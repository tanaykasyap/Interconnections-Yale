{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 8. Style Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: C14_Q1832_GoalLineReliability_PhII_01-30-2024.pdf from Project 1832\n",
      "Skipped Addendum PDF: Q1859Mayacamas_GeothermalAppendix_AC14PhII_Revision1.pdf from Project 1859 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1871Stageline_Energy_StorageAppendix_AC14PhIIAddendum1.pdf from Project 1871 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1875Delilah_Energy_StorageAppendix_AC14PhIIRevision1.pdf from Project 1875 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1881Spectrum_Energy_StorageAppendix_AC14PhIIRevision1.pdf from Project 1881 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1932-Cougar Storage-Appendix_A-C14PhII-Revision1.pdf from Project 1932 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1954Huaso_HybridAppendix_AC14PhIIAddendum1.pdf from Project 1954 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1956SequoiaAppendix_AC14PhIIAddendum1.pdf from Project 1956 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1958Spikes_Peak_SolarAppendix_AC14PhIIAddendum1.pdf from Project 1958 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1959Cornucopia_HybridAppendix_AC14PhIIAddendum1.pdf from Project 1959 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1963Almande_Energy_StorageAppendix_AC14PhIIAddendum1.pdf from Project 1963 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1968PlumAppendix_AC14PhIIAddendum1.pdf from Project 1968 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1973Peaceful_Hollow_BESSAppendix_AC14PhIIAddendum1.pdf from Project 1973 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: Q1992Callinan_Solar_and_StorageAppendix_AC14PhIIRevision1_Final.pdf from Project 1992 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 1a.pdf from Project 2022 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2022 TOT1101-Socorro Peak Solar-Appendix A.pdf from Project 2022 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2022 TOT1101-Socorro Peak Solar-Appendix A-Attachment 2b.pdf from Project 2022 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A.pdf from Project 2022 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2022 TOT1101-Socorro Peak Solar-Appendix A-Attachment 1b.pdf from Project 2022 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 2a.pdf from Project 2022 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 1a_SLD (SH 2 OF SH 2).pdf from Project 2022 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 1a_SLD (SH 1 OF SH 2).pdf from Project 2022 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A.pdf from Project 2023 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 1a.pdf from Project 2023 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A-Attachment 1b.pdf from Project 2023 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A-Attachment 2b.pdf from Project 2023 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A.pdf from Project 2023 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 2a.pdf from Project 2023 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A-Attachment 2b_Addendum.pdf from Project 2023 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 1a_SLD-(Sh1of2).pdf from Project 2023 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A_Addendum.pdf from Project 2023 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 1a_SLD-(Sh2of2).pdf from Project 2023 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A-Attachment 1b.pdf from Project 2025 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A.pdf from Project 2025 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A.pdf from Project 2025 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A-Attachment 2b.pdf from Project 2025 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2027 TOT1114-Harquahala Flats 2-Appendix A-Attachment 1b.pdf from Project 2027 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2027-TOT1114-Harquahala Flats 2-Appendix A-Attachment 2a.pdf from Project 2027 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2027-TOT1114-Harquahala Flats 2-Appendix A.pdf from Project 2027 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2027-TOT1114-Harquahala Flats 2-Appendix A-Attachment 1a.pdf from Project 2027 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2027 TOT1114-Harquahala Flats 2-Appendix A.pdf from Project 2027 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2027 TOT1114-Harquahala Flats 2-Appendix A-Attachment 2b.pdf from Project 2027 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2029 TOT1139-Harquahala Sunrise 2-Appendix A.pdf from Project 2029 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2029-TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 1a.pdf from Project 2029 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2029 TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 1b.pdf from Project 2029 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2029 TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 2b.pdf from Project 2029 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2029-TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 2a.pdf from Project 2029 (No Table 7)\n",
      "Skipped PDF: QC14PII-DCRT-Eastern-Q2029-TOT1139-Harquahala Sunrise 2-Appendix A.pdf from Project 2029 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A_rev1.pdf from Project 2031 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A.pdf from Project 2031 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment 2.pdf from Project 2031 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment 1.pdf from Project 2031 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment1_rev1.pdf from Project 2031 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment 2_Addendum.pdf from Project 2031 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A_Addendum.pdf from Project 2031 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment1_rev1.pdf from Project 2032 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment 1.pdf from Project 2032 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment 2.pdf from Project 2032 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A_rev1.pdf from Project 2032 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A.pdf from Project 2032 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A_Addendum.pdf from Project 2032 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment 2_Addendum.pdf from Project 2032 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment 1.pdf from Project 2033 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment 2.pdf from Project 2033 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment1_rev1.pdf from Project 2033 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A_rev1.pdf from Project 2033 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A.pdf from Project 2033 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment 2_Addendum.pdf from Project 2033 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A_Addendum.pdf from Project 2033 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment 2.pdf from Project 2034 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment 1.pdf from Project 2034 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A_rev1.pdf from Project 2034 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A.pdf from Project 2034 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment1_rev1.pdf from Project 2034 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment 2_Revision.pdf from Project 2034 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A_Revision.pdf from Project 2034 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A_rev1.pdf from Project 2036 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment1_rev1.pdf from Project 2036 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A.pdf from Project 2036 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment 2.pdf from Project 2036 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment 1.pdf from Project 2036 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A_Addendum.pdf from Project 2036 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment 2_Addendum.pdf from Project 2036 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment1_rev1.pdf from Project 2037 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment 1.pdf from Project 2037 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment 2.pdf from Project 2037 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A_rev1.pdf from Project 2037 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A.pdf from Project 2037 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment 2_Revision.pdf from Project 2037 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A_Revision.pdf from Project 2037 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment1_rev1.pdf from Project 2041 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A.pdf from Project 2041 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A_rev1.pdf from Project 2041 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment 2.pdf from Project 2041 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment 1.pdf from Project 2041 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A_Addendum.pdf from Project 2041 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment 2_Addendum.pdf from Project 2041 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A.pdf from Project 2042 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment1_rev1.pdf from Project 2042 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A_rev1.pdf from Project 2042 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment 2.pdf from Project 2042 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment 1.pdf from Project 2042 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment 2_Revision.pdf from Project 2042 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A_Revision.pdf from Project 2042 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment1_rev1.pdf from Project 2043 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment 1.pdf from Project 2043 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment 2.pdf from Project 2043 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A.pdf from Project 2043 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A_rev1.pdf from Project 2043 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment 2_Addendum.pdf from Project 2043 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A_Addendum.pdf from Project 2043 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A_rev1.pdf from Project 2045 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A.pdf from Project 2045 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A-Attachment1_rev1.pdf from Project 2045 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A-Attachment 2.pdf from Project 2045 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A-Attachment 1.pdf from Project 2045 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A-Attachment 2.pdf from Project 2048 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A-Attachment 1.pdf from Project 2048 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A_rev1.pdf from Project 2048 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A.pdf from Project 2048 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A-Attachment1_rev1.pdf from Project 2048 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A.pdf from Project 2049 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A_rev1.pdf from Project 2049 (No Table 7)\n",
      "Skipped PDF: Q2049__Ardilla_PHII_Results_Mtg__Minutes_FINAL.pdf from Project 2049 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment 2.pdf from Project 2049 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment 1.pdf from Project 2049 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment1_rev1.pdf from Project 2049 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A_Revision.pdf from Project 2049 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment 2_Revision.pdf from Project 2049 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment 2.pdf from Project 2050 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment 1.pdf from Project 2050 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A.pdf from Project 2050 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A_rev1.pdf from Project 2050 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment1_rev1.pdf from Project 2050 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment 2_Revision.pdf from Project 2050 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A_Revision.pdf from Project 2050 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment 1.pdf from Project 2051 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment 2.pdf from Project 2051 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment1_rev1.pdf from Project 2051 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A_rev1.pdf from Project 2051 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A.pdf from Project 2051 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment 2_Revision.pdf from Project 2051 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A_Revision.pdf from Project 2051 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment 2.pdf from Project 2052 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment 1.pdf from Project 2052 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A_rev1.pdf from Project 2052 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment1_rev1.pdf from Project 2052 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A.pdf from Project 2052 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A_Revision.pdf from Project 2052 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment 2_Revision.pdf from Project 2052 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1032 Q2055 N.pdf from Project 2055 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2055-TOT1032-Euismod_ApndxA-clean.pdf from Project 2055 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1032 Q2055 N.pdf from Project 2055 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1032 Q2055 E Revision 4-15-24.pdf from Project 2055 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1032 Q2055 E Revision 4-15-24.pdf from Project 2055 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1036 Q2056 Quercus-clean.pdf from Project 2056 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1036 Q2056 N.pdf from Project 2056 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2056-TOT1036-Quercus_ApndxA-clean.pdf from Project 2056 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1036 Q2056 N.pdf from Project 2056 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1036 Q2056 N Addendum.pdf from Project 2056 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: C14.2-North-Q2060-TOT1047-SagebrushBESS_ApndxA-clean.pdf from Project 2060 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1047 Q2060 Sagebrush Energy Storage-clean.pdf from Project 2060 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1047 Q2060 N.pdf from Project 2060 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1047 Q2060 N.pdf from Project 2060 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1047 Q2060 N Addendum 4-15-24.pdf from Project 2060 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1047 Q2060 N Addendum 4-15-24.pdf from Project 2060 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1043 Q2061 N.pdf from Project 2061 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2061-TOT1043-Juniper_ApndxA-clean.pdf from Project 2061 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1043 Q2061 N.pdf from Project 2061 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1043 Q2061 Juniper Storage-clean.pdf from Project 2061 (No Table 7)\n",
      "Skipped Addendum PDF: Q2061-TOT1043-WhirlwindSCDFixAdm-Northern-final.pdf from Project 2061 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1043 Q2061 N Addendum 4-15-24.pdf from Project 2061 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1043 Q2061 N Addendum 4-15-24.pdf from Project 2061 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1052 Q2062 Dorian Solar-clean.pdf from Project 2062 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1052 Q2062 N.pdf from Project 2062 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2062-TOT1052-Dorian_ApndxA-clean.pdf from Project 2062 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1052 Q2062 N.pdf from Project 2062 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1052 Q2062 N Addendum 4-15-24.pdf from Project 2062 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1052 Q2062 N Addendum 4-15-24.pdf from Project 2062 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1059 Q2064 N.pdf from Project 2064 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1059 Q2064 N.pdf from Project 2064 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2064-TOT1059-Elion_ApndxA-clean.pdf from Project 2064 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1059 Q2064 Elion Energy Storage-clean.pdf from Project 2064 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1059 Q2064 N Addendum 4-15-24.pdf from Project 2064 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1059 Q2064 N Addendum 4-15-24.pdf from Project 2064 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1089 Q2066 N.pdf from Project 2066 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1089 Q2066 N.pdf from Project 2066 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2066-TOT1089-Drifter_ApndxA-clean.pdf from Project 2066 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1089 Q2066 Drifter Energy Storage-clean.pdf from Project 2066 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2068-TOT1072-Bobor_ApndxA-clean.pdf from Project 2068 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1072 Q2068 Bobor Storage-clean.pdf from Project 2068 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1072 Q2068 N.pdf from Project 2068 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1072 Q2068 N.pdf from Project 2068 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1072 Q2068 N Addendum 3-26-24.pdf from Project 2068 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1099 Q2075 Fairmont Solar 1-clean.pdf from Project 2075 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2075-TOT1099-Fairmont_ApndxA-clean.pdf from Project 2075 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1099 Q2075 N.pdf from Project 2075 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1099 Q2075 N.pdf from Project 2075 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2078-TOT1097-SoleilCantil_ApndxA-clean.pdf from Project 2078 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1097 Q2078 N.pdf from Project 2078 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1097 Q2078 Soleil Cantil-clean.pdf from Project 2078 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1097 Q2078 N.pdf from Project 2078 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1097 Q2078 N Revision 4-15-24.pdf from Project 2078 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1097 Q2078 N Revision 4-15-24.pdf from Project 2078 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: C14.2-North-Attchmnt1b-ICBuild-TOT1111&Q2080-Rangeland-clean.pdf from Project 2080 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1111 Q2080 Option 2 (IC Build) N.pdf from Project 2080 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1111 Q2080 N.pdf from Project 2080 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1111 Q2080 N.pdf from Project 2080 (No Table 7)\n",
      "Skipped PDF: C14.2-SCE-North-OTB-Q2080&TOT1111-Rangeland-AppendixA-clean.pdf from Project 2080 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Attchmnt1a-SCEBuild-TOT1111&Q2080-Rangeland-clean.pdf from Project 2080 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1111 Q2080 Option 2 (IC Build) N.pdf from Project 2080 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1111 Q2080 N Addendum 4-15-24.pdf from Project 2080 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1111 Q2080 N Addendum 4-15-24.pdf from Project 2080 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1111 Q2080 Option 2 (IC Build) N Addendum 4-15-24.pdf from Project 2080 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1111 Q2080 Option 2 (IC Build) N Addendum 4-15-24.pdf from Project 2080 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1100 Q2081 N.pdf from Project 2081 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1100 Q2081 Mineral King Solar-clean.pdf from Project 2081 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1100 Q2081 N.pdf from Project 2081 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2081-TOT1100-MineralKing_ApndxA-clean.pdf from Project 2081 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2085-TOT1120-Solsken_ApndxA-clean.pdf from Project 2085 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1120 Q2085 N.pdf from Project 2085 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1120 Q2085 Solsken-clean.pdf from Project 2085 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1120 Q2085 N.pdf from Project 2085 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1136 Q2089 N.pdf from Project 2089 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1136 Q2089 N.pdf from Project 2089 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2089-TOT1136-Greasewood_ApndxA-clean.pdf from Project 2089 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1136 Q2089 Greasewood Energy Storage-clean.pdf from Project 2089 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1136 Q2089 N Revision 4-15-24.pdf from Project 2089 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1136 Q2089 N Revision 4-15-24.pdf from Project 2089 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: Q2090__Gwent_Storage_2_PHII_Mtg__Minutes_FINAL.pdf from Project 2090 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1138 Q2090 Gwent Storage 2-clean.pdf from Project 2090 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1138 Q2090 N.pdf from Project 2090 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2090-TOT1138-Gwent2_ApndxA-clean.pdf from Project 2090 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1138 Q2090 N.pdf from Project 2090 (No Table 7)\n",
      "Skipped Addendum PDF: QC14PII-Northern-Attachment1-TOT1138 Q2090 Gwent Storage 2-addendum.pdf from Project 2090 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1138 Q2090 N Addendum 3-26-24.pdf from Project 2090 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1140 Q2091 Maathai Storage-clean.pdf from Project 2091 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1140 Q2091 N.pdf from Project 2091 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1140 Q2091 N.pdf from Project 2091 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2091-TOT1140-Maathai_ApndxA-clean.pdf from Project 2091 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1140 Q2091 N Addendum 4-15-24.pdf from Project 2091 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1140 Q2091 N Addendum 4-15-24.pdf from Project 2091 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1147 Q2092 N.pdf from Project 2092 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1147 Q2092 N.pdf from Project 2092 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1147 Q2092 Rosa Storage-clean.pdf from Project 2092 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2092-TOT1147-Rosa_ApndxA-clean.pdf from Project 2092 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 1  Q2096 TOT1064 NOL.pdf from Project 2096 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 2 TOT1046 Q2097 NOL.pdf from Project 2097 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 1 Q2097 TOT1046 NOL.pdf from Project 2097 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 3 TOT1046 Q2097 NOL.pdf from Project 2097 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Q2097-TOT1046-Sienna2 NOL.pdf from Project 2097 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 3  TOT1080 Q2098 NOL (IC Build).pdf from Project 2098 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 2  TOT1080 Q2098 NOL (IC Build).pdf from Project 2098 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-OTB-Q2098-TOT1080-Cuerno Grande NOL.pdf from Project 2098 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attchmnt1b(ICBuild)-Q2098-TOT1080 NOL.pdf from Project 2098 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attchmnt1a-Q2098-TOT1080 NOL.pdf from Project 2098 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 2  TOT1080 Q2098 NOL (SCE Build).pdf from Project 2098 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 3  TOT1080 Q2098 NOL (SCE Build).pdf from Project 2098 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 1  Q2101 - TOT1091 NOL.pdf from Project 2101 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 3  TOT1091 Q2101 NOL.pdf from Project 2101 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Q2101-TOT1091-Weston NOL.pdf from Project 2101 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 2  TOT1091 Q2101 NOL.pdf from Project 2101 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Q2103-TOT1106-Cady NOL.pdf from Project 2103 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 3  TOT1106 Q2103 NOL.pdf from Project 2103 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 1 Q2103 - TOT1106 NOL.pdf from Project 2103 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 2  TOT1106 Q2103 NOL.pdf from Project 2103 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 2 TOT1109 Q2104 NOL.pdf from Project 2104 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 1 Q2104 - TOT1109 NOL.pdf from Project 2104 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Q2104-TOT1109-DosPalmas NOL.pdf from Project 2104 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 3 TOT1109 Q2104 NOL.pdf from Project 2104 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 2  TOT1125 Q2105 NOL.pdf from Project 2105 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 1  Q2105 - TOT1125 NOL.pdf from Project 2105 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Attachment 3  TOT1125 Q2105 NOL.pdf from Project 2105 (No Table 7)\n",
      "Skipped PDF: QC14PII-SCE-ApndixA-Q2105-TOT1125-Conduit NOL.pdf from Project 2105 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1023 Q2109 M.pdf from Project 2109 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1023 Q2109 M.pdf from Project 2109 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1023 Q2109 M.pdf from Project 2109 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1023 Q2109 M.pdf from Project 2109 (No Table 7)\n",
      "Skipped PDF: QC14PII-Northern-Attachment1-TOT1024 Q2110 Flea Flicker Energy Storage-clean.pdf from Project 2110 (No Table 7)\n",
      "Skipped PDF: C14.2-North-Q2110-TOT1024-FleaFlicker_ApndxA-clean.pdf from Project 2110 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1024 Q2110 N.pdf from Project 2110 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1024 Q2110 N.pdf from Project 2110 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1024 Q2110 N Addendum 4-15-24.pdf from Project 2110 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1024 Q2110 N Addendum 4-15-24.pdf from Project 2110 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1027 Q2111 M.pdf from Project 2111 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1027 Q2111 M.pdf from Project 2111 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A TOT1027 Q2111 M.pdf from Project 2111 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1027 Q2111 M.pdf from Project 2111 (No Table 7)\n",
      "Skipped Addendum PDF: P2RPT-C14_Q2111_FLEETWOOD_PhII_AppendixA_Addendum.pdf from Project 2111 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: P2RPT-C14_Q2111_FLEETWOOD_PhII_Addendum_Attachment2.pdf from Project 2111 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1028 Q2113 M.pdf from Project 2113 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A TOT1028 Q2113 M.pdf from Project 2113 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1028 Q2113 M.pdf from Project 2113 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1028 Q2113 M.pdf from Project 2113 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1029 Q2114 M.pdf from Project 2114 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1029 Q2114 M.pdf from Project 2114 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1029 Q2114 M.pdf from Project 2114 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A TOT1029 Q2114 M.pdf from Project 2114 (No Table 7)\n",
      "Skipped Addendum PDF: P2RPT-C14_Q2114_DIRAC_PhII_Addendum_Attachment2.pdf from Project 2114 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: P2RPT-C14_Q2114_DIRAC_PhII_AppendixA_Addendum.pdf from Project 2114 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1030 Q2115 M.pdf from Project 2115 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A TOT1030 Q2115 M.pdf from Project 2115 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1030 Q2115 M.pdf from Project 2115 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1030 Q2115 M.pdf from Project 2115 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1033 Q2116 M.pdf from Project 2116 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1033 Q2116 M.pdf from Project 2116 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1033 Q2116 M rev1.pdf from Project 2116 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1033 Q2116 M.pdf from Project 2116 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1033 Q2116 M.pdf from Project 2116 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1033 Q2116 M rev1.pdf from Project 2116 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Appx A  TOT1033 Q2116 M - Addendum 1.pdf from Project 2116 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 3  TOT1033 Q2116 M Addendum 1.pdf from Project 2116 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1034 Q2117 M.pdf from Project 2117 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1034 Q2117 M.pdf from Project 2117 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1034 Q2117 M.pdf from Project 2117 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1034 Q2117 M.pdf from Project 2117 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1060 Q2121 M.pdf from Project 2121 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1060 Q2121 M.pdf from Project 2121 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1060 Q2121 M.pdf from Project 2121 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1060 Q2121 M.pdf from Project 2121 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1062 Q2124 M.pdf from Project 2124 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1062 Q2124 M.pdf from Project 2124 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1062 Q2124 M.pdf from Project 2124 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1062 Q2124 M.pdf from Project 2124 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1063 Q2125 M.pdf from Project 2125 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1063 Q2125 M.pdf from Project 2125 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1063 Q2125 M.pdf from Project 2125 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1063 Q2125 M.pdf from Project 2125 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1076 Q2127 M.pdf from Project 2127 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1076 Q2127 M.pdf from Project 2127 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1076 Q2127 M.pdf from Project 2127 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1076 Q2127 M.pdf from Project 2127 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1148 Q2129 M.pdf from Project 2129 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1148 Q2129 M.pdf from Project 2129 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1148 Q2129 M.pdf from Project 2129 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1148 Q2129 M.pdf from Project 2129 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1088 Q2131 M.pdf from Project 2131 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1088 Q2131 M.pdf from Project 2131 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1088 Q2131 M.pdf from Project 2131 (No Table 7)\n",
      "Skipped PDF: Q2131__Merlin_Storage__PhII_Results_Mtg_FINAL.pdf from Project 2131 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1088 Q2131 M.pdf from Project 2131 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1090 Q2134 M.pdf from Project 2134 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1090 Q2134 M.pdf from Project 2134 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1090 Q2134 M.pdf from Project 2134 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1090 Q2134 M.pdf from Project 2134 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1108 Q2136 M.pdf from Project 2136 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1108 Q2136 M.pdf from Project 2136 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1108 Q2136 M.pdf from Project 2136 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1108 Q2136 M.pdf from Project 2136 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1131 Q2137 M v2.pdf from Project 2137 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1131 Q2137 M.pdf from Project 2137 (No Table 7)\n",
      "Skipped PDF: QC14PII - Metro - Attachment 1 - Q2137 - TOT1131 - Mt Baldy Energy Storage-clean.pdf from Project 2137 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Appx A  TOT1131 Q2137 M.pdf from Project 2137 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1131 Q2137 M.pdf from Project 2137 (No Table 7)\n",
      "Skipped Addendum PDF: P2RPT-C14_Q2137_MT_BALDY_ENERGY_STORAGE_PhII_Addendum_Attachment2.pdf from Project 2137 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: P2RPT-C14_Q2137_MT_BALDY_ENERGY_STORAGE_PhII_AppendixA_Addendum.pdf from Project 2137 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 1  TOT1133 Q2138 M.pdf from Project 2138 (No Table 7)\n",
      "Skipped PDF: QC14PII - Metro - Attachment 1 - Q2138 - TOT1133 - Separator-clean.pdf from Project 2138 (No Table 7)\n",
      "Skipped PDF: C14.2-Metro-Q2138-TOT1133-Seperator_ApndxA-clean1.pdf from Project 2138 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1133 Q2138 M.pdf from Project 2138 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1133 Q2138 M.pdf from Project 2138 (No Table 7)\n",
      "Skipped PDF: QC14PII - EOP - Attachment 1 - Q2140 - TOT1056 - Sterling.pdf from Project 2140 (No Table 7)\n",
      "Skipped PDF: C14.2-EOP-Q2140-TOT1056-Sterling_ApndxA-final.pdf from Project 2140 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1056 Q2140 EOP.pdf from Project 2140 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1056 Q2140 EOP.pdf from Project 2140 (No Table 7)\n",
      "Skipped Addendum PDF: QC14 Ph2 Attachment 2  TOT1056 Q2140 EOP Addendum 3-26-24.pdf from Project 2140 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC14PII - EOP - Attachment 1 - Q2141 - TOT1130 - Delamar Energy Storage 2.pdf from Project 2141 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1130 Q2141 EOP.pdf from Project 2141 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1130 Q2141 EOP.pdf from Project 2141 (No Table 7)\n",
      "Skipped PDF: C14.2-EOP-Q2141-TOT1130-Delamar2_ApndxA-final.pdf from Project 2141 (No Table 7)\n",
      "Skipped Addendum PDF: C14.2-EOP-Q2141-TOT1130-Delamar2_ApndxA-Addendum.pdf from Project 2141 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: C14.2-AS-EOP-Q2142-TOT1022-SilverStar_ApndxA-final.pdf from Project 2142 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2142_Att1.pdf from Project 2142 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1022 Q2142 EOP AFS.pdf from Project 2142 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2142_Att2.pdf from Project 2142 (No Table 7)\n",
      "Skipped PDF: Q2142combined Attachment 3 feb162024.pdf from Project 2142 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1022 Q2142 EOP AFS.pdf from Project 2142 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2142_ApndxA-final.pdf from Project 2142 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2142_Att2_Update_2_16.pdf from Project 2142 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2144_Att2_Update_2_16.pdf from Project 2144 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1118 Q2144 EOP AFS.pdf from Project 2144 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1118 Q2144 EOP AFS.pdf from Project 2144 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2144_ApndxA-final.pdf from Project 2144 (No Table 7)\n",
      "Skipped PDF: C14.2-AS-EOP-Q2144-TOT1118-Murray_ApndxA-final.pdf from Project 2144 (No Table 7)\n",
      "Skipped PDF: Q2144combinedAttachment 3 feb162024.pdf from Project 2144 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2144_Att1.pdf from Project 2144 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2144_Att2.pdf from Project 2144 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2145_ApndxA_final.pdf from Project 2145 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2145_Att2_Update_2_16.pdf from Project 2145 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1150 Q2145 EOP AFS.pdf from Project 2145 (No Table 7)\n",
      "Skipped PDF: Q2145combinedAttachment3Feb162024.pdf from Project 2145 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2145_Att1.pdf from Project 2145 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2145_Att2.pdf from Project 2145 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1150 Q2145 EOP AFS.pdf from Project 2145 (No Table 7)\n",
      "Skipped PDF: C14.2-AS-EOP-Q2145-TOT1150-Dandelion_ApndxA-final.pdf from Project 2145 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2146_Att1-final.pdf from Project 2146 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1085 Q2146 EOP AFS.pdf from Project 2146 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2146_Att2_Update_2_16.pdf from Project 2146 (No Table 7)\n",
      "Skipped PDF: Q2146combinedAttachment3Feb162024.pdf from Project 2146 (No Table 7)\n",
      "Skipped PDF: C14.2-AS-EOP-Q2146-TOT1085-WaterRock2_ApndxA.pdf from Project 2146 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1085 Q2146 EOP AFS.pdf from Project 2146 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2146_ApndxA-final.pdf from Project 2146 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2146_Att2-final.pdf from Project 2146 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2147_Att1-final.pdf from Project 2147 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1053 Q2147 EOP AFS.pdf from Project 2147 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1053 Q2147 EOP AFS.pdf from Project 2147 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2147_ApndxA-final.pdf from Project 2147 (No Table 7)\n",
      "Skipped PDF: C14.2-AS-EOP-Q2147-TOT1053-Kawich_ApndxA.pdf from Project 2147 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1054 Q2148 EOP AFS.pdf from Project 2148 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1054 Q2148 EOP AFS.pdf from Project 2148 (No Table 7)\n",
      "Skipped PDF: Q2148-Combined Attachment 3.pdf from Project 2148 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2148_ApndxA-final.pdf from Project 2148 (No Table 7)\n",
      "Skipped PDF: Q2148combinedAttachment3Feb162024.pdf from Project 2148 (No Table 7)\n",
      "Skipped PDF: C14.2-AS-EOP-Q2148-TOT1054-Mosey_ApndxA.pdf from Project 2148 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2148_Att2_Update_2_16.pdf from Project 2148 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1127 Q2149 EOP AFS.pdf from Project 2149 (No Table 7)\n",
      "Skipped PDF: Q2149combinedAttachment3Feb162024.pdf from Project 2149 (No Table 7)\n",
      "Skipped PDF: Q2149-TOT1127-Attachment 2-SCE AS.pdf from Project 2149 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1127 Q2149 EOP AFS.pdf from Project 2149 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2149_Att2_Update_2_16.pdf from Project 2149 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2149_Att2.pdf from Project 2149 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2149_Att1.pdf from Project 2149 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2149_ApndxA-final.pdf from Project 2149 (No Table 7)\n",
      "Skipped PDF: C14.2-AS-EOP-Q2149-TOT1127-Sunbaked_ApndxA.pdf from Project 2149 (No Table 7)\n",
      "Skipped PDF: Q2150combinedAttachment3Feb162024.pdf from Project 2150 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 3  TOT1142 Q2150 EOP AFS.pdf from Project 2150 (No Table 7)\n",
      "Skipped PDF: QC14 Ph2 Attachment 2  TOT1142 Q2150 EOP AFS.pdf from Project 2150 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2150_ApndxA-final.pdf from Project 2150 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2150_Att1-final.pdf from Project 2150 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2150_Att2-final.pdf from Project 2150 (No Table 7)\n",
      "Skipped PDF: QC14P2-Q2150_Att2_Update_2_16.pdf from Project 2150 (No Table 7)\n",
      "Skipped PDF: C14.2-AS-EOP-Q2150-TOT1142-RoughHat3_ApndxA.pdf from Project 2150 (No Table 7)\n",
      "Scraped PDF: C14_Q2153_FrigatebirdStorage_PhII_01-30-2024.pdf from Project 2153\n",
      "Scraped PDF: C14_Q2154_Remy_PhII_01-30-2024.pdf from Project 2154\n",
      "Scraped PDF: C14_Q2157_GnarlyOsage_PhII_01-30-2024.pdf from Project 2157\n",
      "Scraped PDF: C14_Q2161_AlisaSolarEnergyComplex_PhII_01-30-2024.pdf from Project 2161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: C14_Q2162_SolarDeMexicali_PhII_01-30-2024.pdf from Project 2162\n",
      "Scraped PDF: C14_Q2165_Tower1EnergyStorage_PhII_01-30-2024.pdf from Project 2165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: C14_Q2166_Umbriel_PhII_01-30-2024.pdf from Project 2166\n",
      "Scraped PDF: C14_Q2167_HammerheadStorage_PhII_01-30-2024.pdf from Project 2167\n",
      "Scraped PDF: C14_Q2172_Hyder_PhII_01-30-2024.pdf from Project 2172\n",
      "Scraped PDF: C14_Q2173_LagoDomingoStorage_PhII_01-30-2024.pdf from Project 2173\n",
      "Skipped PDF: C4PhII_Q2173_Attachment 1.pdf from Project 2173 (No Table 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: C14_Q2176_YuhaDesertBatteryStorage_PhII_01-30-2024.pdf from Project 2176\n",
      "Scraped PDF: C14_Q2177_BoulderBrushHybrid_PhII_01-30-2024.pdf from Project 2177\n",
      "Scraped Addendum PDF: P2RPT-C14_Q2177_BoulderBrushHybrid_PhII_Addendum2_32624.pdf from Project 2177\n",
      "Scraped Addendum PDF: P2RPT-C14_Q2177_BoulderBrushHybrid_PhII_Addendum_1_03072024.pdf from Project 2177\n",
      "Scraped PDF: C14_Q2178_BellBluffStorage_PhII_01-30-2024.pdf from Project 2178\n",
      "Scraped PDF: C14_Q2180_CargoStorage_PhII_01-30-2024.pdf from Project 2180\n",
      "Scraped PDF: C14_Q2181_PinscherEnergyStorage_PhII_01-30-2024.pdf from Project 2181\n",
      "Skipped PDF: C4PhII_Q2181_Attachment 1.pdf from Project 2181 (No Table 7)\n",
      "Scraped Addendum PDF: P2RPT-C14_Q2181_PinscherEnergyStorage_PhII_Addendum1_4222024.pdf from Project 2181\n",
      "Scraped PDF: C14_Q2182_TaylorStorage_PhII_01-30-2024.pdf from Project 2182\n",
      "Scraped PDF: C14_Q2184_AmberjackEnergy_PhII_01-30-2024.pdf from Project 2184\n",
      "Scraped PDF: C14_Q2185_GatewayEnergyStorage2_PhII_01-30-2024.pdf from Project 2185\n",
      "Scraped PDF: C14_Q2186_SandbarEnergyStorage_PhII_01-30-2024.pdf from Project 2186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:693: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Network Upgrades, Estimated Costs, and Estimated Time to Construct' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: C14_Q2187_EolicaDeRumorosa_PhII_01-30-2024.pdf from Project 2187\n",
      "Scraped PDF: C14_Q2188_GeraniumEnergyStorage_PhII_01-30-2024.pdf from Project 2188\n",
      "Scraped Addendum PDF: P2RPT-C14_Q2188_GeraniumEnergyStorage_PhII_Addendum1_3142024.pdf from Project 2188\n",
      "Scraped PDF: C14_Q2192_PajaroValleyStorage_PhII_01-30-2024.pdf from Project 2192\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/ph2_rawdata_cluster14_style_Q_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/ph2_rawdata_cluster14_style_Q_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 297\n",
      "Total Projects Scraped: 23\n",
      "Total Projects Skipped: 274\n",
      "Total Projects Missing: 62\n",
      "Total PDFs Accessed: 460\n",
      "Total PDFs Scraped: 27\n",
      "Total PDFs Skipped: 433\n",
      "\n",
      "List of Scraped Projects:\n",
      "[1832, 2153, 2154, 2157, 2161, 2162, 2165, 2166, 2167, 2172, 2173, 2176, 2177, 2178, 2180, 2181, 2182, 2184, 2185, 2186, 2187, 2188, 2192]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[1831, 1833, 1835, 1837, 1838, 1840, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1871, 1872, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1983, 1987, 1988, 1992, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2004, 2005, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2025, 2026, 2027, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2039, 2041, 2042, 2043, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2054, 2055, 2056, 2057, 2060, 2061, 2062, 2064, 2065, 2066, 2068, 2069, 2070, 2071, 2072, 2073, 2075, 2078, 2079, 2080, 2081, 2083, 2084, 2085, 2086, 2089, 2090, 2091, 2092, 2093, 2096, 2097, 2098, 2100, 2101, 2103, 2104, 2105, 2106, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2131, 2134, 2135, 2136, 2137, 2138, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2155, 2156, 2163, 2168, 2169, 2170, 2175]\n",
      "\n",
      "List of Missing Projects:\n",
      "[1834, 1836, 1839, 1841, 1868, 1869, 1870, 1873, 1885, 1898, 1936, 1937, 1938, 1939, 1947, 1948, 1961, 1981, 1982, 1984, 1985, 1986, 1989, 1990, 1991, 1993, 2024, 2028, 2038, 2040, 2044, 2053, 2058, 2059, 2063, 2067, 2074, 2076, 2077, 2082, 2087, 2088, 2094, 2095, 2099, 2102, 2107, 2130, 2132, 2133, 2139, 2158, 2159, 2160, 2164, 2171, 2174, 2179, 2183, 2189, 2190, 2191]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['C14_Q1832_GoalLineReliability_PhII_01-30-2024.pdf', 'C14_Q2153_FrigatebirdStorage_PhII_01-30-2024.pdf', 'C14_Q2154_Remy_PhII_01-30-2024.pdf', 'C14_Q2157_GnarlyOsage_PhII_01-30-2024.pdf', 'C14_Q2161_AlisaSolarEnergyComplex_PhII_01-30-2024.pdf', 'C14_Q2162_SolarDeMexicali_PhII_01-30-2024.pdf', 'C14_Q2165_Tower1EnergyStorage_PhII_01-30-2024.pdf', 'C14_Q2166_Umbriel_PhII_01-30-2024.pdf', 'C14_Q2167_HammerheadStorage_PhII_01-30-2024.pdf', 'C14_Q2172_Hyder_PhII_01-30-2024.pdf', 'C14_Q2173_LagoDomingoStorage_PhII_01-30-2024.pdf', 'C14_Q2176_YuhaDesertBatteryStorage_PhII_01-30-2024.pdf', 'C14_Q2177_BoulderBrushHybrid_PhII_01-30-2024.pdf', 'P2RPT-C14_Q2177_BoulderBrushHybrid_PhII_Addendum2_32624.pdf', 'P2RPT-C14_Q2177_BoulderBrushHybrid_PhII_Addendum_1_03072024.pdf', 'C14_Q2178_BellBluffStorage_PhII_01-30-2024.pdf', 'C14_Q2180_CargoStorage_PhII_01-30-2024.pdf', 'C14_Q2181_PinscherEnergyStorage_PhII_01-30-2024.pdf', 'P2RPT-C14_Q2181_PinscherEnergyStorage_PhII_Addendum1_4222024.pdf', 'C14_Q2182_TaylorStorage_PhII_01-30-2024.pdf', 'C14_Q2184_AmberjackEnergy_PhII_01-30-2024.pdf', 'C14_Q2185_GatewayEnergyStorage2_PhII_01-30-2024.pdf', 'C14_Q2186_SandbarEnergyStorage_PhII_01-30-2024.pdf', 'C14_Q2187_EolicaDeRumorosa_PhII_01-30-2024.pdf', 'C14_Q2188_GeraniumEnergyStorage_PhII_01-30-2024.pdf', 'P2RPT-C14_Q2188_GeraniumEnergyStorage_PhII_Addendum1_3142024.pdf', 'C14_Q2192_PajaroValleyStorage_PhII_01-30-2024.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['Q1859Mayacamas_GeothermalAppendix_AC14PhII_Revision1.pdf', 'Q1871Stageline_Energy_StorageAppendix_AC14PhIIAddendum1.pdf', 'Q1875Delilah_Energy_StorageAppendix_AC14PhIIRevision1.pdf', 'Q1881Spectrum_Energy_StorageAppendix_AC14PhIIRevision1.pdf', 'Q1932-Cougar Storage-Appendix_A-C14PhII-Revision1.pdf', 'Q1954Huaso_HybridAppendix_AC14PhIIAddendum1.pdf', 'Q1956SequoiaAppendix_AC14PhIIAddendum1.pdf', 'Q1958Spikes_Peak_SolarAppendix_AC14PhIIAddendum1.pdf', 'Q1959Cornucopia_HybridAppendix_AC14PhIIAddendum1.pdf', 'Q1963Almande_Energy_StorageAppendix_AC14PhIIAddendum1.pdf', 'Q1968PlumAppendix_AC14PhIIAddendum1.pdf', 'Q1973Peaceful_Hollow_BESSAppendix_AC14PhIIAddendum1.pdf', 'Q1992Callinan_Solar_and_StorageAppendix_AC14PhIIRevision1_Final.pdf', 'QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 1a.pdf', 'QC14PII-SCE-Eastern-Q2022 TOT1101-Socorro Peak Solar-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2022 TOT1101-Socorro Peak Solar-Appendix A-Attachment 2b.pdf', 'QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2022 TOT1101-Socorro Peak Solar-Appendix A-Attachment 1b.pdf', 'QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 2a.pdf', 'QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 1a_SLD (SH 2 OF SH 2).pdf', 'QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 1a_SLD (SH 1 OF SH 2).pdf', 'QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A.pdf', 'QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 1a.pdf', 'QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A-Attachment 1b.pdf', 'QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A-Attachment 2b.pdf', 'QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A.pdf', 'QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 2a.pdf', 'QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A-Attachment 2b_Addendum.pdf', 'QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 1a_SLD-(Sh1of2).pdf', 'QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A_Addendum.pdf', 'QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 1a_SLD-(Sh2of2).pdf', 'QC14PII-SCE-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A-Attachment 1b.pdf', 'QC14PII-SCE-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A.pdf', 'QC14PII-DCRT-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A-Attachment 2b.pdf', 'QC14PII-SCE-Eastern-Q2027 TOT1114-Harquahala Flats 2-Appendix A-Attachment 1b.pdf', 'QC14PII-DCRT-Eastern-Q2027-TOT1114-Harquahala Flats 2-Appendix A-Attachment 2a.pdf', 'QC14PII-DCRT-Eastern-Q2027-TOT1114-Harquahala Flats 2-Appendix A.pdf', 'QC14PII-DCRT-Eastern-Q2027-TOT1114-Harquahala Flats 2-Appendix A-Attachment 1a.pdf', 'QC14PII-SCE-Eastern-Q2027 TOT1114-Harquahala Flats 2-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2027 TOT1114-Harquahala Flats 2-Appendix A-Attachment 2b.pdf', 'QC14PII-SCE-Eastern-Q2029 TOT1139-Harquahala Sunrise 2-Appendix A.pdf', 'QC14PII-DCRT-Eastern-Q2029-TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 1a.pdf', 'QC14PII-SCE-Eastern-Q2029 TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 1b.pdf', 'QC14PII-SCE-Eastern-Q2029 TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 2b.pdf', 'QC14PII-DCRT-Eastern-Q2029-TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 2a.pdf', 'QC14PII-DCRT-Eastern-Q2029-TOT1139-Harquahala Sunrise 2-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A_rev1.pdf', 'Q2049__Ardilla_PHII_Results_Mtg__Minutes_FINAL.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment 2_Revision.pdf', 'QC14 Ph2 Attachment 3  TOT1032 Q2055 N.pdf', 'C14.2-North-Q2055-TOT1032-Euismod_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1032 Q2055 N.pdf', 'QC14 Ph2 Attachment 2  TOT1032 Q2055 E Revision 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1032 Q2055 E Revision 4-15-24.pdf', 'QC14PII-Northern-Attachment1-TOT1036 Q2056 Quercus-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1036 Q2056 N.pdf', 'C14.2-North-Q2056-TOT1036-Quercus_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1036 Q2056 N.pdf', 'QC14 Ph2 Attachment 2  TOT1036 Q2056 N Addendum.pdf', 'C14.2-North-Q2060-TOT1047-SagebrushBESS_ApndxA-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1047 Q2060 Sagebrush Energy Storage-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1047 Q2060 N.pdf', 'QC14 Ph2 Attachment 3  TOT1047 Q2060 N.pdf', 'QC14 Ph2 Attachment 2  TOT1047 Q2060 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1047 Q2060 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1043 Q2061 N.pdf', 'C14.2-North-Q2061-TOT1043-Juniper_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1043 Q2061 N.pdf', 'QC14PII-Northern-Attachment1-TOT1043 Q2061 Juniper Storage-clean.pdf', 'Q2061-TOT1043-WhirlwindSCDFixAdm-Northern-final.pdf', 'QC14 Ph2 Attachment 2  TOT1043 Q2061 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1043 Q2061 N Addendum 4-15-24.pdf', 'QC14PII-Northern-Attachment1-TOT1052 Q2062 Dorian Solar-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1052 Q2062 N.pdf', 'C14.2-North-Q2062-TOT1052-Dorian_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1052 Q2062 N.pdf', 'QC14 Ph2 Attachment 3  TOT1052 Q2062 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1052 Q2062 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1059 Q2064 N.pdf', 'QC14 Ph2 Attachment 2  TOT1059 Q2064 N.pdf', 'C14.2-North-Q2064-TOT1059-Elion_ApndxA-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1059 Q2064 Elion Energy Storage-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1059 Q2064 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1059 Q2064 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1089 Q2066 N.pdf', 'QC14 Ph2 Attachment 3  TOT1089 Q2066 N.pdf', 'C14.2-North-Q2066-TOT1089-Drifter_ApndxA-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1089 Q2066 Drifter Energy Storage-clean.pdf', 'C14.2-North-Q2068-TOT1072-Bobor_ApndxA-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1072 Q2068 Bobor Storage-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1072 Q2068 N.pdf', 'QC14 Ph2 Attachment 3  TOT1072 Q2068 N.pdf', 'QC14 Ph2 Attachment 2  TOT1072 Q2068 N Addendum 3-26-24.pdf', 'QC14PII-Northern-Attachment1-TOT1099 Q2075 Fairmont Solar 1-clean.pdf', 'C14.2-North-Q2075-TOT1099-Fairmont_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1099 Q2075 N.pdf', 'QC14 Ph2 Attachment 2  TOT1099 Q2075 N.pdf', 'C14.2-North-Q2078-TOT1097-SoleilCantil_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1097 Q2078 N.pdf', 'QC14PII-Northern-Attachment1-TOT1097 Q2078 Soleil Cantil-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1097 Q2078 N.pdf', 'QC14 Ph2 Attachment 2  TOT1097 Q2078 N Revision 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1097 Q2078 N Revision 4-15-24.pdf', 'C14.2-North-Attchmnt1b-ICBuild-TOT1111&Q2080-Rangeland-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1111 Q2080 Option 2 (IC Build) N.pdf', 'QC14 Ph2 Attachment 3  TOT1111 Q2080 N.pdf', 'QC14 Ph2 Attachment 2  TOT1111 Q2080 N.pdf', 'C14.2-SCE-North-OTB-Q2080&TOT1111-Rangeland-AppendixA-clean.pdf', 'C14.2-North-Attchmnt1a-SCEBuild-TOT1111&Q2080-Rangeland-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1111 Q2080 Option 2 (IC Build) N.pdf', 'QC14 Ph2 Attachment 2  TOT1111 Q2080 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1111 Q2080 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1111 Q2080 Option 2 (IC Build) N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1111 Q2080 Option 2 (IC Build) N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1100 Q2081 N.pdf', 'QC14PII-Northern-Attachment1-TOT1100 Q2081 Mineral King Solar-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1100 Q2081 N.pdf', 'C14.2-North-Q2081-TOT1100-MineralKing_ApndxA-clean.pdf', 'C14.2-North-Q2085-TOT1120-Solsken_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1120 Q2085 N.pdf', 'QC14PII-Northern-Attachment1-TOT1120 Q2085 Solsken-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1120 Q2085 N.pdf', 'QC14 Ph2 Attachment 3  TOT1136 Q2089 N.pdf', 'QC14 Ph2 Attachment 2  TOT1136 Q2089 N.pdf', 'C14.2-North-Q2089-TOT1136-Greasewood_ApndxA-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1136 Q2089 Greasewood Energy Storage-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1136 Q2089 N Revision 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1136 Q2089 N Revision 4-15-24.pdf', 'Q2090__Gwent_Storage_2_PHII_Mtg__Minutes_FINAL.pdf', 'QC14PII-Northern-Attachment1-TOT1138 Q2090 Gwent Storage 2-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1138 Q2090 N.pdf', 'C14.2-North-Q2090-TOT1138-Gwent2_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1138 Q2090 N.pdf', 'QC14PII-Northern-Attachment1-TOT1138 Q2090 Gwent Storage 2-addendum.pdf', 'QC14 Ph2 Attachment 2  TOT1138 Q2090 N Addendum 3-26-24.pdf', 'QC14PII-Northern-Attachment1-TOT1140 Q2091 Maathai Storage-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1140 Q2091 N.pdf', 'QC14 Ph2 Attachment 2  TOT1140 Q2091 N.pdf', 'C14.2-North-Q2091-TOT1140-Maathai_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1140 Q2091 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1140 Q2091 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1147 Q2092 N.pdf', 'QC14 Ph2 Attachment 2  TOT1147 Q2092 N.pdf', 'QC14PII-Northern-Attachment1-TOT1147 Q2092 Rosa Storage-clean.pdf', 'C14.2-North-Q2092-TOT1147-Rosa_ApndxA-clean.pdf', 'QC14PII-SCE-ApndixA-Attachment 1  Q2096 TOT1064 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2 TOT1046 Q2097 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 1 Q2097 TOT1046 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3 TOT1046 Q2097 NOL.pdf', 'QC14PII-SCE-ApndixA-Q2097-TOT1046-Sienna2 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3  TOT1080 Q2098 NOL (IC Build).pdf', 'QC14PII-SCE-ApndixA-Attachment 2  TOT1080 Q2098 NOL (IC Build).pdf', 'QC14PII-SCE-ApndixA-OTB-Q2098-TOT1080-Cuerno Grande NOL.pdf', 'QC14PII-SCE-ApndixA-Attchmnt1b(ICBuild)-Q2098-TOT1080 NOL.pdf', 'QC14PII-SCE-ApndixA-Attchmnt1a-Q2098-TOT1080 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2  TOT1080 Q2098 NOL (SCE Build).pdf', 'QC14PII-SCE-ApndixA-Attachment 3  TOT1080 Q2098 NOL (SCE Build).pdf', 'QC14PII-SCE-ApndixA-Attachment 1  Q2101 - TOT1091 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3  TOT1091 Q2101 NOL.pdf', 'QC14PII-SCE-ApndixA-Q2101-TOT1091-Weston NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2  TOT1091 Q2101 NOL.pdf', 'QC14PII-SCE-ApndixA-Q2103-TOT1106-Cady NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3  TOT1106 Q2103 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 1 Q2103 - TOT1106 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2  TOT1106 Q2103 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2 TOT1109 Q2104 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 1 Q2104 - TOT1109 NOL.pdf', 'QC14PII-SCE-ApndixA-Q2104-TOT1109-DosPalmas NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3 TOT1109 Q2104 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2  TOT1125 Q2105 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 1  Q2105 - TOT1125 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3  TOT1125 Q2105 NOL.pdf', 'QC14PII-SCE-ApndixA-Q2105-TOT1125-Conduit NOL.pdf', 'QC14 Ph2 Attachment 1  TOT1023 Q2109 M.pdf', 'QC14 Ph2 Attachment 3  TOT1023 Q2109 M.pdf', 'QC14 Ph2 Attachment 2  TOT1023 Q2109 M.pdf', 'QC14 Ph2 Appx A  TOT1023 Q2109 M.pdf', 'QC14PII-Northern-Attachment1-TOT1024 Q2110 Flea Flicker Energy Storage-clean.pdf', 'C14.2-North-Q2110-TOT1024-FleaFlicker_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1024 Q2110 N.pdf', 'QC14 Ph2 Attachment 3  TOT1024 Q2110 N.pdf', 'QC14 Ph2 Attachment 2  TOT1024 Q2110 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1024 Q2110 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1027 Q2111 M.pdf', 'QC14 Ph2 Attachment 3  TOT1027 Q2111 M.pdf', 'QC14 Ph2 Appx A TOT1027 Q2111 M.pdf', 'QC14 Ph2 Attachment 1  TOT1027 Q2111 M.pdf', 'P2RPT-C14_Q2111_FLEETWOOD_PhII_AppendixA_Addendum.pdf', 'P2RPT-C14_Q2111_FLEETWOOD_PhII_Addendum_Attachment2.pdf', 'QC14 Ph2 Attachment 3  TOT1028 Q2113 M.pdf', 'QC14 Ph2 Appx A TOT1028 Q2113 M.pdf', 'QC14 Ph2 Attachment 2  TOT1028 Q2113 M.pdf', 'QC14 Ph2 Attachment 1  TOT1028 Q2113 M.pdf', 'QC14 Ph2 Attachment 1  TOT1029 Q2114 M.pdf', 'QC14 Ph2 Attachment 2  TOT1029 Q2114 M.pdf', 'QC14 Ph2 Attachment 3  TOT1029 Q2114 M.pdf', 'QC14 Ph2 Appx A TOT1029 Q2114 M.pdf', 'P2RPT-C14_Q2114_DIRAC_PhII_Addendum_Attachment2.pdf', 'P2RPT-C14_Q2114_DIRAC_PhII_AppendixA_Addendum.pdf', 'QC14 Ph2 Attachment 1  TOT1030 Q2115 M.pdf', 'QC14 Ph2 Appx A TOT1030 Q2115 M.pdf', 'QC14 Ph2 Attachment 3  TOT1030 Q2115 M.pdf', 'QC14 Ph2 Attachment 2  TOT1030 Q2115 M.pdf', 'QC14 Ph2 Attachment 1  TOT1033 Q2116 M.pdf', 'QC14 Ph2 Attachment 3  TOT1033 Q2116 M.pdf', 'QC14 Ph2 Attachment 1  TOT1033 Q2116 M rev1.pdf', 'QC14 Ph2 Attachment 2  TOT1033 Q2116 M.pdf', 'QC14 Ph2 Appx A  TOT1033 Q2116 M.pdf', 'QC14 Ph2 Attachment 2  TOT1033 Q2116 M rev1.pdf', 'QC14 Ph2 Appx A  TOT1033 Q2116 M - Addendum 1.pdf', 'QC14 Ph2 Attachment 3  TOT1033 Q2116 M Addendum 1.pdf', 'QC14 Ph2 Attachment 3  TOT1034 Q2117 M.pdf', 'QC14 Ph2 Appx A  TOT1034 Q2117 M.pdf', 'QC14 Ph2 Attachment 2  TOT1034 Q2117 M.pdf', 'QC14 Ph2 Attachment 1  TOT1034 Q2117 M.pdf', 'QC14 Ph2 Attachment 1  TOT1060 Q2121 M.pdf', 'QC14 Ph2 Attachment 2  TOT1060 Q2121 M.pdf', 'QC14 Ph2 Appx A  TOT1060 Q2121 M.pdf', 'QC14 Ph2 Attachment 3  TOT1060 Q2121 M.pdf', 'QC14 Ph2 Appx A  TOT1062 Q2124 M.pdf', 'QC14 Ph2 Attachment 2  TOT1062 Q2124 M.pdf', 'QC14 Ph2 Attachment 3  TOT1062 Q2124 M.pdf', 'QC14 Ph2 Attachment 1  TOT1062 Q2124 M.pdf', 'QC14 Ph2 Attachment 1  TOT1063 Q2125 M.pdf', 'QC14 Ph2 Attachment 3  TOT1063 Q2125 M.pdf', 'QC14 Ph2 Attachment 2  TOT1063 Q2125 M.pdf', 'QC14 Ph2 Appx A  TOT1063 Q2125 M.pdf', 'QC14 Ph2 Attachment 1  TOT1076 Q2127 M.pdf', 'QC14 Ph2 Attachment 3  TOT1076 Q2127 M.pdf', 'QC14 Ph2 Attachment 2  TOT1076 Q2127 M.pdf', 'QC14 Ph2 Appx A  TOT1076 Q2127 M.pdf', 'QC14 Ph2 Attachment 1  TOT1148 Q2129 M.pdf', 'QC14 Ph2 Attachment 3  TOT1148 Q2129 M.pdf', 'QC14 Ph2 Appx A  TOT1148 Q2129 M.pdf', 'QC14 Ph2 Attachment 2  TOT1148 Q2129 M.pdf', 'QC14 Ph2 Attachment 2  TOT1088 Q2131 M.pdf', 'QC14 Ph2 Appx A  TOT1088 Q2131 M.pdf', 'QC14 Ph2 Attachment 3  TOT1088 Q2131 M.pdf', 'Q2131__Merlin_Storage__PhII_Results_Mtg_FINAL.pdf', 'QC14 Ph2 Attachment 1  TOT1088 Q2131 M.pdf', 'QC14 Ph2 Attachment 1  TOT1090 Q2134 M.pdf', 'QC14 Ph2 Attachment 3  TOT1090 Q2134 M.pdf', 'QC14 Ph2 Appx A  TOT1090 Q2134 M.pdf', 'QC14 Ph2 Attachment 2  TOT1090 Q2134 M.pdf', 'QC14 Ph2 Attachment 2  TOT1108 Q2136 M.pdf', 'QC14 Ph2 Appx A  TOT1108 Q2136 M.pdf', 'QC14 Ph2 Attachment 3  TOT1108 Q2136 M.pdf', 'QC14 Ph2 Attachment 1  TOT1108 Q2136 M.pdf', 'QC14 Ph2 Appx A  TOT1131 Q2137 M v2.pdf', 'QC14 Ph2 Attachment 2  TOT1131 Q2137 M.pdf', 'QC14PII - Metro - Attachment 1 - Q2137 - TOT1131 - Mt Baldy Energy Storage-clean.pdf', 'QC14 Ph2 Appx A  TOT1131 Q2137 M.pdf', 'QC14 Ph2 Attachment 3  TOT1131 Q2137 M.pdf', 'P2RPT-C14_Q2137_MT_BALDY_ENERGY_STORAGE_PhII_Addendum_Attachment2.pdf', 'P2RPT-C14_Q2137_MT_BALDY_ENERGY_STORAGE_PhII_AppendixA_Addendum.pdf', 'QC14 Ph2 Attachment 1  TOT1133 Q2138 M.pdf', 'QC14PII - Metro - Attachment 1 - Q2138 - TOT1133 - Separator-clean.pdf', 'C14.2-Metro-Q2138-TOT1133-Seperator_ApndxA-clean1.pdf', 'QC14 Ph2 Attachment 3  TOT1133 Q2138 M.pdf', 'QC14 Ph2 Attachment 2  TOT1133 Q2138 M.pdf', 'QC14PII - EOP - Attachment 1 - Q2140 - TOT1056 - Sterling.pdf', 'C14.2-EOP-Q2140-TOT1056-Sterling_ApndxA-final.pdf', 'QC14 Ph2 Attachment 3  TOT1056 Q2140 EOP.pdf', 'QC14 Ph2 Attachment 2  TOT1056 Q2140 EOP.pdf', 'QC14 Ph2 Attachment 2  TOT1056 Q2140 EOP Addendum 3-26-24.pdf', 'QC14PII - EOP - Attachment 1 - Q2141 - TOT1130 - Delamar Energy Storage 2.pdf', 'QC14 Ph2 Attachment 3  TOT1130 Q2141 EOP.pdf', 'QC14 Ph2 Attachment 2  TOT1130 Q2141 EOP.pdf', 'C14.2-EOP-Q2141-TOT1130-Delamar2_ApndxA-final.pdf', 'C14.2-EOP-Q2141-TOT1130-Delamar2_ApndxA-Addendum.pdf', 'C14.2-AS-EOP-Q2142-TOT1022-SilverStar_ApndxA-final.pdf', 'QC14P2-Q2142_Att1.pdf', 'QC14 Ph2 Attachment 3  TOT1022 Q2142 EOP AFS.pdf', 'QC14P2-Q2142_Att2.pdf', 'Q2142combined Attachment 3 feb162024.pdf', 'QC14 Ph2 Attachment 2  TOT1022 Q2142 EOP AFS.pdf', 'QC14P2-Q2142_ApndxA-final.pdf', 'QC14P2-Q2142_Att2_Update_2_16.pdf', 'QC14P2-Q2144_Att2_Update_2_16.pdf', 'QC14 Ph2 Attachment 2  TOT1118 Q2144 EOP AFS.pdf', 'QC14 Ph2 Attachment 3  TOT1118 Q2144 EOP AFS.pdf', 'QC14P2-Q2144_ApndxA-final.pdf', 'C14.2-AS-EOP-Q2144-TOT1118-Murray_ApndxA-final.pdf', 'Q2144combinedAttachment 3 feb162024.pdf', 'QC14P2-Q2144_Att1.pdf', 'QC14P2-Q2144_Att2.pdf', 'QC14P2-Q2145_ApndxA_final.pdf', 'QC14P2-Q2145_Att2_Update_2_16.pdf', 'QC14 Ph2 Attachment 3  TOT1150 Q2145 EOP AFS.pdf', 'Q2145combinedAttachment3Feb162024.pdf', 'QC14P2-Q2145_Att1.pdf', 'QC14P2-Q2145_Att2.pdf', 'QC14 Ph2 Attachment 2  TOT1150 Q2145 EOP AFS.pdf', 'C14.2-AS-EOP-Q2145-TOT1150-Dandelion_ApndxA-final.pdf', 'QC14P2-Q2146_Att1-final.pdf', 'QC14 Ph2 Attachment 2  TOT1085 Q2146 EOP AFS.pdf', 'QC14P2-Q2146_Att2_Update_2_16.pdf', 'Q2146combinedAttachment3Feb162024.pdf', 'C14.2-AS-EOP-Q2146-TOT1085-WaterRock2_ApndxA.pdf', 'QC14 Ph2 Attachment 3  TOT1085 Q2146 EOP AFS.pdf', 'QC14P2-Q2146_ApndxA-final.pdf', 'QC14P2-Q2146_Att2-final.pdf', 'QC14P2-Q2147_Att1-final.pdf', 'QC14 Ph2 Attachment 3  TOT1053 Q2147 EOP AFS.pdf', 'QC14 Ph2 Attachment 2  TOT1053 Q2147 EOP AFS.pdf', 'QC14P2-Q2147_ApndxA-final.pdf', 'C14.2-AS-EOP-Q2147-TOT1053-Kawich_ApndxA.pdf', 'QC14 Ph2 Attachment 3  TOT1054 Q2148 EOP AFS.pdf', 'QC14 Ph2 Attachment 2  TOT1054 Q2148 EOP AFS.pdf', 'Q2148-Combined Attachment 3.pdf', 'QC14P2-Q2148_ApndxA-final.pdf', 'Q2148combinedAttachment3Feb162024.pdf', 'C14.2-AS-EOP-Q2148-TOT1054-Mosey_ApndxA.pdf', 'QC14P2-Q2148_Att2_Update_2_16.pdf', 'QC14 Ph2 Attachment 2  TOT1127 Q2149 EOP AFS.pdf', 'Q2149combinedAttachment3Feb162024.pdf', 'Q2149-TOT1127-Attachment 2-SCE AS.pdf', 'QC14 Ph2 Attachment 3  TOT1127 Q2149 EOP AFS.pdf', 'QC14P2-Q2149_Att2_Update_2_16.pdf', 'QC14P2-Q2149_Att2.pdf', 'QC14P2-Q2149_Att1.pdf', 'QC14P2-Q2149_ApndxA-final.pdf', 'C14.2-AS-EOP-Q2149-TOT1127-Sunbaked_ApndxA.pdf', 'Q2150combinedAttachment3Feb162024.pdf', 'QC14 Ph2 Attachment 3  TOT1142 Q2150 EOP AFS.pdf', 'QC14 Ph2 Attachment 2  TOT1142 Q2150 EOP AFS.pdf', 'QC14P2-Q2150_ApndxA-final.pdf', 'QC14P2-Q2150_Att1-final.pdf', 'QC14P2-Q2150_Att2-final.pdf', 'QC14P2-Q2150_Att2_Update_2_16.pdf', 'C14.2-AS-EOP-Q2150-TOT1142-RoughHat3_ApndxA.pdf', 'C4PhII_Q2173_Attachment 1.pdf', 'C4PhII_Q2181_Attachment 1.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['Q1859Mayacamas_GeothermalAppendix_AC14PhII_Revision1.pdf', 'Q1871Stageline_Energy_StorageAppendix_AC14PhIIAddendum1.pdf', 'Q1875Delilah_Energy_StorageAppendix_AC14PhIIRevision1.pdf', 'Q1881Spectrum_Energy_StorageAppendix_AC14PhIIRevision1.pdf', 'Q1932-Cougar Storage-Appendix_A-C14PhII-Revision1.pdf', 'Q1954Huaso_HybridAppendix_AC14PhIIAddendum1.pdf', 'Q1956SequoiaAppendix_AC14PhIIAddendum1.pdf', 'Q1958Spikes_Peak_SolarAppendix_AC14PhIIAddendum1.pdf', 'Q1959Cornucopia_HybridAppendix_AC14PhIIAddendum1.pdf', 'Q1963Almande_Energy_StorageAppendix_AC14PhIIAddendum1.pdf', 'Q1968PlumAppendix_AC14PhIIAddendum1.pdf', 'Q1973Peaceful_Hollow_BESSAppendix_AC14PhIIAddendum1.pdf', 'Q1992Callinan_Solar_and_StorageAppendix_AC14PhIIRevision1_Final.pdf', 'QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 1a_SLD (SH 2 OF SH 2).pdf', 'QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 1a_SLD (SH 1 OF SH 2).pdf', 'QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A-Attachment 2b_Addendum.pdf', 'QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 1a_SLD-(Sh1of2).pdf', 'QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A_Addendum.pdf', 'QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 1a_SLD-(Sh2of2).pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment 2_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A_Addendum.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment 2_Revision.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A_Revision.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment 2_Revision.pdf', 'QC14 Ph2 Attachment 2  TOT1032 Q2055 E Revision 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1032 Q2055 E Revision 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1036 Q2056 N Addendum.pdf', 'QC14 Ph2 Attachment 2  TOT1047 Q2060 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1047 Q2060 N Addendum 4-15-24.pdf', 'Q2061-TOT1043-WhirlwindSCDFixAdm-Northern-final.pdf', 'QC14 Ph2 Attachment 2  TOT1043 Q2061 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1043 Q2061 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1052 Q2062 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1052 Q2062 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1059 Q2064 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1059 Q2064 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1072 Q2068 N Addendum 3-26-24.pdf', 'QC14 Ph2 Attachment 2  TOT1097 Q2078 N Revision 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1097 Q2078 N Revision 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1111 Q2080 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1111 Q2080 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1111 Q2080 Option 2 (IC Build) N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1111 Q2080 Option 2 (IC Build) N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1136 Q2089 N Revision 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1136 Q2089 N Revision 4-15-24.pdf', 'QC14PII-Northern-Attachment1-TOT1138 Q2090 Gwent Storage 2-addendum.pdf', 'QC14 Ph2 Attachment 2  TOT1138 Q2090 N Addendum 3-26-24.pdf', 'QC14 Ph2 Attachment 2  TOT1140 Q2091 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1140 Q2091 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 2  TOT1024 Q2110 N Addendum 4-15-24.pdf', 'QC14 Ph2 Attachment 3  TOT1024 Q2110 N Addendum 4-15-24.pdf', 'P2RPT-C14_Q2111_FLEETWOOD_PhII_AppendixA_Addendum.pdf', 'P2RPT-C14_Q2111_FLEETWOOD_PhII_Addendum_Attachment2.pdf', 'P2RPT-C14_Q2114_DIRAC_PhII_Addendum_Attachment2.pdf', 'P2RPT-C14_Q2114_DIRAC_PhII_AppendixA_Addendum.pdf', 'QC14 Ph2 Attachment 2  TOT1033 Q2116 M rev1.pdf', 'QC14 Ph2 Appx A  TOT1033 Q2116 M - Addendum 1.pdf', 'QC14 Ph2 Attachment 3  TOT1033 Q2116 M Addendum 1.pdf', 'P2RPT-C14_Q2137_MT_BALDY_ENERGY_STORAGE_PhII_Addendum_Attachment2.pdf', 'P2RPT-C14_Q2137_MT_BALDY_ENERGY_STORAGE_PhII_AppendixA_Addendum.pdf', 'QC14 Ph2 Attachment 2  TOT1056 Q2140 EOP Addendum 3-26-24.pdf', 'C14.2-EOP-Q2141-TOT1130-Delamar2_ApndxA-Addendum.pdf', 'P2RPT-C14_Q2177_BoulderBrushHybrid_PhII_Addendum2_32624.pdf', 'P2RPT-C14_Q2177_BoulderBrushHybrid_PhII_Addendum_1_03072024.pdf', 'P2RPT-C14_Q2181_PinscherEnergyStorage_PhII_Addendum1_4222024.pdf', 'P2RPT-C14_Q2188_GeraniumEnergyStorage_PhII_Addendum1_3142024.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['C14_Q1832_GoalLineReliability_PhII_01-30-2024.pdf', 'QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 1a.pdf', 'QC14PII-SCE-Eastern-Q2022 TOT1101-Socorro Peak Solar-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2022 TOT1101-Socorro Peak Solar-Appendix A-Attachment 2b.pdf', 'QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2022 TOT1101-Socorro Peak Solar-Appendix A-Attachment 1b.pdf', 'QC14PII-DCRT-Eastern-Q2022-TOT1101-Socorro Peak-Appendix A-Attachment 2a.pdf', 'QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A.pdf', 'QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 1a.pdf', 'QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A-Attachment 1b.pdf', 'QC14PII-SCE-Eastern-Q2023 TOT1083-Jove-Appendix A-Attachment 2b.pdf', 'QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A.pdf', 'QC14PII-DCRT-Eastern-Q2023-TOT1083-Jove-Appendix A-Attachment 2a.pdf', 'QC14PII-SCE-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A-Attachment 1b.pdf', 'QC14PII-SCE-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A.pdf', 'QC14PII-DCRT-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2025 TOT1113-Ranegras PVS-Appendix A-Attachment 2b.pdf', 'QC14PII-SCE-Eastern-Q2027 TOT1114-Harquahala Flats 2-Appendix A-Attachment 1b.pdf', 'QC14PII-DCRT-Eastern-Q2027-TOT1114-Harquahala Flats 2-Appendix A-Attachment 2a.pdf', 'QC14PII-DCRT-Eastern-Q2027-TOT1114-Harquahala Flats 2-Appendix A.pdf', 'QC14PII-DCRT-Eastern-Q2027-TOT1114-Harquahala Flats 2-Appendix A-Attachment 1a.pdf', 'QC14PII-SCE-Eastern-Q2027 TOT1114-Harquahala Flats 2-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2027 TOT1114-Harquahala Flats 2-Appendix A-Attachment 2b.pdf', 'QC14PII-SCE-Eastern-Q2029 TOT1139-Harquahala Sunrise 2-Appendix A.pdf', 'QC14PII-DCRT-Eastern-Q2029-TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 1a.pdf', 'QC14PII-SCE-Eastern-Q2029 TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 1b.pdf', 'QC14PII-SCE-Eastern-Q2029 TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 2b.pdf', 'QC14PII-DCRT-Eastern-Q2029-TOT1139-Harquahala Sunrise 2-Appendix A-Attachment 2a.pdf', 'QC14PII-DCRT-Eastern-Q2029-TOT1139-Harquahala Sunrise 2-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2031-TOT1031-Marley-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2032-TOT1035-Trolley-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2033-TOT1042-Carmine-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2034-TOT1045-Hub City-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2036-TOT1061-Redonda-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2037-TOT1087-Sargasso-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2041-TOT1095-Athos Storage 2-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2042-TOT1094-Easley-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2043-TOT1068-Picador-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2045-TOT1104-Invictus-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2048-TOT1098-Twin Palms-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A_rev1.pdf', 'Q2049__Ardilla_PHII_Results_Mtg__Minutes_FINAL.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2049-TOT1123-Ardilla-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2050-TOT1107-Eternal 2-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2051-TOT1132-Salvador-Appendix A.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment 2.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment 1.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A_rev1.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A-Attachment1_rev1.pdf', 'QC14PII-SCE-Eastern-Q2052-TOT1128-Bouse-Appendix A.pdf', 'QC14 Ph2 Attachment 3  TOT1032 Q2055 N.pdf', 'C14.2-North-Q2055-TOT1032-Euismod_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1032 Q2055 N.pdf', 'QC14PII-Northern-Attachment1-TOT1036 Q2056 Quercus-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1036 Q2056 N.pdf', 'C14.2-North-Q2056-TOT1036-Quercus_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1036 Q2056 N.pdf', 'C14.2-North-Q2060-TOT1047-SagebrushBESS_ApndxA-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1047 Q2060 Sagebrush Energy Storage-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1047 Q2060 N.pdf', 'QC14 Ph2 Attachment 3  TOT1047 Q2060 N.pdf', 'QC14 Ph2 Attachment 3  TOT1043 Q2061 N.pdf', 'C14.2-North-Q2061-TOT1043-Juniper_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1043 Q2061 N.pdf', 'QC14PII-Northern-Attachment1-TOT1043 Q2061 Juniper Storage-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1052 Q2062 Dorian Solar-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1052 Q2062 N.pdf', 'C14.2-North-Q2062-TOT1052-Dorian_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1052 Q2062 N.pdf', 'QC14 Ph2 Attachment 3  TOT1059 Q2064 N.pdf', 'QC14 Ph2 Attachment 2  TOT1059 Q2064 N.pdf', 'C14.2-North-Q2064-TOT1059-Elion_ApndxA-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1059 Q2064 Elion Energy Storage-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1089 Q2066 N.pdf', 'QC14 Ph2 Attachment 3  TOT1089 Q2066 N.pdf', 'C14.2-North-Q2066-TOT1089-Drifter_ApndxA-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1089 Q2066 Drifter Energy Storage-clean.pdf', 'C14.2-North-Q2068-TOT1072-Bobor_ApndxA-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1072 Q2068 Bobor Storage-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1072 Q2068 N.pdf', 'QC14 Ph2 Attachment 3  TOT1072 Q2068 N.pdf', 'QC14PII-Northern-Attachment1-TOT1099 Q2075 Fairmont Solar 1-clean.pdf', 'C14.2-North-Q2075-TOT1099-Fairmont_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1099 Q2075 N.pdf', 'QC14 Ph2 Attachment 2  TOT1099 Q2075 N.pdf', 'C14.2-North-Q2078-TOT1097-SoleilCantil_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1097 Q2078 N.pdf', 'QC14PII-Northern-Attachment1-TOT1097 Q2078 Soleil Cantil-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1097 Q2078 N.pdf', 'C14.2-North-Attchmnt1b-ICBuild-TOT1111&Q2080-Rangeland-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1111 Q2080 Option 2 (IC Build) N.pdf', 'QC14 Ph2 Attachment 3  TOT1111 Q2080 N.pdf', 'QC14 Ph2 Attachment 2  TOT1111 Q2080 N.pdf', 'C14.2-SCE-North-OTB-Q2080&TOT1111-Rangeland-AppendixA-clean.pdf', 'C14.2-North-Attchmnt1a-SCEBuild-TOT1111&Q2080-Rangeland-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1111 Q2080 Option 2 (IC Build) N.pdf', 'QC14 Ph2 Attachment 2  TOT1100 Q2081 N.pdf', 'QC14PII-Northern-Attachment1-TOT1100 Q2081 Mineral King Solar-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1100 Q2081 N.pdf', 'C14.2-North-Q2081-TOT1100-MineralKing_ApndxA-clean.pdf', 'C14.2-North-Q2085-TOT1120-Solsken_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1120 Q2085 N.pdf', 'QC14PII-Northern-Attachment1-TOT1120 Q2085 Solsken-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1120 Q2085 N.pdf', 'QC14 Ph2 Attachment 3  TOT1136 Q2089 N.pdf', 'QC14 Ph2 Attachment 2  TOT1136 Q2089 N.pdf', 'C14.2-North-Q2089-TOT1136-Greasewood_ApndxA-clean.pdf', 'QC14PII-Northern-Attachment1-TOT1136 Q2089 Greasewood Energy Storage-clean.pdf', 'Q2090__Gwent_Storage_2_PHII_Mtg__Minutes_FINAL.pdf', 'QC14PII-Northern-Attachment1-TOT1138 Q2090 Gwent Storage 2-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1138 Q2090 N.pdf', 'C14.2-North-Q2090-TOT1138-Gwent2_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1138 Q2090 N.pdf', 'QC14PII-Northern-Attachment1-TOT1140 Q2091 Maathai Storage-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1140 Q2091 N.pdf', 'QC14 Ph2 Attachment 2  TOT1140 Q2091 N.pdf', 'C14.2-North-Q2091-TOT1140-Maathai_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 3  TOT1147 Q2092 N.pdf', 'QC14 Ph2 Attachment 2  TOT1147 Q2092 N.pdf', 'QC14PII-Northern-Attachment1-TOT1147 Q2092 Rosa Storage-clean.pdf', 'C14.2-North-Q2092-TOT1147-Rosa_ApndxA-clean.pdf', 'QC14PII-SCE-ApndixA-Attachment 1  Q2096 TOT1064 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2 TOT1046 Q2097 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 1 Q2097 TOT1046 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3 TOT1046 Q2097 NOL.pdf', 'QC14PII-SCE-ApndixA-Q2097-TOT1046-Sienna2 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3  TOT1080 Q2098 NOL (IC Build).pdf', 'QC14PII-SCE-ApndixA-Attachment 2  TOT1080 Q2098 NOL (IC Build).pdf', 'QC14PII-SCE-ApndixA-OTB-Q2098-TOT1080-Cuerno Grande NOL.pdf', 'QC14PII-SCE-ApndixA-Attchmnt1b(ICBuild)-Q2098-TOT1080 NOL.pdf', 'QC14PII-SCE-ApndixA-Attchmnt1a-Q2098-TOT1080 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2  TOT1080 Q2098 NOL (SCE Build).pdf', 'QC14PII-SCE-ApndixA-Attachment 3  TOT1080 Q2098 NOL (SCE Build).pdf', 'QC14PII-SCE-ApndixA-Attachment 1  Q2101 - TOT1091 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3  TOT1091 Q2101 NOL.pdf', 'QC14PII-SCE-ApndixA-Q2101-TOT1091-Weston NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2  TOT1091 Q2101 NOL.pdf', 'QC14PII-SCE-ApndixA-Q2103-TOT1106-Cady NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3  TOT1106 Q2103 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 1 Q2103 - TOT1106 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2  TOT1106 Q2103 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2 TOT1109 Q2104 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 1 Q2104 - TOT1109 NOL.pdf', 'QC14PII-SCE-ApndixA-Q2104-TOT1109-DosPalmas NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3 TOT1109 Q2104 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 2  TOT1125 Q2105 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 1  Q2105 - TOT1125 NOL.pdf', 'QC14PII-SCE-ApndixA-Attachment 3  TOT1125 Q2105 NOL.pdf', 'QC14PII-SCE-ApndixA-Q2105-TOT1125-Conduit NOL.pdf', 'QC14 Ph2 Attachment 1  TOT1023 Q2109 M.pdf', 'QC14 Ph2 Attachment 3  TOT1023 Q2109 M.pdf', 'QC14 Ph2 Attachment 2  TOT1023 Q2109 M.pdf', 'QC14 Ph2 Appx A  TOT1023 Q2109 M.pdf', 'QC14PII-Northern-Attachment1-TOT1024 Q2110 Flea Flicker Energy Storage-clean.pdf', 'C14.2-North-Q2110-TOT1024-FleaFlicker_ApndxA-clean.pdf', 'QC14 Ph2 Attachment 2  TOT1024 Q2110 N.pdf', 'QC14 Ph2 Attachment 3  TOT1024 Q2110 N.pdf', 'QC14 Ph2 Attachment 2  TOT1027 Q2111 M.pdf', 'QC14 Ph2 Attachment 3  TOT1027 Q2111 M.pdf', 'QC14 Ph2 Appx A TOT1027 Q2111 M.pdf', 'QC14 Ph2 Attachment 1  TOT1027 Q2111 M.pdf', 'QC14 Ph2 Attachment 3  TOT1028 Q2113 M.pdf', 'QC14 Ph2 Appx A TOT1028 Q2113 M.pdf', 'QC14 Ph2 Attachment 2  TOT1028 Q2113 M.pdf', 'QC14 Ph2 Attachment 1  TOT1028 Q2113 M.pdf', 'QC14 Ph2 Attachment 1  TOT1029 Q2114 M.pdf', 'QC14 Ph2 Attachment 2  TOT1029 Q2114 M.pdf', 'QC14 Ph2 Attachment 3  TOT1029 Q2114 M.pdf', 'QC14 Ph2 Appx A TOT1029 Q2114 M.pdf', 'QC14 Ph2 Attachment 1  TOT1030 Q2115 M.pdf', 'QC14 Ph2 Appx A TOT1030 Q2115 M.pdf', 'QC14 Ph2 Attachment 3  TOT1030 Q2115 M.pdf', 'QC14 Ph2 Attachment 2  TOT1030 Q2115 M.pdf', 'QC14 Ph2 Attachment 1  TOT1033 Q2116 M.pdf', 'QC14 Ph2 Attachment 3  TOT1033 Q2116 M.pdf', 'QC14 Ph2 Attachment 1  TOT1033 Q2116 M rev1.pdf', 'QC14 Ph2 Attachment 2  TOT1033 Q2116 M.pdf', 'QC14 Ph2 Appx A  TOT1033 Q2116 M.pdf', 'QC14 Ph2 Attachment 3  TOT1034 Q2117 M.pdf', 'QC14 Ph2 Appx A  TOT1034 Q2117 M.pdf', 'QC14 Ph2 Attachment 2  TOT1034 Q2117 M.pdf', 'QC14 Ph2 Attachment 1  TOT1034 Q2117 M.pdf', 'QC14 Ph2 Attachment 1  TOT1060 Q2121 M.pdf', 'QC14 Ph2 Attachment 2  TOT1060 Q2121 M.pdf', 'QC14 Ph2 Appx A  TOT1060 Q2121 M.pdf', 'QC14 Ph2 Attachment 3  TOT1060 Q2121 M.pdf', 'QC14 Ph2 Appx A  TOT1062 Q2124 M.pdf', 'QC14 Ph2 Attachment 2  TOT1062 Q2124 M.pdf', 'QC14 Ph2 Attachment 3  TOT1062 Q2124 M.pdf', 'QC14 Ph2 Attachment 1  TOT1062 Q2124 M.pdf', 'QC14 Ph2 Attachment 1  TOT1063 Q2125 M.pdf', 'QC14 Ph2 Attachment 3  TOT1063 Q2125 M.pdf', 'QC14 Ph2 Attachment 2  TOT1063 Q2125 M.pdf', 'QC14 Ph2 Appx A  TOT1063 Q2125 M.pdf', 'QC14 Ph2 Attachment 1  TOT1076 Q2127 M.pdf', 'QC14 Ph2 Attachment 3  TOT1076 Q2127 M.pdf', 'QC14 Ph2 Attachment 2  TOT1076 Q2127 M.pdf', 'QC14 Ph2 Appx A  TOT1076 Q2127 M.pdf', 'QC14 Ph2 Attachment 1  TOT1148 Q2129 M.pdf', 'QC14 Ph2 Attachment 3  TOT1148 Q2129 M.pdf', 'QC14 Ph2 Appx A  TOT1148 Q2129 M.pdf', 'QC14 Ph2 Attachment 2  TOT1148 Q2129 M.pdf', 'QC14 Ph2 Attachment 2  TOT1088 Q2131 M.pdf', 'QC14 Ph2 Appx A  TOT1088 Q2131 M.pdf', 'QC14 Ph2 Attachment 3  TOT1088 Q2131 M.pdf', 'Q2131__Merlin_Storage__PhII_Results_Mtg_FINAL.pdf', 'QC14 Ph2 Attachment 1  TOT1088 Q2131 M.pdf', 'QC14 Ph2 Attachment 1  TOT1090 Q2134 M.pdf', 'QC14 Ph2 Attachment 3  TOT1090 Q2134 M.pdf', 'QC14 Ph2 Appx A  TOT1090 Q2134 M.pdf', 'QC14 Ph2 Attachment 2  TOT1090 Q2134 M.pdf', 'QC14 Ph2 Attachment 2  TOT1108 Q2136 M.pdf', 'QC14 Ph2 Appx A  TOT1108 Q2136 M.pdf', 'QC14 Ph2 Attachment 3  TOT1108 Q2136 M.pdf', 'QC14 Ph2 Attachment 1  TOT1108 Q2136 M.pdf', 'QC14 Ph2 Appx A  TOT1131 Q2137 M v2.pdf', 'QC14 Ph2 Attachment 2  TOT1131 Q2137 M.pdf', 'QC14PII - Metro - Attachment 1 - Q2137 - TOT1131 - Mt Baldy Energy Storage-clean.pdf', 'QC14 Ph2 Appx A  TOT1131 Q2137 M.pdf', 'QC14 Ph2 Attachment 3  TOT1131 Q2137 M.pdf', 'QC14 Ph2 Attachment 1  TOT1133 Q2138 M.pdf', 'QC14PII - Metro - Attachment 1 - Q2138 - TOT1133 - Separator-clean.pdf', 'C14.2-Metro-Q2138-TOT1133-Seperator_ApndxA-clean1.pdf', 'QC14 Ph2 Attachment 3  TOT1133 Q2138 M.pdf', 'QC14 Ph2 Attachment 2  TOT1133 Q2138 M.pdf', 'QC14PII - EOP - Attachment 1 - Q2140 - TOT1056 - Sterling.pdf', 'C14.2-EOP-Q2140-TOT1056-Sterling_ApndxA-final.pdf', 'QC14 Ph2 Attachment 3  TOT1056 Q2140 EOP.pdf', 'QC14 Ph2 Attachment 2  TOT1056 Q2140 EOP.pdf', 'QC14PII - EOP - Attachment 1 - Q2141 - TOT1130 - Delamar Energy Storage 2.pdf', 'QC14 Ph2 Attachment 3  TOT1130 Q2141 EOP.pdf', 'QC14 Ph2 Attachment 2  TOT1130 Q2141 EOP.pdf', 'C14.2-EOP-Q2141-TOT1130-Delamar2_ApndxA-final.pdf', 'C14.2-AS-EOP-Q2142-TOT1022-SilverStar_ApndxA-final.pdf', 'QC14P2-Q2142_Att1.pdf', 'QC14 Ph2 Attachment 3  TOT1022 Q2142 EOP AFS.pdf', 'QC14P2-Q2142_Att2.pdf', 'Q2142combined Attachment 3 feb162024.pdf', 'QC14 Ph2 Attachment 2  TOT1022 Q2142 EOP AFS.pdf', 'QC14P2-Q2142_ApndxA-final.pdf', 'QC14P2-Q2142_Att2_Update_2_16.pdf', 'QC14P2-Q2144_Att2_Update_2_16.pdf', 'QC14 Ph2 Attachment 2  TOT1118 Q2144 EOP AFS.pdf', 'QC14 Ph2 Attachment 3  TOT1118 Q2144 EOP AFS.pdf', 'QC14P2-Q2144_ApndxA-final.pdf', 'C14.2-AS-EOP-Q2144-TOT1118-Murray_ApndxA-final.pdf', 'Q2144combinedAttachment 3 feb162024.pdf', 'QC14P2-Q2144_Att1.pdf', 'QC14P2-Q2144_Att2.pdf', 'QC14P2-Q2145_ApndxA_final.pdf', 'QC14P2-Q2145_Att2_Update_2_16.pdf', 'QC14 Ph2 Attachment 3  TOT1150 Q2145 EOP AFS.pdf', 'Q2145combinedAttachment3Feb162024.pdf', 'QC14P2-Q2145_Att1.pdf', 'QC14P2-Q2145_Att2.pdf', 'QC14 Ph2 Attachment 2  TOT1150 Q2145 EOP AFS.pdf', 'C14.2-AS-EOP-Q2145-TOT1150-Dandelion_ApndxA-final.pdf', 'QC14P2-Q2146_Att1-final.pdf', 'QC14 Ph2 Attachment 2  TOT1085 Q2146 EOP AFS.pdf', 'QC14P2-Q2146_Att2_Update_2_16.pdf', 'Q2146combinedAttachment3Feb162024.pdf', 'C14.2-AS-EOP-Q2146-TOT1085-WaterRock2_ApndxA.pdf', 'QC14 Ph2 Attachment 3  TOT1085 Q2146 EOP AFS.pdf', 'QC14P2-Q2146_ApndxA-final.pdf', 'QC14P2-Q2146_Att2-final.pdf', 'QC14P2-Q2147_Att1-final.pdf', 'QC14 Ph2 Attachment 3  TOT1053 Q2147 EOP AFS.pdf', 'QC14 Ph2 Attachment 2  TOT1053 Q2147 EOP AFS.pdf', 'QC14P2-Q2147_ApndxA-final.pdf', 'C14.2-AS-EOP-Q2147-TOT1053-Kawich_ApndxA.pdf', 'QC14 Ph2 Attachment 3  TOT1054 Q2148 EOP AFS.pdf', 'QC14 Ph2 Attachment 2  TOT1054 Q2148 EOP AFS.pdf', 'Q2148-Combined Attachment 3.pdf', 'QC14P2-Q2148_ApndxA-final.pdf', 'Q2148combinedAttachment3Feb162024.pdf', 'C14.2-AS-EOP-Q2148-TOT1054-Mosey_ApndxA.pdf', 'QC14P2-Q2148_Att2_Update_2_16.pdf', 'QC14 Ph2 Attachment 2  TOT1127 Q2149 EOP AFS.pdf', 'Q2149combinedAttachment3Feb162024.pdf', 'Q2149-TOT1127-Attachment 2-SCE AS.pdf', 'QC14 Ph2 Attachment 3  TOT1127 Q2149 EOP AFS.pdf', 'QC14P2-Q2149_Att2_Update_2_16.pdf', 'QC14P2-Q2149_Att2.pdf', 'QC14P2-Q2149_Att1.pdf', 'QC14P2-Q2149_ApndxA-final.pdf', 'C14.2-AS-EOP-Q2149-TOT1127-Sunbaked_ApndxA.pdf', 'Q2150combinedAttachment3Feb162024.pdf', 'QC14 Ph2 Attachment 3  TOT1142 Q2150 EOP AFS.pdf', 'QC14 Ph2 Attachment 2  TOT1142 Q2150 EOP AFS.pdf', 'QC14P2-Q2150_ApndxA-final.pdf', 'QC14P2-Q2150_Att1-final.pdf', 'QC14P2-Q2150_Att2-final.pdf', 'QC14P2-Q2150_Att2_Update_2_16.pdf', 'C14.2-AS-EOP-Q2150-TOT1142-RoughHat3_ApndxA.pdf', 'C14_Q2153_FrigatebirdStorage_PhII_01-30-2024.pdf', 'C14_Q2154_Remy_PhII_01-30-2024.pdf', 'C14_Q2157_GnarlyOsage_PhII_01-30-2024.pdf', 'C14_Q2161_AlisaSolarEnergyComplex_PhII_01-30-2024.pdf', 'C14_Q2162_SolarDeMexicali_PhII_01-30-2024.pdf', 'C14_Q2165_Tower1EnergyStorage_PhII_01-30-2024.pdf', 'C14_Q2166_Umbriel_PhII_01-30-2024.pdf', 'C14_Q2167_HammerheadStorage_PhII_01-30-2024.pdf', 'C14_Q2172_Hyder_PhII_01-30-2024.pdf', 'C14_Q2173_LagoDomingoStorage_PhII_01-30-2024.pdf', 'C4PhII_Q2173_Attachment 1.pdf', 'C14_Q2176_YuhaDesertBatteryStorage_PhII_01-30-2024.pdf', 'C14_Q2177_BoulderBrushHybrid_PhII_01-30-2024.pdf', 'C14_Q2178_BellBluffStorage_PhII_01-30-2024.pdf', 'C14_Q2180_CargoStorage_PhII_01-30-2024.pdf', 'C14_Q2181_PinscherEnergyStorage_PhII_01-30-2024.pdf', 'C4PhII_Q2181_Attachment 1.pdf', 'C14_Q2182_TaylorStorage_PhII_01-30-2024.pdf', 'C14_Q2184_AmberjackEnergy_PhII_01-30-2024.pdf', 'C14_Q2185_GatewayEnergyStorage2_PhII_01-30-2024.pdf', 'C14_Q2186_SandbarEnergyStorage_PhII_01-30-2024.pdf', 'C14_Q2187_EolicaDeRumorosa_PhII_01-30-2024.pdf', 'C14_Q2188_GeraniumEnergyStorage_PhII_01-30-2024.pdf', 'C14_Q2192_PajaroValleyStorage_PhII_01-30-2024.pdf']\n",
      "\n",
      "List of Style N PDFs (Skipped due to 'Network Upgrade Type'):\n",
      "[]\n",
      "\n",
      "Total Number of Style N PDFs: 0\n",
      "\n",
      "Number of Original PDFs Scraped: 23\n",
      "Number of Addendum PDFs Scraped: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:1086: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/3039312557.py:1086: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY =\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/ph2_rawdata_cluster14_style_Q_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/ph2_rawdata_cluster14_style_Q_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/ph2_scraping_cluster14_style_Q_log.txt\"\n",
    "PROJECT_RANGE = range(1831, 2193)  # Inclusive range for q_ids in Clusters 13\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "style_n_pdfs = []  # List to track style N PDFs\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "original_has_table7 = {}  # Dictionary to track if original PDFs have table7\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters, but keeps parentheses.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            # collapse internal whitespace\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            # strip out everything except letters, digits, spaces, and parentheses\n",
    "            header = re.sub(r'[^a-z0-9\\s\\(\\)]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    elif value is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(value).replace('\\n', ' ').strip()\n",
    "     \n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Conditionally Assigned Network Upgrades\",\n",
    "        \"Local Off-Peak Network Upgrade\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if  re.search(rf\"\\b{re.escape(phrase)}\\b(?=\\d|\\W|$)\", title, re.IGNORECASE):\n",
    "        \n",
    "         #re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        # Updated Cluster Extraction\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        if '14' in clusters:\n",
    "            cluster_number = '14'\n",
    "        elif clusters:\n",
    "            cluster_number = max(clusters, key=lambda x: int(x))  # Choose the highest cluster number found\n",
    "        else:\n",
    "            cluster_number = '14'  # Default to 12 if not found\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 7-1\" to \"Table 7-5\" with hyphen or dot\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*8[-.]([1-2])\\b\", text, re.IGNORECASE):\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 7-1 to 7-6 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1  # Plus two to include possible continuation\n",
    "\n",
    "            print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end}\", file=log_file)\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*8[-.]([1-2])[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(3).strip()\n",
    "                                break\n",
    "\n",
    "                    if table_title:\n",
    "                        if re.search(r\"\\b8-7\\b\", table_title, re.IGNORECASE):\n",
    "                            print(f\"Skipping Table 7-7 on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            continue  # Skip Table 7-7\n",
    "\n",
    "                        # New Table 7 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        # Handle ADNU-specific grouping\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    # Group all adnu rows into one 'upgrade' row\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    # If 'type of upgrade' exists, just rename adnu if needed\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row is none, replace only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' first row for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            # Non-ADNU new tables\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row is none, replace only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for the first row in new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(\"Duplicate columns detected in new table. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table\n",
    "                        if not extracted_tables:\n",
    "                            print(f\"No previous Table 7 detected to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        last_table = extracted_tables[-1]\n",
    "                        expected_columns = last_table.columns.tolist()\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "\n",
    "                        # Check if the first row is a header row\n",
    "                        # As per your latest instruction, we will treat all continuation table rows as data points\n",
    "                        # without any header detection\n",
    "                        # However, you mentioned checking if there is a header row first, so we'll implement that\n",
    "\n",
    "                        # Detect if first row is a header\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\", \"MW at POI\"]\n",
    "                        first_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            any(re.search(rf\"\\b{kw}\\b\", clean_string_cell(cell).lower()) for kw in header_keywords)\n",
    "                            for cell in first_row\n",
    "                        )\n",
    "\n",
    "                        if is_header_row:\n",
    "                            # Handle header row in continuation table\n",
    "                            headers = clean_column_headers(first_row)\n",
    "                            data_rows = data_rows[1:]  # Exclude header row\n",
    "\n",
    "                            # Update expected_columns by adding new columns if any\n",
    "                            new_columns = [col for col in headers if col not in expected_columns]\n",
    "                            if new_columns:\n",
    "                                expected_columns.extend(new_columns)\n",
    "                                print(f\"Added new columns from continuation table: {new_columns}\", file=log_file)\n",
    "\n",
    "                            # Create a mapping of new columns to add with default NaN\n",
    "                            for new_col in new_columns:\n",
    "                                last_table[new_col] = pd.NA\n",
    "\n",
    "                            # Reindex last_table to include new columns\n",
    "                            last_table = last_table.reindex(columns=expected_columns)\n",
    "                            extracted_tables[-1] = last_table\n",
    "\n",
    "                            # Update 'type of upgrade' column in the first row if needed\n",
    "                            if \"type of upgrade\" in headers:\n",
    "                                type_upgrade_idx = headers.index(\"type of upgrade\")\n",
    "                                if pd.isna(data_rows[0][type_upgrade_idx]) or data_rows[0][type_upgrade_idx] == \"\":\n",
    "                                    data_rows[0][type_upgrade_idx] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'type of upgrade' first row for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            elif \"upgrade\" in headers:\n",
    "                                upgrade_idx = headers.index(\"upgrade\")\n",
    "                                if pd.isna(data_rows[0][upgrade_idx]) or data_rows[0][upgrade_idx] == \"\":\n",
    "                                    data_rows[0][upgrade_idx] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'upgrade' first row for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' or 'upgrade' does not exist, add it\n",
    "                                headers.append(\"type of upgrade\")\n",
    "                                expected_columns.append(\"type of upgrade\")\n",
    "                                for idx, row in enumerate(data_rows):\n",
    "                                    data_rows[idx].append(specific_phrase)\n",
    "                                print(f\"Added 'type of upgrade' column and filled with '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                            # Handle ADNU-specific logic if applicable\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                if \"adnu\" in headers:\n",
    "                                    if \"upgrade\" not in headers:\n",
    "                                        # Rename 'adnu' to 'upgrade'\n",
    "                                        adnu_idx = headers.index(\"adnu\")\n",
    "                                        headers[adnu_idx] = \"upgrade\"\n",
    "                                        for row in data_rows:\n",
    "                                            row[adnu_idx] = \" \".join([str(cell) for cell in row[adnu_idx] if pd.notna(cell)])\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in continuation ADNU table.\", file=log_file)\n",
    "                                # Ensure 'type of upgrade' column is filled\n",
    "                                if \"type of upgrade\" not in headers:\n",
    "                                    headers.append(\"type of upgrade\")\n",
    "                                    expected_columns.append(\"type of upgrade\")\n",
    "                                    for row in data_rows:\n",
    "                                        row.append(specific_phrase)\n",
    "                                    print(\"Added 'type of upgrade' column with specific phrase for continuation ADNU table.\", file=log_file)\n",
    "\n",
    "                        else:\n",
    "                            # No header row detected, treat all rows as data points\n",
    "                            print(f\"No header row detected in continuation table on page {page_number + 1}, table {table_index + 1}. Treating all rows as data.\", file=log_file)\n",
    "\n",
    "                        # Create DataFrame for continuation table\n",
    "                        if is_header_row:\n",
    "                            try:\n",
    "                                df_continuation = pd.DataFrame(data_rows, columns=headers)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "                        else:\n",
    "                            # Create DataFrame with expected_columns\n",
    "                            # Handle cases where continuation table has more columns\n",
    "                            standardized_data = []\n",
    "                            for row in data_rows:\n",
    "                                if len(row) < len(expected_columns):\n",
    "                                    # Insert 'type of upgrade' or 'upgrade' with specific_phrase\n",
    "                                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                        # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                        missing_cols = len(expected_columns) - len(row)\n",
    "                                        #row += [specific_phrase] * missing_cols\n",
    "                                        data_rows = [row[:7] + [specific_phrase] + row[7:] for row in data_rows]\n",
    "                                        print(f\"Inserted '{specific_phrase}' for missing columns in ADNU continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                    else:\n",
    "                                        # For non-ADNU tables, assume missing \"type of upgrade\" column\n",
    "                                        missing_cols = len(expected_columns) - len(row)\n",
    "                                        #row += [specific_phrase] * missing_cols\n",
    "                                        data_rows = [ [specific_phrase]  for row in data_rows]\n",
    "                                        print(f\"Inserted '{specific_phrase}' for missing columns in non-ADNU continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                elif len(row) > len(expected_columns):\n",
    "                                    # Add new columns with default names\n",
    "                                    extra_cols = len(row) - len(expected_columns)\n",
    "                                    for i in range(extra_cols):\n",
    "                                        new_col_name = f\"column{len(expected_columns) + 1 + i}\"\n",
    "                                        expected_columns.append(new_col_name)\n",
    "                                        last_table[new_col_name] = pd.NA\n",
    "                                        print(f\"Added new column '{new_col_name}' for extra data in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                    row = row[:len(expected_columns)]\n",
    "\n",
    "                                row_dict = dict(zip(expected_columns, [clean_string_cell(cell) for cell in row]))\n",
    "\n",
    "                                # Handle 'type of upgrade' column\n",
    "                                if \"type of upgrade\" in row_dict and (pd.isna(row_dict[\"type of upgrade\"]) or row_dict[\"type of upgrade\"] == \"\"):\n",
    "                                    row_dict[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'type of upgrade' for a row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                                standardized_data.append(row_dict)\n",
    "\n",
    "                            try:\n",
    "                                df_continuation = pd.DataFrame(standardized_data, columns=expected_columns)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "\n",
    "\n",
    "                             # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                if \"type of upgrade\" in df_continuation.columns:\n",
    "                                    first_row = df_continuation.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                        df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                else:\n",
    "                                    # If \"type of upgrade\" column does not exist, add it\n",
    "                                    df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                            else:\n",
    "                                # General Handling for other tables\n",
    "                                if \"type of upgrade\" in df_continuation.columns:\n",
    "                                    first_row = df_continuation.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                        df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                else:\n",
    "                                    # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                    df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "\n",
    "\n",
    "                        # Handle ADNU-specific logic in continuation tables\n",
    "                        #if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                        #    print(\"Handling ADNU-specific logic in continuation table.\", file=log_file)\n",
    "                        #    if \"upgrade\" in df_continuation.columns and \"adnu\" not in df_continuation.columns:\n",
    "                        #        # Ensure 'upgrade' column is present\n",
    "                        #        if \"upgrade\" not in df_continuation.columns:\n",
    "                        #            df_continuation[\"upgrade\"] = specific_phrase\n",
    "                        #            print(\"Added 'upgrade' column to continuation ADNU table.\", file=log_file)\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\", file=log_file)\n",
    "                            df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                        print(f\"Appended continuation table data to the last extracted table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 7 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After processing all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 7 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 7 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 7 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 7-1 to 7-5.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*8[-.]([1-3])\\b\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def has_network_upgrade_type_column(pdf_path, log_file):\n",
    "    \"\"\"Checks if any table in the PDF has a column header 'Network Upgrade Type'.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables()\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    headers = clean_column_headers(tab[0])\n",
    "                    if \"network upgrade type\" in headers:\n",
    "                        print(f\"Found 'Network Upgrade Type' in PDF {pdf_path} on page {page_number}, table {table_index}.\", file=log_file)\n",
    "                        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking 'Network Upgrade Type' in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path, log_file):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' or 'Revision' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            print(f\"Extracted Text: {text}\", file= log_file)  # Debugging step\n",
    "            # Case-insensitive check for 'Addendum' or 'Revision'\n",
    "            text_lower = text.lower()\n",
    "            return \"addendum\" in text_lower or \"revision\" in text_lower\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    \"\"\"\n",
    "    Appends a suffix to duplicate headers to make them unique.\n",
    "\n",
    "    Args:\n",
    "        headers (list): List of column headers.\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique column headers.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "    SKIP_PROJECTS = {1860, 2003, 2006}\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "\n",
    "\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            # Skip the projects in the SKIP_PROJECTS set\n",
    "            if project_id in SKIP_PROJECTS:\n",
    "                print(f\"Skipping Project {project_id} (marked to skip)\", file=log_file)\n",
    "                continue\n",
    "\n",
    "         \n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"03_phase_2_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Separate PDFs into originals and addendums\n",
    "            list_pdfs = [pdf for pdf in os.listdir(project_path) if pdf.endswith(\".pdf\")]\n",
    "            originals = []\n",
    "            addendums = []\n",
    "            for pdf_name in list_pdfs:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                if is_addendum(pdf_path, log_file):\n",
    "                    addendums.append(pdf_name)\n",
    "                else:\n",
    "                    originals.append(pdf_name)\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Process original PDFs first\n",
    "            for pdf_name in originals:\n",
    "                \n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    # Still check if original has table7\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                original_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "\n",
    "                    if not has_table7:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    # Extract Table 7 and merge\n",
    "                    df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False)\n",
    "                    if not df.empty:\n",
    "                        core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        project_scraped = True\n",
    "                        total_pdfs_scraped += 1\n",
    "                        print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                    else:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Then process addendum PDFs\n",
    "            for pdf_name in addendums:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                addendum_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "\n",
    "                    if not has_table7:\n",
    "                        if original_has_table7.get(project_id, False):\n",
    "                            # Attempt to scrape alternative tables is no longer needed\n",
    "                            # According to the latest request, alternative table scraping is removed\n",
    "                            # Therefore, we skip addendum PDFs that do not have Table 7\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7 and original does not have Table 7)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7 and original does not have Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not is_add and not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    if is_add and base_data_extracted:\n",
    "                        # For addendums, use the extracted base data\n",
    "                        table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                        if table7_data.empty and original_has_table7.get(project_id, False):\n",
    "                            # Scrape alternative tables is removed, so skip if no data\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        if not table7_data.empty:\n",
    "                            # Merge base data with Table 7 data\n",
    "                            merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                            merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                            core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                            scraped_pdfs.append(pdf_name)\n",
    "                            scraped_projects.add(project_id)\n",
    "                            project_scraped = True\n",
    "                            total_pdfs_scraped += 1\n",
    "                            print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    # Optionally, print to ipynb\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "    # Rest of the code remains unchanged...\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nList of Style N PDFs (Skipped due to 'Network Upgrade Type'):\")\n",
    "    print(style_n_pdfs)\n",
    "\n",
    "    print(\"\\nTotal Number of Style N PDFs:\", len(style_n_pdfs))\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.applymap(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized and Totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 1832\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2153\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2157\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2161\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2162\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: CANU, Total Rows Present?: False\n",
      "Creating Total row for CANU\n",
      "\n",
      "Processing q_id: 2165\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2166\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: CANU, Total Rows Present?: False\n",
      "Creating Total row for CANU\n",
      "\n",
      "Processing q_id: 2167\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2172\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2173\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 2176\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: CANU, Total Rows Present?: False\n",
      "Creating Total row for CANU\n",
      "\n",
      "Processing q_id: 2177\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2178\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2180\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2181\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: OPNU, Total Rows Present?: False\n",
      "Creating Total row for OPNU\n",
      "\n",
      "Processing q_id: 2182\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2184\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2185\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2186\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2187\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: CANU, Total Rows Present?: False\n",
      "Creating Total row for CANU\n",
      "\n",
      "Processing q_id: 2188\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 2192\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "     q_id  cluster req_deliverability   latitude   longitude  capacity  \\\n",
      "0   1832       14               Full  33.119900 -117.098900       NaN   \n",
      "1   1832       14               Full  33.119900 -117.098900       NaN   \n",
      "2   2153       14               Full  32.692393 -117.135500       NaN   \n",
      "3   2153       14               Full  32.692393 -117.135500       NaN   \n",
      "4   2157       14               Full  33.126667 -117.326944       NaN   \n",
      "5   2157       14               Full  33.126667 -117.326944       NaN   \n",
      "6   2161       14               Full  32.883153 -113.639271       NaN   \n",
      "7   2162       14               Full  32.614000 -115.674800       NaN   \n",
      "8   2162       14               Full  32.614000 -115.674800       NaN   \n",
      "9   2162       14               Full  32.614000 -115.674800       NaN   \n",
      "10  2165       14               Full        NaN         NaN       NaN   \n",
      "11  2165       14               Full        NaN         NaN       NaN   \n",
      "12  2166       14               Full  32.711475 -115.141371       NaN   \n",
      "13  2166       14               Full  32.711475 -115.141371       NaN   \n",
      "14  2166       14               Full  32.711475 -115.141371       NaN   \n",
      "15  2167       14               Full  32.909657 -117.107210       NaN   \n",
      "16  2167       14               Full  32.909657 -117.107210       NaN   \n",
      "17  2172       14               Full  33.036500 -113.315500       NaN   \n",
      "18  2172       14               Full  33.036500 -113.315500       NaN   \n",
      "19  2173       14               Full  32.650220 -116.283700       NaN   \n",
      "20  2173       14               Full  32.650220 -116.283700       NaN   \n",
      "21  2173       14               Full  32.650220 -116.283700       NaN   \n",
      "22  2176       14               Full  32.728000 -115.714000       NaN   \n",
      "23  2176       14               Full  32.728000 -115.714000       NaN   \n",
      "24  2176       14               Full  32.728000 -115.714000       NaN   \n",
      "25  2177       14               Full  32.759906 -116.287100       NaN   \n",
      "26  2177       14               Full  32.759906 -116.287100       NaN   \n",
      "27  2178       14               Full  32.812000 -116.665000       NaN   \n",
      "28  2178       14               Full  32.812000 -116.665000       NaN   \n",
      "29  2180       14               Full  32.567000 -116.919000       NaN   \n",
      "30  2180       14               Full  32.567000 -116.919000       NaN   \n",
      "31  2181       14               None  32.620702 -116.169819       NaN   \n",
      "32  2181       14               None  32.620702 -116.169819       NaN   \n",
      "33  2181       14               None  32.620702 -116.169819       NaN   \n",
      "34  2181       14               None  32.620702 -116.169819       NaN   \n",
      "35  2182       14               None  32.958678 -117.106358       NaN   \n",
      "36  2182       14               None  32.958678 -117.106358       NaN   \n",
      "37  2184       14               Full  32.798318 -116.792606       NaN   \n",
      "38  2184       14               Full  32.798318 -116.792606       NaN   \n",
      "39  2185       14               Full  32.570161 -116.910957       NaN   \n",
      "40  2185       14               Full  32.570161 -116.910957       NaN   \n",
      "41  2186       14               Full  33.204724 -117.327382       NaN   \n",
      "42  2186       14               Full  33.204724 -117.327382       NaN   \n",
      "43  2187       14               Full  32.459417 -116.166258       NaN   \n",
      "44  2187       14               Full  32.459417 -116.166258       NaN   \n",
      "45  2187       14               Full  32.459417 -116.166258       NaN   \n",
      "46  2188       14               Full  32.672100 -116.977474       NaN   \n",
      "47  2188       14               Full  32.672100 -116.977474       NaN   \n",
      "48  2192       14               Full  32.619733 -116.948728       NaN   \n",
      "49  2192       14               Full  32.619733 -116.948728       NaN   \n",
      "\n",
      "                             point_of_interconnection type_of_upgrade upgrade  \\\n",
      "0   ESCO Substation 69 kV (Gen-Tie sharing with Go...    Total PTO_IF           \n",
      "1   ESCO Substation 69 kV (Gen-Tie sharing with Go...       Total RNU           \n",
      "2   Silvergate Substation 230 kV (Gen-tie sharing ...    Total PTO_IF           \n",
      "3   Silvergate Substation 230 kV (Gen-tie sharing ...       Total RNU           \n",
      "4                            Encina Substation 138 kV    Total PTO_IF           \n",
      "5                            Encina Substation 138 kV       Total RNU           \n",
      "6                     Hoodoo Wash - North Gila 500 kV       Total RNU           \n",
      "7   Imperial Valley Substation 230 kV (Gen-tie sha...    Total PTO_IF           \n",
      "8   Imperial Valley Substation 230 kV (Gen-tie sha...       Total RNU           \n",
      "9   Imperial Valley Substation 230 kV (Gen-tie sha...      Total CANU           \n",
      "10                       Ocean Ranch Substation 69 kV    Total PTO_IF           \n",
      "11                       Ocean Ranch Substation 69 kV       Total RNU           \n",
      "12           North Gila - Imperial Valley 500 kV Line    Total PTO_IF           \n",
      "13           North Gila - Imperial Valley 500 kV Line       Total RNU           \n",
      "14           North Gila - Imperial Valley 500 kV Line      Total CANU           \n",
      "15                           Scripps Substation 69 kV    Total PTO_IF           \n",
      "16                           Scripps Substation 69 kV       Total RNU           \n",
      "17          Hoodoo Wash  Hassayampa 500 kV Line (APS)    Total PTO_IF           \n",
      "18          Hoodoo Wash  Hassayampa 500 kV Line (APS)       Total RNU           \n",
      "19                   Boulevard East 138 kV Substation    Total PTO_IF           \n",
      "20                   Boulevard East 138 kV Substation       Total RNU           \n",
      "21                   Boulevard East 138 kV Substation      Total LDNU           \n",
      "22  Imperial Valley Substation 230 kV (Loop-in wit...    Total PTO_IF           \n",
      "23  Imperial Valley Substation 230 kV (Loop-in wit...       Total RNU           \n",
      "24  Imperial Valley Substation 230 kV (Loop-in wit...      Total CANU           \n",
      "25  Suncrest  Ocotillo 500 kV line (Lost Valley Sw...    Total PTO_IF           \n",
      "26  Suncrest  Ocotillo 500 kV line (Lost Valley Sw...       Total RNU           \n",
      "27                         Suncrest Substation 230 kV    Total PTO_IF           \n",
      "28                         Suncrest Substation 230 kV       Total RNU           \n",
      "29                        Otay Mesa Switchyard 230 kV    Total PTO_IF           \n",
      "30                        Otay Mesa Switchyard 230 kV       Total RNU           \n",
      "31                    Carrizo Gorge Switchyard 138 kV    Total PTO_IF           \n",
      "32                    Carrizo Gorge Switchyard 138 kV       Total RNU           \n",
      "33                    Carrizo Gorge Switchyard 138 kV      Total LDNU           \n",
      "34                    Carrizo Gorge Switchyard 138 kV      Total OPNU           \n",
      "35                        Chicarita Substation 138 kV    Total PTO_IF           \n",
      "36                        Chicarita Substation 138 kV       Total RNU           \n",
      "37    Loveland  Alpine 69kV line (Loop-in Switchyard)    Total PTO_IF           \n",
      "38    Loveland  Alpine 69kV line (Loop-in Switchyard)       Total RNU           \n",
      "39  Otay Mesa Substation 230 kV (Gen-tie share wit...    Total PTO_IF           \n",
      "40  Otay Mesa Substation 230 kV (Gen-tie share wit...       Total RNU           \n",
      "41                     San Luis Rey Substation 230 kV    Total PTO_IF           \n",
      "42                     San Luis Rey Substation 230 kV       Total RNU           \n",
      "43                      East County Substation 500 kV    Total PTO_IF           \n",
      "44                      East County Substation 500 kV       Total RNU           \n",
      "45                      East County Substation 500 kV      Total CANU           \n",
      "46                            Miguel Substation 69 kV    Total PTO_IF           \n",
      "47                            Miguel Substation 69 kV       Total RNU           \n",
      "48                        Salt Creek Substation 69 kV    Total PTO_IF           \n",
      "49                        Salt Creek Substation 69 kV       Total RNU           \n",
      "\n",
      "   description cost_allocation_factor  estimated_cost_x_1000  \\\n",
      "0                                                        0.0   \n",
      "1                                                      814.0   \n",
      "2                                                        0.0   \n",
      "3                                                    99114.0   \n",
      "4                                                        0.0   \n",
      "5                                                      564.0   \n",
      "6                                                    21980.0   \n",
      "7                                                        0.0   \n",
      "8                                                      933.0   \n",
      "9                                                     1800.0   \n",
      "10                                                       0.0   \n",
      "11                                                     180.0   \n",
      "12                                                       0.0   \n",
      "13                                                  192952.0   \n",
      "14                                                    3432.0   \n",
      "15                                                       0.0   \n",
      "16                                                     180.0   \n",
      "17                                                       0.0   \n",
      "18                                                   93726.0   \n",
      "19                                                       0.0   \n",
      "20                                                     197.0   \n",
      "21                                                    9953.0   \n",
      "22                                                       0.0   \n",
      "23                                                    1396.0   \n",
      "24                                                    5400.0   \n",
      "25                                                       0.0   \n",
      "26                                                     863.0   \n",
      "27                                                       0.0   \n",
      "28                                                   45087.0   \n",
      "29                                                       0.0   \n",
      "30                                                  128800.0   \n",
      "31                                                       0.0   \n",
      "32                                                    2415.0   \n",
      "33                                                    7464.0   \n",
      "34                                                       0.0   \n",
      "35                                                       0.0   \n",
      "36                                                     180.0   \n",
      "37                                                       0.0   \n",
      "38                                                   27259.0   \n",
      "39                                                       0.0   \n",
      "40                                                  105869.0   \n",
      "41                                                       0.0   \n",
      "42                                                   11016.0   \n",
      "43                                                       0.0   \n",
      "44                                                  139183.0   \n",
      "45                                                    1344.0   \n",
      "46                                                       0.0   \n",
      "47                                                     180.0   \n",
      "48                                                       0.0   \n",
      "49                                                     180.0   \n",
      "\n",
      "    escalated_cost_x_1000 total_estimated_cost_x_1000  \\\n",
      "0                  404.78                               \n",
      "1                  869.00                               \n",
      "2                    0.00                               \n",
      "3               113387.00                               \n",
      "4                    0.00                               \n",
      "5                  601.00                               \n",
      "6                23409.00                               \n",
      "7                    0.00                               \n",
      "8                 1012.00                               \n",
      "9                 1968.00                               \n",
      "10                   0.00                               \n",
      "11                 192.00                               \n",
      "12                   0.00                               \n",
      "13              220739.00                               \n",
      "14                3936.00                               \n",
      "15                   0.00                               \n",
      "16                 192.00                               \n",
      "17                   0.00                               \n",
      "18              107223.00                               \n",
      "19                   0.00                               \n",
      "20                 210.00                               \n",
      "21               10639.00                               \n",
      "22                   0.00                               \n",
      "23                1512.00                               \n",
      "24                5856.00                               \n",
      "25                 666.00                               \n",
      "26                 933.00                               \n",
      "27                   0.00                               \n",
      "28               51580.00                               \n",
      "29                   0.00                               \n",
      "30              147348.00                               \n",
      "31                   0.00                               \n",
      "32                2581.00                               \n",
      "33                7979.00                               \n",
      "34                   0.00                               \n",
      "35                   0.00                               \n",
      "36                 192.00                               \n",
      "37                   0.00                               \n",
      "38               31184.00                               \n",
      "39                1084.62                               \n",
      "40              121114.00                               \n",
      "41                   0.00                               \n",
      "42               11920.00                               \n",
      "43                   0.00                               \n",
      "44              159226.00                               \n",
      "45                1536.00                               \n",
      "46                   0.00                               \n",
      "47                 192.00                               \n",
      "48                   0.00                               \n",
      "49                 192.00                               \n",
      "\n",
      "    total_estimated_cost_x_1000_escalated estimated_time_to_construct  \\\n",
      "0                                  808.74                               \n",
      "1                                    0.00                               \n",
      "2                                 8283.77                               \n",
      "3                                    0.00                               \n",
      "4                                  373.45                               \n",
      "5                                    0.00                               \n",
      "6                                    0.00                               \n",
      "7                                    0.00                               \n",
      "8                                    0.00                               \n",
      "9                                    0.00                               \n",
      "10                                2117.15                               \n",
      "11                                   0.00                               \n",
      "12                                4443.95                               \n",
      "13                                   0.00                               \n",
      "14                                   0.00                               \n",
      "15                                2117.15                               \n",
      "16                                   0.00                               \n",
      "17                                4443.95                               \n",
      "18                                   0.00                               \n",
      "19                                5267.65                               \n",
      "20                                   0.00                               \n",
      "21                                   0.00                               \n",
      "22                                 378.13                               \n",
      "23                                   0.00                               \n",
      "24                                   0.00                               \n",
      "25                                1322.00                               \n",
      "26                                   0.00                               \n",
      "27                               13949.48                               \n",
      "28                                   0.00                               \n",
      "29                               13949.48                               \n",
      "30                                   0.00                               \n",
      "31                                1941.65                               \n",
      "32                                   0.00                               \n",
      "33                                   0.00                               \n",
      "34                                   0.00                               \n",
      "35                                2896.26                               \n",
      "36                                   0.00                               \n",
      "37                                1628.14                               \n",
      "38                                   0.00                               \n",
      "39                                2169.24                               \n",
      "40                                   0.00                               \n",
      "41                                5695.65                               \n",
      "42                                   0.00                               \n",
      "43                                9474.40                               \n",
      "44                                   0.00                               \n",
      "45                                   0.00                               \n",
      "46                                6432.17                               \n",
      "47                                   0.00                               \n",
      "48                                2867.59                               \n",
      "49                                   0.00                               \n",
      "\n",
      "   upgrade_classification item  adnu_cost_rate_x_1000_escalated  \n",
      "0                           no                                0  \n",
      "1                           no                                0  \n",
      "2                           no                                0  \n",
      "3                           no                                0  \n",
      "4                           no                                0  \n",
      "5                           no                                0  \n",
      "6                           no                                0  \n",
      "7                           no                                0  \n",
      "8                           no                                0  \n",
      "9                           no                                0  \n",
      "10                          no                                0  \n",
      "11                          no                                0  \n",
      "12                          no                                0  \n",
      "13                          no                                0  \n",
      "14                          no                                0  \n",
      "15                          no                                0  \n",
      "16                          no                                0  \n",
      "17                          no                                0  \n",
      "18                          no                                0  \n",
      "19                          no                                0  \n",
      "20                          no                                0  \n",
      "21                          no                                0  \n",
      "22                          no                                0  \n",
      "23                          no                                0  \n",
      "24                          no                                0  \n",
      "25                          no                                0  \n",
      "26                          no                                0  \n",
      "27                          no                                0  \n",
      "28                          no                                0  \n",
      "29                          no                                0  \n",
      "30                          no                                0  \n",
      "31                          no                                0  \n",
      "32                          no                                0  \n",
      "33                          no                                0  \n",
      "34                          no                                0  \n",
      "35                          no                                0  \n",
      "36                          no                                0  \n",
      "37                          no                                0  \n",
      "38                          no                                0  \n",
      "39                          no                                0  \n",
      "40                          no                                0  \n",
      "41                          no                                0  \n",
      "42                          no                                0  \n",
      "43                          no                                0  \n",
      "44                          no                                0  \n",
      "45                          no                                0  \n",
      "46                          no                                0  \n",
      "47                          no                                0  \n",
      "48                          no                                0  \n",
      "49                          no                                0  \n",
      "Itemized rows saved to 'costs_phase_2_cluster_14_style_Q_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_14_style_Q_total.csv'.\n",
      "['PTO_IF' 'RNU' 'CANU' 'LDNU' 'OPNU']\n",
      "[1832 2153 2157 2161 2162 2165 2166 2167 2172 2173 2176 2177 2178 2180\n",
      " 2181 2182 2184 2185 2186 2187 2188 2192]\n",
      "[14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1186774963.py:222: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1186774963.py:222: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1186774963.py:222: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1186774963.py:441: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('q_id', group_keys=False).apply(ffill_qid)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1186774963.py:632: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['9' '9' '36' '9' '12' '48' '36' '18' '9' '9' '9']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[mask, time_col] = times\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/ph2_rawdata_cluster14_style_Q_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "df['q_id'] = df['q_id'].replace(1170, 2185)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def move_dollar_values(df, src_col, dst_col):\n",
    "    \"\"\"\n",
    "    Moves pure dollar amounts (e.g. \"$3625.89\", \"$3,300.00\", \"$1,084.62\") \n",
    "    from src_col into dst_col (as numeric), and blanks out src_col.\n",
    "    \"\"\"\n",
    "    # 1) Column existence check\n",
    "    if src_col not in df.columns or dst_col not in df.columns:\n",
    "        raise KeyError(f\"Columns {src_col!r} or {dst_col!r} not found in DataFrame\")\n",
    "\n",
    "    # 2) Trim whitespace\n",
    "    s = df[src_col].astype(str).str.strip()\n",
    "\n",
    "    # 3) Match patterns like \"$123\", \"$1,234\", \"$12,345.67\"\n",
    "    pattern = r\"^\\$\\d{1,3}(?:,\\d{3})*(?:\\.\\d{1,2})?$\"\n",
    "    mask = s.str.match(pattern, na=False)\n",
    "\n",
    "    # 4) Move into dst_col (stripped of \"$\" and \",\", cast to float)\n",
    "    cleaned = (\n",
    "        s[mask]\n",
    "        .str.replace(r\"[\\$,]\", \"\", regex=True)\n",
    "        .astype(float)\n",
    "    )\n",
    "    df.loc[mask, dst_col] = cleaned\n",
    "\n",
    "    # 5) Blank out the original\n",
    "    df.loc[mask, src_col] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "df = df[~df.apply(lambda row: row.astype(str).isin([\"Allocated\",\"Cost\", \"(Escalated\",\"$k)\",\"(Note 1)\", \"Total NU\", \"Cost (2023\", \"Project\", \"Allocation\", \"Upgrade\", \"Total NU Cost (2023 $k)\",\n",
    "                                                    \"(2023 $k)\", \"Time to\", \"Construct\", \"(Months)\", \"(Notes 2\", \"and 4)\"\n",
    "                                                     ]).any(), axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'cost allocation factor' in df.columns:\n",
    "#    df['description'] = df['cost allocation factor'].apply(extract_non_numeric_text)\n",
    "#    df['cost_allocation_factor'] = df['cost allocation factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"project\",\n",
    "            \"upgrade\",\n",
    "            \n",
    "            \"unnamed_2\" ],\n",
    "\n",
    "        \"estimated_cost_x_1000\": [\n",
    "\n",
    "            \"estimated cost x 1000\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"allocated cost (2023k)\",\n",
    "            \"allocated_cost\",\n",
    "            \"assigned cost\",\n",
    "            \"allocated cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "            \"column13\",\n",
    "            \"column17\",\n",
    "            \"allocated cost (2023 k)\",\n",
    "            \"allocated cost (2023 k) (note 1)\",\n",
    "            \"allocated cost (2022 k)\",\n",
    "            \"allocated cost (2023k)\"\n",
    "            \n",
    "\n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"allocated cost (escalated k) (note 1)\"\n",
    "\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"assigned cost escalated\",\n",
    "             \"column19\",\n",
    "             \"column23\",\n",
    "             \"estimated cost x 1000 escalated (note 1)\",\n",
    "             \"allocated cost (escalated k) (note 1)\",\n",
    "             \n",
    "\n",
    "\n",
    "        ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "             \"column27\",\n",
    "             \"column25\",\n",
    "             \"column29\",\n",
    "             \"estimated time to construct (months) (notes 2 and 4)\",\n",
    "             \"estimated time to construct (months) (note 3)\",\n",
    "             \"estimated time to construct (months) (notes 2 and 5)\",\n",
    "             \"estimated time to construct (months) (note 2 and 5)\",\n",
    "\n",
    "\n",
    "        ],\n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\",\n",
    "            \"column5\",\n",
    "            \n",
    "            \"total nu cost (2023 k)\",\n",
    "            \"total nu cost (2022 k)\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\",\n",
    "            \"total nu cost (escalated k) (note 1)\",\n",
    "            \"total nu cost (escalate d k) (note 1)\"\n",
    "        ],\n",
    "       \n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "\n",
    "        \"adnu_cost_rate_escalated_x_1000\": [\n",
    "        \"cost rate escalated\",\n",
    "        ],\n",
    "\n",
    "        \"description\": [\"description\"],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "            \"project allocation\",\n",
    "            \"column7\",\n",
    "            \"column11\"\n",
    "\n",
    "        ],\n",
    "        \"estimated cost x 1000 escalated with itcca\": [\n",
    "            \"estimated cost x 1000 escalated with itcca\",\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "if 'upgrade' in df.columns:\n",
    "    df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    "#if 'cost_allocation_factor' in df.columns:\n",
    "#    df['description'] = df['cost_allocation_factor'].apply(extract_non_numeric_text)\n",
    "#    df['cost_allocation_factor'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values    \n",
    "\n",
    "\n",
    "# Clear cost_allocation_factor for rows where upgrade is \"Total\" (case-insensitive)\n",
    "df.loc[df['upgrade'].str.lower() == 'total', 'cost_allocation_factor'] = None  # or use \"\" if you prefer an empty string\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "#df['description'] = df['description'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    "\n",
    "#df.drop(['unnamed_3', 'unnamed_15', 'unnamed_18', 'unnamed_16', 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "df.drop(['total nu'\t,'cost', 'estimated', 'allocated', '2154','estimated cost x', \n",
    "           'ks remy', 'estimated time to', '2161', 'alisa solar energy complex',\n",
    "         'column21', 'column3', 'column9',\t'column15', 'column33',\t'column31',\t'cost allocation', \"allocated cost (escalated k) with itcca (note 2)\", \"estimated cost x 1000 escalated with itcca (note 2)\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade']= df['type_of_upgrade'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = move_dollar_values(df, 'description','escalated_cost_x_1000')\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def move_description_to_upgrade(df):\n",
    "    \"\"\"Moves description to upgrade if type_of_upgrade is 'PTO' and upgrade is empty.\"\"\"\n",
    "    \n",
    "    # Ensure columns are strings and replace NaNs with empty strings for processing\n",
    "    df['upgrade'] = df['upgrade'].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
    "    df['description'] = df['description'].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
    "\n",
    "    # Debug: Print before update\n",
    "    #print(\"Before update (only PTO rows):\")\n",
    "    #print(df[df['type_of_upgrade'] == 'PTO'][['type_of_upgrade', 'upgrade', 'description']])\n",
    "\n",
    "    # Apply row-wise transformation\n",
    "    def move_if_empty(row):\n",
    "        if row['type_of_upgrade'] == 'PTO' and row['upgrade'].strip() == \"\" and row['description'].strip() != \"\":\n",
    "            row['upgrade'] = row['description']  # Move description to upgrade\n",
    "            row['description'] = None # Clear description\n",
    "        return row\n",
    "\n",
    "    df = df.apply(move_if_empty, axis=1)\n",
    "\n",
    "    # Debug: Print after update\n",
    "    #print(\"\\nAfter update (only PTO rows):\")\n",
    "    #print(df[df['type_of_upgrade'] == 'PTO'][['type_of_upgrade', 'upgrade', 'description']])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply function\n",
    "#df = move_description_to_upgrade(df)\n",
    "\n",
    "\n",
    "# Your list of upgrade phrases\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    "def simple_move_upgrades(df, desc_col='description', upg_col='upgrade'):\n",
    "    # Ensure the upgrade column exists\n",
    "    df[upg_col] = df.get(upg_col, None)\n",
    "\n",
    "    # Iterate row by row\n",
    "    for i, desc in df[desc_col].fillna(\"\").items():\n",
    "        for ph in upgrade_phrases:\n",
    "            if ph in desc:\n",
    "                # Set the upgrade column to the found phrase\n",
    "                df.at[i, upg_col] = ph\n",
    "                # Remove the phrase from the description\n",
    "                df.at[i, desc_col] = desc.replace(ph, \"\").strip()\n",
    "                break  # stop after the first match\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = simple_move_upgrades(df)\n",
    "\n",
    "\n",
    "df = df[df['upgrade'].fillna('').astype(str).str.strip() != '']\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "df['tot'] = df.apply(\n",
    "    lambda row: 'yes' if (\n",
    "        (pd.notna(row.get('upgrade')) and 'Total' in str(row['upgrade']))  \n",
    "    ) else 'no',\n",
    "    axis=1\n",
    ") \n",
    "\n",
    "# Now extract ONLY \"Total\" rows with a foolproof match\n",
    "total_rows_df = df[df['tot'] == 'yes']\n",
    "\n",
    "total_rows_df = total_rows_df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "total_rows_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_total_network.csv', index=False) \n",
    "df = df[df['upgrade'].str.strip().str.lower() != 'total']\n",
    "df.drop('tot', axis=1, inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def classify_and_fill_upgrades(df):\n",
    "    \"\"\"Moves upgrade phrases to upgrade_classification, forward-fills (including type_of_upgrade), and removes redundant rows.\"\"\"\n",
    "\n",
    "    # 1) Normalize\n",
    "    df['upgrade'] = df['upgrade'].astype(str).fillna(\"\")\n",
    "    df['upgrade_classification'] = None\n",
    "\n",
    "    # 2) Mark the phrase‐rows\n",
    "    df.loc[df['upgrade'].isin(upgrade_phrases), 'upgrade_classification'] = df['upgrade']\n",
    "\n",
    "    # 3) Single pass per‐q_id ffill\n",
    "    def ffill_qid(group):\n",
    "        current = None\n",
    "        for idx in group.index:\n",
    "            up = group.at[idx, 'upgrade'].strip()\n",
    "\n",
    "            if up in upgrade_phrases:\n",
    "                current = up\n",
    "            elif up.lower() == \"total\":\n",
    "                current = None\n",
    "\n",
    "            # fill classification\n",
    "            group.at[idx, 'upgrade_classification'] = current\n",
    "\n",
    "            # forward‐fill type_of_upgrade whenever it’s blank\n",
    "            if not group.at[idx, 'type_of_upgrade'] or pd.isna(group.at[idx, 'type_of_upgrade']):\n",
    "                group.at[idx, 'type_of_upgrade'] = current or \"\"\n",
    "\n",
    "        return group\n",
    "\n",
    "    df = df.groupby('q_id', group_keys=False).apply(ffill_qid)\n",
    "\n",
    "    # 4) drop the original phrase‐marker rows\n",
    "    df = df[~df['upgrade'].isin(upgrade_phrases)]\n",
    "\n",
    "    # 5) finally, coerce the “master” classification back into type_of_upgrade\n",
    "    df.loc[df['upgrade_classification'] == 'LDNU',                     'type_of_upgrade'] = 'LDNU'\n",
    "    df.loc[df['upgrade_classification'].isin(['CANU-GR','CANU']),      'type_of_upgrade'] = 'CANU'\n",
    "    df.loc[df['upgrade_classification'] == 'PNU',                      'type_of_upgrade'] = 'PNU'\n",
    "    df.loc[df['upgrade_classification'].isin(['IRNU','GRNU','IRNU-A']), 'type_of_upgrade'] = 'RNU'\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df = classify_and_fill_upgrades(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/ph2_cluster_14_style_Q.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"IRNU\": 'RNU',\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    " \"Total IRNU\": 'RNU',\n",
    "}\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)  \n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by removing:\n",
    "      - Dollar signs ($)\n",
    "      - Asterisks (*)\n",
    "      - Any parenthesized content \"(...)\" (Notes or otherwise)\n",
    "      - Commas\n",
    "    Then converts to a numeric float, returning pd.NA on failure.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # 1) Remove $ and *\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        # 2) Remove anything in parentheses, e.g. \"(Note 6 and 7)\"\n",
    "        value = re.sub(r'\\([^)]*\\)', '', value)\n",
    "        # 3) Remove commas and trim spaces\n",
    "        value = value.replace(',', '').strip()\n",
    "    # 4) Convert to numeric, coercing invalid to NaN\n",
    "    return pd.to_numeric(value, errors='coerce')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000', 'adnu_cost_rate_x_1000_escalated']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    " \n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def move_time_numbers(df, description_col='description', time_col='estimated_time_to_construct'):\n",
    "    \"\"\"\n",
    "    Moves standalone integer values (e.g., 35, 9, 12) from `description_col`\n",
    "    into `time_col`, and blanks out the original description cell.\n",
    "    Only moves cells that are purely an integer, whether stored as a number\n",
    "    or as a digit‐only string.\n",
    "    \"\"\"\n",
    "    # 1) sanity check\n",
    "    if description_col not in df.columns or time_col not in df.columns:\n",
    "        raise KeyError(f\"Columns {description_col!r} or {time_col!r} not found in DataFrame\")\n",
    "    \n",
    "    # 2) helper: detect a “pure” integer cell\n",
    "    def is_pure_int(val):\n",
    "        if pd.isna(val):\n",
    "            return False\n",
    "        # if it’s actually numeric, check it’s an integer\n",
    "        if isinstance(val, (int, float)):\n",
    "            return float(val).is_integer()\n",
    "        # if it’s a string, strip and see if it’s all digits\n",
    "        if isinstance(val, str):\n",
    "            s = val.strip()\n",
    "            return bool(re.fullmatch(r\"\\d+\", s))\n",
    "        return False\n",
    "    \n",
    "    # 3) build mask\n",
    "    mask = df[description_col].apply(is_pure_int)\n",
    "    \n",
    "    # 4) Move values\n",
    "    #   * cast numeric → int → str so our time_col is consistently string/int\n",
    "    times = df.loc[mask, description_col].apply(lambda v: str(int(float(v))) if not pd.isna(v) else \"\")\n",
    "    df.loc[mask, time_col] = times\n",
    "    \n",
    "    # 5) blank out description\n",
    "    df.loc[mask, description_col] = \"\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = move_time_numbers(df, 'description', 'estimated_time_to_construct')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'adnu_cost_rate_x_1000_escalated']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ((group['type_of_upgrade'] == f\"Total {upgrade}\") & (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['type_of_upgrade'] == f\"Total {upgrade}\") & (group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_14_style_Q_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_14_style_Q_total.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n",
    "\n",
    "\n",
    "#df.to_csv('Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 14/03_raw/rawdata_cluster14_style_Q.csv')\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 2177\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: OPNU, Total Rows Present?: False\n",
      "Creating Total row for OPNU\n",
      "\n",
      "Processing q_id: 2181\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: OPNU, Total Rows Present?: False\n",
      "Creating Total row for OPNU\n",
      "\n",
      "Processing q_id: 2188\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "    q_id  cluster req_deliverability   latitude   longitude  capacity  \\\n",
      "0  2177       14               Full  32.759906 -116.287100       NaN   \n",
      "1  2177       14               Full  32.759906 -116.287100       NaN   \n",
      "2  2177       14               Full  32.759906 -116.287100       NaN   \n",
      "3  2181       14               None  32.620702 -116.169819       NaN   \n",
      "4  2181       14               None  32.620702 -116.169819       NaN   \n",
      "5  2181       14               None  32.620702 -116.169819       NaN   \n",
      "6  2181       14               None  32.620702 -116.169819       NaN   \n",
      "7  2188       14               Full  32.672100 -116.977474       NaN   \n",
      "8  2188       14               Full  32.672100 -116.977474       NaN   \n",
      "\n",
      "                            point_of_interconnection type_of_upgrade upgrade  \\\n",
      "0  Suncrest  Ocotillo 500 kV line (Lost Valley Sw...    Total PTO_IF           \n",
      "1  Suncrest  Ocotillo 500 kV line (Lost Valley Sw...       Total RNU           \n",
      "2  Suncrest  Ocotillo 500 kV line (Lost Valley Sw...      Total OPNU           \n",
      "3                    Carrizo Gorge Switchyard 138 kV    Total PTO_IF           \n",
      "4                    Carrizo Gorge Switchyard 138 kV       Total RNU           \n",
      "5                    Carrizo Gorge Switchyard 138 kV      Total LDNU           \n",
      "6                    Carrizo Gorge Switchyard 138 kV      Total OPNU           \n",
      "7                            Miguel Substation 69 kV    Total PTO_IF           \n",
      "8                            Miguel Substation 69 kV       Total RNU           \n",
      "\n",
      "  description cost_allocation_factor  estimated_cost_x_1000  \\\n",
      "0                                                       0.0   \n",
      "1                                                     863.0   \n",
      "2                                                   81339.0   \n",
      "3                                                       0.0   \n",
      "4                                                    2415.0   \n",
      "5                                                    7464.0   \n",
      "6                                                       0.0   \n",
      "7                                                       0.0   \n",
      "8                                                     180.0   \n",
      "\n",
      "   escalated_cost_x_1000 total_estimated_cost_x_1000  \\\n",
      "0                 1332.0                               \n",
      "1                  950.0                               \n",
      "2                90886.0                               \n",
      "3                    0.0                               \n",
      "4                 2581.0                               \n",
      "5                 7979.0                               \n",
      "6                    0.0                               \n",
      "7                    0.0                               \n",
      "8                  192.0                               \n",
      "\n",
      "   total_estimated_cost_x_1000_escalated estimated_time_to_construct  \\\n",
      "0                                   0.00                               \n",
      "1                                   0.00                               \n",
      "2                                   0.00                               \n",
      "3                                1941.65                               \n",
      "4                                   0.00                               \n",
      "5                                   0.00                               \n",
      "6                                   0.00                               \n",
      "7                                6432.17                               \n",
      "8                                   0.00                               \n",
      "\n",
      "  upgrade_classification item  adnu_cost_rate_x_1000_escalated  \n",
      "0                          no                                0  \n",
      "1                          no                                0  \n",
      "2                          no                                0  \n",
      "3                          no                                0  \n",
      "4                          no                                0  \n",
      "5                          no                                0  \n",
      "6                          no                                0  \n",
      "7                          no                                0  \n",
      "8                          no                                0  \n",
      "Itemized rows saved to 'costs_phase_2_cluster_14_style_Q_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_14_style_Q_total.csv'.\n",
      "['PTO_IF' 'RNU' 'OPNU' 'LDNU']\n",
      "[2177 2181 2188]\n",
      "[14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/763911015.py:196: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/763911015.py:448: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('q_id', group_keys=False).apply(ffill_qid)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/ph2_rawdata_cluster14_style_Q_addendums.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "df['q_id'] = df['q_id'].replace(1170, 2185)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "\n",
    "def move_dollar_values(df, src_col, dst_col):\n",
    "    \"\"\"\n",
    "    For each row in df:\n",
    "      – If df[src_col] is exactly a dollar amount (e.g. \"$3625.89\", \"$3300\"), \n",
    "        move that string into df[dst_col] and set df[src_col] = NaN.\n",
    "      – Otherwise, leave both cols alone.\n",
    "    \"\"\"\n",
    "    # Ensure both columns exist\n",
    "    if src_col not in df or dst_col not in df:\n",
    "        raise KeyError(f\"Columns {src_col!r} or {dst_col!r} not in DataFrame\")\n",
    "\n",
    "    # Define mask of “pure” dollar amounts\n",
    "    is_dollar = df[src_col].astype(str).str.match(r\"^\\$\\d+(?:\\.\\d{1,2})?$\")\n",
    "\n",
    "    # Move\n",
    "    df.loc[is_dollar, dst_col] = df.loc[is_dollar, src_col]\n",
    "    df.loc[is_dollar, src_col] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#df = move_dollar_values(df, \n",
    "#                        src_col='type of upgrade', \n",
    "#                        dst_col='allocated cost')\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "df = df[~df.apply(lambda row: row.astype(str).isin([\"Allocated\",\"Cost\", \"(Escalated\",\"$k)\",\"(Note 1)\", \"Total NU\", \"Cost (2023\", \"Project\", \"Allocation\", \" Upgrade\", \"Total NU Cost (2023 $k)\" ]).any(), axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'cost allocation factor' in df.columns:\n",
    "#    df['description'] = df['cost allocation factor'].apply(extract_non_numeric_text)\n",
    "#    df['cost_allocation_factor'] = df['cost allocation factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "            \"project\",\n",
    "            \"unnamed_2\" ],\n",
    "\n",
    "            \"estimated_cost_x_1000\": [\n",
    "\n",
    "            \"estimated cost x 1000\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"allocated_cost\",\n",
    "            \"assigned cost\",\n",
    "            \"allocated cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "            \"column13\",\n",
    "            \"column17\",\n",
    "            \"allocated estimated cost\",\n",
    "            \"allocated cost (2023 k)\",\n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"assigned cost escalated\",\n",
    "             \"column19\",\n",
    "             \"column23\",\n",
    "             \"estimated cost\",\n",
    "             \"estimated cost (escalated k) (note 1)\",\n",
    "             \"allocated estimated cost (escalated k) (note 1)\",\n",
    "             \"allocated cost (escalated k) (note 1)\",\n",
    "\n",
    "\n",
    "        ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "             \"column27\",\n",
    "             \"column25\",\n",
    "             \"column29\",\n",
    "             \"estimated time to construct (months) (note 3)\",\n",
    "\n",
    "\n",
    "        ],\n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\",\n",
    "            \"column5\",\n",
    "            \"total nu cost (2023 k)\",\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\",\n",
    "            \"total nu cost (escalated k) (note 1)\",\n",
    "        ],\n",
    "       \n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "\n",
    "        \"adnu_cost_rate_escalated_x_1000\": [\n",
    "        \"cost rate escalated\",\n",
    "        ],\n",
    "\n",
    "        \"description\": [\"description\"],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "            \"project allocation\",\n",
    "            \"column7\",\n",
    "            \"column11\"\n",
    "\n",
    "        ],\n",
    "        \"estimated cost x 1000 escalated with itcca\": [\n",
    "            \"estimated cost x 1000 escalated with itcca\",\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "if 'upgrade' in df.columns:\n",
    "    df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    "#if 'cost_allocation_factor' in df.columns:\n",
    "#    df['description'] = df['cost_allocation_factor'].apply(extract_non_numeric_text)\n",
    "#    df['cost_allocation_factor'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values    \n",
    "\n",
    "\n",
    "# Clear cost_allocation_factor for rows where upgrade is \"Total\" (case-insensitive)\n",
    "df.loc[df['upgrade'].str.lower() == 'total', 'cost_allocation_factor'] = None  # or use \"\" if you prefer an empty string\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    "\n",
    "#df.drop(['unnamed_3', 'unnamed_15', 'unnamed_18', 'unnamed_16', 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "cols = [\n",
    "    'total nu', 'cost', 'estimated', 'allocated', 'allocated cost  with itcca',\n",
    "    '2154', 'estimated cost x', 'estimated cost x 1000 escalated with itcca',\n",
    "    'ks remy', 'estimated time to', '2161', 'alisa solar energy complex',\n",
    "    'column21', 'column3', 'column9', 'column15', 'column33', 'column31',\n",
    "    'cost allocation'\n",
    "]\n",
    "\n",
    "df.drop(columns=cols, errors='ignore', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade']= df['type_of_upgrade'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by removing:\n",
    "      - Dollar signs: $\n",
    "      - Asterisks: *\n",
    "      - Any parenthetical notes like \"(Note 2)\" or \"(Notes 6 and 7)\"\n",
    "      - Any other parenthesized content\n",
    "    Then attempts to convert to a numeric (float). Returns pd.NA if invalid.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "    \n",
    "        # 2) remove any \"(Note ...)\" patterns\n",
    "        value = re.sub(r'\\(Note[s]?\\s*\\d+(?:\\s*and\\s*\\d+)*\\)', '', value)\n",
    "        # 3) remove any other parentheses and their contents\n",
    "        value = re.sub(r'\\(.*?\\)', '', value)\n",
    "        # 4) strip leading/trailing whitespace\n",
    "        value = value.strip()\n",
    "\n",
    "    try:\n",
    "        # Try numeric conversion\n",
    "        return value\n",
    "    except (ValueError, TypeError):\n",
    "        return pd.NA\n",
    "    \n",
    "df['description'] = df['description'].apply(clean)  \n",
    "\n",
    "df['cost_allocation_factor'] = df['cost_allocation_factor'].apply(clean)\n",
    "\n",
    "df['description'] = df['description'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def move_description_to_upgrade(df):\n",
    "    \"\"\"Moves description to upgrade if type_of_upgrade is 'PTO' and upgrade is empty.\"\"\"\n",
    "    \n",
    "    # Ensure columns are strings and replace NaNs with empty strings for processing\n",
    "    df['upgrade'] = df['upgrade'].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
    "    df['description'] = df['description'].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
    "\n",
    "    # Debug: Print before update\n",
    "    #print(\"Before update (only PTO rows):\")\n",
    "    #print(df[df['type_of_upgrade'] == 'PTO'][['type_of_upgrade', 'upgrade', 'description']])\n",
    "\n",
    "    # Apply row-wise transformation\n",
    "    def move_if_empty(row):\n",
    "        if row['type_of_upgrade'] == 'PTO' and row['upgrade'].strip() == \"\" and row['description'].strip() != \"\":\n",
    "            row['upgrade'] = row['description']  # Move description to upgrade\n",
    "            row['description'] = None # Clear description\n",
    "        return row\n",
    "\n",
    "    df = df.apply(move_if_empty, axis=1)\n",
    "\n",
    "    # Debug: Print after update\n",
    "    #print(\"\\nAfter update (only PTO rows):\")\n",
    "    #print(df[df['type_of_upgrade'] == 'PTO'][['type_of_upgrade', 'upgrade', 'description']])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply function\n",
    "#df = move_description_to_upgrade(df)\n",
    "\n",
    "\n",
    "# Your list of upgrade phrases\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    "def simple_move_upgrades(df, desc_col='description', upg_col='upgrade'):\n",
    "    # Ensure the upgrade column exists\n",
    "    df[upg_col] = df.get(upg_col, None)\n",
    "\n",
    "    # Iterate row by row\n",
    "    for i, desc in df[desc_col].fillna(\"\").items():\n",
    "        for ph in upgrade_phrases:\n",
    "            if ph in desc:\n",
    "                # Set the upgrade column to the found phrase\n",
    "                df.at[i, upg_col] = ph\n",
    "                # Remove the phrase from the description\n",
    "                df.at[i, desc_col] = desc.replace(ph, \"\").strip()\n",
    "                break  # stop after the first match\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = simple_move_upgrades(df)\n",
    "\n",
    "\n",
    "df = df[df['upgrade'].fillna('').astype(str).str.strip() != '']\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "df['tot'] = df.apply(\n",
    "    lambda row: 'yes' if (\n",
    "        (pd.notna(row.get('upgrade')) and 'Total' in str(row['upgrade']))  \n",
    "    ) else 'no',\n",
    "    axis=1\n",
    ") \n",
    "\n",
    "# Now extract ONLY \"Total\" rows with a foolproof match\n",
    "total_rows_df = df[df['tot'] == 'yes']\n",
    "\n",
    "total_rows_df = total_rows_df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "total_rows_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_total_network_addendum.csv', index=False) \n",
    "df = df[df['upgrade'].str.strip().str.lower() != 'total']\n",
    "df.drop('tot', axis=1, inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def classify_and_fill_upgrades(df):\n",
    "    \"\"\"Moves upgrade phrases to upgrade_classification, forward-fills (including type_of_upgrade), and removes redundant rows.\"\"\"\n",
    "\n",
    "    # 1) Normalize\n",
    "    df['upgrade'] = df['upgrade'].astype(str).fillna(\"\")\n",
    "    df['upgrade_classification'] = None\n",
    "\n",
    "    # 2) Mark the phrase‐rows\n",
    "    df.loc[df['upgrade'].isin(upgrade_phrases), 'upgrade_classification'] = df['upgrade']\n",
    "\n",
    "    # 3) Single pass per‐q_id ffill\n",
    "    def ffill_qid(group):\n",
    "        current = None\n",
    "        for idx in group.index:\n",
    "            up = group.at[idx, 'upgrade'].strip()\n",
    "\n",
    "            if up in upgrade_phrases:\n",
    "                current = up\n",
    "            elif up.lower() == \"total\":\n",
    "                current = None\n",
    "\n",
    "            # fill classification\n",
    "            group.at[idx, 'upgrade_classification'] = current\n",
    "\n",
    "            # forward‐fill type_of_upgrade whenever it’s blank\n",
    "            if not group.at[idx, 'type_of_upgrade'] or pd.isna(group.at[idx, 'type_of_upgrade']):\n",
    "                group.at[idx, 'type_of_upgrade'] = current or \"\"\n",
    "\n",
    "        return group\n",
    "\n",
    "    df = df.groupby('q_id', group_keys=False).apply(ffill_qid)\n",
    "\n",
    "    # 4) drop the original phrase‐marker rows\n",
    "    df = df[~df['upgrade'].isin(upgrade_phrases)]\n",
    "\n",
    "    # 5) finally, coerce the “master” classification back into type_of_upgrade\n",
    "    df.loc[df['upgrade_classification'] == 'LDNU',                     'type_of_upgrade'] = 'LDNU'\n",
    "    df.loc[df['upgrade_classification'].isin(['CANU-GR','CANU']),      'type_of_upgrade'] = 'CANU'\n",
    "    df.loc[df['upgrade_classification'] == 'PNU',                      'type_of_upgrade'] = 'PNU'\n",
    "    df.loc[df['upgrade_classification'].isin(['IRNU','GRNU','IRNU-A']), 'type_of_upgrade'] = 'RNU'\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df = classify_and_fill_upgrades(df)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/ph2_cluster_14_style_Q_addendum.csv', index=False)  \n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"IRNU\": 'RNU',\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    " \"Total IRNU\": 'RNU',\n",
    "}\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)  \n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/cluster_14_style_Q_addendum.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by removing:\n",
    "      - Dollar signs ($)\n",
    "      - Asterisks (*)\n",
    "      - Any parenthesized content \"(...)\" (Notes or otherwise)\n",
    "      - Commas\n",
    "    Then converts to a numeric float, returning pd.NA on failure.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # 1) Remove $ and *\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        # 2) Remove anything in parentheses, e.g. \"(Note 6 and 7)\"\n",
    "        value = re.sub(r'\\([^)]*\\)', '', value)\n",
    "        # 3) Remove commas and trim spaces\n",
    "        value = value.replace(',', '').strip()\n",
    "    # 4) Convert to numeric, coercing invalid to NaN\n",
    "    return pd.to_numeric(value, errors='coerce')    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000', 'adnu_cost_rate_x_1000_escalated']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'adnu_cost_rate_x_1000_escalated']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ((group['type_of_upgrade'] == f\"Total {upgrade}\") & (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['type_of_upgrade'] == f\"Total {upgrade}\") & (group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_itemized_addendums.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_total_addendums.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_14_style_Q_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_14_style_Q_total.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n",
    "\n",
    "\n",
    "#df.to_csv('Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 14/03_raw/rawdata_cluster14_style_Q.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge addendum and orginal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing itemized: q_id=2177, type_of_upgrade=PTO_IF\n",
      "Length of addendum_rows: 1\n",
      "Length of original_rows: 1\n",
      "Processing itemized: q_id=2177, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 3\n",
      "Length of original_rows: 3\n",
      "Processing itemized: q_id=2177, type_of_upgrade=OPNU\n",
      "Length of addendum_rows: 3\n",
      "Length of original_rows: 0\n",
      "Processing itemized: q_id=2181, type_of_upgrade=PTO_IF\n",
      "Length of addendum_rows: 1\n",
      "Length of original_rows: 1\n",
      "Processing itemized: q_id=2181, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 3\n",
      "Length of original_rows: 3\n",
      "Processing itemized: q_id=2181, type_of_upgrade=OPNU\n",
      "Length of addendum_rows: 2\n",
      "Length of original_rows: 2\n",
      "Processing itemized: q_id=2181, type_of_upgrade=LDNU\n",
      "Length of addendum_rows: 1\n",
      "Length of original_rows: 1\n",
      "Processing itemized: q_id=2188, type_of_upgrade=PTO_IF\n",
      "Length of addendum_rows: 1\n",
      "Length of original_rows: 1\n",
      "Processing itemized: q_id=2188, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 1\n",
      "Length of original_rows: 1\n",
      "Processing total: q_id=2177, type_of_upgrade=PTO_IF\n",
      "Processing total: q_id=2177, type_of_upgrade=RNU\n",
      "Processing total: q_id=2177, type_of_upgrade=OPNU\n",
      "Processing total: q_id=2181, type_of_upgrade=PTO_IF\n",
      "Processing total: q_id=2181, type_of_upgrade=RNU\n",
      "Processing total: q_id=2181, type_of_upgrade=OPNU\n",
      "Processing total: q_id=2181, type_of_upgrade=LDNU\n",
      "Processing total: q_id=2188, type_of_upgrade=PTO_IF\n",
      "Processing total: q_id=2188, type_of_upgrade=RNU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:192: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  updated_itemized[col] = updated_itemized[col].replace('', np.nan)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:193: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  updated_total[col] = updated_total[col].replace('', np.nan)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:203: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:208: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:203: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:208: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:203: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_98286/1603589718.py:208: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Load a CSV file and ensure specific columns are treated as character, others as numeric.\n",
    "    \"\"\"\n",
    " # Get columns available in the dataset\n",
    "    available_columns = pd.read_csv(file_path, nrows=0).columns\n",
    "    \n",
    "    # Restrict to char_columns that are present in the dataset\n",
    "    char_columns_in_dataset = [col for col in char_columns if col in available_columns]\n",
    "    \n",
    "    # Load the dataset, treating char_columns_in_dataset as strings\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype={col: str for col in char_columns_in_dataset},\n",
    "        na_values=[],  # Disable automatic NaN interpretation\n",
    "        keep_default_na=False  # Prevent treating \"None\" as NaN\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert all other columns to numeric\n",
    "    #for col in df.columns:\n",
    "    #    if col not in char_columns:\n",
    "    #        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_data(df, file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Save a dataframe to a CSV file, ensuring specific columns are treated as character.\n",
    "    \"\"\"\n",
    "    for col in char_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def merge_with_addendums(itemized, itemized_addendums, total, total_addendums):\n",
    "    # Add an 'original' column to the datasets\n",
    "    itemized['original'] = \"yes\"\n",
    "    total['original'] = \"yes\"\n",
    "    \n",
    "    # Preserve the original row order\n",
    "    itemized['row_order'] = pd.to_numeric(itemized.index, errors=\"coerce\")\n",
    "    total['row_order'] = pd.to_numeric(total.index, errors=\"coerce\")\n",
    "    \n",
    "    # Ensure q_id is numeric for comparison\n",
    "    itemized['q_id'] = pd.to_numeric(itemized['q_id'], errors=\"coerce\")\n",
    "    itemized_addendums['q_id'] = pd.to_numeric(itemized_addendums['q_id'], errors=\"coerce\")\n",
    "    total['q_id'] = pd.to_numeric(total['q_id'], errors=\"coerce\")\n",
    "    total_addendums['q_id'] = pd.to_numeric(total_addendums['q_id'], errors=\"coerce\")\n",
    "    \n",
    "    # Columns for conditional replacement\n",
    "    conditional_columns = [\"req_deliverability\", \"latitude\", \"longitude\", \"capacity\", \"point_of_interconnection\"]\n",
    "    \n",
    "    # --- Process itemized data (unchanged) ---\n",
    "    updated_itemized_rows = []\n",
    "    for q_id in itemized_addendums['q_id'].unique():\n",
    "        for upgrade_type in itemized_addendums['type_of_upgrade'].unique():\n",
    "            addendum_rows = itemized_addendums[\n",
    "                (itemized_addendums['q_id'] == q_id) &\n",
    "                (itemized_addendums['type_of_upgrade'] == upgrade_type)\n",
    "            ]\n",
    "            if not addendum_rows.empty:\n",
    "                mask = (itemized['q_id'] == q_id) & (itemized['type_of_upgrade'] == upgrade_type)\n",
    "                original_rows = itemized[mask]\n",
    "                print(f\"Processing itemized: q_id={q_id}, type_of_upgrade={upgrade_type}\")\n",
    "                print(f\"Length of addendum_rows: {len(addendum_rows)}\")\n",
    "                print(f\"Length of original_rows: {len(original_rows)}\")\n",
    "                # For specified columns, replace if addendum values are non-empty\n",
    "                for col in conditional_columns:\n",
    "                    if col in addendum_rows.columns and col in original_rows.columns:\n",
    "                        addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
    "                        addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
    "                        addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
    "                # Align lengths\n",
    "                original_rows = original_rows.reset_index(drop=True)\n",
    "                addendum_rows = addendum_rows.reset_index(drop=True)\n",
    "                if len(addendum_rows) > len(original_rows):\n",
    "                    extra_rows = pd.DataFrame({col: pd.NA for col in original_rows.columns},\n",
    "                                              index=range(len(addendum_rows) - len(original_rows)))\n",
    "                    original_rows = pd.concat([original_rows, extra_rows], ignore_index=True)\n",
    "                elif len(addendum_rows) < len(original_rows):\n",
    "                    original_rows = original_rows.iloc[:len(addendum_rows)].reset_index(drop=True)\n",
    "                itemized.loc[mask, 'original'] = \"no\"\n",
    "                updated_itemized_rows.append(\n",
    "                    addendum_rows.assign(original=\"no\", row_order=original_rows['row_order'].values[:len(addendum_rows)])\n",
    "                )\n",
    "                itemized = itemized[~mask]\n",
    "    if updated_itemized_rows:\n",
    "        updated_itemized = pd.concat([itemized] + updated_itemized_rows, ignore_index=True)\n",
    "    else:\n",
    "        updated_itemized = itemized.copy()\n",
    "    updated_itemized[\"row_order\"] = pd.to_numeric(updated_itemized[\"row_order\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "    updated_itemized = updated_itemized.sort_values(by=\"row_order\").drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    \n",
    "    # --- Process total data ---\n",
    "    updated_total_rows = []\n",
    "    for q_id in total_addendums['q_id'].unique():\n",
    "        for upgrade_type in total_addendums['type_of_upgrade'].unique():\n",
    "            addendum_row = total_addendums[\n",
    "                (total_addendums['q_id'] == q_id) &\n",
    "                (total_addendums['type_of_upgrade'] == upgrade_type)\n",
    "            ]\n",
    "            if not addendum_row.empty:\n",
    "                mask = (total['q_id'] == q_id) & (total['type_of_upgrade'] == upgrade_type)\n",
    "                original_row = total[mask]\n",
    "                print(f\"Processing total: q_id={q_id}, type_of_upgrade={upgrade_type}\")\n",
    "                # If no matching original row exists, create a default row_order column\n",
    "                if original_row.empty:\n",
    "                    original_row = pd.DataFrame({'row_order': [pd.NA] * len(addendum_row)}, index=addendum_row.index)\n",
    "                else:\n",
    "                    original_row = original_row.reset_index(drop=True)\n",
    "                addendum_row = addendum_row.reset_index(drop=True)\n",
    "                if len(addendum_row) > len(original_row):\n",
    "                    extra_rows = pd.DataFrame({col: pd.NA for col in original_row.columns},\n",
    "                                              index=range(len(addendum_row) - len(original_row)))\n",
    "                    original_row = pd.concat([original_row, extra_rows], ignore_index=True)\n",
    "                elif len(addendum_row) < len(original_row):\n",
    "                    original_row = original_row.iloc[:len(addendum_row)].reset_index(drop=True)\n",
    "                for col in conditional_columns:\n",
    "                    if col in addendum_row.columns and col in original_row.columns:\n",
    "                        addendum_row[col] = addendum_row[col].replace(\"\", pd.NA)\n",
    "                        addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
    "                        addendum_row[col] = addendum_row[col].fillna(\"\")\n",
    "                total.loc[mask, 'original'] = \"no\"\n",
    "                updated_total_rows.append(\n",
    "                    addendum_row.assign(original=\"no\", row_order=original_row['row_order'].values[:len(addendum_row)])\n",
    "                )\n",
    "                total = total[~mask]\n",
    "    if updated_total_rows:\n",
    "        updated_total = pd.concat([total] + updated_total_rows, ignore_index=True)\n",
    "    else:\n",
    "        updated_total = total.copy()\n",
    "    updated_total[\"row_order\"] = pd.to_numeric(updated_total[\"row_order\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "    updated_total = updated_total.sort_values(by=\"row_order\").drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Fill missing columns with zeros in the updated datasets\n",
    "    for col in set(itemized.columns) - set(updated_itemized.columns):\n",
    "        updated_itemized[col] = 0\n",
    "    for col in set(total.columns) - set(updated_total.columns):\n",
    "        updated_total[col] = 0\n",
    "\n",
    "    # Move the 'original' column to the last position\n",
    "    updated_itemized = updated_itemized[[col for col in updated_itemized.columns if col != 'original'] + ['original']]\n",
    "    updated_total = updated_total[[col for col in updated_total.columns if col != 'original'] + ['original']]\n",
    "    \n",
    "    if \"row_order\" in updated_itemized.columns:\n",
    "        updated_itemized = updated_itemized.drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    if \"row_order\" in updated_total.columns:\n",
    "        updated_total = updated_total.drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    \n",
    "    return updated_itemized, updated_total\n",
    "\n",
    "\n",
    "# Define the character columns\n",
    "char_columns = [\n",
    "    \"req_deliverability\", \"point_of_interconnection\", \"type_of_upgrade\",\n",
    "    \"upgrade\", \"description\", \"estimated_time_to_construct\", \"original\", \"item\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "itemized = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_itemized.csv\", char_columns)\n",
    "itemized_addendums = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_itemized_addendums.csv\", char_columns)\n",
    "total = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_total.csv\", char_columns)\n",
    "total_addendums = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_total_addendums.csv\", char_columns)\n",
    "\n",
    "\n",
    "updated_itemized, updated_total = merge_with_addendums(itemized, itemized_addendums, total, total_addendums)\n",
    "\n",
    "# Drop the specified columns from the updated datasets\n",
    "columns_to_drop = [ \"upgrade_classification\",\"estimated\", \"caiso_queue\", \"project_type\", \"dependent_system_upgrade\"]\n",
    "\n",
    "# For the itemized dataset\n",
    "updated_itemized = updated_itemized.drop(columns=[col for col in columns_to_drop if col in updated_itemized.columns], errors='ignore')\n",
    "\n",
    "# For the total dataset\n",
    "updated_total = updated_total.drop(columns=[col for col in columns_to_drop if col in updated_total.columns], errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "# List of columns to process with ffill and bfill\n",
    "columns_to_fill = [\"point_of_interconnection\", \"latitude\", \"longitude\", \"req_deliverability\", \"capacity\"]\n",
    "\n",
    "# Replace empty strings with NaN for the specified columns\n",
    "for col in columns_to_fill:\n",
    "    updated_itemized[col] = updated_itemized[col].replace('', np.nan)\n",
    "    updated_total[col] = updated_total[col].replace('', np.nan)\n",
    "\n",
    "# Sort by q_id while maintaining other column order (stable sorting)\n",
    "updated_itemized = updated_itemized.sort_values(by=[\"q_id\"], kind=\"stable\").reset_index(drop=True)\n",
    "updated_total = updated_total.sort_values(by=[\"q_id\"], kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "# Apply forward-fill and backward-fill for the specified columns within each q_id group\n",
    "for col in columns_to_fill:\n",
    "    updated_itemized[col] = (\n",
    "        updated_itemized.groupby(\"q_id\")[col]\n",
    "        .apply(lambda group: group.ffill().bfill())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    updated_total[col] = (\n",
    "        updated_total.groupby(\"q_id\")[col]\n",
    "        .apply(lambda group: group.ffill().bfill())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# Replace NaN back with empty strings for consistency\n",
    "for col in columns_to_fill:\n",
    "    updated_itemized[col] = updated_itemized[col].replace(np.nan, '')\n",
    "    updated_total[col] = updated_total[col].replace(np.nan, '')\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the updated datasets\n",
    "save_data(updated_itemized, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/01_clean/costs_phase_2_cluster_14_style_Q_itemized_updated.csv\", char_columns)\n",
    "save_data(updated_total, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/01_clean/costs_phase_2_cluster_14_style_Q_total_updated.csv\", char_columns)\n",
    "\n",
    "\n",
    "\n",
    "# Save the results\n",
    "save_data(updated_itemized, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/01_clean/costs_phase_2_cluster_14_style_Q_itemized_updated.csv\", char_columns)\n",
    "save_data(updated_total, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/01_clean/costs_phase_2_cluster_14_style_Q_total_updated.csv\", char_columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
