{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4 Style R- table 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:280: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:280: SyntaxWarning: invalid escape sequence '\\$'\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_93272/1017319466.py:280: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: 11AS737399-Q667_Revision_II_to_Appendix_A_1282013_final.pdf from Project 667\n",
      "Scraped PDF: 11AS737399-Appendix_A__Q667_11092012.pdf from Project 667\n",
      "Skipped PDF: 11AS703562-QC34PII_Appendix_A_PGE_Q688.pdf from Project 688 (No Table 3 or Attachment data)\n",
      "Skipped PDF: C3C4P2-North of Lugo-Q695-Dixie Comstock Geothermal-AppendixA.pdf from Project 695 (No Table 3 or Attachment data)\n",
      "Skipped PDF: C3C4P2-Metro-Q702AppendixA-final.pdf from Project 702 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700201-Appendix_A__Q705_Frontier_Blackwell_QC3C4_Ph_II_Study_ReportFinal_CMB_5NOV2012.pdf from Project 705 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700272-Appendix_A__Q708.pdf from Project 708 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700375-Appendix_A__Q709.pdf from Project 709 (No Table 3 or Attachment data)\n",
      "Skipped PDF: C3C4P2-SCE EOP-Q714-AppendixA.pdf from Project 714 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS708367-HHR_Q714_and_Sandy_Valley_Phase_I_SEGS_Q740_PhII_RM_Min__FINAL.pdf from Project 714 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701175-Appendix_A__Q720_Addendum_4.pdf from Project 720 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701175-Appendix_A_PGE_Q720.pdf from Project 720 (No Table 3 or Attachment data)\n",
      "Skipped PDF: C3C4P2-Northern-Q738-Oasis 20MW-AppendixA.pdf from Project 738 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS708680-El_Segundo_Energy_Ctr_Q702__Oasis_20_MW_Q738_Ph_II_RM_Min__FINAL.pdf from Project 738 (No Table 3 or Attachment data)\n",
      "Skipped PDF: C3C4P2-SCE EOP-Q740-AppendixA.pdf from Project 740 (No Table 3 or Attachment data)\n",
      "Skipped PDF: C3C4P2-SCE EOP-Q740-Revised Appendix A.pdf from Project 740 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS702744-Appendix_A__Q744_Redwood_Solar_Farm_QC3C4_Ph_II_Study_ReportFinal_CMB5NOV2012.pdf from Project 744 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS717303-C3C4P2NorthernQ746RE_AstoriaAppendixA.pdf from Project 746 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS717303-SCE_Recurrent_Projects_Ph_II_RM_Min__Final.pdf from Project 746 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS702880-Appendix_A__Q765.pdf from Project 765 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Q768_TOT585 Phase 2 - WABSRB - SLD.pdf from Project 768 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS711971-C3C4P2NorthernQ768SP_Antelope_DSRAppendixA.pdf from Project 768 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS703152-Appendix_A__Q775_Twissleman_I_QC3C4_Ph_II_Study_ReportFinal_CMB_5NOV2012.pdf from Project 775 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS708542-Q789_Revision_II_to_Appendix_A_1282013_final.pdf from Project 789\n",
      "Scraped PDF: Revised Appendix A - Q789_11-21-2012.pdf from Project 789\n",
      "Scraped PDF: Appendix A - C789_11-09-2012.pdf from Project 789\n",
      "Scraped PDF: Appendix A - Q794_11-09-2012.pdf from Project 794\n",
      "Scraped Addendum PDF: 11AS708846-Q794_Revision_II_to_Appendix_A_1282013_final.pdf from Project 794\n",
      "Scraped PDF: Revised Appendix A - Q794_11-21-2012.pdf from Project 794\n",
      "Skipped PDF: 11AS708846-Soitec_PhII_RM_Min__Final.pdf from Project 794 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737469-Kingbird_Solar_A_Q795__Kingbird_Solar_B_Q796_PhII_RM_Min__FINAL.pdf from Project 795 (No Table 3 or Attachment data)\n",
      "Skipped PDF: C3C4P2-Northern-Q795-Kingbird Solar A-AppendixA.pdf from Project 795 (No Table 3 or Attachment data)\n",
      "Skipped PDF: C3C4P2-Northern-Q796-Kingbird Solar B-AppendixA.pdf from Project 796 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737434-Kingbird_Solar_A_Q795__Kingbird_Solar_B_Q796_PhII_RM_Min__FINAL.pdf from Project 796 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS711623-C3C4P2EasternQ798AppendixAFinal.pdf from Project 798 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS711623-QC34PIIQ798Addendum.pdf from Project 798 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699264-QC34PII_Appendix_A_PGE_Q799.pdf from Project 799 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699264-Attachment_10_Q799_Phase_2.pdf from Project 799 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699298-Q800C34PHIIAppendixANovember52012.pdf from Project 800 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699747-Q805C34PHIIAppendixANovember52012.pdf from Project 805 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700096-Q809C34PHIIAppendixANovember52012.pdf from Project 809 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700725-Q814C34PHIIAppendixANovember52012.pdf from Project 814 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700760-Q815C34PHIIAppendixANovember152012.pdf from Project 815 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700794-Q816C34PHIIAppendixANovember52012.pdf from Project 816 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701279-QC34PII_Appendix_A_PGE_Q820.pdf from Project 820 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701857-QC34PII_Appendix_A_PGE_Q825.pdf from Project 825 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS703986-QC34PII_Appendix_A_PGE_Q829.pdf from Project 829 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS703986-QC34PII_Revised_Appendix_A_Q829.pdf from Project 829 (No Table 3 or Attachment data)\n",
      "Skipped PDF: C3C4P2-Eastern-Q831AppendixA-Final.pdf from Project 831 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS737779-Appendix_A__C837_12302011_final.pdf from Project 837\n",
      "Scraped PDF: 11AS737779-Jacumba_II_Solar_Farm_Q837_Appendix_A__11092012.pdf from Project 837\n",
      "Scraped PDF: 11AS716919-Q838_Revision_II_to_Appendix_A_1282013_final.pdf from Project 838\n",
      "Scraped PDF: 11AS716919-Revised_Appendix_A__Q838_11212012.pdf from Project 838\n",
      "Scraped PDF: 11AS716919-Appendix_A__Q838_11092012.pdf from Project 838\n",
      "Skipped PDF: 11AS708812-ADDENDUM_Q856_Tehachpi__Wind_Energy_Storage_PhII_Study.pdf from Project 856 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS708812-Tehachapi_Wind_Energy_Storage_Q856_Ph_II_RM_Min__FINAL.pdf from Project 856 (No Table 3 or Attachment data)\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/03_raw/ph2_rawdata_cluster4_style_R_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/03_raw/ph2_rawdata_cluster4_style_R_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 124\n",
      "Total Projects Scraped: 5\n",
      "Total Projects Skipped: 119\n",
      "Total Projects Missing: 0\n",
      "Total PDFs Accessed: 56\n",
      "Total PDFs Scraped: 13\n",
      "Total PDFs Skipped: 43\n",
      "\n",
      "List of Scraped Projects:\n",
      "['667', '789', '794', '837', '838']\n",
      "\n",
      "List of Skipped Projects:\n",
      "['668', '669', '670', '671', '674', '676', '678', '679', '680', '681', '683', '684', '685', '686', '687', '688', '692', '695', '696', '697', '698', '700', '702', '703', '704', '705', '706', '707', '708', '709', '712', '714', '716', '717', '720', '723', '725', '729', '730', '732', '736', '737', '738', '739', '740', '741', '744', '746', '751', '752', '756', '762', '764', '765', '766', '767', '768', '769', '770', '771', '774', '775', '778', '779', '781', '782', '783', '784', '785', '786', '788', '790', '791', '792', '793', '795', '796', '797', '798', '799', '800', '801', '804', '805', '806', '807', '809', '812', '813', '814', '815', '816', '820', '823', '824', '825', '829', '831', '834', '836', '839', '840', '841', '842', '843', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '857', '858']\n",
      "\n",
      "List of Missing Projects:\n",
      "[]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['11AS737399-Q667_Revision_II_to_Appendix_A_1282013_final.pdf', '11AS737399-Appendix_A__Q667_11092012.pdf', '11AS708542-Q789_Revision_II_to_Appendix_A_1282013_final.pdf', 'Revised Appendix A - Q789_11-21-2012.pdf', 'Appendix A - C789_11-09-2012.pdf', 'Appendix A - Q794_11-09-2012.pdf', '11AS708846-Q794_Revision_II_to_Appendix_A_1282013_final.pdf', 'Revised Appendix A - Q794_11-21-2012.pdf', '11AS737779-Appendix_A__C837_12302011_final.pdf', '11AS737779-Jacumba_II_Solar_Farm_Q837_Appendix_A__11092012.pdf', '11AS716919-Q838_Revision_II_to_Appendix_A_1282013_final.pdf', '11AS716919-Revised_Appendix_A__Q838_11212012.pdf', '11AS716919-Appendix_A__Q838_11092012.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['11AS703562-QC34PII_Appendix_A_PGE_Q688.pdf', 'C3C4P2-North of Lugo-Q695-Dixie Comstock Geothermal-AppendixA.pdf', 'C3C4P2-Metro-Q702AppendixA-final.pdf', '11AS700201-Appendix_A__Q705_Frontier_Blackwell_QC3C4_Ph_II_Study_ReportFinal_CMB_5NOV2012.pdf', '11AS700272-Appendix_A__Q708.pdf', '11AS700375-Appendix_A__Q709.pdf', 'C3C4P2-SCE EOP-Q714-AppendixA.pdf', '11AS708367-HHR_Q714_and_Sandy_Valley_Phase_I_SEGS_Q740_PhII_RM_Min__FINAL.pdf', '11AS701175-Appendix_A__Q720_Addendum_4.pdf', '11AS701175-Appendix_A_PGE_Q720.pdf', 'C3C4P2-Northern-Q738-Oasis 20MW-AppendixA.pdf', '11AS708680-El_Segundo_Energy_Ctr_Q702__Oasis_20_MW_Q738_Ph_II_RM_Min__FINAL.pdf', 'C3C4P2-SCE EOP-Q740-AppendixA.pdf', 'C3C4P2-SCE EOP-Q740-Revised Appendix A.pdf', '11AS702744-Appendix_A__Q744_Redwood_Solar_Farm_QC3C4_Ph_II_Study_ReportFinal_CMB5NOV2012.pdf', '11AS717303-C3C4P2NorthernQ746RE_AstoriaAppendixA.pdf', '11AS717303-SCE_Recurrent_Projects_Ph_II_RM_Min__Final.pdf', '11AS702880-Appendix_A__Q765.pdf', 'Q768_TOT585 Phase 2 - WABSRB - SLD.pdf', '11AS711971-C3C4P2NorthernQ768SP_Antelope_DSRAppendixA.pdf', '11AS703152-Appendix_A__Q775_Twissleman_I_QC3C4_Ph_II_Study_ReportFinal_CMB_5NOV2012.pdf', '11AS708846-Soitec_PhII_RM_Min__Final.pdf', '11AS737469-Kingbird_Solar_A_Q795__Kingbird_Solar_B_Q796_PhII_RM_Min__FINAL.pdf', 'C3C4P2-Northern-Q795-Kingbird Solar A-AppendixA.pdf', 'C3C4P2-Northern-Q796-Kingbird Solar B-AppendixA.pdf', '11AS737434-Kingbird_Solar_A_Q795__Kingbird_Solar_B_Q796_PhII_RM_Min__FINAL.pdf', '10AS711623-C3C4P2EasternQ798AppendixAFinal.pdf', '10AS711623-QC34PIIQ798Addendum.pdf', '11AS699264-QC34PII_Appendix_A_PGE_Q799.pdf', '11AS699264-Attachment_10_Q799_Phase_2.pdf', '11AS699298-Q800C34PHIIAppendixANovember52012.pdf', '11AS699747-Q805C34PHIIAppendixANovember52012.pdf', '11AS700096-Q809C34PHIIAppendixANovember52012.pdf', '11AS700725-Q814C34PHIIAppendixANovember52012.pdf', '11AS700760-Q815C34PHIIAppendixANovember152012.pdf', '11AS700794-Q816C34PHIIAppendixANovember52012.pdf', '11AS701279-QC34PII_Appendix_A_PGE_Q820.pdf', '11AS701857-QC34PII_Appendix_A_PGE_Q825.pdf', '11AS703986-QC34PII_Appendix_A_PGE_Q829.pdf', '11AS703986-QC34PII_Revised_Appendix_A_Q829.pdf', 'C3C4P2-Eastern-Q831AppendixA-Final.pdf', '11AS708812-ADDENDUM_Q856_Tehachpi__Wind_Energy_Storage_PhII_Study.pdf', '11AS708812-Tehachapi_Wind_Energy_Storage_Q856_Ph_II_RM_Min__FINAL.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['11AS737399-Q667_Revision_II_to_Appendix_A_1282013_final.pdf', '11AS701175-Appendix_A__Q720_Addendum_4.pdf', '11AS708542-Q789_Revision_II_to_Appendix_A_1282013_final.pdf', '11AS708846-Q794_Revision_II_to_Appendix_A_1282013_final.pdf', '11AS737469-Kingbird_Solar_A_Q795__Kingbird_Solar_B_Q796_PhII_RM_Min__FINAL.pdf', '11AS737434-Kingbird_Solar_A_Q795__Kingbird_Solar_B_Q796_PhII_RM_Min__FINAL.pdf', '10AS711623-QC34PIIQ798Addendum.pdf', '11AS716919-Q838_Revision_II_to_Appendix_A_1282013_final.pdf', '11AS708812-ADDENDUM_Q856_Tehachpi__Wind_Energy_Storage_PhII_Study.pdf', '11AS708812-Tehachapi_Wind_Energy_Storage_Q856_Ph_II_RM_Min__FINAL.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['11AS737399-Appendix_A__Q667_11092012.pdf', '11AS703562-QC34PII_Appendix_A_PGE_Q688.pdf', 'C3C4P2-North of Lugo-Q695-Dixie Comstock Geothermal-AppendixA.pdf', 'C3C4P2-Metro-Q702AppendixA-final.pdf', '11AS700201-Appendix_A__Q705_Frontier_Blackwell_QC3C4_Ph_II_Study_ReportFinal_CMB_5NOV2012.pdf', '11AS700272-Appendix_A__Q708.pdf', '11AS700375-Appendix_A__Q709.pdf', 'C3C4P2-SCE EOP-Q714-AppendixA.pdf', '11AS708367-HHR_Q714_and_Sandy_Valley_Phase_I_SEGS_Q740_PhII_RM_Min__FINAL.pdf', '11AS701175-Appendix_A_PGE_Q720.pdf', 'C3C4P2-Northern-Q738-Oasis 20MW-AppendixA.pdf', '11AS708680-El_Segundo_Energy_Ctr_Q702__Oasis_20_MW_Q738_Ph_II_RM_Min__FINAL.pdf', 'C3C4P2-SCE EOP-Q740-AppendixA.pdf', 'C3C4P2-SCE EOP-Q740-Revised Appendix A.pdf', '11AS702744-Appendix_A__Q744_Redwood_Solar_Farm_QC3C4_Ph_II_Study_ReportFinal_CMB5NOV2012.pdf', '11AS717303-C3C4P2NorthernQ746RE_AstoriaAppendixA.pdf', '11AS717303-SCE_Recurrent_Projects_Ph_II_RM_Min__Final.pdf', '11AS702880-Appendix_A__Q765.pdf', 'Q768_TOT585 Phase 2 - WABSRB - SLD.pdf', '11AS711971-C3C4P2NorthernQ768SP_Antelope_DSRAppendixA.pdf', '11AS703152-Appendix_A__Q775_Twissleman_I_QC3C4_Ph_II_Study_ReportFinal_CMB_5NOV2012.pdf', 'Revised Appendix A - Q789_11-21-2012.pdf', 'Appendix A - C789_11-09-2012.pdf', 'Appendix A - Q794_11-09-2012.pdf', 'Revised Appendix A - Q794_11-21-2012.pdf', '11AS708846-Soitec_PhII_RM_Min__Final.pdf', 'C3C4P2-Northern-Q795-Kingbird Solar A-AppendixA.pdf', 'C3C4P2-Northern-Q796-Kingbird Solar B-AppendixA.pdf', '10AS711623-C3C4P2EasternQ798AppendixAFinal.pdf', '11AS699264-QC34PII_Appendix_A_PGE_Q799.pdf', '11AS699264-Attachment_10_Q799_Phase_2.pdf', '11AS699298-Q800C34PHIIAppendixANovember52012.pdf', '11AS699747-Q805C34PHIIAppendixANovember52012.pdf', '11AS700096-Q809C34PHIIAppendixANovember52012.pdf', '11AS700725-Q814C34PHIIAppendixANovember52012.pdf', '11AS700760-Q815C34PHIIAppendixANovember152012.pdf', '11AS700794-Q816C34PHIIAppendixANovember52012.pdf', '11AS701279-QC34PII_Appendix_A_PGE_Q820.pdf', '11AS701857-QC34PII_Appendix_A_PGE_Q825.pdf', '11AS703986-QC34PII_Appendix_A_PGE_Q829.pdf', '11AS703986-QC34PII_Revised_Appendix_A_Q829.pdf', 'C3C4P2-Eastern-Q831AppendixA-Final.pdf', '11AS737779-Appendix_A__C837_12302011_final.pdf', '11AS737779-Jacumba_II_Solar_Farm_Q837_Appendix_A__11092012.pdf', '11AS716919-Revised_Appendix_A__Q838_11212012.pdf', '11AS716919-Appendix_A__Q838_11092012.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 9\n",
      "Number of Addendum PDFs Scraped: 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "import inflect\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY =\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/03_raw/ph2_rawdata_cluster4_style_R_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/03_raw/ph2_rawdata_cluster4_style_R_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/03_raw/ph2_scraping_cluster4_style_R_log.txt\"\n",
    "PROJECT_RANGE = range(667, 860)   # Inclusive range for q_ids in Clusters 4\n",
    "\n",
    "\n",
    "# Read the CSV file containing processed projects (with q_id column)\n",
    "processed_csv_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters/costs_phase_2_all_clusters_total.csv\"  # UPDATE THIS PATH\n",
    "processed_df = pd.read_csv(processed_csv_path)\n",
    "# Convert q_id values to numeric then to int for filtering\n",
    "processed_q_ids = pd.to_numeric(processed_df['q_id'], errors='coerce').dropna().astype(int).unique()\n",
    "projects_to_process = sorted([q_id for q_id in PROJECT_RANGE if q_id not in processed_q_ids])\n",
    "\n",
    " \n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing, removing unwanted characters, and singularizing words.\"\"\"\n",
    "    p = inflect.engine()\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "            # Correct any mis‐spellings of “type of upgrade”\n",
    "            header = re.sub(r'\\btype of upgr\\s*ade\\b', 'type of upgrade', header)\n",
    "            words = header.split()\n",
    "            singular_words = [p.singular_noun(word) if p.singular_noun(word) else word for word in words]\n",
    "            header = \" \".join(singular_words)\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback if none found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "    new_order = existing_desired + remaining\n",
    "    df = df[new_order]\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"\n",
    "    Ensures each row in data_rows has exactly len(headers) columns.\n",
    "    If a row is too short, it is padded with empty strings.\n",
    "    If too long, it is truncated.\n",
    "    \"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"] * (col_count - len(row)))\n",
    "\n",
    "def extract_table2(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "    table_settings_list = [\n",
    "        {\"horizontal_strategy\": \"text\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 1},\n",
    "        {\"horizontal_strategy\": \"lines\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 2},\n",
    "    ]\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*(?:2|B\\.1)\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 2 found in the PDF.\", file=log_file)\n",
    "                return None\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Table 2 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "            extraction_successful = False\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 2...\", file=log_file)\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    poi_col_index = cell_index\n",
    "                                    adjacent_col_index = poi_col_index + 1\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            poi_value_parts = []\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "                                            if poi_value_parts:\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                            if extraction_successful:\n",
    "                                break\n",
    "                        if extraction_successful:\n",
    "                            break\n",
    "                    if extraction_successful:\n",
    "                        break\n",
    "                if extraction_successful:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 2 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "    if not extraction_successful:\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            print(\"Point of Interconnection not found in Table 2.\", file=log_file)\n",
    "            return None\n",
    "    return point_of_interconnection\n",
    "\n",
    "def fix_column_names(columns):\n",
    "    \"\"\"\n",
    "    Renames duplicate and empty column names.\n",
    "    Duplicate names are suffixed with _1, _2, etc.\n",
    "    Empty or whitespace-only names are replaced with unnamed_1, unnamed_2, etc.\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    counts = {}\n",
    "    unnamed_count = 1\n",
    "    for col in columns:\n",
    "        # Treat empty or whitespace-only names as unnamed.\n",
    "        if not col or col.strip() == \"\":\n",
    "            new_col = f\"unnamed_{unnamed_count}\"\n",
    "            unnamed_count += 1\n",
    "        else:\n",
    "            new_col = col.strip()\n",
    "        if new_col in counts:\n",
    "            new_col_with_suffix = f\"{new_col}_{counts[new_col]}\"\n",
    "            counts[new_col] += 1\n",
    "            new_cols.append(new_col_with_suffix)\n",
    "        else:\n",
    "            counts[new_col] = 1\n",
    "            new_cols.append(new_col)\n",
    "    return new_cols\n",
    "\n",
    "def post_process_columns(df, log_file):\n",
    "    \"\"\"\n",
    "    Post-processes DataFrame column names:\n",
    "      1. For any column named 'unnamed_#' (or empty), look at its first non-empty cell.\n",
    "         If that cell is not a dollar amount (i.e. does not match /^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$/)\n",
    "         and it contains 2 or 3 words, then rename the column to that value (after cleaning).\n",
    "         If a column already exists with that name, merge the data from the renamed column into the\n",
    "         existing column and drop the renamed column.\n",
    "      2. If a column is named \"Needed For\", then rename it to \"description\" (merging with an existing\n",
    "         description column if necessary).\n",
    "    \"\"\"\n",
    "    # Process unnamed columns.\n",
    "    for col in list(df.columns):\n",
    "        if col.lower().startswith(\"unnamed_\") or col.strip() == \"\":\n",
    "            # Find the first non-empty cell in this column.\n",
    "            first_non_empty = None\n",
    "            for val in df[col]:\n",
    "                cell_val = \"\"\n",
    "                if isinstance(val, str):\n",
    "                    cell_val = val.strip()\n",
    "                elif val is not None:\n",
    "                    cell_val = str(val).strip()\n",
    "                if cell_val:\n",
    "                    first_non_empty = cell_val\n",
    "                    break\n",
    "            if first_non_empty:\n",
    "                # Check if the value is a dollar amount.\n",
    "                if not re.match(r\"^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$\", first_non_empty):\n",
    "                    words = first_non_empty.split()\n",
    "                    if 2 <= len(words) <= 3:\n",
    "                        # Clean the candidate name.\n",
    "                        new_name = clean_column_headers([first_non_empty])[0]\n",
    "                        log_file.write(f\"Renaming column '{col}' to '{new_name}' based on first non-empty value '{first_non_empty}'.\\n\")\n",
    "                        if new_name in df.columns and new_name != col:\n",
    "                            # Merge the two columns: fill empty cells in existing new_name from the renamed col.\n",
    "                            for idx in df.index:\n",
    "                                existing_val = df.at[idx, new_name]\n",
    "                                candidate_val = df.at[idx, col]\n",
    "                                if (pd.isna(existing_val) or existing_val == \"\") and (not pd.isna(candidate_val) and candidate_val != \"\"):\n",
    "                                    df.at[idx, new_name] = candidate_val\n",
    "                            df.drop(columns=[col], inplace=True)\n",
    "                        else:\n",
    "                            df.rename(columns={col: new_name}, inplace=True)\n",
    "    # Process \"Needed For\" column: rename or merge it into \"description\".\n",
    "    if \"Needed For\" in df.columns:\n",
    "        if \"description\" in df.columns:\n",
    "            log_file.write(\"Merging 'Needed For' column into existing 'description' column.\\n\")\n",
    "            for idx in df.index:\n",
    "                desc_val = df.at[idx, \"description\"]\n",
    "                needed_for_val = df.at[idx, \"Needed For\"]\n",
    "                if (pd.isna(desc_val) or desc_val == \"\") and (not pd.isna(needed_for_val) and needed_for_val != \"\"):\n",
    "                    df.at[idx, \"description\"] = needed_for_val\n",
    "            df.drop(columns=[\"Needed For\"], inplace=True)\n",
    "        else:\n",
    "            log_file.write(\"Renaming 'Needed For' column to 'description'.\\n\")\n",
    "            df.rename(columns={\"Needed For\": \"description\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def extract_table3(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 3 data   the provided PDF.\n",
    "     \n",
    "      2. Renaming of duplicate/empty columns (using fix_column_names) and then post-processing\n",
    "         unnamed columns as described.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 10\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain either Table 3 patterns or Attachment 1/Attachment 2.\n",
    "            table3_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*11[-.]([1-2])\\b\", text, re.IGNORECASE):\n",
    "                    #re.search(r\"Attachment\\s*[12]\", text, re.IGNORECASE)):\n",
    "                    table3_pages.append(i)\n",
    "            if not table3_pages:\n",
    "                print(\"No Table 10  found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "            first_page = table3_pages[0]\n",
    "            last_page = table3_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Candidate pages start on {scrape_start + 1} and end on {scrape_end}\", file=log_file)\n",
    "            # Process each page that might contain table data.\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                # This variable keeps track of the bottom y-coordinate of the previous table on the page.\n",
    "                previous_table_bottom = None\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "                    table_bbox = table.bbox  # (x0, top, x1, bottom)\n",
    "                    # Define the title region for the table: above the table bounding box.\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*11[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(2).strip()\n",
    "                                break\n",
    "                        \n",
    "                  \n",
    "                    # Extract the specific phrase using the refined table title.\n",
    "                    if table_title:\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New table detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        # Rename header 'type' to 'type of upgrade' if needed.\n",
    "                        if \"type\" in headers and \"type of upgrade\" not in headers:\n",
    "                            headers = [(\"type of upgrade\" if h == \"type\" else h) for h in headers]\n",
    "                        if \"need for\" in headers:\n",
    "                            headers = [(\"description\" if h == \"need for\" else h) for h in headers]  \n",
    "                    \n",
    "                        # Apply the duplicate/empty column fixing.\n",
    "                        headers = fix_column_names(headers)\n",
    "                        data_rows = tab[1:]\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        if \"allocated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"allocated\"], inplace=True)\n",
    "                            print(f\"Dropped 'Max of' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate x \" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate x \"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file) \n",
    "\n",
    "                        if \"3339615 9\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"3339615 9\"], inplace=True)\n",
    "                            print(f\"Dropped '3339615 9' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)     \n",
    "                            \n",
    "                        if \"6 steady state reliability and posttransient voltage stability\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"6 steady state reliability and posttransient voltage stability\"], inplace=True)\n",
    "                            print(f\"Dropped '6 steady state reliability and posttransient voltage stability' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)  \n",
    "\n",
    "\n",
    "\n",
    "                        if \"escalated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"escalated\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)    \n",
    "\n",
    "\n",
    "                        # Also, if the DataFrame has a column named \"type\" (and not already \"type of upgrade\"), rename it.\n",
    "                        if 'type' in df_new.columns and 'type of upgrade' not in df_new.columns:\n",
    "                            df_new.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        # Special handling for ADNU tables if needed.\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        # Fix duplicate and unnamed columns in the new table.\n",
    "                        df_new.columns = fix_column_names(df_new.columns.tolist())\n",
    "                        # Now apply the post-processing of column names:\n",
    "                        df_new = post_process_columns(df_new, log_file)\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation table branch.\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous table title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"Continuation table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "                        # Use the number of columns from the last extracted table as expected.\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            data_rows = data_rows[1:]\n",
    "                        # Ensure every row has the same length as expected_headers.\n",
    "                        adjust_rows_length(data_rows, expected_headers)\n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "                        # Rename column 'type' if needed.\n",
    "                        if 'type' in df_continuation.columns and 'type of upgrade' not in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        if \"need for\" in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={\"need for\": \"description\"}, inplace=True)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        # Fix duplicate and unnamed columns in the continuation table.\n",
    "                        df_continuation.columns = fix_column_names(df_continuation.columns.tolist())\n",
    "                        # Post-process the columns in the continuation table.\n",
    "                        df_continuation = post_process_columns(df_continuation, log_file)\n",
    "                        # Concatenate the continuation table with the previous extracted table.\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                    # Update the previous_table_bottom for the page using the current table's bbox.\n",
    "                    previous_table_bottom = table_bbox[3]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 10 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "        print(\"\\nConcatenating all extracted Table 10/Attachment data...\", file=log_file)\n",
    "        try:\n",
    "            table3_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table3_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 10/Attachment data extracted.\", file=log_file)\n",
    "        table3_data = pd.DataFrame()\n",
    "    return table3_data\n",
    "\n",
    "\n",
    "def extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 10 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table3_data = extract_table3(pdf_path, log_file, is_addendum)\n",
    "    if table3_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        overlapping_columns = base_data.columns.intersection(table3_data.columns).difference(['point_of_interconnection'])\n",
    "        table3_data = table3_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        base_data_repeated = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "        try:\n",
    "\n",
    "                        # Concatenate base data with Table 8 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "           # if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "           #     merged_df[\"is_duplicate\"] = merged_df.duplicated(subset=[\"q_id\", \"type of upgrade\", \"upgrade\"], keep=\"first\")\n",
    "            #    merged_df = merged_df[merged_df[\"is_duplicate\"] == False].drop(columns=[\"is_duplicate\"])\n",
    "            #    print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade'.\", file=log_file)\n",
    "\n",
    "\n",
    "            if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "                # Identify rows where 'type of upgrade' and 'upgrade' are not empty\n",
    "                non_empty_rows = merged_df[\n",
    "                    merged_df[\"type of upgrade\"].notna() & merged_df[\"upgrade\"].notna() &\n",
    "                    (merged_df[\"type of upgrade\"].str.strip() != \"\") & (merged_df[\"upgrade\"].str.strip() != \"\")\n",
    "                ]\n",
    "\n",
    "                # Group by q_id, type of upgrade, and upgrade, keeping the first occurrence\n",
    "                grouped_df = non_empty_rows.groupby([\"q_id\", \"type of upgrade\", \"upgrade\"], as_index=False).first()\n",
    "\n",
    "                # Get the original order of the rows in merged_df before filtering\n",
    "                merged_df[\"original_index\"] = merged_df.index\n",
    "\n",
    "                # Combine unique grouped rows with originally empty rows\n",
    "                final_df = pd.concat([\n",
    "                    grouped_df,\n",
    "                    merged_df[merged_df[\"type of upgrade\"].isna() | (merged_df[\"type of upgrade\"].str.strip() == \"\") |\n",
    "                            merged_df[\"upgrade\"].isna() | (merged_df[\"upgrade\"].str.strip() == \"\")]\n",
    "                ], ignore_index=True, sort=False)\n",
    "\n",
    "                # Restore the original order of the rows based on the saved index\n",
    "                final_df.sort_values(by=\"original_index\", inplace=True)\n",
    "                final_df.drop(columns=[\"original_index\"], inplace=True)\n",
    "                merged_df = final_df\n",
    "\n",
    "                print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade', excluding empty rows while preserving order.\", file=log_file)\n",
    "\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            print(f\"Merged base data with Table 3 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 3 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data\n",
    "\n",
    "def check_has_table3(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 3 or Attachment 1/Attachment 2.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*11[-.]?\\d*\", text, re.IGNORECASE):\n",
    "                    #re.search(r\"Attachment\\s*[12]\", text, re.IGNORECASE))\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' or 'Revision' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "             \n",
    "            # Case-insensitive check for 'Addendum' or 'Revision'\n",
    "            text_lower = text.lower()\n",
    "            return \"addendum\" in text_lower or \"revision\" in text_lower\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "        text = clean_string_cell(text)\n",
    "        #queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = str(project_id) #queue_id.group(1) if queue_id else \n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = 3\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "        point_of_interconnection = extract_table2(pdf_path, log_file)\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "    df = df.map(clean_string_cell)\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "    #if 'q_id' in df.columns:\n",
    "    #    df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "'''\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "            project_scraped = False\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    try:\n",
    "                        has_table3 = check_has_table3(pdf_path)\n",
    "                        if not has_table3:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\", file=log_file)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "                        if is_add and base_data_extracted:\n",
    "                            table3_data = extract_table3(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table3_data.empty:\n",
    "                                merged_df = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table3_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            df = extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "'''\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"\n",
    "    Processes all PDFs in the directories within BASE_DIRECTORY whose numeric prefix is in PROJECT_RANGE.\n",
    "    This allows folders like '641' and '641AA' (if 641 is in the PROJECT_RANGE) to be processed,\n",
    "    and uses the full folder name (e.g. '641AA') as the project id (q_id).\n",
    "    \"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    " \n",
    "\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        # List all subdirectories in BASE_DIRECTORY that have a numeric prefix.\n",
    "        folders = [\n",
    "            folder for folder in os.listdir(BASE_DIRECTORY)\n",
    "            if os.path.isdir(os.path.join(BASE_DIRECTORY, folder)) and re.match(r'^(\\d+)', folder)\n",
    "        ]\n",
    "    \n",
    "\n",
    "\n",
    "        def sort_key(folder):\n",
    "            match = re.match(r'^(\\d+)', folder)\n",
    "            if match:\n",
    "                numeric = int(match.group(1))\n",
    "                return (numeric, folder)\n",
    "            return (float('inf'), folder)\n",
    "\n",
    "        # Sort the folders in ascending order.\n",
    "        sorted_folders = sorted(folders, key=sort_key)\n",
    "\n",
    "        # Process each folder in sorted order.\n",
    "        for folder in sorted_folders:\n",
    "            folder_path = os.path.join(BASE_DIRECTORY, folder)\n",
    "            match = re.match(r'^(\\d+)', folder)\n",
    "            if not match:\n",
    "                continue  # Skip if there is no numeric prefix.\n",
    "            numeric_part = int(match.group(1))\n",
    "            # Process the folder only if its numeric part is in the desired range.\n",
    "            if numeric_part not in PROJECT_RANGE:\n",
    "                continue\n",
    "\n",
    "            # Use the full folder name as the project identifier (q_id).\n",
    "            project_id = folder  # e.g., \"641AA\" or \"641\"\n",
    "            project_path = os.path.join(folder_path, \"03_phase_2_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    try:\n",
    "                        has_table3 = check_has_table3(pdf_path)\n",
    "                        if not has_table3:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\", file=log_file)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "                        if is_add and base_data_extracted:\n",
    "                            table3_data = extract_table3(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table3_data.empty:\n",
    "                                merged_df = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table3_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            df = extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "    # Save results and print summary as before.\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized and Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: ['q_id', 'cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection', 'type of upgrade', 'unnamed_10', 'unnamed_11', 'unnamed_4', 'unnamed_6', 'unnamed_13', 'estimated', 'unnamed_3', 'delivery network upgrade', 'type of', 'unnamed_7', 'unnamed_15', 'unnamed_18', 'unnamed_5', 'unnamed_8', 'unnamed_16', 'estimated cost x', 'unnamed_19', 'unnamed_9', 'unnamed_12', 'unnamed_1', '84 month', '6 month', 'unnamed_14', 'estimated cost', 'estimated_1', '12 month', 'unnamed_17', 'reliability network upgrade', '14 affected system']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/03_raw/ph2_rawdata_cluster4_style_R_originals.csv\", dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "\n",
    "\n",
    "#df.columns = clean_column_headers(df.columns)\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "#df = df.map(clean_string_cell)\n",
    "#df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "print(\"After cleaning:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q945 PTO first and only row esc cost 41.4 is entered in the pdf as 41,4, have to manually update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 667\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 789\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 794\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 837\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 838\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "     q_id  cluster req_deliverability  latitude  longitude  capacity  \\\n",
      "0    667        3               Full       NaN        NaN       NaN   \n",
      "1    667        3               Full       NaN        NaN       NaN   \n",
      "2    667        3               Full       NaN        NaN       NaN   \n",
      "3    789        3               Full       NaN        NaN       NaN   \n",
      "4    789        3               Full       NaN        NaN       NaN   \n",
      "5    789        3               Full       NaN        NaN       NaN   \n",
      "6    794        3               None       NaN        NaN       NaN   \n",
      "7    794        3               None       NaN        NaN       NaN   \n",
      "8    794        3               None       NaN        NaN       NaN   \n",
      "9    837        3               Full    32.627   -116.126       NaN   \n",
      "10   837        3               Full    32.627   -116.126       NaN   \n",
      "11   837        3               Full    32.627   -116.126       NaN   \n",
      "12   838        3               Full       NaN        NaN       NaN   \n",
      "13   838        3               Full       NaN        NaN       NaN   \n",
      "14   838        3               Full       NaN        NaN       NaN   \n",
      "\n",
      "                          point_of_interconnection type_of_upgrade  \\\n",
      "0   230 kV bus at the proposed New Imperial Valley      Total LDNU   \n",
      "1   230 kV bus at the proposed New Imperial Valley    Total PTO_IF   \n",
      "2   230 kV bus at the proposed New Imperial Valley       Total RNU   \n",
      "3           69 kV bus at Boulevard East Substation      Total LDNU   \n",
      "4           69 kV bus at Boulevard East Substation    Total PTO_IF   \n",
      "5           69 kV bus at Boulevard East Substation       Total RNU   \n",
      "6               138 kV bus at Boulevard Substation      Total LDNU   \n",
      "7               138 kV bus at Boulevard Substation    Total PTO_IF   \n",
      "8               138 kV bus at Boulevard Substation       Total RNU   \n",
      "9             138 kV bus at East County Substation      Total LDNU   \n",
      "10            138 kV bus at East County Substation    Total PTO_IF   \n",
      "11            138 kV bus at East County Substation       Total RNU   \n",
      "12        230 kV bus at Imperial Valley Substation      Total LDNU   \n",
      "13        230 kV bus at Imperial Valley Substation    Total PTO_IF   \n",
      "14        230 kV bus at Imperial Valley Substation       Total RNU   \n",
      "\n",
      "   type_of_upgrade_2 upgrade description cost_allocation_factor  \\\n",
      "0                                                                 \n",
      "1                                                                 \n",
      "2                                                                 \n",
      "3                                                                 \n",
      "4                                                                 \n",
      "5                                                                 \n",
      "6                                                                 \n",
      "7                                                                 \n",
      "8                                                                 \n",
      "9                                                                 \n",
      "10                                                                \n",
      "11                                                                \n",
      "12                                                                \n",
      "13                                                                \n",
      "14                                                                \n",
      "\n",
      "    estimated_cost_x_1000 escalated_cost_x_1000 estimated_time_to_construct  \\\n",
      "0                    84.0                                                     \n",
      "1                  4517.0                                                     \n",
      "2                  3680.0                                                     \n",
      "3                     0.0                                                     \n",
      "4                  2665.0                                                     \n",
      "5                  4149.0                                                     \n",
      "6                  1000.0                                                     \n",
      "7                  2084.0                                                     \n",
      "8                  3021.0                                                     \n",
      "9                  1000.0                                                     \n",
      "10                 2032.0                                                     \n",
      "11                 1576.0                                                     \n",
      "12                    0.0                                                     \n",
      "13                  226.0                                                     \n",
      "14                 6814.0                                                     \n",
      "\n",
      "   item  \n",
      "0    no  \n",
      "1    no  \n",
      "2    no  \n",
      "3    no  \n",
      "4    no  \n",
      "5    no  \n",
      "6    no  \n",
      "7    no  \n",
      "8    no  \n",
      "9    no  \n",
      "10   no  \n",
      "11   no  \n",
      "12   no  \n",
      "13   no  \n",
      "14   no  \n",
      "👉 Duplicate column names in df: []\n",
      "Itemized rows saved to 'costs_phase_2_cluster_4_style_R_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_4_style_R_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU']\n",
      "[667 789 794 837 838]\n",
      "[3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_93272/4225489517.py:361: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_93272/4225489517.py:361: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_93272/4225489517.py:361: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_93272/4225489517.py:462: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean[required_cols] = df_clean[required_cols].applymap(lambda x: str(x).strip())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_93272/4225489517.py:906: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/03_raw/ph2_rawdata_cluster4_style_R_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_cost_allocation(df, source_col, target_col=\"cost_allocation_factor\"):\n",
    "    \"\"\"\n",
    "    Extracts percentage values from a specified source column and moves them into a target column.\n",
    "    \n",
    "    - A percentage value is defined as a string that, when stripped of whitespace,\n",
    "      fully matches a pattern of digits (with optional commas or periods) followed by a percent sign.\n",
    "    - If a cell in the source column matches this pattern, its value is placed into the target column,\n",
    "      and the source column cell is cleared (set to an empty string).\n",
    "    - If the cell does not match a percentage pattern, it is left untouched in the source column.\n",
    "    \n",
    "    Parameters:\n",
    "      df         : pandas DataFrame.\n",
    "      source_col : string, the name of the column to scan for percentage values.\n",
    "      target_col : string, the name of the column to store the extracted percentage values.\n",
    "                   Defaults to \"cost_allocation_factor\".\n",
    "    \n",
    "    Returns:\n",
    "      The DataFrame with the updated columns.\n",
    "    \"\"\"\n",
    "    # Define a regex pattern to match a percentage value (e.g., \"78.25%\").\n",
    "    # The pattern allows digits, commas, and periods, followed immediately by a \"%\" (ignoring leading/trailing spaces).\n",
    "    pattern = r\"^\\s*[\\d,\\.]+%\\s*$\"\n",
    "    \n",
    "    def extract_percentage(text):\n",
    "        # If text matches the percentage pattern, return the stripped text; otherwise, return None.\n",
    "        if isinstance(text, str) and re.fullmatch(pattern, text):\n",
    "            return text.strip()\n",
    "        return None\n",
    "\n",
    "    def clear_percentage(text):\n",
    "        # If text matches the percentage pattern, clear it (return an empty string).\n",
    "        # Otherwise, return the text stripped of surrounding whitespace.\n",
    "        if isinstance(text, str) and re.fullmatch(pattern, text):\n",
    "            return \"\"\n",
    "        if isinstance(text, str):\n",
    "            return text.strip()\n",
    "        return text\n",
    "\n",
    "    # Create (or overwrite) the target column with extracted percentage values from the source column.\n",
    "    df[target_col] = df[source_col].apply(extract_percentage)\n",
    "    # In the source column, remove any percentage values (leaving other text intact).\n",
    "    df[source_col] = df[source_col].apply(clear_percentage)\n",
    "    \n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "def filter_numeric_costs(df, col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame and column name, this function extracts the numeric cost from each cell,\n",
    "    converting values with an optional '$' sign (and possible commas) to floats.\n",
    "    If a valid numeric cost cannot be extracted, the cell is set to NaN.\n",
    "    \n",
    "    Parameters:\n",
    "      df  : pandas DataFrame.\n",
    "      col : string, the name of the column to process.\n",
    "      \n",
    "    Returns:\n",
    "      The original DataFrame with the specified column converted to numeric values (or NaN if conversion fails).\n",
    "    \"\"\"\n",
    "    def extract_numeric(value):\n",
    "        value_str = str(value)\n",
    "        # This regex matches an optional '$', optional spaces, and a number with commas and an optional decimal part.\n",
    "        match = re.search(r'\\$?\\s*([\\d,]+(?:\\.\\d+)?)', value_str)\n",
    "        if match:\n",
    "            num_str = match.group(1).replace(',', '')\n",
    "            try:\n",
    "                return float(num_str)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    # Apply the extraction function to the specified column.\n",
    "    df[col] = df[col].apply(extract_numeric)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def extract_months_values(df, col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame and column name, this function extracts text patterns matching\n",
    "    durations expressed in months (e.g., \"32 months\", \"43 Month\", \"23 Months\", \"22Months\", or \"21-29months\").\n",
    "    If a valid pattern is found, it returns the matched text; otherwise, it returns an empty string.\n",
    "    \n",
    "    Parameters:\n",
    "        df  : pandas DataFrame.\n",
    "        col : string, the name of the column to process.\n",
    "        \n",
    "    Returns:\n",
    "        The DataFrame with the specified column updated.\n",
    "    \"\"\"\n",
    "    def extract_months(text):\n",
    "        text = str(text)\n",
    "        # Pattern explanation:\n",
    "        #   \\d+          : one or more digits\n",
    "        #   (?:-\\d+)?    : optionally, a hyphen followed by one or more digits (to capture ranges like 21-29)\n",
    "        #   \\s*          : optional whitespace\n",
    "        #   [Mm]onths?   : \"month\" or \"months\" (case insensitive for the first letter)\n",
    "        pattern = r'(\\d+(?:-\\d+)?\\s*[Mm]onths?)'\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group(1) if match else \"\"\n",
    "    \n",
    "    df[col] = df[col].apply(extract_months)\n",
    "    return df\n",
    "\n",
    "def move_months_values(df, source_col, target_col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame, this function extracts text patterns matching durations expressed in months\n",
    "    (e.g., \"32 months\", \"43 Month\", \"23 Months\", \"22Months\", or \"21-29months\") from the source column,\n",
    "    moves the extracted text to the target column, and removes it from the source column.\n",
    "    \n",
    "    Parameters:\n",
    "        df         : pandas DataFrame.\n",
    "        source_col : string, the name of the column to extract the month text from.\n",
    "        target_col : string, the name of the column where the extracted month text will be moved.\n",
    "        \n",
    "    Returns:\n",
    "        The updated DataFrame with the month values moved.\n",
    "    \"\"\"\n",
    "    # Pattern explanation:\n",
    "    #   \\d+          : one or more digits\n",
    "    #   (?:-\\d+)?    : optionally, a hyphen and one or more digits (to capture ranges like 21-29)\n",
    "    #   \\s*          : optional whitespace\n",
    "    #   [Mm]onths?   : \"month\" or \"months\" (case insensitive for the first letter)\n",
    "    pattern = r'(\\d+(?:-\\d+)?\\s*[Mm]onths?)'\n",
    "    \n",
    "    def process_text(text):\n",
    "        text = str(text)\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            extracted = match.group(1)\n",
    "            # Remove the extracted text from the source text and clean up extra spaces\n",
    "            updated_text = re.sub(pattern, \"\", text).strip()\n",
    "            return extracted, updated_text\n",
    "        else:\n",
    "            return \"\", text\n",
    "\n",
    "    # Prepare lists to store the extracted month text and the updated source text\n",
    "    extracted_vals = []\n",
    "    updated_source_vals = []\n",
    "    \n",
    "    for val in df[source_col]:\n",
    "        ext, updated = process_text(val)\n",
    "        extracted_vals.append(ext)\n",
    "        updated_source_vals.append(updated)\n",
    "    \n",
    "    # Create/update the target column with the extracted month text\n",
    "    df[target_col] = extracted_vals\n",
    "    # Replace the source column values with the text after removal of the month text\n",
    "    df[source_col] = updated_source_vals\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Filter numeric costs in 'estimated_cost_x_1000' and 'escalated_cost_x_1000' columns\n",
    "df = filter_numeric_costs(df, 'unnamed_11')\n",
    "\n",
    "#df = filter_numeric_costs(df, 'unnamed_10')\n",
    "\n",
    "df = filter_numeric_costs(df, 'estimated')\n",
    "\n",
    "df = filter_numeric_costs(df, 'estimated cost x')\n",
    "\n",
    "\n",
    "df = extract_months_values(df, 'estimated_1')\n",
    "df = move_months_values(df, 'unnamed_13', 'estimated time to construct')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['cost_allocation_factor']= None\n",
    "\n",
    "\n",
    "df = extract_cost_allocation(df, \"unnamed_8\", \"cost_allocation_factor\")\n",
    "\n",
    "df = extract_cost_allocation(df, \"unnamed_9\", \"cost_allocation_factor\")\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'unnamed_9' in df.columns:\n",
    " #  df['unnamed_9'] = df['unnamed_9'].apply(extract_non_numeric_text)\n",
    "  # df['unnamed_9'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "\n",
    "\n",
    "#if 'unnamed_8' in df.columns:\n",
    " #  df['unnamed_8'] = df['unnamed_8'].apply(extract_non_numeric_text)\n",
    "  # df['unnamed_8'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "             \"unnamed_4\",\n",
    "             \"eastern area sp\",\n",
    "            ],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"MW\",\n",
    "            \n",
    "        ],   \n",
    "\n",
    "        \"description\": [\"description\",\n",
    "                         \"unnamed_6\" ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [ \n",
    "            \n",
    "            \"estimated time to construct\", \"6 month\", \"unnamed_17\",\n",
    "                                         \"84 month\", \"24 month\", \"48 month\", \"estimated_1\",\"60 month\",\"12 month\", \n",
    "                                         \"3648 month\"\n",
    "                                         ],\n",
    "\n",
    "        \"type_of_upgrade_2\": [\"unnamed_1\", \"delivery network upgrade\", \"reliability network upgrade\",],\n",
    "\n",
    "\n",
    "\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \n",
    "\n",
    "            \"estimated cost x 1000\",\n",
    "            \"estimated_cost\",\n",
    "            \"estimated cost\",\n",
    "            \"estimated\",\n",
    "            \"estimated cost x\",\n",
    "            \n",
    "            \"estimated cost x 1000 constant dollar_1\",\n",
    "            \"estimated cost x 1000 constant dollar\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"estimated cost x 1000 constant dollar_1\",\n",
    "            \"estimated cost x\",\n",
    "            \"allocated_cost\",\n",
    "            \"assigned cost\",\n",
    "            \"allocated cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "           \n",
    "            \"unnamed_11\",\n",
    "           \n",
    "\n",
    "             \n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \n",
    "            \"escalated cost x 1000 constant dollar\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"assigned cost escalated\",\n",
    "              \"unnamed_10\",\n",
    "             \"unnamed_13\",\n",
    "            \n",
    "            \n",
    "             \n",
    "\n",
    "        ],\n",
    "\n",
    "         \n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "       \n",
    "         \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost_allocation_factor\",\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocation\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "            \"project allocation\",\n",
    "            \"percent allocation\",\n",
    "            \"unnamed_8\",\n",
    "            \"unnamed_7\",\n",
    "            \n",
    "           \n",
    "\n",
    "        ],\n",
    "       \n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "#if 'upgrade' in df.columns:\n",
    " #   df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    "\n",
    "#df.drop(['unnamed_3', 'unnamed_15', 'unnamed_18', 'unnamed_16', 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "df.drop(['unnamed_3','unnamed_5', 'unnamed_8', 'unnamed_9','unnamed_12', 'unnamed_14', 'unnamed_15', 'unnamed_16','unnamed_18','unnamed_19' , \"14 affected system\",'see prior study','type of'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "#df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "\n",
    " \n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Processes the DataFrame as follows:\n",
    "    \n",
    "    1. Drops any rows where any of these columns are empty or blank:\n",
    "       - 'upgrade', 'description', 'cost_allocation_factor',\n",
    "         'estimated_time_to_construct', 'type_of_upgrade_2', 'estimated_cost_x_1000'\n",
    "    \n",
    "    2. For each remaining row, if the value in 'type_of_upgrade' starts with\n",
    "       'SCE', 'SDG&E', or 'PG&E' (or is empty after stripping),\n",
    "       then the value in 'type_of_upgrade_2' is replaced with the value from 'type_of_upgrade'.\n",
    "       \n",
    "    Parameters:\n",
    "        df: pandas DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        A cleaned DataFrame with the above processing applied.\n",
    "    \"\"\"\n",
    "    # Define the required columns\n",
    "    required_cols = [\n",
    "        \"upgrade\", \"description\", \"cost_allocation_factor\",\n",
    "        \"estimated_time_to_construct\", \"type_of_upgrade_2\", \"estimated_cost_x_1000\"\n",
    "    ]\n",
    "    \n",
    "       # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Replace NaN with empty strings for checking emptiness\n",
    "    df_clean[required_cols] = df_clean[required_cols].fillna(\"\")\n",
    "\n",
    "    # Convert all required columns to strings and strip whitespace\n",
    "    df_clean[required_cols] = df_clean[required_cols].applymap(lambda x: str(x).strip())\n",
    "    \n",
    "    \n",
    " # Drop rows where all required columns are empty\n",
    "    df_clean = df_clean[~(df_clean[required_cols].apply(lambda row: all(row == \"\"), axis=1))]\n",
    "    \n",
    " \n",
    "    \n",
    "    # Define a function to update type_of_upgrade_2 if needed.\n",
    "    def update_type(row):\n",
    "        # Get the value from type_of_upgrade (converted to string and stripped)\n",
    "        val = str(row.get(\"type_of_upgrade\", \"\")).strip()\n",
    "        # If the value is empty or starts with SCE, SDG&E, or PG&E, then update type_of_upgrade_2\n",
    "        if val == \"\" or re.match(r'^(SCE|SDG&E|PG&E)', val):\n",
    "            row[\"type_of_upgrade\"] = row[\"type_of_upgrade_2\"]\n",
    "        return row\n",
    "\n",
    "\n",
    "    # Apply the function row-wise\n",
    "    df_clean = df_clean.apply(update_type, axis=1)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "df = process_dataframe(df)\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"type_of_upgrade_2\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "\n",
    "df = reorder_columns(df)\n",
    "\n",
    "df= df[df['q_id']!= '667']\n",
    " \n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].replace(\"\", np.nan).ffill() \n",
    "\n",
    "\n",
    "df= df[df['type_of_upgrade']!= '12. Local Furnishing Bonds']\n",
    "df= df[df['type_of_upgrade']!= '(when applicable):']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/03_raw/cluster_4_style_R.csv', index=False)\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "df['tot'] = df.apply(\n",
    "    lambda row: 'yes' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) \n",
    "        ) else 'no',\n",
    "    axis=1\n",
    ") \n",
    "\n",
    "# Now extract ONLY \"Total\" rows with a foolproof match\n",
    "total_rows_df = df[df['tot'] == 'yes']\n",
    "\n",
    "total_rows_df = total_rows_df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "total_rows_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/02_intermediate/costs_phase_2_cluster_4_style_R_total_network.csv', index=False) \n",
    "df = df[df['type_of_upgrade'].str.strip().str.lower() != 'total']\n",
    "df = df[df['type_of_upgrade'].str.strip().str.lower() != 'total cost']\n",
    " \n",
    "df.drop('tot', axis=1, inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)  \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    \"PTO’s Interconnection Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTO’s Interconnectio n Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTOs Interconnection Facilities\": \"PTO_IF\",\n",
    "    \"PTOs Interconnectio n Facilities\": \"PTO_IF\",\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Delivery Network\": \"ADNU\",\n",
    " \"Plan of Service Reliability Network Upgrades\": \"RNU\",\n",
    " \"Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Reliability Network Upgrades\": \"RNU\",\n",
    " \"SDG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Upgrades\": \"LDNU\",\n",
    " \"SCE Distribution Upgrades\": \"LDNU\",\n",
    " \"SCE Reliability Network Upgrades for Short Circuit duty\": \"RNU\",\n",
    " \"SCE Network Upgrades\": \"RNU\",\n",
    " \"Plan of Service Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Upgrades, Estimated Costs, and Estimated Time to Construct Summary for C565 - Continued\": \"LDNU\",\n",
    " \"Upgrades, Estimated Costs, and Estimated Time to Construct Summary for C565 -\": \"LDNU\",\n",
    " \"Reliability Network Upgrades to Physically Interconnect\": \"RNU\",\n",
    "\n",
    " \"Reliability Network Upgrades\": \"RNU\",\n",
    "    \"Local Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Area Deliverability Upgrades\": \"ADNU\",\n",
    "    \"Escalated Cost and Time to Construct for Interconnection Facilities, Reliability Network Upgrades, and Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Distribution\": \"ADNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "#df = stable_sort_by_type_of_upgrade(df)  \n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = df[df[\"type_of_upgrade\"] != \"may\"]    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ( (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    # 1) Diagnose\n",
    "    dups = df.columns[df.columns.duplicated()]\n",
    "    print(\"👉 Duplicate column names in df:\", dups.tolist())\n",
    "\n",
    "    # 2) Drop perfect duplicates\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    \"\"\"\n",
    "    Removes the word 'month' or 'months' (case insensitive) from the value.\n",
    "    Leaves behind any numbers or number ranges (e.g. \"6\", \"6-12\").\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Remove 'month' or 'months' (case-insensitive), optionally with spaces around them.\n",
    "        cleaned_value = re.sub(r'(?i)\\s*months?\\s*', '', value)\n",
    "        \n",
    "        return cleaned_value.strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Then apply it to your column, for example with Pandas:\n",
    "df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "         \n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "if 'upgrade' in df.columns:\n",
    "    df['upgrade'] = df['upgrade'].ffill()      \n",
    "\n",
    "\n",
    "df.drop('type_of_upgrade_2', axis=1, inplace=True, errors='ignore') \n",
    "\n",
    "#df= reorder_columns(df)\n",
    "\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/02_intermediate/costs_phase_2_cluster_4_style_R_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/02_intermediate/costs_phase_2_cluster_4_style_R_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_4_style_R_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_4_style_R_total.csv'.\")\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/03_raw/ph2_rawdata_cluster4_style_R_addendums.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"escalated cost x 1000 (note 1)\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"estimated cost x 1000 escalated without itcca (note 1)\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"estimated cost x 1000 escalated (note 1)\",\n",
    "            \"escalated costs x 1000 (note 1)\",\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"estimated cost x 1000\",\n",
    "            \"allocated_cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "            \"estimated_time_months_to_construct_note_1\",\n",
    "            \"estimated time (months) to construct (note 1)\",\n",
    "            \"estimated time (months) to construct (note 2)\",\n",
    "            \"estimated time (months to construct) (note 2)\",\n",
    "            \"estimated time to construct (note 2)\",\n",
    "            \"estimated time to construct (note 3)\",\n",
    "            \"estimated time (months) to execute (note 2)\",\n",
    "            \"estimated time (months) to execute(note 2)\",\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "#df.drop('incremental deliverability', axis=1, inplace=True)\n",
    "#df.drop('dependent system upgrade', axis=1, inplace=True)\n",
    "\n",
    "#STEP 2: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    " \n",
    "\n",
    "\n",
    "required_columns = ['type_of_upgrade', 'cost_allocation_factor']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "\n",
    "df = df[\n",
    "    ~df['type_of_upgrade'].str.contains(r'Precursor Network Upgrades \\(PNU\\)|Estimated in Service Date', na=False)\n",
    "]\n",
    " \n",
    " \n",
    "# Step 3: Rename 'Grand Total' to 'Total' in total_estimated_cost_x_1000\n",
    "if 'total_estimated_cost_x_1000' in df.columns:\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Grand Total', 'Total')\n",
    "\n",
    "# Step 4: Move 'Total' from total_estimated_cost_x_1000 to cost_allocation_factor\n",
    "if 'total_estimated_cost_x_1000' in df.columns and 'cost_allocation_factor' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: 'Total' if str(row['total_estimated_cost_x_1000']) == 'Total' else row['cost_allocation_factor'],\n",
    "        axis=1\n",
    "    )\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Total', None)\n",
    "\n",
    "df = df[df[\"description\"] != \"Total Allocated\"]    \n",
    "\n",
    "if 'description' in df.columns and 'cost_allocation_factor' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: 'Total' if str(row['description']) == 'Total' else row['cost_allocation_factor'],\n",
    "        axis=1\n",
    "    )\n",
    "    df['description'] = df['description'].replace('Total', None)\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "   \n",
    "#df.drop(['interconnection_facility_element', 'cost_subject_to_itcc',\t'total_cost_excluding_itcc_note_1'], axis=1, inplace=True)\n",
    "\n",
    "# Step 4: Clean the type of upgrade column\n",
    "   \n",
    " \n",
    "df['type_of_upgrade'] = (\n",
    "    df['type_of_upgrade']\n",
    "    .fillna('')  # Temporarily replace NaN with an empty string\n",
    "    .str.replace(r'\\(Note \\d+\\)', '', regex=True)  # Remove (Note digit)\n",
    "    .str.strip()  # Strip leading/trailing whitespace\n",
    "    .str.title()  # Capitalize the first letter of each word\n",
    "    .str.replace(r'Upgrades$', 'Upgrade', regex=True)  # Fix plural endings\n",
    "    .replace('', pd.NA)  # Convert empty strings back to NaN\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'Reliability Network Upgrade': 'RNU',\n",
    "    'Reliability Network Upgrades': 'RNU',\n",
    "    'Local Delivery Network Upgrade': 'LDNU',\n",
    "    'Local Delivery Network': 'LDNU',\n",
    "    'Potential Local Delivery Network Upgrade': 'LDNU',\n",
    "    \"Ptos Interconnection Facilities\": 'PTO_IF',\n",
    "    'Area Delivery Network Upgrade': 'ADNU',\n",
    "    'Reliability Network Upgrade To Physically Interconnect': 'RNU',\n",
    "    \"Reliability Network upgrade To Physically Interconnect\": \"RNU\",\n",
    "    'Escalated Cost And Time To Construct For Reliability Network Upgrade': 'RNU',\n",
    "     'Pto': 'PTO_IF',\n",
    "    \"Other Potential Network Upgrade\": \"OPNU\",\n",
    "    \"Conditionally Assigned Network Upgrade\": \"CANU\",\n",
    "    \"Canus\": \"CANU\",\n",
    "        \"Escalated Cost And Time To Construct For Reliability Network Upgrade4\": \"RNU\",\n",
    "    \"Escalated Cost And Time To Construct For Reliability Network Upgrade3\": \"RNU\",\n",
    "    \"Local Off-Peak Network Upgrade\": \"LOPNU\",\n",
    "    'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Canu': 'CANU',\n",
    "  \n",
    " 'Total ADNU': 'ADNU',\n",
    "  'Ptos Interconnect Ion Facilities' : 'PTO_IF',\n",
    "  'Local Off- Peak Network Upgrade': 'LOPNU',\n",
    " 'P Os Interconnection Facilities': 'PTO_IF',\n",
    "  \n",
    "\n",
    " \n",
    "}\n",
    "\n",
    "\n",
    "#Step 4: Apply mapping and ffill type of upgrade column\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()  \n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# build a boolean mask: True for rows where cost contains \"(…)\"\n",
    "#has_paren = df['estimated_cost_x_1000'].astype(str).str.contains(r'\\([^)]*\\)')\n",
    "# drop those rows\n",
    "#df = df.loc[~has_paren].copy()\n",
    "\n",
    "def drop_rows_with_star_in_costs(df, cost_cols):\n",
    "    \"\"\"\n",
    "    Remove any row where any of the cost_cols contains a '*' character.\n",
    "    \"\"\"\n",
    "    mask = pd.Series(False, index=df.index)\n",
    "    for col in cost_cols:\n",
    "        if col in df.columns:\n",
    "            # star anywhere in the string\n",
    "            mask = mask | df[col].astype(str).str.contains(r\"\\*\", regex=True)\n",
    "    # keep only rows without a star\n",
    "    return df.loc[~mask].reset_index(drop=True)\n",
    "\n",
    "def drop_rows_with_dash_in_time(df, time_col):\n",
    "    \"\"\"\n",
    "    Remove any row where the time_col contains a dash '-' (e.g. '-').\n",
    "    \"\"\"\n",
    "    if time_col in df.columns:\n",
    "        mask = df[time_col].astype(str).str.contains(r\"^-+$\", regex=True)\n",
    "        return df.loc[~mask].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def keep_second_entry_in_cells(df, columns):\n",
    "    \"\"\"\n",
    "    For each column in columns, if the cell contains multiple space‑separated entries,\n",
    "    keep only the second one.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        def pick_second(cell):\n",
    "            parts = re.findall(r\"[\\d\\.\\$%,]+\", str(cell))\n",
    "            return parts[1] if len(parts) > 1 else (parts[0] if parts else cell)\n",
    "        df[col] = df[col].apply(pick_second)\n",
    "    return df\n",
    "\n",
    "# ── Integration ──\n",
    "# Place this just before your Step 7 clean_currency block:\n",
    "\n",
    "# 1) drop any row where estimated or escalated cost has '*'\n",
    " \n",
    "\n",
    "# 2) drop any row where estimated_time_to_construct is just a dash\n",
    "df = drop_rows_with_dash_in_time(df, 'estimated_time_to_construct')\n",
    "\n",
    "# 3) if multiple entries exist in a cell, keep only the second\n",
    " \n",
    "# Now proceed with your clean_currency step…\n",
    "\n",
    "\n",
    "    \n",
    "# Step 4: Remove $ signs and convert to numeric\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "         \n",
    "    try:\n",
    "        return value\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "df['cost_allocation_factor'] = df['cost_allocation_factor'].apply(clean_currency)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000', 'adnu_cost_rate_x_1000_escalated']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated',   'adnu_cost_rate_x_1000_escalated']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "                # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = group[\n",
    "            (group['type_of_upgrade'] == upgrade) & (group['item'] == 'no')\n",
    "        ].shape[0] > 0\n",
    "        \n",
    "        if total_exists:\n",
    "             \n",
    "            continue\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "        if not total_exists:\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate specified columns from the existing row\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns (single row, so it remains the same)\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate the specified columns from the first row in the group\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 4: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "# Step 9: Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()\n",
    "\n",
    "if 'upgrade' in df.columns and 'type_of_upgrade' in df.columns and 'q_id' in df.columns:\n",
    "    df['upgrade'] = df.groupby(['q_id', 'type_of_upgrade'])['upgrade'].ffill()\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "# build a mask of exactly the rows you want to drop\n",
    "mask = (\n",
    "    (df['type_of_upgrade'] == 'PTO_IF') &\n",
    "    (df['upgrade'] == 'None') &\n",
    "    (df['description'] == 'None') &\n",
    "    (df['cost_allocation_factor'] == 0) &\n",
    "    (df['estimated_cost_x_1000'] == 0) &\n",
    "    (df['escalated_cost_x_1000'] == 0) &\n",
    "    (df['estimated_time_to_construct'] == 0)\n",
    ")\n",
    "\n",
    " \n",
    "\n",
    "# if you want to do it in-place instead:\n",
    "df.drop(df[mask].index, inplace=True)\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/02_intermediate/costs_phase_2_cluster_4_style_R_itemized_addendums.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/02_intermediate/costs_phase_2_cluster_4_style_R_total_addendums.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_4_style_R_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_4_style_R_total.csv'.\")\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge- Complete replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Load a CSV file and ensure specific columns are treated as character.\n",
    "    \"\"\"\n",
    "    available_columns = pd.read_csv(file_path, nrows=0).columns\n",
    "    char_cols = [c for c in char_columns if c in available_columns]\n",
    "    return pd.read_csv(\n",
    "        file_path,\n",
    "        dtype={c: str for c in char_cols},\n",
    "        na_values=[], \n",
    "        keep_default_na=False\n",
    "    )\n",
    "\n",
    "def save_data(df, file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Save a dataframe to CSV, forcing certain columns to string.\n",
    "    \"\"\"\n",
    "    for col in char_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def merge_with_addendums(itemized, itemized_addendums, total, total_addendums):\n",
    "    # mark originals & keep row order\n",
    "    itemized['original'] = \"yes\"\n",
    "    total['original']    = \"yes\"\n",
    "    itemized['row_order'] = itemized.index\n",
    "    total['row_order']    = total.index\n",
    "\n",
    "    # ensure numeric q_id\n",
    "    for df in (itemized, itemized_addendums, total, total_addendums):\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors=\"coerce\")\n",
    "\n",
    "    conditional_columns = [\n",
    "        \"req_deliverability\",\"latitude\",\"longitude\",\n",
    "        \"capacity\",\"point_of_interconnection\"\n",
    "    ]\n",
    "\n",
    "    # --- ITEMIZED: replace only matching (q_id, type_of_upgrade) blocks ---\n",
    "    updated_itemized_rows = []\n",
    "    # iterate over each unique (q_id, type_of_upgrade) in the addendums\n",
    "    for q, t in itemized_addendums[['q_id','type_of_upgrade']].drop_duplicates().itertuples(index=False):\n",
    "        adds = itemized_addendums[\n",
    "            (itemized_addendums['q_id'] == q) &\n",
    "            (itemized_addendums['type_of_upgrade'] == t)\n",
    "        ].reset_index(drop=True)\n",
    "        orig = itemized[\n",
    "            (itemized['q_id'] == q) &\n",
    "            (itemized['type_of_upgrade'] == t)\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # combine conditional columns\n",
    "        for col in conditional_columns:\n",
    "            if col in adds.columns and col in orig.columns:\n",
    "                adds[col] = (\n",
    "                    adds[col].replace(\"\", pd.NA)\n",
    "                              .combine_first(orig[col])\n",
    "                              .fillna(\"\")\n",
    "                )\n",
    "\n",
    "        # carry over or pad row_order\n",
    "        if 'row_order' in orig:\n",
    "            ro = orig['row_order'].tolist()\n",
    "            if len(ro) < len(adds):\n",
    "                ro += [pd.NA] * (len(adds) - len(ro))\n",
    "        else:\n",
    "            ro = [pd.NA] * len(adds)\n",
    "\n",
    "        adds = adds.assign(original=\"no\", row_order=ro[:len(adds)])\n",
    "\n",
    "        # drop only those matching (q_id, type_of_upgrade) from the master\n",
    "        itemized = itemized[\n",
    "            ~((itemized['q_id'] == q) & (itemized['type_of_upgrade'] == t))\n",
    "        ]\n",
    "\n",
    "        updated_itemized_rows.append(adds)\n",
    "\n",
    "    # stitch back untouched originals + updated blocks\n",
    "    updated_itemized = pd.concat(\n",
    "        [itemized] + updated_itemized_rows,\n",
    "        ignore_index=True\n",
    "    ) if updated_itemized_rows else itemized.copy()\n",
    "\n",
    "    updated_itemized['row_order'] = updated_itemized['row_order'].fillna(-1).astype(int)\n",
    "    updated_itemized = (\n",
    "        updated_itemized\n",
    "        .sort_values('row_order')\n",
    "        .drop(columns='row_order')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # --- TOTAL: per type_of_upgrade (unchanged logic) ---\n",
    "    updated_total_rows = []\n",
    "    for q in total_addendums['q_id'].unique():\n",
    "        for t in total_addendums['type_of_upgrade'].unique():\n",
    "            adds = total_addendums[\n",
    "                (total_addendums['q_id']==q)&\n",
    "                (total_addendums['type_of_upgrade']==t)\n",
    "            ].reset_index(drop=True)\n",
    "            if adds.empty:\n",
    "                continue\n",
    "\n",
    "            mask = (total['q_id']==q)&(total['type_of_upgrade']==t)\n",
    "            orig = total[mask].reset_index(drop=True)\n",
    "            if orig.empty:\n",
    "                orig = pd.DataFrame({'row_order':[pd.NA]*len(adds)}, index=adds.index)\n",
    "\n",
    "            # align lengths\n",
    "            if len(adds) > len(orig):\n",
    "                extra = pd.DataFrame({c: pd.NA for c in orig.columns},\n",
    "                                     index=range(len(adds)-len(orig)))\n",
    "                orig = pd.concat([orig, extra], ignore_index=True)\n",
    "            elif len(adds) < len(orig):\n",
    "                orig = orig.iloc[:len(adds)].reset_index(drop=True)\n",
    "\n",
    "            for col in conditional_columns:\n",
    "                if col in adds.columns and col in orig.columns:\n",
    "                    adds[col] = (\n",
    "                        adds[col].replace(\"\", pd.NA)\n",
    "                                  .combine_first(orig[col])\n",
    "                                  .fillna(\"\")\n",
    "                    )\n",
    "\n",
    "            total.loc[mask, 'original'] = \"no\"\n",
    "            updated_total_rows.append(\n",
    "                adds.assign(original=\"no\", row_order=orig['row_order'].tolist()[:len(adds)])\n",
    "            )\n",
    "            total = total[~mask]\n",
    "\n",
    "    updated_total = pd.concat([total] + updated_total_rows, ignore_index=True) \\\n",
    "                    if updated_total_rows else total.copy()\n",
    "    updated_total['row_order'] = updated_total['row_order'].fillna(-1).astype(int)\n",
    "    updated_total = (\n",
    "        updated_total\n",
    "        .sort_values('row_order')\n",
    "        .drop(columns='row_order')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # move 'original' to end\n",
    "    def move_last(df):\n",
    "        cols = [c for c in df.columns if c!='original'] + ['original']\n",
    "        return df[cols]\n",
    "\n",
    "    return move_last(updated_itemized), move_last(updated_total)\n",
    "\n",
    "\n",
    "# ── main script ──\n",
    "\n",
    "char_columns = [\n",
    "    \"req_deliverability\",\"point_of_interconnection\",\"type_of_upgrade\",\n",
    "    \"upgrade\",\"description\",\"estimated_time_to_construct\",\"original\",\"item\"\n",
    "]\n",
    "\n",
    "itemized = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 4/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_4_style_R_itemized.csv\",\n",
    "    char_columns\n",
    ")\n",
    "itemized_addendums = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 4/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_4_style_R_itemized_addendums.csv\",\n",
    "    char_columns\n",
    ")\n",
    "total = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 4/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_4_style_R_total.csv\",\n",
    "    char_columns\n",
    ")\n",
    "total_addendums = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 4/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_4_style_R_total_addendums.csv\",\n",
    "    char_columns\n",
    ")\n",
    "\n",
    "updated_itemized, updated_total = merge_with_addendums(\n",
    "    itemized, itemized_addendums, total, total_addendums\n",
    ")\n",
    "\n",
    "# drop unwanted columns\n",
    "to_drop = [\n",
    "    \"upgrade_classification\",\"estimated\",\"caiso_queue\",\n",
    "    \"project_type\",\"dependent_system_upgrade\"\n",
    "]\n",
    "updated_itemized = updated_itemized.drop(columns=[c for c in to_drop if c in updated_itemized], errors='ignore')\n",
    "updated_total   = updated_total.drop(columns=[c for c in to_drop if c in updated_total],   errors='ignore')\n",
    "\n",
    "# fill & sort\n",
    "fill_cols = [\n",
    "    \"point_of_interconnection\",\"latitude\",\"longitude\",\n",
    "    \"req_deliverability\",\"capacity\"\n",
    "]\n",
    "for df in (updated_itemized, updated_total):\n",
    "    for c in fill_cols:\n",
    "        df[c] = df[c].replace('', np.nan)\n",
    "    df.sort_values('q_id', kind='stable', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    for c in fill_cols:\n",
    "        df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
    "    for c in fill_cols:\n",
    "        df[c].replace(np.nan, '', inplace=True)\n",
    "\n",
    "# save\n",
    "save_data(\n",
    "    updated_itemized,\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 4/01_clean/\"\n",
    "    \"costs_phase_2_cluster_4_style_R_itemized_updated.csv\",\n",
    "    char_columns\n",
    ")\n",
    "save_data(\n",
    "    updated_total,\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 4/01_clean/\"\n",
    "    \"costs_phase_2_cluster_4_style_R_total_updated.csv\",\n",
    "    char_columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Scraped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orignals only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded itemized data from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4//02_intermediate/costs_phase_2_cluster_4_style_R_itemized.csv\n",
      "Loaded totals data from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4//02_intermediate/costs_phase_2_cluster_4_style_R_total.csv\n",
      "\n",
      "Q_ids with missing upgrades:\n",
      "  Q_id 667 is missing upgrades: ADNU\n",
      "  Q_id 789 is missing upgrades: ADNU\n",
      "  Q_id 794 is missing upgrades: ADNU\n",
      "  Q_id 837 is missing upgrades: ADNU\n",
      "  Q_id 838 is missing upgrades: ADNU\n",
      "\n",
      "Mismatches saved to '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/mismatches.csv'.\n",
      "\n",
      "Direct Matches (Exact Point of Interconnection Names):\n",
      "                         point_of_interconnection        q_id\n",
      "3  230 KV BUS AT THE PROPOSED NEW IMPERIAL VALLEY  [667, 838]\n",
      "\n",
      "Fuzzy Matches (>=80% Similarity in Point of Interconnection):\n",
      "                       point_of_interconnection_1     q_ids_1  \\\n",
      "0  230 KV BUS AT THE PROPOSED NEW IMPERIAL VALLEY  [667, 838]   \n",
      "1          69 KV BUS AT BOULEVARD EAST SUBSTATION       [789]   \n",
      "2          69 KV BUS AT BOULEVARD EAST SUBSTATION       [789]   \n",
      "3              138 KV BUS AT BOULEVARD SUBSTATION       [794]   \n",
      "\n",
      "                 point_of_interconnection_2 q_ids_2  similarity_score  \n",
      "0  230 KV BUS AT IMPERIAL VALLEY SUBSTATION   [838]                84  \n",
      "1        138 KV BUS AT BOULEVARD SUBSTATION   [794]                94  \n",
      "2      138 KV BUS AT EAST COUNTY SUBSTATION   [837]                82  \n",
      "3      138 KV BUS AT EAST COUNTY SUBSTATION   [837]                83  \n",
      "Matched Q_ids saved to '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/matched_qids.csv'.\n",
      "\n",
      "Total checks performed: 15\n",
      "Total mismatches found: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "# Paths to the CSV files\n",
    "ITEMIZED_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4//02_intermediate/costs_phase_2_cluster_4_style_R_itemized.csv'\n",
    "TOTALS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4//02_intermediate/costs_phase_2_cluster_4_style_R_total.csv'\n",
    "\n",
    "# Columns in totals_df that hold the reported total costs\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "# Upgrade types to check\n",
    "REQUIRED_UPGRADES = ['PTO_IF', 'RNU', 'LDNU', 'ADNU']\n",
    "\n",
    "# Output paths\n",
    "MISMATCHES_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/mismatches.csv'\n",
    "MATCHED_QIDS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/matched_qids.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "def load_csv(path, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {dataset_name} from {path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Load datasets\n",
    "itemized_df = load_csv(ITEMIZED_CSV_PATH, \"itemized data\")\n",
    "totals_df = load_csv(TOTALS_CSV_PATH, \"totals data\")\n",
    "\n",
    "# ---------------------- Data Cleaning ---------------------- #\n",
    "\n",
    "def clean_text(df, column):\n",
    "    \"\"\"\n",
    "    Cleans text data by stripping leading/trailing spaces and converting to uppercase.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        df[column] = df[column].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        print(f\"Warning: '{column}' column is missing in the dataset. Filling with 'UNKNOWN'.\")\n",
    "        df[column] = 'UNKNOWN'\n",
    "    return df\n",
    "\n",
    "# Clean 'type_of_upgrade' and 'point_of_interconnection' in both datasets\n",
    "itemized_df = clean_text(itemized_df, 'type_of_upgrade')\n",
    "itemized_df = clean_text(itemized_df, 'point_of_interconnection')\n",
    "\n",
    "totals_df = clean_text(totals_df, 'type_of_upgrade')\n",
    "totals_df = clean_text(totals_df, 'point_of_interconnection')\n",
    "\n",
    "# ---------------------- Data Preparation ---------------------- #\n",
    "\n",
    "# Ensure necessary columns exist in itemized_df\n",
    "required_itemized_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', 'estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in required_itemized_columns:\n",
    "    if col not in itemized_df.columns:\n",
    "        print(f\"Warning: '{col}' column is missing in the itemized dataset.\")\n",
    "        if col in ['q_id', 'type_of_upgrade', 'point_of_interconnection']:\n",
    "            itemized_df[col] = 'UNKNOWN'\n",
    "        else:\n",
    "            itemized_df[col] = 0\n",
    "\n",
    "# Ensure necessary columns exist in totals_df\n",
    "required_totals_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in required_totals_columns:\n",
    "    if col not in totals_df.columns:\n",
    "        print(f\"Error: '{col}' column is missing in the totals dataset. Cannot proceed.\")\n",
    "        exit(1)\n",
    "\n",
    "# Convert cost columns to numeric, coercing errors to NaN and filling with 0\n",
    "cost_columns_itemized = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in cost_columns_itemized:\n",
    "    itemized_df[col] = pd.to_numeric(itemized_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "cost_columns_totals = [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in cost_columns_totals:\n",
    "    totals_df[col] = pd.to_numeric(totals_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# ---------------------- Calculate Manual Totals ---------------------- #\n",
    "\n",
    "# Group itemized data by q_id and type_of_upgrade and calculate sums\n",
    "itemized_grouped = itemized_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    'estimated_cost_x_1000': 'sum',\n",
    "    'escalated_cost_x_1000': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "itemized_grouped['manual_total'] = itemized_grouped.apply(\n",
    "    lambda row: row['estimated_cost_x_1000'] if row['estimated_cost_x_1000'] > 0 else row['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Prepare Totals Data ---------------------- #\n",
    "\n",
    "# Group totals data by q_id and type_of_upgrade and calculate sums\n",
    "totals_grouped = totals_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "    TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "totals_grouped['reported_total'] = totals_grouped.apply(\n",
    "    lambda row: row[TOTALS_ESTIMATED_COLUMN] if row[TOTALS_ESTIMATED_COLUMN] > 0 else row[TOTALS_ESCALATED_COLUMN],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Merge Data ---------------------- #\n",
    "\n",
    "# Merge the itemized and totals data on q_id and type_of_upgrade\n",
    "comparison_df = pd.merge(\n",
    "    itemized_grouped,\n",
    "    totals_grouped[['q_id', 'type_of_upgrade', 'reported_total']],\n",
    "    on=['q_id', 'type_of_upgrade'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---------------------- Check for Missing Upgrades ---------------------- #\n",
    "\n",
    "# Identify q_ids that are missing any of the required upgrades\n",
    "missing_upgrades_report = []\n",
    "for q_id in comparison_df['q_id'].unique():\n",
    "    upgrades_present = comparison_df[comparison_df['q_id'] == q_id]['type_of_upgrade'].unique()\n",
    "    missing_upgrades = [upgrade for upgrade in REQUIRED_UPGRADES if upgrade not in upgrades_present]\n",
    "    if missing_upgrades:\n",
    "        missing_upgrades_report.append((q_id, missing_upgrades))\n",
    "\n",
    "# Report missing upgrades\n",
    "if missing_upgrades_report:\n",
    "    print(\"\\nQ_ids with missing upgrades:\")\n",
    "    for q_id, missing in missing_upgrades_report:\n",
    "        print(f\"  Q_id {q_id} is missing upgrades: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nAll q_ids have all required upgrades.\")\n",
    "\n",
    "# ---------------------- Compare Totals and Identify Mismatches ---------------------- #\n",
    "\n",
    "# Initialize list to store mismatches\n",
    "mismatches = []\n",
    "\n",
    "# Iterate through each row to compare manual_total with reported_total\n",
    "for index, row in comparison_df.iterrows():\n",
    "    q_id = row['q_id']\n",
    "    upgrade = row['type_of_upgrade']\n",
    "    manual_total = row['manual_total']\n",
    "    reported_total = row['reported_total']\n",
    "    \n",
    "    # Determine if both manual_total and reported_total are zero\n",
    "    if manual_total == 0.0 and reported_total == 0.0:\n",
    "        continue  # No mismatch\n",
    "    # Determine if manual_total is zero and reported_total is missing or zero\n",
    "    elif manual_total == 0.0 and (pd.isna(row['reported_total']) or reported_total == 0.0):\n",
    "        continue  # No mismatch\n",
    "    # If reported_total is missing (NaN) and manual_total is not zero\n",
    "    elif pd.isna(row['reported_total']) and manual_total != 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: Missing\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': 'Missing'\n",
    "        })\n",
    "    # If manual_total is not zero and reported_total is zero\n",
    "    elif manual_total != 0.0 and reported_total == 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: 0.0\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # If both totals are non-zero but differ beyond tolerance\n",
    "    elif abs(manual_total - reported_total) > 1e-2:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: {reported_total}\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # Else, totals match; do nothing\n",
    "\n",
    "# Create a DataFrame for mismatches\n",
    "mismatches_df = pd.DataFrame(mismatches, columns=['q_id', 'type_of_upgrade', 'manual_total', 'reported_total'])\n",
    "\n",
    "# Save mismatches to a CSV file\n",
    "try:\n",
    "    mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "    print(f\"\\nMismatches saved to '{MISMATCHES_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving mismatches CSV: {e}\")\n",
    "\n",
    "# ---------------------- Point of Interconnection Matching ---------------------- #\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from itemized dataset\n",
    "itemized_poi = itemized_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from totals dataset\n",
    "totals_poi = totals_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Merge both to have a complete list of q_id and point_of_interconnection\n",
    "all_poi = pd.concat([itemized_poi, totals_poi]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ---------------------- Direct Match Identification ---------------------- #\n",
    "\n",
    "# Group by point_of_interconnection to find q_ids sharing the same point_of_interconnection\n",
    "direct_matches = all_poi.groupby('point_of_interconnection')['q_id'].apply(list).reset_index()\n",
    "\n",
    "# Filter groups with more than one q_id (i.e., shared points_of_interconnection)\n",
    "direct_matches = direct_matches[direct_matches['q_id'].apply(len) > 1]\n",
    "\n",
    "print(\"\\nDirect Matches (Exact Point of Interconnection Names):\")\n",
    "if not direct_matches.empty:\n",
    "    print(direct_matches)\n",
    "else:\n",
    "    print(\"No direct matches found.\")\n",
    "\n",
    "# ---------------------- Fuzzy Match Identification ---------------------- #\n",
    "\n",
    "# Prepare list of points_of_interconnection for fuzzy matching\n",
    "poi_list = all_poi['point_of_interconnection'].unique().tolist()\n",
    "\n",
    "# Initialize list to store fuzzy matches\n",
    "fuzzy_matches = []\n",
    "\n",
    "# Iterate through each point_of_interconnection to find similar ones\n",
    "for i, poi in enumerate(poi_list):\n",
    "    # Compare with the rest of the points to avoid redundant comparisons\n",
    "    similar_pois = process.extract(poi, poi_list[i+1:], scorer=fuzz.token_set_ratio)\n",
    "    \n",
    "    # Filter matches with similarity >= 80%\n",
    "    for match_poi, score in similar_pois:\n",
    "        if score >= 80:\n",
    "            # Retrieve q_ids for both points_of_interconnection\n",
    "            qids_poi1 = all_poi[all_poi['point_of_interconnection'] == poi]['q_id'].tolist()\n",
    "            qids_poi2 = all_poi[all_poi['point_of_interconnection'] == match_poi]['q_id'].tolist()\n",
    "            \n",
    "            # Append the matched pairs with their points_of_interconnection and similarity score\n",
    "            fuzzy_matches.append({\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_ids_1': qids_poi1,\n",
    "                'point_of_interconnection_2': match_poi,\n",
    "                'q_ids_2': qids_poi2,\n",
    "                'similarity_score': score\n",
    "            })\n",
    "\n",
    "# Convert fuzzy matches to DataFrame\n",
    "fuzzy_matches_df = pd.DataFrame(fuzzy_matches)\n",
    "\n",
    "print(\"\\nFuzzy Matches (>=80% Similarity in Point of Interconnection):\")\n",
    "if not fuzzy_matches_df.empty:\n",
    "    print(fuzzy_matches_df)\n",
    "else:\n",
    "    print(\"No fuzzy matches found.\")\n",
    "\n",
    "# ---------------------- Save Matched Q_ids to CSV ---------------------- #\n",
    "\n",
    "# For clarity, create a combined DataFrame for direct and fuzzy matches\n",
    "\n",
    "# Direct matches: list each pair of q_ids sharing the same point_of_interconnection\n",
    "direct_matches_expanded = []\n",
    "for _, row in direct_matches.iterrows():\n",
    "    qids = row['q_id']\n",
    "    poi = row['point_of_interconnection']\n",
    "    # Generate all possible unique pairs\n",
    "    for i in range(len(qids)):\n",
    "        for j in range(i+1, len(qids)):\n",
    "            direct_matches_expanded.append({\n",
    "                'match_type': 'Direct',\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_id_1': qids[i],\n",
    "                'point_of_interconnection_2': poi,\n",
    "                'q_id_2': qids[j],\n",
    "                'similarity_score': 100\n",
    "            })\n",
    "\n",
    "# Fuzzy matches: already have pairs\n",
    "fuzzy_matches_expanded = []\n",
    "for _, row in fuzzy_matches_df.iterrows():\n",
    "    fuzzy_matches_expanded.append({\n",
    "        'match_type': 'Fuzzy',\n",
    "        'point_of_interconnection_1': row['point_of_interconnection_1'],\n",
    "        'q_id_1': row['q_ids_1'],\n",
    "        'point_of_interconnection_2': row['point_of_interconnection_2'],\n",
    "        'q_id_2': row['q_ids_2'],\n",
    "        'similarity_score': row['similarity_score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "matched_qids_df = pd.DataFrame(direct_matches_expanded + fuzzy_matches_expanded)\n",
    "\n",
    "# Save matched q_ids to CSV\n",
    "try:\n",
    "    matched_qids_df.to_csv(MATCHED_QIDS_CSV_PATH, index=False)\n",
    "    print(f\"Matched Q_ids saved to '{MATCHED_QIDS_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving matched Q_ids CSV: {e}\")\n",
    "\n",
    "# ---------------------- Summary ---------------------- #\n",
    "\n",
    "# Print a summary\n",
    "total_checked = len(comparison_df)\n",
    "total_mismatches = len(mismatches_df)\n",
    "print(f\"\\nTotal checks performed: {total_checked}\")\n",
    "print(f\"Total mismatches found: {total_mismatches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
