{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C7 Style Q- table 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: C7 Ph II Appendix A - Q1010 Dyer Summit Wind Repower.pdf from Project 1010\n",
      "Scraped PDF: C7 Ph II Appendix A - Q1011 GHS Project.pdf from Project 1011\n",
      "Scraped PDF: C7 Ph II Appendix A - Q1014 Paradise Cut.pdf from Project 1014\n",
      "Scraped PDF: C7 Ph II Appendix A - Q1016 Richmond Steam Turbine.pdf from Project 1016\n",
      "Scraped PDF: C7 Ph II Appendix A - Q1021 Stockton Biomass.pdf from Project 1021\n",
      "Skipped Addendum PDF: Q1021_Stockton Biomass_C7PHII_Report Addendum #1.pdf from Project 1021 (No Table 7)\n",
      "Scraped PDF: Q1027 Blackbriar_Appendix A-Individual Report -C7Ph II.pdf from Project 1027\n",
      "Skipped PDF: C7PhII - Attachment 1-Allocation of Network Upgrades for Cost Estimates-Q1028.pdf from Project 1028 (No Table 7)\n",
      "Scraped PDF: Q1028 Little Bear Solar1_Appendix A-Individual Report-C7Ph II.pdf from Project 1028\n",
      "Scraped PDF: Q1029 Little Bear Solar 2_Appendix A-Individual Report-C7Ph II.pdf from Project 1029\n",
      "Skipped PDF: C7PhII - Attachment 1-Allocation of Network Upgrades for Cost Estimates-Q1029.pdf from Project 1029 (No Table 7)\n",
      "Scraped PDF: Q1030 South Lake Solar_Appendix A-Individual Report-C7Ph II.pdf from Project 1030\n",
      "Scraped PDF: Q1031 Stonecrop_Appendix A-Individual Report-C7Ph II.pdf from Project 1031\n",
      "Scraped PDF: Q1032 Tranquillity 8_Appendix A-Individual Report-C7Ph II.pdf from Project 1032\n",
      "Scraped PDF: Q1033 Broken Spoke_Appendix A-Individual Report C7Ph II.pdf from Project 1033\n",
      "Scraped PDF: Q1036 Mustang Two_Appendix A-Individual Report-C7Ph II.pdf from Project 1036\n",
      "Scraped PDF: Q1038 Pandora Solar_Appendix A-Individual Report_C7Ph II.pdf from Project 1038\n",
      "Scraped PDF: 14AS870088-QC7PhII_Q1040_Arlington_Valley_Solar_Energy_II_Addm2toAppendix_A_1032016.pdf from Project 1040\n",
      "Scraped PDF: QC7PhII_Q1040_Arlington Valley Solar Energy II_Appendix A_11-24-2015.pdf from Project 1040\n",
      "Scraped Addendum PDF: QC7PhII_Q1040_Arlington Valley Solar Energy II_Addm1toAppendix A_12-9-2015.pdf from Project 1040\n",
      "Scraped PDF: QC7PhII_Q1045_Chula Vista ERC Appendix A_11-24-2015.pdf from Project 1045\n",
      "Scraped Addendum PDF: QC7PhII_Q1045_Chula Vista ERC_Addm1toAppendix A_12-9-2015.pdf from Project 1045\n",
      "Scraped PDF: QC7PhII_Q1047_El Cajon ERC_Appendix A_11-24-2015.pdf from Project 1047\n",
      "Scraped Addendum PDF: QC7PhII_Q1047_El Cajon ERC_Addm1toAppendix A_12-9-2015.pdf from Project 1047\n",
      "Scraped PDF: QC7PhII_Q1048_Escondido ERC_Appendix A_11-24-2015.pdf from Project 1048\n",
      "Scraped Addendum PDF: QC7PhII_Q1048_Escondido ERC_Addm1toAppendix A_12-9-2015.pdf from Project 1048\n",
      "Scraped PDF: QC7PhII_Q1050_Honey Badger_Appendix A_11-24-2015.pdf from Project 1050\n",
      "Scraped Addendum PDF: QC7PhII_Q1050_Honey Badger_Addm1toAppendix A_12-9-2015.pdf from Project 1050\n",
      "Scraped PDF: QC7PhII_Q1053_Mesquite Solar 3_Appendix A_11-24-2015.pdf from Project 1053\n",
      "Scraped Addendum PDF: QC7PhII_Q1053_Mesquite Solar 3_Addm1toAppendix A_12-9-2015.pdf from Project 1053\n",
      "Scraped PDF: QC7PhII_Q1058_Pio Pico Capacity Increase_Appendix A_11-24-2015.pdf from Project 1058\n",
      "Scraped Addendum PDF: QC7PhII_Q1058_Pio Pico Capacity Increase_Addm1toAppendix A_12-9-2015.pdf from Project 1058\n",
      "Skipped Addendum PDF: QC7PhII_Q1058_Pio Pico Capacity Increase_Addm2_04-08-2016.pdf from Project 1058 (No Table 7)\n",
      "Scraped PDF: QC7PhII_Q1062_White Wing Ranch Solar _Appendix A_11-24-2015.pdf from Project 1062\n",
      "Scraped Addendum PDF: QC7PhII_Q1062_White Wing Ranch Solar_Addm1toAppendix A_1-25-2016.pdf from Project 1062\n",
      "Skipped PDF: QC7PII-SCE-VEA-EOP-Q1064-ARESNevada-Appendix A.pdf from Project 1064 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-VEA-EOP-Q1064-ARESNevada-Appendix A-Attachment 1.pdf from Project 1064 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-EOP-Q1064-ARESNEvada-Appendix A-Attachment 2.pdf from Project 1064 (No Table 7)\n",
      "Skipped PDF: QC7PII-VEA-EOP-Q1064-ARESNevada-Appendix A-Attachment 2.pdf from Project 1064 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-EOP-Q1064-ARESNEvada-Appendix A-Attachment 2a.pdf from Project 1064 (No Table 7)\n",
      "Skipped Addendum PDF: QC7PII-SCE-VEA-EOP-Q1064-Appendix A_Addendum01.pdf from Project 1064 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC7PII-SCE-VEA-EOP-Q1064-Appendix A_Addendum02.pdf from Project 1064 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC7PII-VEA-Q1065-CaballoLoco-Appendix A-Attachment 2.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-VEA-EOP-Q1065-CaballoLoco-Appendix A.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-VEA-EOP-Q1065-CaballoLoco-Appendix A-Attachment 1.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-EOP-Q1065-CaballoLoco-Appendix A-Attachment 2a.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-EOP-Q1065-CaballoLoco-Appendix A-Attachment 2.pdf from Project 1065 (No Table 7)\n",
      "Skipped Addendum PDF: QC7PII-SCE-VEA-EOP-Q1065-Appendix A_Addendum-01.pdf from Project 1065 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC7PII-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2a.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 1.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A.pdf from Project 1066 (No Table 7)\n",
      "Skipped Addendum PDF: QC7PII-SCE-VEA-EOP-Q1066-Appendix A_Addendum01.pdf from Project 1066 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC7PII-SCE-VEA-EOP-Q1066-Appendix A_Addendum-02.pdf from Project 1066 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC7PII-SCE-EOP-Q1069-Apendix A-BoulderSolar-Atchmnt1.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-EOP-Q1069-Appendix A-BoulderSolar.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: C7PII-SCE-EOP-Q1069-Appendix A-BoulderSolar-Attachment 2a.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: C7PII-SCE-EOP-Q1069-Appendix A-BoulderSolar-Attachment 2.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Eastern_Q1070-Jupiter Solar_Appendix A-Report.pdf from Project 1070 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Eastern_Q1070-Jupiter Solar_Attachment 2-Cost.pdf from Project 1070 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Eastern_Q1071-Quartz4Solar_Appendix A_Report.pdf from Project 1071 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Eastern_Q1071-Quartz4Solar_Attachment 2-Cost.pdf from Project 1071 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A-Attachment 2.pdf from Project 1074 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A-Attachment 1.pdf from Project 1074 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A-Attachment 2a.pdf from Project 1074 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A.pdf from Project 1074 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A.pdf from Project 1076 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A-Attachment 2a.pdf from Project 1076 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A-Attachment 2.pdf from Project 1076 (No Table 7)\n",
      "Skipped PDF: QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A-Attachment 1.pdf from Project 1076 (No Table 7)\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/03_raw/ph2_rawdata_cluster7_style_Q_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/03_raw/ph2_rawdata_cluster7_style_Q_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 65\n",
      "Total Projects Scraped: 22\n",
      "Total Projects Skipped: 43\n",
      "Total Projects Missing: 18\n",
      "Total PDFs Accessed: 70\n",
      "Total PDFs Scraped: 31\n",
      "Total PDFs Skipped: 39\n",
      "\n",
      "List of Scraped Projects:\n",
      "[1010, 1011, 1014, 1016, 1021, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1036, 1038, 1040, 1045, 1047, 1048, 1050, 1053, 1058, 1062]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[1007, 1013, 1015, 1018, 1019, 1020, 1023, 1024, 1026, 1035, 1037, 1043, 1046, 1049, 1051, 1052, 1055, 1056, 1057, 1059, 1060, 1061, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1080, 1083, 1084, 1087, 1088, 1089]\n",
      "\n",
      "List of Missing Projects:\n",
      "[1008, 1009, 1012, 1017, 1022, 1025, 1034, 1039, 1041, 1042, 1044, 1054, 1078, 1079, 1081, 1082, 1085, 1086]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['C7 Ph II Appendix A - Q1010 Dyer Summit Wind Repower.pdf', 'C7 Ph II Appendix A - Q1011 GHS Project.pdf', 'C7 Ph II Appendix A - Q1014 Paradise Cut.pdf', 'C7 Ph II Appendix A - Q1016 Richmond Steam Turbine.pdf', 'C7 Ph II Appendix A - Q1021 Stockton Biomass.pdf', 'Q1027 Blackbriar_Appendix A-Individual Report -C7Ph II.pdf', 'Q1028 Little Bear Solar1_Appendix A-Individual Report-C7Ph II.pdf', 'Q1029 Little Bear Solar 2_Appendix A-Individual Report-C7Ph II.pdf', 'Q1030 South Lake Solar_Appendix A-Individual Report-C7Ph II.pdf', 'Q1031 Stonecrop_Appendix A-Individual Report-C7Ph II.pdf', 'Q1032 Tranquillity 8_Appendix A-Individual Report-C7Ph II.pdf', 'Q1033 Broken Spoke_Appendix A-Individual Report C7Ph II.pdf', 'Q1036 Mustang Two_Appendix A-Individual Report-C7Ph II.pdf', 'Q1038 Pandora Solar_Appendix A-Individual Report_C7Ph II.pdf', '14AS870088-QC7PhII_Q1040_Arlington_Valley_Solar_Energy_II_Addm2toAppendix_A_1032016.pdf', 'QC7PhII_Q1040_Arlington Valley Solar Energy II_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1040_Arlington Valley Solar Energy II_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1045_Chula Vista ERC Appendix A_11-24-2015.pdf', 'QC7PhII_Q1045_Chula Vista ERC_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1047_El Cajon ERC_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1047_El Cajon ERC_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1048_Escondido ERC_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1048_Escondido ERC_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1050_Honey Badger_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1050_Honey Badger_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1053_Mesquite Solar 3_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1053_Mesquite Solar 3_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1058_Pio Pico Capacity Increase_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1058_Pio Pico Capacity Increase_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1062_White Wing Ranch Solar _Appendix A_11-24-2015.pdf', 'QC7PhII_Q1062_White Wing Ranch Solar_Addm1toAppendix A_1-25-2016.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['Q1021_Stockton Biomass_C7PHII_Report Addendum #1.pdf', 'C7PhII - Attachment 1-Allocation of Network Upgrades for Cost Estimates-Q1028.pdf', 'C7PhII - Attachment 1-Allocation of Network Upgrades for Cost Estimates-Q1029.pdf', 'QC7PhII_Q1058_Pio Pico Capacity Increase_Addm2_04-08-2016.pdf', 'QC7PII-SCE-VEA-EOP-Q1064-ARESNevada-Appendix A.pdf', 'QC7PII-SCE-VEA-EOP-Q1064-ARESNevada-Appendix A-Attachment 1.pdf', 'QC7PII-SCE-EOP-Q1064-ARESNEvada-Appendix A-Attachment 2.pdf', 'QC7PII-VEA-EOP-Q1064-ARESNevada-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-EOP-Q1064-ARESNEvada-Appendix A-Attachment 2a.pdf', 'QC7PII-SCE-VEA-EOP-Q1064-Appendix A_Addendum01.pdf', 'QC7PII-SCE-VEA-EOP-Q1064-Appendix A_Addendum02.pdf', 'QC7PII-VEA-Q1065-CaballoLoco-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-VEA-EOP-Q1065-CaballoLoco-Appendix A.pdf', 'QC7PII-SCE-VEA-EOP-Q1065-CaballoLoco-Appendix A-Attachment 1.pdf', 'QC7PII-SCE-EOP-Q1065-CaballoLoco-Appendix A-Attachment 2a.pdf', 'QC7PII-SCE-EOP-Q1065-CaballoLoco-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-VEA-EOP-Q1065-Appendix A_Addendum-01.pdf', 'QC7PII-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2a.pdf', 'QC7PII-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 1.pdf', 'QC7PII-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A.pdf', 'QC7PII-SCE-VEA-EOP-Q1066-Appendix A_Addendum01.pdf', 'QC7PII-SCE-VEA-EOP-Q1066-Appendix A_Addendum-02.pdf', 'QC7PII-SCE-EOP-Q1069-Apendix A-BoulderSolar-Atchmnt1.pdf', 'QC7PII-SCE-EOP-Q1069-Appendix A-BoulderSolar.pdf', 'C7PII-SCE-EOP-Q1069-Appendix A-BoulderSolar-Attachment 2a.pdf', 'C7PII-SCE-EOP-Q1069-Appendix A-BoulderSolar-Attachment 2.pdf', 'QC7PII-SCE-Eastern_Q1070-Jupiter Solar_Appendix A-Report.pdf', 'QC7PII-SCE-Eastern_Q1070-Jupiter Solar_Attachment 2-Cost.pdf', 'QC7PII-SCE-Eastern_Q1071-Quartz4Solar_Appendix A_Report.pdf', 'QC7PII-SCE-Eastern_Q1071-Quartz4Solar_Attachment 2-Cost.pdf', 'QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A-Attachment 1.pdf', 'QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A-Attachment 2a.pdf', 'QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A.pdf', 'QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A.pdf', 'QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A-Attachment 2a.pdf', 'QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A-Attachment 1.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['Q1021_Stockton Biomass_C7PHII_Report Addendum #1.pdf', 'QC7PhII_Q1040_Arlington Valley Solar Energy II_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1045_Chula Vista ERC_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1047_El Cajon ERC_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1048_Escondido ERC_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1050_Honey Badger_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1053_Mesquite Solar 3_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1058_Pio Pico Capacity Increase_Addm1toAppendix A_12-9-2015.pdf', 'QC7PhII_Q1058_Pio Pico Capacity Increase_Addm2_04-08-2016.pdf', 'QC7PhII_Q1062_White Wing Ranch Solar_Addm1toAppendix A_1-25-2016.pdf', 'QC7PII-SCE-VEA-EOP-Q1064-Appendix A_Addendum01.pdf', 'QC7PII-SCE-VEA-EOP-Q1064-Appendix A_Addendum02.pdf', 'QC7PII-SCE-VEA-EOP-Q1065-Appendix A_Addendum-01.pdf', 'QC7PII-SCE-VEA-EOP-Q1066-Appendix A_Addendum01.pdf', 'QC7PII-SCE-VEA-EOP-Q1066-Appendix A_Addendum-02.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['C7 Ph II Appendix A - Q1010 Dyer Summit Wind Repower.pdf', 'C7 Ph II Appendix A - Q1011 GHS Project.pdf', 'C7 Ph II Appendix A - Q1014 Paradise Cut.pdf', 'C7 Ph II Appendix A - Q1016 Richmond Steam Turbine.pdf', 'C7 Ph II Appendix A - Q1021 Stockton Biomass.pdf', 'Q1027 Blackbriar_Appendix A-Individual Report -C7Ph II.pdf', 'C7PhII - Attachment 1-Allocation of Network Upgrades for Cost Estimates-Q1028.pdf', 'Q1028 Little Bear Solar1_Appendix A-Individual Report-C7Ph II.pdf', 'Q1029 Little Bear Solar 2_Appendix A-Individual Report-C7Ph II.pdf', 'C7PhII - Attachment 1-Allocation of Network Upgrades for Cost Estimates-Q1029.pdf', 'Q1030 South Lake Solar_Appendix A-Individual Report-C7Ph II.pdf', 'Q1031 Stonecrop_Appendix A-Individual Report-C7Ph II.pdf', 'Q1032 Tranquillity 8_Appendix A-Individual Report-C7Ph II.pdf', 'Q1033 Broken Spoke_Appendix A-Individual Report C7Ph II.pdf', 'Q1036 Mustang Two_Appendix A-Individual Report-C7Ph II.pdf', 'Q1038 Pandora Solar_Appendix A-Individual Report_C7Ph II.pdf', '14AS870088-QC7PhII_Q1040_Arlington_Valley_Solar_Energy_II_Addm2toAppendix_A_1032016.pdf', 'QC7PhII_Q1040_Arlington Valley Solar Energy II_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1045_Chula Vista ERC Appendix A_11-24-2015.pdf', 'QC7PhII_Q1047_El Cajon ERC_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1048_Escondido ERC_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1050_Honey Badger_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1053_Mesquite Solar 3_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1058_Pio Pico Capacity Increase_Appendix A_11-24-2015.pdf', 'QC7PhII_Q1062_White Wing Ranch Solar _Appendix A_11-24-2015.pdf', 'QC7PII-SCE-VEA-EOP-Q1064-ARESNevada-Appendix A.pdf', 'QC7PII-SCE-VEA-EOP-Q1064-ARESNevada-Appendix A-Attachment 1.pdf', 'QC7PII-SCE-EOP-Q1064-ARESNEvada-Appendix A-Attachment 2.pdf', 'QC7PII-VEA-EOP-Q1064-ARESNevada-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-EOP-Q1064-ARESNEvada-Appendix A-Attachment 2a.pdf', 'QC7PII-VEA-Q1065-CaballoLoco-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-VEA-EOP-Q1065-CaballoLoco-Appendix A.pdf', 'QC7PII-SCE-VEA-EOP-Q1065-CaballoLoco-Appendix A-Attachment 1.pdf', 'QC7PII-SCE-EOP-Q1065-CaballoLoco-Appendix A-Attachment 2a.pdf', 'QC7PII-SCE-EOP-Q1065-CaballoLoco-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2a.pdf', 'QC7PII-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 1.pdf', 'QC7PII-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A.pdf', 'QC7PII-SCE-EOP-Q1069-Apendix A-BoulderSolar-Atchmnt1.pdf', 'QC7PII-SCE-EOP-Q1069-Appendix A-BoulderSolar.pdf', 'C7PII-SCE-EOP-Q1069-Appendix A-BoulderSolar-Attachment 2a.pdf', 'C7PII-SCE-EOP-Q1069-Appendix A-BoulderSolar-Attachment 2.pdf', 'QC7PII-SCE-Eastern_Q1070-Jupiter Solar_Appendix A-Report.pdf', 'QC7PII-SCE-Eastern_Q1070-Jupiter Solar_Attachment 2-Cost.pdf', 'QC7PII-SCE-Eastern_Q1071-Quartz4Solar_Appendix A_Report.pdf', 'QC7PII-SCE-Eastern_Q1071-Quartz4Solar_Attachment 2-Cost.pdf', 'QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A-Attachment 1.pdf', 'QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A-Attachment 2a.pdf', 'QC7PII-SCE-Northern-Q1074-Garland2ES-Appendix A.pdf', 'QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A.pdf', 'QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A-Attachment 2a.pdf', 'QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A-Attachment 2.pdf', 'QC7PII-SCE-Northern-Q1076-WillowSprings3-Appendix A-Attachment 1.pdf']\n",
      "\n",
      "List of Style N PDFs (Skipped due to 'Network Upgrade Type'):\n",
      "[]\n",
      "\n",
      "Total Number of Style N PDFs: 0\n",
      "\n",
      "Number of Original PDFs Scraped: 23\n",
      "Number of Addendum PDFs Scraped: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/1015372725.py:1113: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/1015372725.py:1113: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY =\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/03_raw/ph2_rawdata_cluster7_style_Q_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/03_raw/ph2_rawdata_cluster7_style_Q_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/03_raw/ph2_scraping_cluster7_style_Q_log.txt\"\n",
    "PROJECT_RANGE = range(1007, 1090)   # Inclusive range for q_ids in Clusters 7\n",
    "\n",
    "\n",
    "# Read the CSV file containing processed projects (with q_id column)\n",
    "processed_csv_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters/costs_phase_2_all_clusters_total.csv\"  # UPDATE THIS PATH\n",
    "processed_df = pd.read_csv(processed_csv_path)\n",
    "# Convert q_id values to numeric then to int for filtering\n",
    "processed_q_ids = pd.to_numeric(processed_df['q_id'], errors='coerce').dropna().astype(int).unique()\n",
    "projects_to_process = sorted([q_id for q_id in PROJECT_RANGE if q_id not in processed_q_ids])\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "style_n_pdfs = []  # List to track style N PDFs\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "original_has_table7 = {}  # Dictionary to track if original PDFs have table7\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters, but keeps parentheses.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            # collapse internal whitespace\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            # strip out everything except letters, digits, spaces, and parentheses\n",
    "            header = re.sub(r'[^a-z0-9\\s\\(\\)]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    elif value is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(value).replace('\\n', ' ').strip()\n",
    "     \n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Conditionally Assigned Network Upgrades\",\n",
    "        \"Local Off-Peak Network Upgrade\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if  re.search(rf\"\\b{re.escape(phrase)}\\b(?=\\d|\\W|$)\", title, re.IGNORECASE):\n",
    "        \n",
    "         #re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        # Updated Cluster Extraction\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        if '7' in clusters:\n",
    "            cluster_number = '7'\n",
    "        elif clusters:\n",
    "            cluster_number = max(clusters, key=lambda x: int(x))  # Choose the highest cluster number found\n",
    "        else:\n",
    "            cluster_number = '7'  # Default to 7 if not found\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 7-1\" to \"Table 7-5\" with hyphen or dot\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*8[-.]([1-5])\\b\", text, re.IGNORECASE):\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 7-1 to 7-6 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus two to include possible continuation\n",
    "\n",
    "            print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end}\", file=log_file)\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*8[-.]([1-6])[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(3).strip()\n",
    "                                break\n",
    "\n",
    "                    if table_title:\n",
    "                        if re.search(r\"PTO Interconnection Facilities Cost Estimate Summary\", table_title, re.IGNORECASE):\n",
    "                            print(f\"Skipping Table 8-1 PTO on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                    if table_title:\n",
    "                        if re.search(r\"\\b8-7\\b\", table_title, re.IGNORECASE):\n",
    "                            print(f\"Skipping Table 7-7 on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            continue  # Skip Table 7-7\n",
    "\n",
    "                        \n",
    "\n",
    "                        # New Table 7 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                            # ← INSERT this block:\n",
    "                        if df_new.empty:\n",
    "                            # store an empty DF with the right columns,\n",
    "                            # so that continuation blocks can append to it\n",
    "                            extracted_tables.append(pd.DataFrame(columns=headers))\n",
    "                            print(f\"Header-only Table 7 (‘{specific_phrase}’) detected on page {page_number+1}; waiting for continuation…\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        # Handle ADNU-specific grouping\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    # Group all adnu rows into one 'upgrade' row\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    # If 'type of upgrade' exists, just rename adnu if needed\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row is none, replace only first row if needed\n",
    "                                if df_new.empty:\n",
    "                                    # should never happen once you’ve done step 1, but safe to check\n",
    "                                    continue\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' first row for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            # Non-ADNU new tables\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row is none, replace only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for the first row in new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(\"Duplicate columns detected in new table. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table\n",
    "                        if not extracted_tables:\n",
    "                            print(f\"No previous Table 7 detected to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        last_table = extracted_tables[-1]\n",
    "                        expected_columns = last_table.columns.tolist()\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "\n",
    "                        # Check if the first row is a header row\n",
    "                        # As per your latest instruction, we will treat all continuation table rows as data points\n",
    "                        # without any header detection\n",
    "                        # However, you mentioned checking if there is a header row first, so we'll implement that\n",
    "\n",
    "                        # Detect if first row is a header\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\", \"MW at POI\"]\n",
    "                        first_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            any(re.search(rf\"\\b{kw}\\b\", clean_string_cell(cell).lower()) for kw in header_keywords)\n",
    "                            for cell in first_row\n",
    "                        )\n",
    "\n",
    "                        if is_header_row:\n",
    "                            # Handle header row in continuation table\n",
    "                            headers = clean_column_headers(first_row)\n",
    "                            data_rows = data_rows[1:]  # Exclude header row\n",
    "\n",
    "                            # Update expected_columns by adding new columns if any\n",
    "                            new_columns = [col for col in headers if col not in expected_columns]\n",
    "                            if new_columns:\n",
    "                                expected_columns.extend(new_columns)\n",
    "                                print(f\"Added new columns from continuation table: {new_columns}\", file=log_file)\n",
    "\n",
    "                            # Create a mapping of new columns to add with default NaN\n",
    "                            for new_col in new_columns:\n",
    "                                last_table[new_col] = pd.NA\n",
    "\n",
    "                            # Reindex last_table to include new columns\n",
    "                            last_table = last_table.reindex(columns=expected_columns)\n",
    "                            extracted_tables[-1] = last_table\n",
    "\n",
    "                            # Update 'type of upgrade' column in the first row if needed\n",
    "                            if \"type of upgrade\" in headers:\n",
    "                                type_upgrade_idx = headers.index(\"type of upgrade\")\n",
    "                                if pd.isna(data_rows[0][type_upgrade_idx]) or data_rows[0][type_upgrade_idx] == \"\":\n",
    "                                    data_rows[0][type_upgrade_idx] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'type of upgrade' first row for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            elif \"upgrade\" in headers:\n",
    "                                upgrade_idx = headers.index(\"upgrade\")\n",
    "                                if pd.isna(data_rows[0][upgrade_idx]) or data_rows[0][upgrade_idx] == \"\":\n",
    "                                    data_rows[0][upgrade_idx] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'upgrade' first row for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' or 'upgrade' does not exist, add it\n",
    "                                headers.append(\"type of upgrade\")\n",
    "                                expected_columns.append(\"type of upgrade\")\n",
    "                                for idx, row in enumerate(data_rows):\n",
    "                                    data_rows[idx].append(specific_phrase)\n",
    "                                print(f\"Added 'type of upgrade' column and filled with '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                            # Handle ADNU-specific logic if applicable\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                if \"adnu\" in headers:\n",
    "                                    if \"upgrade\" not in headers:\n",
    "                                        # Rename 'adnu' to 'upgrade'\n",
    "                                        adnu_idx = headers.index(\"adnu\")\n",
    "                                        headers[adnu_idx] = \"upgrade\"\n",
    "                                        for row in data_rows:\n",
    "                                            row[adnu_idx] = \" \".join([str(cell) for cell in row[adnu_idx] if pd.notna(cell)])\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in continuation ADNU table.\", file=log_file)\n",
    "                                # Ensure 'type of upgrade' column is filled\n",
    "                                if \"type of upgrade\" not in headers:\n",
    "                                    headers.append(\"type of upgrade\")\n",
    "                                    expected_columns.append(\"type of upgrade\")\n",
    "                                    for row in data_rows:\n",
    "                                        row.append(specific_phrase)\n",
    "                                    print(\"Added 'type of upgrade' column with specific phrase for continuation ADNU table.\", file=log_file)\n",
    "\n",
    "                        else:\n",
    "                            # No header row detected, treat all rows as data points\n",
    "                            print(f\"No header row detected in continuation table on page {page_number + 1}, table {table_index + 1}. Treating all rows as data.\", file=log_file)\n",
    "\n",
    "                        # Create DataFrame for continuation table\n",
    "                        if is_header_row:\n",
    "                            try:\n",
    "                                df_continuation = pd.DataFrame(data_rows, columns=headers)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "                        else:\n",
    "                            # Create DataFrame with expected_columns\n",
    "                            # Handle cases where continuation table has more columns\n",
    "                            standardized_data = []\n",
    "                            for row in data_rows:\n",
    "                                if len(row) < len(expected_columns):\n",
    "                                    # Insert 'type of upgrade' or 'upgrade' with specific_phrase\n",
    "                                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                        # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                        missing_cols = len(expected_columns) - len(row)\n",
    "                                        #row += [specific_phrase] * missing_cols\n",
    "                                        data_rows = [row[:7] + [specific_phrase] + row[7:] for row in data_rows]\n",
    "                                        print(f\"Inserted '{specific_phrase}' for missing columns in ADNU continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                    else:\n",
    "                                        # For non-ADNU tables, assume missing \"type of upgrade\" column\n",
    "                                        missing_cols = len(expected_columns) - len(row)\n",
    "                                        #row += [specific_phrase] * missing_cols\n",
    "                                        data_rows = [ [specific_phrase]  for row in data_rows]\n",
    "                                        print(f\"Inserted '{specific_phrase}' for missing columns in non-ADNU continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                elif len(row) > len(expected_columns):\n",
    "                                    # Add new columns with default names\n",
    "                                    extra_cols = len(row) - len(expected_columns)\n",
    "                                    for i in range(extra_cols):\n",
    "                                        new_col_name = f\"column{len(expected_columns) + 1 + i}\"\n",
    "                                        expected_columns.append(new_col_name)\n",
    "                                        last_table[new_col_name] = pd.NA\n",
    "                                        print(f\"Added new column '{new_col_name}' for extra data in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                    row = row[:len(expected_columns)]\n",
    "\n",
    "                                row_dict = dict(zip(expected_columns, [clean_string_cell(cell) for cell in row]))\n",
    "\n",
    "                                # Handle 'type of upgrade' column\n",
    "                                if \"type of upgrade\" in row_dict and (pd.isna(row_dict[\"type of upgrade\"]) or row_dict[\"type of upgrade\"] == \"\"):\n",
    "                                    row_dict[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'type of upgrade' for a row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                                standardized_data.append(row_dict)\n",
    "\n",
    "                            try:\n",
    "                                df_continuation = pd.DataFrame(standardized_data, columns=expected_columns)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "\n",
    "\n",
    "                             # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                if \"type of upgrade\" in df_continuation.columns:\n",
    "                                    first_row = df_continuation.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                        df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                else:\n",
    "                                    # If \"type of upgrade\" column does not exist, add it\n",
    "                                    df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                            else:\n",
    "                                # General Handling for other tables\n",
    "                                if \"type of upgrade\" in df_continuation.columns:\n",
    "                                    first_row = df_continuation.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                        df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                else:\n",
    "                                    # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                    df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "\n",
    "\n",
    "                        # Handle ADNU-specific logic in continuation tables\n",
    "                        #if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                        #    print(\"Handling ADNU-specific logic in continuation table.\", file=log_file)\n",
    "                        #    if \"upgrade\" in df_continuation.columns and \"adnu\" not in df_continuation.columns:\n",
    "                        #        # Ensure 'upgrade' column is present\n",
    "                        #        if \"upgrade\" not in df_continuation.columns:\n",
    "                        #            df_continuation[\"upgrade\"] = specific_phrase\n",
    "                        #            print(\"Added 'upgrade' column to continuation ADNU table.\", file=log_file)\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\", file=log_file)\n",
    "                            df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                        print(f\"Appended continuation table data to the last extracted table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 7 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After processing all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 7 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 7 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 7 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 7-1 to 7-5.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*8[-.]([1-4])\\b\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def has_network_upgrade_type_column(pdf_path, log_file):\n",
    "    \"\"\"Checks if any table in the PDF has a column header 'Network Upgrade Type'.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables()\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    headers = clean_column_headers(tab[0])\n",
    "                    if \"network upgrade type\" in headers:\n",
    "                        print(f\"Found 'Network Upgrade Type' in PDF {pdf_path} on page {page_number}, table {table_index}.\", file=log_file)\n",
    "                        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking 'Network Upgrade Type' in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path, log_file):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' or 'Revision' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            print(f\"Extracted Text: {text}\", file= log_file)  # Debugging step\n",
    "            # Case-insensitive check for 'Addendum' or 'Revision'\n",
    "            text_lower = text.lower()\n",
    "            return \"addendum\" in text_lower or \"revision\" in text_lower\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    \"\"\"\n",
    "    Appends a suffix to duplicate headers to make them unique.\n",
    "\n",
    "    Args:\n",
    "        headers (list): List of column headers.\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique column headers.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "    SKIP_PROJECTS = {1860, 2003, 2006}\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "\n",
    "\n",
    "        for project_id in projects_to_process:\n",
    "            \n",
    "            # Skip the projects in the SKIP_PROJECTS set\n",
    "            if project_id in SKIP_PROJECTS:\n",
    "                print(f\"Skipping Project {project_id} (marked to skip)\", file=log_file)\n",
    "                continue\n",
    "\n",
    "         \n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"03_phase_2_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Separate PDFs into originals and addendums\n",
    "            list_pdfs = [pdf for pdf in os.listdir(project_path) if pdf.endswith(\".pdf\")]\n",
    "            originals = []\n",
    "            addendums = []\n",
    "            for pdf_name in list_pdfs:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                if is_addendum(pdf_path, log_file):\n",
    "                    addendums.append(pdf_name)\n",
    "                else:\n",
    "                    originals.append(pdf_name)\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Process original PDFs first\n",
    "            for pdf_name in originals:\n",
    "                \n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    # Still check if original has table7\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                original_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "\n",
    "                    if not has_table7:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    # Extract Table 7 and merge\n",
    "                    df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False)\n",
    "                    if not df.empty:\n",
    "                        core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        project_scraped = True\n",
    "                        total_pdfs_scraped += 1\n",
    "                        print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                    else:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Then process addendum PDFs\n",
    "            for pdf_name in addendums:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                addendum_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "\n",
    "                    if not has_table7:\n",
    "                        if original_has_table7.get(project_id, False):\n",
    "                            # Attempt to scrape alternative tables is no longer needed\n",
    "                            # According to the latest request, alternative table scraping is removed\n",
    "                            # Therefore, we skip addendum PDFs that do not have Table 7\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7 and original does not have Table 7)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7 and original does not have Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not is_add and not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    if is_add and base_data_extracted:\n",
    "                        # For addendums, use the extracted base data\n",
    "                        table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                        if table7_data.empty and original_has_table7.get(project_id, False):\n",
    "                            # Scrape alternative tables is removed, so skip if no data\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        if not table7_data.empty:\n",
    "                            # Merge base data with Table 7 data\n",
    "                            merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                            merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                            core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                            scraped_pdfs.append(pdf_name)\n",
    "                            scraped_projects.add(project_id)\n",
    "                            project_scraped = True\n",
    "                            total_pdfs_scraped += 1\n",
    "                            print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    # Optionally, print to ipynb\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "    # Rest of the code remains unchanged...\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nList of Style N PDFs (Skipped due to 'Network Upgrade Type'):\")\n",
    "    print(style_n_pdfs)\n",
    "\n",
    "    print(\"\\nTotal Number of Style N PDFs:\", len(style_n_pdfs))\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.applymap(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1010, 1011,1016, 1021 got scraped but the tables were missed due to some indexing error, need to do them seperatly and merge back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ok this got fixed by the following changes:\n",
    "    df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "\n",
    "    # ← INSERT this block:\n",
    "    if df_new.empty:\n",
    "        # store an empty DF with the right columns,\n",
    "        # so that continuation blocks can append to it\n",
    "        extracted_tables.append(pd.DataFrame(columns=headers))\n",
    "        print(f\"Header-only Table 7 (‘{specific_phrase}’) detected on page {page_number+1}; waiting for continuation…\", file=log_file)\n",
    "        continue\n",
    "\n",
    "    if df_new.empty:\n",
    "        # should never happen once you’ve done step 1, but safe to check\n",
    "        continue\n",
    "    first_row = df_new.iloc[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized and Addendums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note for 1437, all tables are table 7 but LDNU is table 7 with only one row so manually added it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: ['q_id', 'cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection', 'type of upgrade', 'upgrade', 'description', 'cost allocation factor', 'escalated costs x 1000', 'estimated time (months) to construct (note 1)', 'estimated cost x 1000', 'estimated cost x 1000 escalated (note 1)', 'estimated time to construct', 'project size (mw)', 'estimated cost x 1000 escalated', 'adnu cost rate x 1000', 'estimated cost x 1000 escalated with itcca (note 2)', 'total estimated cost x 1000 escalated', 'estimated time to construct (note 3)', 'Unnamed: 22', 'cost rate x 1000', 'estimated time to construct (note 2)', 'total estimated cost x 1000 escalalted']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/03_raw/ph2_rawdata_cluster7_style_Q_originals.csv\", dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "\n",
    "\n",
    "#df.columns = clean_column_headers(df.columns)\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "#df = df.map(clean_string_cell)\n",
    "#df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "print(\"After cleaning:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_2_cluster_7_style_Q_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_7_style_Q_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU' 'ADNU']\n",
      "[1010 1011 1014 1016 1021 1027 1028 1029 1030 1031 1032 1033 1036 1038\n",
      " 1040 1045 1047 1048 1050 1053 1058 1062]\n",
      "[7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/4263063990.py:83: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/4263063990.py:83: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/03_raw/ph2_rawdata_cluster7_style_Q_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            'estimated cost x 1000 (od year) (note 1)',\n",
    "            \"estimated cost x 1000 escalated (note 1)\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            'Esc',\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "             \"estimated cost x 1000 escalated without itcca (note 1)\",\n",
    "            \"estimated cost x 1000 (od year) (note 1)\",\n",
    "             \"estimated cost x 1000 (od year) (note 1)\",\n",
    "             \"estimated_cost_x_1000_escalated_note_1\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"estimated cost x 1000\",\n",
    "            \"allocated_cost\",\n",
    "            \"sum of allocated constant cost\"\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "            \"estimated time to construct (note 1)\",\n",
    "            \"estimated time (months) to construct (note 1)\",\n",
    "            \"estimated time to construct (note 3)\",\n",
    "            \"estimated time to construct (note 2)\",\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"project size (mw)\"\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "#df.drop('incremental deliverability', axis=1, inplace=True)\n",
    "#df.drop('dependent system upgrade', axis=1, inplace=True)\n",
    "#df.drop('upgrade_classification', axis=1, inplace=True)\n",
    "\n",
    "#STEP 2: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    " \n",
    "\n",
    "required_columns = ['type_of_upgrade', 'cost_allocation_factor']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "\n",
    "df = df[\n",
    "    ~df['type_of_upgrade'].str.contains(r'Precursor Network Upgrades \\(PNU\\)|Estimated in Service Date', na=False)\n",
    "]\n",
    "\n",
    "df = df[~df.apply(lambda row: row.astype(str).isin([\"Total Of This\", \"of this\"]).any(), axis=1)]\n",
    " \n",
    " \n",
    "# Step 3: Rename 'Grand Total' to 'Total' in total_estimated_cost_x_1000\n",
    "if 'total_estimated_cost_x_1000' in df.columns:\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Grand Total', 'Total')\n",
    "\n",
    "# Step 4: Move 'Total' from total_estimated_cost_x_1000 to cost_allocation_factor\n",
    "if 'total_estimated_cost_x_1000' in df.columns and 'cost_allocation_factor' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: 'Total' if str(row['total_estimated_cost_x_1000']) == 'Total' else row['cost_allocation_factor'],\n",
    "        axis=1\n",
    "    )\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Total', None)\n",
    "\n",
    "df = df[df[\"description\"] != \"Total Allocated\"]    \n",
    "\n",
    "if 'description' in df.columns and 'cost_allocation_factor' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: 'Total' if str(row['description']) == 'Total' else row['cost_allocation_factor'],\n",
    "        axis=1\n",
    "    )\n",
    "    df['description'] = df['description'].replace('Total', None)\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "   \n",
    "df.drop( [ 'estimated_cost_x_1000_escalated_with_itcca_note_2',\t \t'estimated_cost_x_1000_escalated_with_itcca_note_2' ], axis=1, inplace=True)\n",
    "\n",
    "# Step 5: Clean the type of upgrade column\n",
    "   \n",
    " \n",
    "df['type_of_upgrade'] = (\n",
    "    df['type_of_upgrade']\n",
    "    .fillna('')  # Temporarily replace NaN with an empty string\n",
    "    .str.replace(r'\\(Note \\d+\\)', '', regex=True)  # Remove (Note digit)\n",
    "    .str.strip()  # Strip leading/trailing whitespace\n",
    "    .str.title()  # Capitalize the first letter of each word\n",
    "    .str.replace(r'Upgrades$', 'Upgrade', regex=True)  # Fix plural endings\n",
    "    .replace('', pd.NA)  # Convert empty strings back to NaN\n",
    ")\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'Reliability Network Upgrade': 'RNU',\n",
    "    'Reliability Network Upgrades': 'RNU',\n",
    "    'Local Delivery Network Upgrade': 'LDNU',\n",
    "    'Local Delivery Network': 'LDNU',\n",
    "    'Potential Local Delivery Network Upgrade': 'LDNU',\n",
    "    \"Ptos Interconnection Facilities\": 'PTO_IF',\n",
    "    'Escalated Cost And Time To Construct For Interconnection Facilities - If': 'PTO_IF',\n",
    "    'Area Delivery Network Upgrade': 'ADNU',\n",
    "    'Reliability Network Upgrade To Physically Interconnect': 'RNU',\n",
    "    \"Reliability Network upgrade To Physically Interconnect\": \"RNU\",\n",
    "     'Pto': 'PTO_IF',\n",
    "    \"Other Potential Network Upgrade\": \"OPNU\",\n",
    "    \"Conditionally Assigned Network Upgrade\": \"CANU\",\n",
    "    \"Canus\": \"CANU\",\n",
    "    'Local  Delivery  Network  Upgrade': 'LDNU',\n",
    "        \"Escalated Cost And Time To Construct For Reliability Network Upgrade4\": \"RNU\",\n",
    "    \"Escalated Cost And Time To Construct For Reliability Network Upgrade3\": \"RNU\",\n",
    "    'Escalated Cost And Time To Construct For Reliability Network Upgrade': \"RNU\",\n",
    "    'Rnus, Estimated Costs, And Estimated Time To Construct Summary': \"RNU\",\n",
    "    \"Local Off-Peak Network Upgrade\": \"LOPNU\",\n",
    "    'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Canu': 'CANU',\n",
    "  \n",
    " 'Total ADNU': 'ADNU',\n",
    "  'Ptos Interconnect Ion Facilities' : 'PTO_IF',\n",
    "  'Local Off- Peak Network Upgrade': 'LOPNU',\n",
    " 'P Os Interconnection Facilities': 'PTO_IF',\n",
    "  \n",
    "\n",
    " \n",
    "}\n",
    "\n",
    "\n",
    "#Step 6: Apply mapping and ffill type of upgrade column\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()  \n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# build a boolean mask: True for rows where cost contains \"(…)\"\n",
    "has_paren = df['estimated_cost_x_1000'].astype(str).str.contains(r'\\([^)]*\\)')\n",
    "# drop those rows\n",
    "df = df.loc[~has_paren].copy()\n",
    "\n",
    "def drop_rows_with_star_in_costs(df, cost_cols):\n",
    "    \"\"\"\n",
    "    Remove any row where any of the cost_cols contains a '*' character.\n",
    "    \"\"\"\n",
    "    mask = pd.Series(False, index=df.index)\n",
    "    for col in cost_cols:\n",
    "        if col in df.columns:\n",
    "            # star anywhere in the string\n",
    "            mask = mask | df[col].astype(str).str.contains(r\"\\*\", regex=True)\n",
    "    # keep only rows without a star\n",
    "    return df.loc[~mask].reset_index(drop=True)\n",
    "\n",
    "def drop_rows_with_dash_in_time(df, time_col):\n",
    "    \"\"\"\n",
    "    Remove any row where the time_col contains a dash '-' (e.g. '-').\n",
    "    \"\"\"\n",
    "    if time_col in df.columns:\n",
    "        mask = df[time_col].astype(str).str.contains(r\"^-+$\", regex=True)\n",
    "        return df.loc[~mask].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def keep_second_entry_in_cells(df, columns):\n",
    "    \"\"\"\n",
    "    For each column in columns, if the cell contains multiple space‑separated entries,\n",
    "    keep only the second one.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        def pick_second(cell):\n",
    "            parts = re.findall(r\"[\\d\\.\\$%,]+\", str(cell))\n",
    "            return parts[1] if len(parts) > 1 else (parts[0] if parts else cell)\n",
    "        df[col] = df[col].apply(pick_second)\n",
    "    return df\n",
    "\n",
    "# ── Integration ──\n",
    "# Place this just before your Step 7 clean_currency block:\n",
    "\n",
    "# 1) drop any row where estimated or escalated cost has '*'\n",
    "df = drop_rows_with_star_in_costs(\n",
    "    df,\n",
    "    cost_cols=[\n",
    "        'estimated_cost_x_1000',\n",
    "        'escalated_cost_x_1000',\n",
    "        'total_estimated_cost_x_1000',\n",
    "        'total_estimated_cost_x_1000_escalated'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2) drop any row where estimated_time_to_construct is just a dash\n",
    "df = drop_rows_with_dash_in_time(df, 'estimated_time_to_construct')\n",
    "\n",
    " \n",
    "# Now proceed with your clean_currency step…\n",
    "\n",
    "\n",
    "    \n",
    "# Step 7: Remove $ signs and convert to numeric\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000', 'adnu_cost_rate_x_1000_escalated']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated',   'adnu_cost_rate_x_1000_escalated']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "                # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = group[\n",
    "            (group['type_of_upgrade'] == upgrade) & (group['item'] == 'no')\n",
    "        ].shape[0] > 0\n",
    "        \n",
    "        if total_exists:\n",
    "             \n",
    "            continue\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "        if not total_exists:\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate specified columns from the existing row\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns (single row, so it remains the same)\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate the specified columns from the first row in the group\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 7: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "# Step 9: Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()\n",
    "\n",
    "if 'upgrade' in df.columns and 'type_of_upgrade' in df.columns and 'q_id' in df.columns:\n",
    "    df['upgrade'] = df.groupby(['q_id', 'type_of_upgrade'])['upgrade'].ffill()\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        val = df.at[i, 'type_of_upgrade']\n",
    "        # only do the equality check if val is not NA\n",
    "        if pd.notna(val) and val == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = val\n",
    "\n",
    "df.dropna(subset=['type_of_upgrade'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "# build a mask of exactly the rows you want to drop\n",
    "mask = (\n",
    "    (df['type_of_upgrade'] == 'PTO_IF') &\n",
    "    (df['upgrade'] == 'None') &\n",
    "    (df['description'] == 'None') &\n",
    "    (df['cost_allocation_factor'] == 0) &\n",
    "    (df['estimated_cost_x_1000'] == 0) &\n",
    "    (df['escalated_cost_x_1000'] == 0) &\n",
    "    (df['estimated_time_to_construct'] == 0)\n",
    ")\n",
    "\n",
    " \n",
    "\n",
    "# if you want to do it in-place instead:\n",
    "df.drop(df[mask].index, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/02_intermediate/costs_phase_2_cluster_7_style_Q_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/02_intermediate/costs_phase_2_cluster_7_style_Q_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_7_style_Q_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_7_style_Q_total.csv'.\")\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_2_cluster_7_style_Q_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_7_style_Q_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU' 'ADNU']\n",
      "[1040 1045 1047 1048 1050 1053 1058 1062]\n",
      "[7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/3751230650.py:77: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/3751230650.py:77: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/03_raw/ph2_rawdata_cluster7_style_Q_addendums.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"estimated cost x 1000 escalated without itcca (note 1)\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"estimated cost x 1000 escalated (note 1)\"\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"estimated cost x 1000\",\n",
    "            \"allocated_cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "            \"estimated_time_months_to_construct_note_1\",\n",
    "            \"estimated time (months) to construct (note 1)\",\n",
    "            \"estimated time to construct (note 2)\",\n",
    "            \"estimated time to construct (note 3)\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "#df.drop('incremental deliverability', axis=1, inplace=True)\n",
    "#df.drop('dependent system upgrade', axis=1, inplace=True)\n",
    "\n",
    "#STEP 2: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "df.drop('estimated_cost_x_1000_escalated_with_itcca_note_2', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "required_columns = ['type_of_upgrade', 'cost_allocation_factor']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "\n",
    "df = df[\n",
    "    ~df['type_of_upgrade'].str.contains(r'Precursor Network Upgrades \\(PNU\\)|Estimated in Service Date', na=False)\n",
    "]\n",
    " \n",
    " \n",
    "# Step 3: Rename 'Grand Total' to 'Total' in total_estimated_cost_x_1000\n",
    "if 'total_estimated_cost_x_1000' in df.columns:\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Grand Total', 'Total')\n",
    "\n",
    "# Step 4: Move 'Total' from total_estimated_cost_x_1000 to cost_allocation_factor\n",
    "if 'total_estimated_cost_x_1000' in df.columns and 'cost_allocation_factor' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: 'Total' if str(row['total_estimated_cost_x_1000']) == 'Total' else row['cost_allocation_factor'],\n",
    "        axis=1\n",
    "    )\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Total', None)\n",
    "\n",
    "df = df[df[\"description\"] != \"Total Allocated\"]    \n",
    "\n",
    "if 'description' in df.columns and 'cost_allocation_factor' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: 'Total' if str(row['description']) == 'Total' else row['cost_allocation_factor'],\n",
    "        axis=1\n",
    "    )\n",
    "    df['description'] = df['description'].replace('Total', None)\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "   \n",
    "#df.drop(['interconnection_facility_element', 'cost_subject_to_itcc',\t'total_cost_excluding_itcc_note_1'], axis=1, inplace=True)\n",
    "\n",
    "# Step 5: Clean the type of upgrade column\n",
    "   \n",
    " \n",
    "df['type_of_upgrade'] = (\n",
    "    df['type_of_upgrade']\n",
    "    .fillna('')  # Temporarily replace NaN with an empty string\n",
    "    .str.replace(r'\\(Note \\d+\\)', '', regex=True)  # Remove (Note digit)\n",
    "    .str.strip()  # Strip leading/trailing whitespace\n",
    "    .str.title()  # Capitalize the first letter of each word\n",
    "    .str.replace(r'Upgrades$', 'Upgrade', regex=True)  # Fix plural endings\n",
    "    .replace('', pd.NA)  # Convert empty strings back to NaN\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'Reliability Network Upgrade': 'RNU',\n",
    "    'Reliability Network Upgrades': 'RNU',\n",
    "    'Local Delivery Network Upgrade': 'LDNU',\n",
    "    'Local Delivery Network': 'LDNU',\n",
    "    'Potential Local Delivery Network Upgrade': 'LDNU',\n",
    "    \"Ptos Interconnection Facilities\": 'PTO_IF',\n",
    "    'Area Delivery Network Upgrade': 'ADNU',\n",
    "    'Reliability Network Upgrade To Physically Interconnect': 'RNU',\n",
    "    \"Reliability Network upgrade To Physically Interconnect\": \"RNU\",\n",
    "    'Escalated Cost And Time To Construct For Reliability Network Upgrade': 'RNU',\n",
    "     'Pto': 'PTO_IF',\n",
    "    \"Other Potential Network Upgrade\": \"OPNU\",\n",
    "    \"Conditionally Assigned Network Upgrade\": \"CANU\",\n",
    "    \"Canus\": \"CANU\",\n",
    "        \"Escalated Cost And Time To Construct For Reliability Network Upgrade4\": \"RNU\",\n",
    "    \"Escalated Cost And Time To Construct For Reliability Network Upgrade3\": \"RNU\",\n",
    "    \"Local Off-Peak Network Upgrade\": \"LOPNU\",\n",
    "    'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Canu': 'CANU',\n",
    "  \n",
    " 'Total ADNU': 'ADNU',\n",
    "  'Ptos Interconnect Ion Facilities' : 'PTO_IF',\n",
    "  'Local Off- Peak Network Upgrade': 'LOPNU',\n",
    " 'P Os Interconnection Facilities': 'PTO_IF',\n",
    "  \n",
    "\n",
    " \n",
    "}\n",
    "\n",
    "\n",
    "#Step 6: Apply mapping and ffill type of upgrade column\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()  \n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# build a boolean mask: True for rows where cost contains \"(…)\"\n",
    "#has_paren = df['estimated_cost_x_1000'].astype(str).str.contains(r'\\([^)]*\\)')\n",
    "# drop those rows\n",
    "#df = df.loc[~has_paren].copy()\n",
    "\n",
    "def drop_rows_with_star_in_costs(df, cost_cols):\n",
    "    \"\"\"\n",
    "    Remove any row where any of the cost_cols contains a '*' character.\n",
    "    \"\"\"\n",
    "    mask = pd.Series(False, index=df.index)\n",
    "    for col in cost_cols:\n",
    "        if col in df.columns:\n",
    "            # star anywhere in the string\n",
    "            mask = mask | df[col].astype(str).str.contains(r\"\\*\", regex=True)\n",
    "    # keep only rows without a star\n",
    "    return df.loc[~mask].reset_index(drop=True)\n",
    "\n",
    "def drop_rows_with_dash_in_time(df, time_col):\n",
    "    \"\"\"\n",
    "    Remove any row where the time_col contains a dash '-' (e.g. '-').\n",
    "    \"\"\"\n",
    "    if time_col in df.columns:\n",
    "        mask = df[time_col].astype(str).str.contains(r\"^-+$\", regex=True)\n",
    "        return df.loc[~mask].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def keep_second_entry_in_cells(df, columns):\n",
    "    \"\"\"\n",
    "    For each column in columns, if the cell contains multiple space‑separated entries,\n",
    "    keep only the second one.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        def pick_second(cell):\n",
    "            parts = re.findall(r\"[\\d\\.\\$%,]+\", str(cell))\n",
    "            return parts[1] if len(parts) > 1 else (parts[0] if parts else cell)\n",
    "        df[col] = df[col].apply(pick_second)\n",
    "    return df\n",
    "\n",
    "# ── Integration ──\n",
    "# Place this just before your Step 7 clean_currency block:\n",
    "\n",
    "# 1) drop any row where estimated or escalated cost has '*'\n",
    " \n",
    "\n",
    "# 2) drop any row where estimated_time_to_construct is just a dash\n",
    "df = drop_rows_with_dash_in_time(df, 'estimated_time_to_construct')\n",
    "\n",
    "# 3) if multiple entries exist in a cell, keep only the second\n",
    " \n",
    "# Now proceed with your clean_currency step…\n",
    "\n",
    "\n",
    "    \n",
    "# Step 7: Remove $ signs and convert to numeric\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "         \n",
    "    try:\n",
    "        return value\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "df['cost_allocation_factor'] = df['cost_allocation_factor'].apply(clean_currency)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000', 'adnu_cost_rate_x_1000_escalated']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated',   'adnu_cost_rate_x_1000_escalated']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "                # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = group[\n",
    "            (group['type_of_upgrade'] == upgrade) & (group['item'] == 'no')\n",
    "        ].shape[0] > 0\n",
    "        \n",
    "        if total_exists:\n",
    "             \n",
    "            continue\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "        if not total_exists:\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate specified columns from the existing row\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns (single row, so it remains the same)\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate the specified columns from the first row in the group\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 7: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "# Step 9: Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()\n",
    "\n",
    "if 'upgrade' in df.columns and 'type_of_upgrade' in df.columns and 'q_id' in df.columns:\n",
    "    df['upgrade'] = df.groupby(['q_id', 'type_of_upgrade'])['upgrade'].ffill()\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "# build a mask of exactly the rows you want to drop\n",
    "mask = (\n",
    "    (df['type_of_upgrade'] == 'PTO_IF') &\n",
    "    (df['upgrade'] == 'None') &\n",
    "    (df['description'] == 'None') &\n",
    "    (df['cost_allocation_factor'] == 0) &\n",
    "    #(df['estimated_cost_x_1000'] == 0) &\n",
    "    (df['escalated_cost_x_1000'] == 0) &\n",
    "    (df['estimated_time_to_construct'] == 0)\n",
    ")\n",
    "\n",
    " \n",
    "\n",
    "# if you want to do it in-place instead:\n",
    "df.drop(df[mask].index, inplace=True)\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/02_intermediate/costs_phase_2_cluster_7_style_Q_itemized_addendums.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/02_intermediate/costs_phase_2_cluster_7_style_Q_total_addendums.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_7_style_Q_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_7_style_Q_total.csv'.\")\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge- Complete replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/2317753129.py:210: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/2317753129.py:210: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/2317753129.py:210: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/2317753129.py:212: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].replace(np.nan, '', inplace=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/2317753129.py:210: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/2317753129.py:210: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/2317753129.py:210: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_66086/2317753129.py:212: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].replace(np.nan, '', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Load a CSV file and ensure specific columns are treated as character.\n",
    "    \"\"\"\n",
    "    available_columns = pd.read_csv(file_path, nrows=0).columns\n",
    "    char_cols = [c for c in char_columns if c in available_columns]\n",
    "    return pd.read_csv(\n",
    "        file_path,\n",
    "        dtype={c: str for c in char_cols},\n",
    "        na_values=[], \n",
    "        keep_default_na=False\n",
    "    )\n",
    "\n",
    "def save_data(df, file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Save a dataframe to CSV, forcing certain columns to string.\n",
    "    \"\"\"\n",
    "    for col in char_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def merge_with_addendums(itemized, itemized_addendums, total, total_addendums):\n",
    "    # mark originals & keep row order\n",
    "    itemized['original'] = \"yes\"\n",
    "    total['original']    = \"yes\"\n",
    "    itemized['row_order'] = itemized.index\n",
    "    total['row_order']    = total.index\n",
    "\n",
    "    # ensure numeric q_id\n",
    "    for df in (itemized, itemized_addendums, total, total_addendums):\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors=\"coerce\")\n",
    "\n",
    "    conditional_columns = [\n",
    "        \"req_deliverability\",\"latitude\",\"longitude\",\n",
    "        \"capacity\",\"point_of_interconnection\"\n",
    "    ]\n",
    "\n",
    "    # --- ITEMIZED: replace only matching (q_id, type_of_upgrade) blocks ---\n",
    "    updated_itemized_rows = []\n",
    "    # iterate over each unique (q_id, type_of_upgrade) in the addendums\n",
    "    for q, t in itemized_addendums[['q_id','type_of_upgrade']].drop_duplicates().itertuples(index=False):\n",
    "        adds = itemized_addendums[\n",
    "            (itemized_addendums['q_id'] == q) &\n",
    "            (itemized_addendums['type_of_upgrade'] == t)\n",
    "        ].reset_index(drop=True)\n",
    "        orig = itemized[\n",
    "            (itemized['q_id'] == q) &\n",
    "            (itemized['type_of_upgrade'] == t)\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # combine conditional columns\n",
    "        for col in conditional_columns:\n",
    "            if col in adds.columns and col in orig.columns:\n",
    "                adds[col] = (\n",
    "                    adds[col].replace(\"\", pd.NA)\n",
    "                              .combine_first(orig[col])\n",
    "                              .fillna(\"\")\n",
    "                )\n",
    "\n",
    "        # carry over or pad row_order\n",
    "        if 'row_order' in orig:\n",
    "            ro = orig['row_order'].tolist()\n",
    "            if len(ro) < len(adds):\n",
    "                ro += [pd.NA] * (len(adds) - len(ro))\n",
    "        else:\n",
    "            ro = [pd.NA] * len(adds)\n",
    "\n",
    "        adds = adds.assign(original=\"no\", row_order=ro[:len(adds)])\n",
    "\n",
    "        # drop only those matching (q_id, type_of_upgrade) from the master\n",
    "        itemized = itemized[\n",
    "            ~((itemized['q_id'] == q) & (itemized['type_of_upgrade'] == t))\n",
    "        ]\n",
    "\n",
    "        updated_itemized_rows.append(adds)\n",
    "\n",
    "    # stitch back untouched originals + updated blocks\n",
    "    updated_itemized = pd.concat(\n",
    "        [itemized] + updated_itemized_rows,\n",
    "        ignore_index=True\n",
    "    ) if updated_itemized_rows else itemized.copy()\n",
    "\n",
    "    updated_itemized['row_order'] = updated_itemized['row_order'].fillna(-1).astype(int)\n",
    "    updated_itemized = (\n",
    "        updated_itemized\n",
    "        .sort_values('row_order')\n",
    "        .drop(columns='row_order')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # --- TOTAL: per type_of_upgrade (unchanged logic) ---\n",
    "    updated_total_rows = []\n",
    "    for q in total_addendums['q_id'].unique():\n",
    "        for t in total_addendums['type_of_upgrade'].unique():\n",
    "            adds = total_addendums[\n",
    "                (total_addendums['q_id']==q)&\n",
    "                (total_addendums['type_of_upgrade']==t)\n",
    "            ].reset_index(drop=True)\n",
    "            if adds.empty:\n",
    "                continue\n",
    "\n",
    "            mask = (total['q_id']==q)&(total['type_of_upgrade']==t)\n",
    "            orig = total[mask].reset_index(drop=True)\n",
    "            if orig.empty:\n",
    "                orig = pd.DataFrame({'row_order':[pd.NA]*len(adds)}, index=adds.index)\n",
    "\n",
    "            # align lengths\n",
    "            if len(adds) > len(orig):\n",
    "                extra = pd.DataFrame({c: pd.NA for c in orig.columns},\n",
    "                                     index=range(len(adds)-len(orig)))\n",
    "                orig = pd.concat([orig, extra], ignore_index=True)\n",
    "            elif len(adds) < len(orig):\n",
    "                orig = orig.iloc[:len(adds)].reset_index(drop=True)\n",
    "\n",
    "            for col in conditional_columns:\n",
    "                if col in adds.columns and col in orig.columns:\n",
    "                    adds[col] = (\n",
    "                        adds[col].replace(\"\", pd.NA)\n",
    "                                  .combine_first(orig[col])\n",
    "                                  .fillna(\"\")\n",
    "                    )\n",
    "\n",
    "            total.loc[mask, 'original'] = \"no\"\n",
    "            updated_total_rows.append(\n",
    "                adds.assign(original=\"no\", row_order=orig['row_order'].tolist()[:len(adds)])\n",
    "            )\n",
    "            total = total[~mask]\n",
    "\n",
    "    updated_total = pd.concat([total] + updated_total_rows, ignore_index=True) \\\n",
    "                    if updated_total_rows else total.copy()\n",
    "    updated_total['row_order'] = updated_total['row_order'].fillna(-1).astype(int)\n",
    "    updated_total = (\n",
    "        updated_total\n",
    "        .sort_values('row_order')\n",
    "        .drop(columns='row_order')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # move 'original' to end\n",
    "    def move_last(df):\n",
    "        cols = [c for c in df.columns if c!='original'] + ['original']\n",
    "        return df[cols]\n",
    "\n",
    "    return move_last(updated_itemized), move_last(updated_total)\n",
    "\n",
    "\n",
    "# ── main script ──\n",
    "\n",
    "char_columns = [\n",
    "    \"req_deliverability\",\"point_of_interconnection\",\"type_of_upgrade\",\n",
    "    \"upgrade\",\"description\",\"estimated_time_to_construct\",\"original\",\"item\"\n",
    "]\n",
    "\n",
    "itemized = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 7/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_7_style_Q_itemized.csv\",\n",
    "    char_columns\n",
    ")\n",
    "itemized_addendums = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 7/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_7_style_Q_itemized_addendums.csv\",\n",
    "    char_columns\n",
    ")\n",
    "total = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 7/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_7_style_Q_total.csv\",\n",
    "    char_columns\n",
    ")\n",
    "total_addendums = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 7/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_7_style_Q_total_addendums.csv\",\n",
    "    char_columns\n",
    ")\n",
    "\n",
    "updated_itemized, updated_total = merge_with_addendums(\n",
    "    itemized, itemized_addendums, total, total_addendums\n",
    ")\n",
    "\n",
    "# drop unwanted columns\n",
    "to_drop = [\n",
    "    \"upgrade_classification\",\"estimated\",\"caiso_queue\",\n",
    "    \"project_type\",\"dependent_system_upgrade\"\n",
    "]\n",
    "updated_itemized = updated_itemized.drop(columns=[c for c in to_drop if c in updated_itemized], errors='ignore')\n",
    "updated_total   = updated_total.drop(columns=[c for c in to_drop if c in updated_total],   errors='ignore')\n",
    "\n",
    "# fill & sort\n",
    "fill_cols = [\n",
    "    \"point_of_interconnection\",\"latitude\",\"longitude\",\n",
    "    \"req_deliverability\",\"capacity\"\n",
    "]\n",
    "for df in (updated_itemized, updated_total):\n",
    "    for c in fill_cols:\n",
    "        df[c] = df[c].replace('', np.nan)\n",
    "    df.sort_values('q_id', kind='stable', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    for c in fill_cols:\n",
    "        df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
    "    for c in fill_cols:\n",
    "        df[c].replace(np.nan, '', inplace=True)\n",
    "\n",
    "# save\n",
    "save_data(\n",
    "    updated_itemized,\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 7/01_clean/\"\n",
    "    \"costs_phase_2_cluster_7_style_Q_itemized_updated.csv\",\n",
    "    char_columns\n",
    ")\n",
    "save_data(\n",
    "    updated_total,\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 7/01_clean/\"\n",
    "    \"costs_phase_2_cluster_7_style_Q_total_updated.csv\",\n",
    "    char_columns\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
