{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster 12 Style Q- Table 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: Q1548-Loriner Storage-Appendix_A-C12PhII.pdf from Project 1548\n",
      "Scraped PDF: Q1550-Tanager Storage-Appendix_A-C12PhII.pdf from Project 1550\n",
      "Scraped PDF: Q1552-Cormorant Storage-Appendix_A-C12PhII.pdf from Project 1552\n",
      "Scraped PDF: Q1553-Jewelflower Storage-Appendix_A-C12PhII.pdf from Project 1553\n",
      "Scraped PDF: Q1557-Noosa Energy Storage-Appendix_A-C12PhII.pdf from Project 1557\n",
      "Scraped PDF: Q1558-Hydaspes-Appendix_A-C12PhII.pdf from Project 1558\n",
      "Skipped PDF: Q1565 - Dynamo Solar Phase 2 Report_Rev0_2020.10.16.pdf from Project 1565 (No Table 7)\n",
      "Scraped PDF: Q1565-Dynamo Solar-Appendix_A-C12PhII.pdf from Project 1565\n",
      "Scraped PDF: Q1568-Air Station 1-Appendix_A-C12PhII.pdf from Project 1568\n",
      "Scraped PDF: Q1586-Arcturus_C12PhII_Appendix_A.pdf from Project 1586\n",
      "Scraped PDF: Q1587-Kingsroad Hybrid Solar_C12PhII_Appendix_A.pdf from Project 1587\n",
      "Scraped PDF: Q1591-Carthage_C12PhII_Appendix_A.pdf from Project 1591\n",
      "Scraped PDF: Q1593-Pelicans Jaw Hybrid Solar-Appendix_A-C12PhII.pdf from Project 1593\n",
      "Skipped Addendum PDF: Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Addendum5_Updated.pdf from Project 1593 (No Table 7)\n",
      "Skipped Addendum PDF: Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Report_Addendum5.pdf from Project 1593 (No Table 7)\n",
      "Scraped PDF: Q1594-Grand Lake Hybrid Solar-Appendix_A-C12PhII.pdf from Project 1594\n",
      "Scraped PDF: Q1596-Buttonbush Solar Hybrid Energy Center-Appendix_A-C12PhII_Addendum.pdf from Project 1596\n",
      "Scraped PDF: Q1596-Buttonbush Solar Hybrid Energy Center_C12PhII_Appendix_A.pdf from Project 1596\n",
      "Skipped Addendum PDF: Q1596_ButtonbushSolarHybridEnergyCenter_Appendix_A_Addendum5.pdf from Project 1596 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A.pdf from Project 1604 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A-Attachment 2.pdf from Project 1604 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A-Attachment 1.pdf from Project 1604 (No Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A-Attachment 2 addendum.pdf from Project 1604 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1608-Avocet-AppendixA-Attachment 1.pdf from Project 1608 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1608-Avocet-AppendixA-Attachment 2.pdf from Project 1608 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1608-Avocet-AppendixA.pdf from Project 1608 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1611-CommerceBESS-AppendixA-Attachment 2.pdf from Project 1611 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1611-CommerceBESS-AppendixA-Attachment 1.pdf from Project 1611 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1611-CommerceBESS-AppendixA.pdf from Project 1611 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1612-Speedway-AppendixA.pdf from Project 1612 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1612-Speedway-AppendixA-Attachment 1.pdf from Project 1612 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1612-Speedway-AppendixA-Attachment 2.pdf from Project 1612 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Revision.pdf from Project 1615 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Attachment 1_Revision.pdf from Project 1615 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Attachment 1.pdf from Project 1615 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Attachment 2.pdf from Project 1615 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA.pdf from Project 1615 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A.pdf from Project 1617 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A-Attachment 1.pdf from Project 1617 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A-Attachment 2.pdf from Project 1617 (No Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A-Attachment 2 addendum.pdf from Project 1617 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1619-Goldback-AppendixA.pdf from Project 1619 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1619-Goldback-AppendixA-Attachment 2.pdf from Project 1619 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1619-Goldback-AppendixA-Attachment1.pdf from Project 1619 (No Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Northern-Q1619-Goldback-AppendixA-Addendum.pdf from Project 1619 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1625-Angeleno-AppendixA.pdf from Project 1625 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1625-Angeleno-AppendixA-Attachment 1.pdf from Project 1625 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1625-Angeleno-AppendixA-Attachment 2.pdf from Project 1625 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1628-FortTejon-AppendixA.pdf from Project 1628 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1628-FortTejon-AppendixA-Attachment 2.pdf from Project 1628 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1628-FortTejon-AppendixA-Attachment 1.pdf from Project 1628 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1629-Humidor1-AppendixA-Attachment 2.pdf from Project 1629 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1629-Humidor1-AppendixA-Attachment 1.pdf from Project 1629 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1629-Humidor1-AppendixA.pdf from Project 1629 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1631-Glenfeliz-AppendixA-Attachment 2.pdf from Project 1631 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1631-Glenfeliz-AppendixA.pdf from Project 1631 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1631-Glenfeliz-AppendixA-Attachment 1.pdf from Project 1631 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1632-SanbornHybrid3-AppendixA.pdf from Project 1632 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1632-SanbornHybrid3-AppendixA-Attachment 1.pdf from Project 1632 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Northern-Q1632-SanbornHybrid3-AppendixA-Attachment 2.pdf from Project 1632 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1636-Appendix A.pdf from Project 1636 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1636-Appendix A-Attachment2.pdf from Project 1636 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1636-Appendix A-Attachment1.pdf from Project 1636 (No Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Eastern-Q1636-AppendixA-Addendum.pdf from Project 1636 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Eastern-Q1636-Appendix A-Attachment2-addendum.pdf from Project 1636 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1641-Appendix A-Attachment2.pdf from Project 1641 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1641-Appendix A-Attachment1.pdf from Project 1641 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1641-Appendix A.pdf from Project 1641 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1642-Appendix A.pdf from Project 1642 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1642-Appendix A-Attachment2.pdf from Project 1642 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1642-Appendix A-Attachment1.pdf from Project 1642 (No Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Eastern-Q1642-AppendixA-Addendum.pdf from Project 1642 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Eastern-Q1642-Appendix A-Attachment2-addendum.pdf from Project 1642 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1643-Appendix A-Attachment2.pdf from Project 1643 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1643-Appendix A-Attachment1.pdf from Project 1643 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1643-Appendix A.pdf from Project 1643 (No Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Eastern-Q1643-AppendixA-Addendum.pdf from Project 1643 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Eastern-Q1643-Appendix A-Attachment2-addendum.pdf from Project 1643 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1645-Appendix A-Attachment1.pdf from Project 1645 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1645-Appendix A-Attachment2.pdf from Project 1645 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1645-Appendix A.pdf from Project 1645 (No Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Eastern-Q1645-Appendix A-Attachment2-addendum.pdf from Project 1645 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Eastern-Q1645-Appendix A-Addendum.pdf from Project 1645 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1646-Appendix A-Attachment1.pdf from Project 1646 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1646-Appendix A-Attachment2.pdf from Project 1646 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Eastern-Q1646-Appendix A.pdf from Project 1646 (No Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Eastern-Q1646-Appendix A-Attachment2-addendum.pdf from Project 1646 (No Table 7 and original does not have Table 7)\n",
      "Skipped Addendum PDF: QC12PII-SCE-Eastern-Q1646-AppendixA-Addendum.pdf from Project 1646 (No Table 7 and original does not have Table 7)\n",
      "Skipped PDF: QC12PII-Q1647-TOT905-Attachment 1.pdf from Project 1647 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Attachment 2 Q1647 TOT905.pdf from Project 1647 (No Table 7)\n",
      "Skipped PDF: QC12PII-SCE-Attachment 2 Q1649-TOT931.pdf from Project 1649 (No Table 7)\n",
      "Skipped PDF: QC12PII-Q1649-Appendix A-Attachment 1pdf.pdf from Project 1649 (No Table 7)\n",
      "Skipped PDF: C12P2-GLW-Q1649-Appendix A-GLW-Attachment 2-addendum.pdf from Project 1649 (No Table 7)\n",
      "Skipped PDF: EOP-AS-Q1650&TOT949-RoughHat_AppendixA-Addendum.pdf from Project 1650 (No Table 7)\n",
      "Skipped PDF: C12P2-GLW-Q1650-Appendix A-GLW-Attachment 2-addendum.pdf from Project 1650 (No Table 7)\n",
      "Skipped PDF: QC12PII-Q1650-Appendix A-Attachment 1.pdf from Project 1650 (No Table 7)\n",
      "Skipped PDF: EOP-AS-Q1650&TOT949-RoughHat_AppendixA.pdf from Project 1650 (No Table 7)\n",
      "Skipped PDF: EOP-AS-Q1654&TOT950-YellowPine3_AppendixA.pdf from Project 1654 (No Table 7)\n",
      "Skipped PDF: C12P2-GLW-Q1654-Appendix A-GLW-Attachment 2-addendum.pdf from Project 1654 (No Table 7)\n",
      "Skipped PDF: QC12PII-Q1654-Appendix A-Attachment 1.pdf from Project 1654 (No Table 7)\n",
      "Skipped PDF: EOP-AS-Q1654&TOT950-YellowPine3_AppendixA-Addendum.pdf from Project 1654 (No Table 7)\n",
      "Skipped PDF: EOP-AS-Q1655&TOT961-BonanzaPeak_AppendixA.pdf from Project 1655 (No Table 7)\n",
      "Skipped PDF: QC12PII-Q1655-Appendix A-Attachment 1.pdf from Project 1655 (No Table 7)\n",
      "Skipped PDF: EOP-AS-Q1655&TOT961-BonanzaPeak_AppendixA-Addendum.pdf from Project 1655 (No Table 7)\n",
      "Skipped PDF: C12P2-GLW-Q1655-Appendix A-GLW-Attachment 2-addendum.pdf from Project 1655 (No Table 7)\n",
      "Scraped PDF: QC12PhII_Q1670_Peregrine_Storage_Appendix_A_final_11-20-2020.pdf from Project 1670\n",
      "Scraped PDF: QC12PhII_Q1673_Nighthawk_Storage_Appendix_A_final_11-20-2020.pdf from Project 1673\n",
      "Scraped Addendum PDF: QC12PhII_Q1673_Nighthawk_Storage_Appendix_A_Addendum1_1122021.pdf from Project 1673\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_rawdata_cluster12_style_Q_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_rawdata_cluster12_style_Q_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 98\n",
      "Total Projects Scraped: 16\n",
      "Total Projects Skipped: 82\n",
      "Total Projects Missing: 38\n",
      "Total PDFs Accessed: 127\n",
      "Total PDFs Scraped: 18\n",
      "Total PDFs Skipped: 90\n",
      "\n",
      "List of Scraped Projects:\n",
      "[1548, 1550, 1552, 1553, 1557, 1558, 1565, 1568, 1586, 1587, 1591, 1593, 1594, 1596, 1670, 1673]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[1541, 1542, 1543, 1546, 1549, 1554, 1555, 1556, 1559, 1561, 1564, 1566, 1573, 1574, 1578, 1581, 1582, 1584, 1590, 1592, 1595, 1597, 1598, 1599, 1600, 1601, 1603, 1604, 1605, 1608, 1609, 1610, 1611, 1612, 1613, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1625, 1626, 1628, 1629, 1631, 1632, 1635, 1636, 1637, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1653, 1654, 1655, 1656, 1657, 1658, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1671, 1672, 1674, 1675]\n",
      "\n",
      "List of Missing Projects:\n",
      "[1544, 1545, 1547, 1551, 1560, 1562, 1563, 1567, 1569, 1570, 1571, 1572, 1575, 1576, 1577, 1579, 1580, 1583, 1585, 1588, 1589, 1602, 1606, 1607, 1614, 1623, 1624, 1627, 1630, 1633, 1634, 1638, 1639, 1651, 1652, 1659, 1660, 1676]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['Q1548-Loriner Storage-Appendix_A-C12PhII.pdf', 'Q1550-Tanager Storage-Appendix_A-C12PhII.pdf', 'Q1552-Cormorant Storage-Appendix_A-C12PhII.pdf', 'Q1553-Jewelflower Storage-Appendix_A-C12PhII.pdf', 'Q1557-Noosa Energy Storage-Appendix_A-C12PhII.pdf', 'Q1558-Hydaspes-Appendix_A-C12PhII.pdf', 'Q1565-Dynamo Solar-Appendix_A-C12PhII.pdf', 'Q1568-Air Station 1-Appendix_A-C12PhII.pdf', 'Q1586-Arcturus_C12PhII_Appendix_A.pdf', 'Q1587-Kingsroad Hybrid Solar_C12PhII_Appendix_A.pdf', 'Q1591-Carthage_C12PhII_Appendix_A.pdf', 'Q1593-Pelicans Jaw Hybrid Solar-Appendix_A-C12PhII.pdf', 'Q1594-Grand Lake Hybrid Solar-Appendix_A-C12PhII.pdf', 'Q1596-Buttonbush Solar Hybrid Energy Center-Appendix_A-C12PhII_Addendum.pdf', 'Q1596-Buttonbush Solar Hybrid Energy Center_C12PhII_Appendix_A.pdf', 'QC12PhII_Q1670_Peregrine_Storage_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1673_Nighthawk_Storage_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1673_Nighthawk_Storage_Appendix_A_Addendum1_1122021.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['Q1565 - Dynamo Solar Phase 2 Report_Rev0_2020.10.16.pdf', 'Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Addendum5_Updated.pdf', 'Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Report_Addendum5.pdf', 'Q1596_ButtonbushSolarHybridEnergyCenter_Appendix_A_Addendum5.pdf', 'QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A.pdf', 'QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A-Attachment 2.pdf', 'QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A-Attachment 1.pdf', 'QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A-Attachment 2 addendum.pdf', 'QC12PII-SCE-Metro-Q1608-Avocet-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Metro-Q1608-Avocet-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Metro-Q1608-Avocet-AppendixA.pdf', 'QC12PII-SCE-Metro-Q1611-CommerceBESS-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Metro-Q1611-CommerceBESS-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Metro-Q1611-CommerceBESS-AppendixA.pdf', 'QC12PII-SCE-Metro-Q1612-Speedway-AppendixA.pdf', 'QC12PII-SCE-Metro-Q1612-Speedway-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Metro-Q1612-Speedway-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Revision.pdf', 'QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Attachment 1_Revision.pdf', 'QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA.pdf', 'QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A.pdf', 'QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A-Attachment 1.pdf', 'QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A-Attachment 2.pdf', 'QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A-Attachment 2 addendum.pdf', 'QC12PII-SCE-Northern-Q1619-Goldback-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1619-Goldback-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1619-Goldback-AppendixA-Attachment1.pdf', 'QC12PII-SCE-Northern-Q1619-Goldback-AppendixA-Addendum.pdf', 'QC12PII-SCE-Northern-Q1625-Angeleno-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1625-Angeleno-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Northern-Q1625-Angeleno-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1628-FortTejon-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1628-FortTejon-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1628-FortTejon-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Northern-Q1629-Humidor1-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1629-Humidor1-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Northern-Q1629-Humidor1-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1631-Glenfeliz-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1631-Glenfeliz-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1631-Glenfeliz-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Northern-Q1632-SanbornHybrid3-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1632-SanbornHybrid3-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Northern-Q1632-SanbornHybrid3-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Eastern-Q1636-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1636-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1636-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1636-AppendixA-Addendum.pdf', 'QC12PII-SCE-Eastern-Q1636-Appendix A-Attachment2-addendum.pdf', 'QC12PII-SCE-Eastern-Q1641-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1641-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1641-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1642-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1642-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1642-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1642-AppendixA-Addendum.pdf', 'QC12PII-SCE-Eastern-Q1642-Appendix A-Attachment2-addendum.pdf', 'QC12PII-SCE-Eastern-Q1643-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1643-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1643-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1643-AppendixA-Addendum.pdf', 'QC12PII-SCE-Eastern-Q1643-Appendix A-Attachment2-addendum.pdf', 'QC12PII-SCE-Eastern-Q1645-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1645-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1645-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1645-Appendix A-Attachment2-addendum.pdf', 'QC12PII-SCE-Eastern-Q1645-Appendix A-Addendum.pdf', 'QC12PII-SCE-Eastern-Q1646-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1646-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1646-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1646-Appendix A-Attachment2-addendum.pdf', 'QC12PII-SCE-Eastern-Q1646-AppendixA-Addendum.pdf', 'QC12PII-Q1647-TOT905-Attachment 1.pdf', 'QC12PII-SCE-Attachment 2 Q1647 TOT905.pdf', 'QC12PII-SCE-Attachment 2 Q1649-TOT931.pdf', 'QC12PII-Q1649-Appendix A-Attachment 1pdf.pdf', 'C12P2-GLW-Q1649-Appendix A-GLW-Attachment 2-addendum.pdf', 'EOP-AS-Q1650&TOT949-RoughHat_AppendixA-Addendum.pdf', 'C12P2-GLW-Q1650-Appendix A-GLW-Attachment 2-addendum.pdf', 'QC12PII-Q1650-Appendix A-Attachment 1.pdf', 'EOP-AS-Q1650&TOT949-RoughHat_AppendixA.pdf', 'EOP-AS-Q1654&TOT950-YellowPine3_AppendixA.pdf', 'C12P2-GLW-Q1654-Appendix A-GLW-Attachment 2-addendum.pdf', 'QC12PII-Q1654-Appendix A-Attachment 1.pdf', 'EOP-AS-Q1654&TOT950-YellowPine3_AppendixA-Addendum.pdf', 'EOP-AS-Q1655&TOT961-BonanzaPeak_AppendixA.pdf', 'QC12PII-Q1655-Appendix A-Attachment 1.pdf', 'EOP-AS-Q1655&TOT961-BonanzaPeak_AppendixA-Addendum.pdf', 'C12P2-GLW-Q1655-Appendix A-GLW-Attachment 2-addendum.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Addendum5_Updated.pdf', 'Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Report_Addendum5.pdf', 'Q1596_ButtonbushSolarHybridEnergyCenter_Appendix_A_Addendum5.pdf', 'QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A-Attachment 2 addendum.pdf', 'QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A-Attachment 2 addendum.pdf', 'QC12PII-SCE-Northern-Q1619-Goldback-AppendixA-Addendum.pdf', 'QC12PII-SCE-Eastern-Q1636-AppendixA-Addendum.pdf', 'QC12PII-SCE-Eastern-Q1636-Appendix A-Attachment2-addendum.pdf', 'QC12PII-SCE-Eastern-Q1642-AppendixA-Addendum.pdf', 'QC12PII-SCE-Eastern-Q1642-Appendix A-Attachment2-addendum.pdf', 'QC12PII-SCE-Eastern-Q1643-AppendixA-Addendum.pdf', 'QC12PII-SCE-Eastern-Q1643-Appendix A-Attachment2-addendum.pdf', 'QC12PII-SCE-Eastern-Q1645-Appendix A-Attachment2-addendum.pdf', 'QC12PII-SCE-Eastern-Q1645-Appendix A-Addendum.pdf', 'QC12PII-SCE-Eastern-Q1646-Appendix A-Attachment2-addendum.pdf', 'QC12PII-SCE-Eastern-Q1646-AppendixA-Addendum.pdf', 'QC12PhII_Q1673_Nighthawk_Storage_Appendix_A_Addendum1_1122021.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['Q1548-Loriner Storage-Appendix_A-C12PhII.pdf', 'Q1550-Tanager Storage-Appendix_A-C12PhII.pdf', 'Q1552-Cormorant Storage-Appendix_A-C12PhII.pdf', 'Q1553-Jewelflower Storage-Appendix_A-C12PhII.pdf', 'Q1557-Noosa Energy Storage-Appendix_A-C12PhII.pdf', 'Q1558-Hydaspes-Appendix_A-C12PhII.pdf', 'Q1565 - Dynamo Solar Phase 2 Report_Rev0_2020.10.16.pdf', 'Q1565-Dynamo Solar-Appendix_A-C12PhII.pdf', 'Q1568-Air Station 1-Appendix_A-C12PhII.pdf', 'Q1586-Arcturus_C12PhII_Appendix_A.pdf', 'Q1587-Kingsroad Hybrid Solar_C12PhII_Appendix_A.pdf', 'Q1591-Carthage_C12PhII_Appendix_A.pdf', 'Q1593-Pelicans Jaw Hybrid Solar-Appendix_A-C12PhII.pdf', 'Q1594-Grand Lake Hybrid Solar-Appendix_A-C12PhII.pdf', 'Q1596-Buttonbush Solar Hybrid Energy Center-Appendix_A-C12PhII_Addendum.pdf', 'Q1596-Buttonbush Solar Hybrid Energy Center_C12PhII_Appendix_A.pdf', 'QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A.pdf', 'QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A-Attachment 2.pdf', 'QC12PII-SCE-NOL-Q1604-Aratina2-Appendix A-Attachment 1.pdf', 'QC12PII-SCE-Metro-Q1608-Avocet-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Metro-Q1608-Avocet-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Metro-Q1608-Avocet-AppendixA.pdf', 'QC12PII-SCE-Metro-Q1611-CommerceBESS-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Metro-Q1611-CommerceBESS-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Metro-Q1611-CommerceBESS-AppendixA.pdf', 'QC12PII-SCE-Metro-Q1612-Speedway-AppendixA.pdf', 'QC12PII-SCE-Metro-Q1612-Speedway-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Metro-Q1612-Speedway-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Revision.pdf', 'QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Attachment 1_Revision.pdf', 'QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Metro-Q1615-Kestrel-AppendixA.pdf', 'QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A.pdf', 'QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A-Attachment 1.pdf', 'QC12PII-SCE-NOL-Q1617-SEGSExpansionHybrid-Appendix A-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1619-Goldback-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1619-Goldback-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1619-Goldback-AppendixA-Attachment1.pdf', 'QC12PII-SCE-Northern-Q1625-Angeleno-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1625-Angeleno-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Northern-Q1625-Angeleno-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1628-FortTejon-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1628-FortTejon-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1628-FortTejon-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Northern-Q1629-Humidor1-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1629-Humidor1-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Northern-Q1629-Humidor1-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1631-Glenfeliz-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Northern-Q1631-Glenfeliz-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1631-Glenfeliz-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Northern-Q1632-SanbornHybrid3-AppendixA.pdf', 'QC12PII-SCE-Northern-Q1632-SanbornHybrid3-AppendixA-Attachment 1.pdf', 'QC12PII-SCE-Northern-Q1632-SanbornHybrid3-AppendixA-Attachment 2.pdf', 'QC12PII-SCE-Eastern-Q1636-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1636-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1636-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1641-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1641-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1641-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1642-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1642-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1642-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1643-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1643-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1643-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1645-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1645-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1645-Appendix A.pdf', 'QC12PII-SCE-Eastern-Q1646-Appendix A-Attachment1.pdf', 'QC12PII-SCE-Eastern-Q1646-Appendix A-Attachment2.pdf', 'QC12PII-SCE-Eastern-Q1646-Appendix A.pdf', 'QC12PII-Q1647-TOT905-Attachment 1.pdf', 'QC12PII-SCE-Attachment 2 Q1647 TOT905.pdf', 'QC12PII-SCE-Attachment 2 Q1649-TOT931.pdf', 'QC12PII-Q1649-Appendix A-Attachment 1pdf.pdf', 'C12P2-GLW-Q1649-Appendix A-GLW-Attachment 2-addendum.pdf', 'EOP-AS-Q1650&TOT949-RoughHat_AppendixA-Addendum.pdf', 'C12P2-GLW-Q1650-Appendix A-GLW-Attachment 2-addendum.pdf', 'QC12PII-Q1650-Appendix A-Attachment 1.pdf', 'EOP-AS-Q1650&TOT949-RoughHat_AppendixA.pdf', 'EOP-AS-Q1654&TOT950-YellowPine3_AppendixA.pdf', 'C12P2-GLW-Q1654-Appendix A-GLW-Attachment 2-addendum.pdf', 'QC12PII-Q1654-Appendix A-Attachment 1.pdf', 'EOP-AS-Q1654&TOT950-YellowPine3_AppendixA-Addendum.pdf', 'EOP-AS-Q1655&TOT961-BonanzaPeak_AppendixA.pdf', 'QC12PII-Q1655-Appendix A-Attachment 1.pdf', 'EOP-AS-Q1655&TOT961-BonanzaPeak_AppendixA-Addendum.pdf', 'C12P2-GLW-Q1655-Appendix A-GLW-Attachment 2-addendum.pdf', 'QC12PhII_Q1670_Peregrine_Storage_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1673_Nighthawk_Storage_Appendix_A_final_11-20-2020.pdf']\n",
      "\n",
      "List of Style N PDFs (Skipped due to 'Network Upgrade Type'):\n",
      "['QC12PhII_Q1656_Gypsy_ESS_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1657_Sandpiper_Storage_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1658_Cardinal_Storage_Appendix_A_final_11-20-2020.pdf', 'P2RPT-QC12PhII_Q1658_Cardinal_Storage_Appendix_A_Addendum1_12282020.pdf', 'QC12PhII_Q1661_Obsidian_Wind_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1661_Obsidian_Wind_Appendix_A_final_11-20-2020_v2.pdf', 'QC12PhII_Q1662_Ventasso_Energy_Storage_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1663_Hoodini_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1663_Hoodini_Appendix_A_Addendum1_01122021.pdf', 'QC12PhII_Q1664_Cucapa_Energia_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1664_Cucapa_Energia_Appendix_A_final_11-20-2020_v2.pdf', 'QC12PhII_Q1665_Kingsley_Solar_Farm_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1665_Kingsley_Solar_Farm_Appendix_A_final_11-20-2020_v2.pdf', 'QC12PhII_Q1667_Wistaria_Ranch_Solar_2_Appendix_A_final_11-20-2020_v2.pdf', 'QC12PhII_Q1667_Wistaria_Ranch_Solar_2_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1669_Pome_BESS_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1671_Marine_Depot_Appendix_A_final_11-20-2020.pdf', 'P2RPT-QC12PhII_Q1671_Marine_Depot_Appendix_A_Addendum1_12282020.pdf', 'QC12PhII_Q1675_Resava_Energy_Storage_Appendix_A_final_11-20-2020.pdf']\n",
      "\n",
      "Total Number of Style N PDFs: 19\n",
      "\n",
      "Number of Original PDFs Scraped: 17\n",
      "Number of Addendum PDFs Scraped: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_8618/3402201393.py:1086: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_8618/3402201393.py:1086: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY =\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_rawdata_cluster12_style_Q_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_rawdata_cluster12_style_Q_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_scraping_cluster12_style_Q_log.txt\"\n",
    "PROJECT_RANGE = range(1541, 1677) # Inclusive range for q_ids in Clusters 13\n",
    "\n",
    "\n",
    "# Read the CSV file containing processed projects (with q_id column)\n",
    "processed_csv_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters/costs_phase_2_all_clusters_total.csv\"  # UPDATE THIS PATH\n",
    "processed_df = pd.read_csv(processed_csv_path)\n",
    "# Convert q_id values to numeric then to int for filtering\n",
    "processed_q_ids = pd.to_numeric(processed_df['q_id'], errors='coerce').dropna().astype(int).unique()\n",
    "projects_to_process = sorted([q_id for q_id in PROJECT_RANGE if q_id not in processed_q_ids])\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "style_n_pdfs = []  # List to track style N PDFs\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "original_has_table7 = {}  # Dictionary to track if original PDFs have table7\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters, but keeps parentheses.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            # collapse internal whitespace\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            # strip out everything except letters, digits, spaces, and parentheses\n",
    "            header = re.sub(r'[^a-z0-9\\s\\(\\)]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    elif value is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(value).replace('\\n', ' ').strip()\n",
    "     \n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Conditionally Assigned Network Upgrades\",\n",
    "        \"Local Off-Peak Network Upgrade\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if  re.search(rf\"\\b{re.escape(phrase)}\\b(?=\\d|\\W|$)\", title, re.IGNORECASE):\n",
    "        \n",
    "         #re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        # Updated Cluster Extraction\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        if '12' in clusters:\n",
    "            cluster_number = '12'\n",
    "        elif clusters:\n",
    "            cluster_number = max(clusters, key=lambda x: int(x))  # Choose the highest cluster number found\n",
    "        else:\n",
    "            cluster_number = '12'  # Default to 12 if not found\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 7-1\" to \"Table 7-5\" with hyphen or dot\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*8[-.]([1-2])\\b\", text, re.IGNORECASE):\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 7-1 to 7-6 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1  # Plus two to include possible continuation\n",
    "\n",
    "            print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end}\", file=log_file)\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*8[-.]([1-2])[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(3).strip()\n",
    "                                break\n",
    "\n",
    "                    if table_title:\n",
    "                        if re.search(r\"\\b8-7\\b\", table_title, re.IGNORECASE):\n",
    "                            print(f\"Skipping Table 7-7 on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            continue  # Skip Table 7-7\n",
    "\n",
    "                        # New Table 7 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        # Handle ADNU-specific grouping\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    # Group all adnu rows into one 'upgrade' row\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    # If 'type of upgrade' exists, just rename adnu if needed\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row is none, replace only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' first row for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            # Non-ADNU new tables\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row is none, replace only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for the first row in new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(\"Duplicate columns detected in new table. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table\n",
    "                        if not extracted_tables:\n",
    "                            print(f\"No previous Table 7 detected to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        last_table = extracted_tables[-1]\n",
    "                        expected_columns = last_table.columns.tolist()\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "\n",
    "                        # Check if the first row is a header row\n",
    "                        # As per your latest instruction, we will treat all continuation table rows as data points\n",
    "                        # without any header detection\n",
    "                        # However, you mentioned checking if there is a header row first, so we'll implement that\n",
    "\n",
    "                        # Detect if first row is a header\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\", \"MW at POI\"]\n",
    "                        first_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            any(re.search(rf\"\\b{kw}\\b\", clean_string_cell(cell).lower()) for kw in header_keywords)\n",
    "                            for cell in first_row\n",
    "                        )\n",
    "\n",
    "                        if is_header_row:\n",
    "                            # Handle header row in continuation table\n",
    "                            headers = clean_column_headers(first_row)\n",
    "                            data_rows = data_rows[1:]  # Exclude header row\n",
    "\n",
    "                            # Update expected_columns by adding new columns if any\n",
    "                            new_columns = [col for col in headers if col not in expected_columns]\n",
    "                            if new_columns:\n",
    "                                expected_columns.extend(new_columns)\n",
    "                                print(f\"Added new columns from continuation table: {new_columns}\", file=log_file)\n",
    "\n",
    "                            # Create a mapping of new columns to add with default NaN\n",
    "                            for new_col in new_columns:\n",
    "                                last_table[new_col] = pd.NA\n",
    "\n",
    "                            # Reindex last_table to include new columns\n",
    "                            last_table = last_table.reindex(columns=expected_columns)\n",
    "                            extracted_tables[-1] = last_table\n",
    "\n",
    "                            # Update 'type of upgrade' column in the first row if needed\n",
    "                            if \"type of upgrade\" in headers:\n",
    "                                type_upgrade_idx = headers.index(\"type of upgrade\")\n",
    "                                if pd.isna(data_rows[0][type_upgrade_idx]) or data_rows[0][type_upgrade_idx] == \"\":\n",
    "                                    data_rows[0][type_upgrade_idx] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'type of upgrade' first row for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            elif \"upgrade\" in headers:\n",
    "                                upgrade_idx = headers.index(\"upgrade\")\n",
    "                                if pd.isna(data_rows[0][upgrade_idx]) or data_rows[0][upgrade_idx] == \"\":\n",
    "                                    data_rows[0][upgrade_idx] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'upgrade' first row for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' or 'upgrade' does not exist, add it\n",
    "                                headers.append(\"type of upgrade\")\n",
    "                                expected_columns.append(\"type of upgrade\")\n",
    "                                for idx, row in enumerate(data_rows):\n",
    "                                    data_rows[idx].append(specific_phrase)\n",
    "                                print(f\"Added 'type of upgrade' column and filled with '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                            # Handle ADNU-specific logic if applicable\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                if \"adnu\" in headers:\n",
    "                                    if \"upgrade\" not in headers:\n",
    "                                        # Rename 'adnu' to 'upgrade'\n",
    "                                        adnu_idx = headers.index(\"adnu\")\n",
    "                                        headers[adnu_idx] = \"upgrade\"\n",
    "                                        for row in data_rows:\n",
    "                                            row[adnu_idx] = \" \".join([str(cell) for cell in row[adnu_idx] if pd.notna(cell)])\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in continuation ADNU table.\", file=log_file)\n",
    "                                # Ensure 'type of upgrade' column is filled\n",
    "                                if \"type of upgrade\" not in headers:\n",
    "                                    headers.append(\"type of upgrade\")\n",
    "                                    expected_columns.append(\"type of upgrade\")\n",
    "                                    for row in data_rows:\n",
    "                                        row.append(specific_phrase)\n",
    "                                    print(\"Added 'type of upgrade' column with specific phrase for continuation ADNU table.\", file=log_file)\n",
    "\n",
    "                        else:\n",
    "                            # No header row detected, treat all rows as data points\n",
    "                            print(f\"No header row detected in continuation table on page {page_number + 1}, table {table_index + 1}. Treating all rows as data.\", file=log_file)\n",
    "\n",
    "                        # Create DataFrame for continuation table\n",
    "                        if is_header_row:\n",
    "                            try:\n",
    "                                df_continuation = pd.DataFrame(data_rows, columns=headers)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "                        else:\n",
    "                            # Create DataFrame with expected_columns\n",
    "                            # Handle cases where continuation table has more columns\n",
    "                            standardized_data = []\n",
    "                            for row in data_rows:\n",
    "                                if len(row) < len(expected_columns):\n",
    "                                    # Insert 'type of upgrade' or 'upgrade' with specific_phrase\n",
    "                                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                        # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                        missing_cols = len(expected_columns) - len(row)\n",
    "                                        #row += [specific_phrase] * missing_cols\n",
    "                                        data_rows = [row[:7] + [specific_phrase] + row[7:] for row in data_rows]\n",
    "                                        print(f\"Inserted '{specific_phrase}' for missing columns in ADNU continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                    else:\n",
    "                                        # For non-ADNU tables, assume missing \"type of upgrade\" column\n",
    "                                        missing_cols = len(expected_columns) - len(row)\n",
    "                                        #row += [specific_phrase] * missing_cols\n",
    "                                        data_rows = [ [specific_phrase]  for row in data_rows]\n",
    "                                        print(f\"Inserted '{specific_phrase}' for missing columns in non-ADNU continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                elif len(row) > len(expected_columns):\n",
    "                                    # Add new columns with default names\n",
    "                                    extra_cols = len(row) - len(expected_columns)\n",
    "                                    for i in range(extra_cols):\n",
    "                                        new_col_name = f\"column{len(expected_columns) + 1 + i}\"\n",
    "                                        expected_columns.append(new_col_name)\n",
    "                                        last_table[new_col_name] = pd.NA\n",
    "                                        print(f\"Added new column '{new_col_name}' for extra data in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                    row = row[:len(expected_columns)]\n",
    "\n",
    "                                row_dict = dict(zip(expected_columns, [clean_string_cell(cell) for cell in row]))\n",
    "\n",
    "                                # Handle 'type of upgrade' column\n",
    "                                if \"type of upgrade\" in row_dict and (pd.isna(row_dict[\"type of upgrade\"]) or row_dict[\"type of upgrade\"] == \"\"):\n",
    "                                    row_dict[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'type of upgrade' for a row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                                standardized_data.append(row_dict)\n",
    "\n",
    "                            try:\n",
    "                                df_continuation = pd.DataFrame(standardized_data, columns=expected_columns)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "\n",
    "\n",
    "                             # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                if \"type of upgrade\" in df_continuation.columns:\n",
    "                                    first_row = df_continuation.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                        df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                else:\n",
    "                                    # If \"type of upgrade\" column does not exist, add it\n",
    "                                    df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                            else:\n",
    "                                # General Handling for other tables\n",
    "                                if \"type of upgrade\" in df_continuation.columns:\n",
    "                                    first_row = df_continuation.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                        df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                else:\n",
    "                                    # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                    df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "\n",
    "\n",
    "                        # Handle ADNU-specific logic in continuation tables\n",
    "                        #if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                        #    print(\"Handling ADNU-specific logic in continuation table.\", file=log_file)\n",
    "                        #    if \"upgrade\" in df_continuation.columns and \"adnu\" not in df_continuation.columns:\n",
    "                        #        # Ensure 'upgrade' column is present\n",
    "                        #        if \"upgrade\" not in df_continuation.columns:\n",
    "                        #            df_continuation[\"upgrade\"] = specific_phrase\n",
    "                        #            print(\"Added 'upgrade' column to continuation ADNU table.\", file=log_file)\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\", file=log_file)\n",
    "                            df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                        print(f\"Appended continuation table data to the last extracted table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 7 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After processing all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 7 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 7 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 7 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 7-1 to 7-5.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*8[-.]([1-3])\\b\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def has_network_upgrade_type_column(pdf_path, log_file):\n",
    "    \"\"\"Checks if any table in the PDF has a column header 'Network Upgrade Type'.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables()\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    headers = clean_column_headers(tab[0])\n",
    "                    if \"network upgrade type\" in headers:\n",
    "                        print(f\"Found 'Network Upgrade Type' in PDF {pdf_path} on page {page_number}, table {table_index}.\", file=log_file)\n",
    "                        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking 'Network Upgrade Type' in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path, log_file):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' or 'Revision' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            print(f\"Extracted Text: {text}\", file= log_file)  # Debugging step\n",
    "            # Case-insensitive check for 'Addendum' or 'Revision'\n",
    "            text_lower = text.lower()\n",
    "            return \"addendum\" in text_lower or \"revision\" in text_lower\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    \"\"\"\n",
    "    Appends a suffix to duplicate headers to make them unique.\n",
    "\n",
    "    Args:\n",
    "        headers (list): List of column headers.\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique column headers.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "    SKIP_PROJECTS = {1860, 2003, 2006}\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "\n",
    "\n",
    "        for project_id in projects_to_process:\n",
    "            \n",
    "            # Skip the projects in the SKIP_PROJECTS set\n",
    "            if project_id in SKIP_PROJECTS:\n",
    "                print(f\"Skipping Project {project_id} (marked to skip)\", file=log_file)\n",
    "                continue\n",
    "\n",
    "         \n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"03_phase_2_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Separate PDFs into originals and addendums\n",
    "            list_pdfs = [pdf for pdf in os.listdir(project_path) if pdf.endswith(\".pdf\")]\n",
    "            originals = []\n",
    "            addendums = []\n",
    "            for pdf_name in list_pdfs:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                if is_addendum(pdf_path, log_file):\n",
    "                    addendums.append(pdf_name)\n",
    "                else:\n",
    "                    originals.append(pdf_name)\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Process original PDFs first\n",
    "            for pdf_name in originals:\n",
    "                \n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    # Still check if original has table7\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                original_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "\n",
    "                    if not has_table7:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    # Extract Table 7 and merge\n",
    "                    df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False)\n",
    "                    if not df.empty:\n",
    "                        core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        project_scraped = True\n",
    "                        total_pdfs_scraped += 1\n",
    "                        print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                    else:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Then process addendum PDFs\n",
    "            for pdf_name in addendums:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                addendum_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "\n",
    "                    if not has_table7:\n",
    "                        if original_has_table7.get(project_id, False):\n",
    "                            # Attempt to scrape alternative tables is no longer needed\n",
    "                            # According to the latest request, alternative table scraping is removed\n",
    "                            # Therefore, we skip addendum PDFs that do not have Table 7\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7 and original does not have Table 7)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7 and original does not have Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not is_add and not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    if is_add and base_data_extracted:\n",
    "                        # For addendums, use the extracted base data\n",
    "                        table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                        if table7_data.empty and original_has_table7.get(project_id, False):\n",
    "                            # Scrape alternative tables is removed, so skip if no data\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        if not table7_data.empty:\n",
    "                            # Merge base data with Table 7 data\n",
    "                            merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                            merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                            core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                            scraped_pdfs.append(pdf_name)\n",
    "                            scraped_projects.add(project_id)\n",
    "                            project_scraped = True\n",
    "                            total_pdfs_scraped += 1\n",
    "                            print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    # Optionally, print to ipynb\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "    # Rest of the code remains unchanged...\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nList of Style N PDFs (Skipped due to 'Network Upgrade Type'):\")\n",
    "    print(style_n_pdfs)\n",
    "\n",
    "    print(\"\\nTotal Number of Style N PDFs:\", len(style_n_pdfs))\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.applymap(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: Q1548-Loriner Storage-Appendix_A-C12PhII.pdf from Project 1548\n",
      "Scraped PDF: Q1550-Tanager Storage-Appendix_A-C12PhII.pdf from Project 1550\n",
      "Scraped PDF: Q1552-Cormorant Storage-Appendix_A-C12PhII.pdf from Project 1552\n",
      "Scraped PDF: Q1553-Jewelflower Storage-Appendix_A-C12PhII.pdf from Project 1553\n",
      "Scraped PDF: Q1557-Noosa Energy Storage-Appendix_A-C12PhII.pdf from Project 1557\n",
      "Scraped PDF: Q1558-Hydaspes-Appendix_A-C12PhII.pdf from Project 1558\n",
      "Skipped PDF: Q1565 - Dynamo Solar Phase 2 Report_Rev0_2020.10.16.pdf from Project 1565 (No Table 7)\n",
      "Scraped PDF: Q1565-Dynamo Solar-Appendix_A-C12PhII.pdf from Project 1565\n",
      "Scraped PDF: Q1568-Air Station 1-Appendix_A-C12PhII.pdf from Project 1568\n",
      "Scraped PDF: Q1586-Arcturus_C12PhII_Appendix_A.pdf from Project 1586\n",
      "Scraped PDF: Q1587-Kingsroad Hybrid Solar_C12PhII_Appendix_A.pdf from Project 1587\n",
      "Scraped PDF: Q1591-Carthage_C12PhII_Appendix_A.pdf from Project 1591\n",
      "Scraped PDF: Q1593-Pelicans Jaw Hybrid Solar-Appendix_A-C12PhII.pdf from Project 1593\n",
      "Scraped Addendum PDF: Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Addendum5_Updated.pdf from Project 1593\n",
      "Scraped Addendum PDF: Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Report_Addendum5.pdf from Project 1593\n",
      "Scraped PDF: Q1594-Grand Lake Hybrid Solar-Appendix_A-C12PhII.pdf from Project 1594\n",
      "Scraped PDF: Q1596-Buttonbush Solar Hybrid Energy Center-Appendix_A-C12PhII_Addendum.pdf from Project 1596\n",
      "Scraped PDF: Q1596-Buttonbush Solar Hybrid Energy Center_C12PhII_Appendix_A.pdf from Project 1596\n",
      "Scraped Addendum PDF: Q1596_ButtonbushSolarHybridEnergyCenter_Appendix_A_Addendum5.pdf from Project 1596\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_rawdata_cluster12_style_Q_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_rawdata_cluster12_style_Q_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 75\n",
      "Total Projects Scraped: 14\n",
      "Total Projects Skipped: 61\n",
      "Total Projects Missing: 38\n",
      "Total PDFs Accessed: 41\n",
      "Total PDFs Scraped: 18\n",
      "Total PDFs Skipped: 1\n",
      "\n",
      "List of Scraped Projects:\n",
      "[1548, 1550, 1552, 1553, 1557, 1558, 1565, 1568, 1586, 1587, 1591, 1593, 1594, 1596]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[1541, 1542, 1543, 1546, 1549, 1554, 1555, 1556, 1559, 1561, 1564, 1566, 1573, 1574, 1578, 1581, 1582, 1584, 1590, 1592, 1595, 1597, 1598, 1599, 1600, 1601, 1603, 1605, 1609, 1610, 1613, 1616, 1618, 1620, 1621, 1622, 1626, 1635, 1637, 1640, 1644, 1648, 1653, 1656, 1657, 1658, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675]\n",
      "\n",
      "List of Missing Projects:\n",
      "[1544, 1545, 1547, 1551, 1560, 1562, 1563, 1567, 1569, 1570, 1571, 1572, 1575, 1576, 1577, 1579, 1580, 1583, 1585, 1588, 1589, 1602, 1606, 1607, 1614, 1623, 1624, 1627, 1630, 1633, 1634, 1638, 1639, 1651, 1652, 1659, 1660, 1676]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['Q1548-Loriner Storage-Appendix_A-C12PhII.pdf', 'Q1550-Tanager Storage-Appendix_A-C12PhII.pdf', 'Q1552-Cormorant Storage-Appendix_A-C12PhII.pdf', 'Q1553-Jewelflower Storage-Appendix_A-C12PhII.pdf', 'Q1557-Noosa Energy Storage-Appendix_A-C12PhII.pdf', 'Q1558-Hydaspes-Appendix_A-C12PhII.pdf', 'Q1565-Dynamo Solar-Appendix_A-C12PhII.pdf', 'Q1568-Air Station 1-Appendix_A-C12PhII.pdf', 'Q1586-Arcturus_C12PhII_Appendix_A.pdf', 'Q1587-Kingsroad Hybrid Solar_C12PhII_Appendix_A.pdf', 'Q1591-Carthage_C12PhII_Appendix_A.pdf', 'Q1593-Pelicans Jaw Hybrid Solar-Appendix_A-C12PhII.pdf', 'Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Addendum5_Updated.pdf', 'Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Report_Addendum5.pdf', 'Q1594-Grand Lake Hybrid Solar-Appendix_A-C12PhII.pdf', 'Q1596-Buttonbush Solar Hybrid Energy Center-Appendix_A-C12PhII_Addendum.pdf', 'Q1596-Buttonbush Solar Hybrid Energy Center_C12PhII_Appendix_A.pdf', 'Q1596_ButtonbushSolarHybridEnergyCenter_Appendix_A_Addendum5.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['Q1565 - Dynamo Solar Phase 2 Report_Rev0_2020.10.16.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Addendum5_Updated.pdf', 'Q1593Pelicans_Jaw_Hybrid_Solar_Appendix_A_Report_Addendum5.pdf', 'Q1596_ButtonbushSolarHybridEnergyCenter_Appendix_A_Addendum5.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['Q1548-Loriner Storage-Appendix_A-C12PhII.pdf', 'Q1550-Tanager Storage-Appendix_A-C12PhII.pdf', 'Q1552-Cormorant Storage-Appendix_A-C12PhII.pdf', 'Q1553-Jewelflower Storage-Appendix_A-C12PhII.pdf', 'Q1557-Noosa Energy Storage-Appendix_A-C12PhII.pdf', 'Q1558-Hydaspes-Appendix_A-C12PhII.pdf', 'Q1565 - Dynamo Solar Phase 2 Report_Rev0_2020.10.16.pdf', 'Q1565-Dynamo Solar-Appendix_A-C12PhII.pdf', 'Q1568-Air Station 1-Appendix_A-C12PhII.pdf', 'Q1586-Arcturus_C12PhII_Appendix_A.pdf', 'Q1587-Kingsroad Hybrid Solar_C12PhII_Appendix_A.pdf', 'Q1591-Carthage_C12PhII_Appendix_A.pdf', 'Q1593-Pelicans Jaw Hybrid Solar-Appendix_A-C12PhII.pdf', 'Q1594-Grand Lake Hybrid Solar-Appendix_A-C12PhII.pdf', 'Q1596-Buttonbush Solar Hybrid Energy Center-Appendix_A-C12PhII_Addendum.pdf', 'Q1596-Buttonbush Solar Hybrid Energy Center_C12PhII_Appendix_A.pdf']\n",
      "\n",
      "List of Style N PDFs (Skipped due to 'Network Upgrade Type'):\n",
      "['QC12PhII_Q1656_Gypsy_ESS_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1657_Sandpiper_Storage_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1658_Cardinal_Storage_Appendix_A_final_11-20-2020.pdf', 'P2RPT-QC12PhII_Q1658_Cardinal_Storage_Appendix_A_Addendum1_12282020.pdf', 'QC12PhII_Q1661_Obsidian_Wind_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1661_Obsidian_Wind_Appendix_A_final_11-20-2020_v2.pdf', 'QC12PhII_Q1662_Ventasso_Energy_Storage_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1663_Hoodini_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1663_Hoodini_Appendix_A_Addendum1_01122021.pdf', 'QC12PhII_Q1664_Cucapa_Energia_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1664_Cucapa_Energia_Appendix_A_final_11-20-2020_v2.pdf', 'QC12PhII_Q1665_Kingsley_Solar_Farm_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1665_Kingsley_Solar_Farm_Appendix_A_final_11-20-2020_v2.pdf', 'QC12PhII_Q1667_Wistaria_Ranch_Solar_2_Appendix_A_final_11-20-2020_v2.pdf', 'QC12PhII_Q1667_Wistaria_Ranch_Solar_2_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1669_Pome_BESS_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1670_Peregrine_Storage_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1671_Marine_Depot_Appendix_A_final_11-20-2020.pdf', 'P2RPT-QC12PhII_Q1671_Marine_Depot_Appendix_A_Addendum1_12282020.pdf', 'QC12PhII_Q1673_Nighthawk_Storage_Appendix_A_final_11-20-2020.pdf', 'QC12PhII_Q1673_Nighthawk_Storage_Appendix_A_Addendum1_1122021.pdf', 'QC12PhII_Q1675_Resava_Energy_Storage_Appendix_A_final_11-20-2020.pdf']\n",
      "\n",
      "Total Number of Style N PDFs: 22\n",
      "\n",
      "Number of Original PDFs Scraped: 15\n",
      "Number of Addendum PDFs Scraped: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_8618/1422236648.py:1167: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_8618/1422236648.py:1167: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_rawdata_cluster12_style_Q_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_rawdata_cluster12_style_Q_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_scraping_cluster12_style_Q_log.txt\"\n",
    "PROJECT_RANGE = range(1541, 1677)  # Inclusive range for q_ids in Clusters 12\n",
    "\n",
    "\n",
    "# Read the CSV file containing processed projects (with q_id column)\n",
    "processed_csv_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters/costs_phase_2_all_clusters_total.csv\"  # UPDATE THIS PATH\n",
    "processed_df = pd.read_csv(processed_csv_path)\n",
    "# Convert q_id values to numeric then to int for filtering\n",
    "processed_q_ids = pd.to_numeric(processed_df['q_id'], errors='coerce').dropna().astype(int).unique()\n",
    "projects_to_process = sorted([q_id for q_id in PROJECT_RANGE if q_id not in processed_q_ids])\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "style_n_pdfs = []  # List to track style N PDFs\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "original_has_table7 = {}  # Dictionary to track if original PDFs have table7\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Conditionally Assigned Network Upgrades\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        # Updated Cluster Extraction\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        if '12' in clusters:\n",
    "            cluster_number = '12'\n",
    "        elif clusters:\n",
    "            cluster_number = max(clusters, key=lambda x: int(x))  # Choose the highest cluster number found\n",
    "        else:\n",
    "            cluster_number = '12'  # Default to 12 if not found\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 7-1\" to \"Table 7-5\" with hyphen or dot\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if  re.search(r\"Table\\s*8[-.]([2-6])\\b\", text, re.IGNORECASE): #re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]([1-5])\\b\", text, re.IGNORECASE):\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "\n",
    "                     \n",
    "\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 7-1 to 7-5 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2\n",
    "\n",
    "            print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end}\", file=log_file)\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*8[-.]([2-6])[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(3).strip()\n",
    "                                break\n",
    "\n",
    "                    if table_title:\n",
    "                        # New Table 7 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        # Handle new ADNU tables (grouping logic)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    # Group all adnu rows into one 'upgrade' row\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    # If 'type of upgrade' exists, just rename adnu if needed\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row none, original logic replaced only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' first row for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            # Non-ADNU new tables\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If exist and none in first row original logic replaced only first row\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for the first row in new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(\"Duplicate columns detected in new table. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous Table 7 title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "\n",
    "                        # Check if the number of columns matches\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue  # No table to continue with\n",
    "\n",
    "                        # Define expected columns based on the last extracted table\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "\n",
    "                        # Detect header row in continuation\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\", \"MW at POI\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            actual_header_row = data_rows[0]\n",
    "                            actual_headers = clean_column_headers(actual_header_row)\n",
    "\n",
    "                            # Go back to previous page bounding box to update table title if needed\n",
    "                            if page_number > 0:\n",
    "                                previous_page = pdf.pages[page_number - 1]\n",
    "                                bbox_lower_region = (0, table_bbox[1], previous_page.width, previous_page.height)\n",
    "                                title_text_previous = previous_page.within_bbox(bbox_lower_region).extract_text() or \"\"\n",
    "                                new_table_title = None\n",
    "                                if title_text_previous:\n",
    "                                    title_lines_prev = title_text_previous.split('\\n')[::-1]\n",
    "                                    for line in title_lines_prev:\n",
    "                                        line = line.strip()\n",
    "                                        match_prev = re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]([1-5])[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                                        if match_prev:\n",
    "                                            new_table_title = match_prev.group(3).strip()\n",
    "                                            break\n",
    "                                if new_table_title:\n",
    "                                    specific_phrase = extract_specific_phrase(new_table_title)\n",
    "                                    print(f\"Updated table title from previous page: '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                else:\n",
    "                                    print(\"No table title found in previous page region. Using existing specific_phrase.\", file=log_file)\n",
    "                            else:\n",
    "                                print(\"No previous page available for title extraction for continuation table. Using existing specific_phrase.\", file=log_file)\n",
    "\n",
    "                            # Handle continuation ADNU or non-ADNU\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                                print(\"Continuation ADNU table detected. No grouping, just rename and type of upgrade handling.\", file=log_file)\n",
    "                                if \"type of upgrade\" not in data_rows[0]:\n",
    "                                    # Insert 'type of upgrade' column at the beginning\n",
    "                                    print(\"Inserting 'type of upgrade' column with specific phrase in continuation table.\", file=log_file)\n",
    "                                    data_rows = [ [specific_phrase] + row for row in data_rows ]\n",
    "\n",
    "                                    if \"ADNU\" in data_rows[0]:\n",
    "                                        print(\"Handling continuation for 'Area Delivery Network Upgrade' table\", file=log_file)\n",
    "                                        print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\", file=log_file)\n",
    "                                        # Find the index where \"ADNU\" occurs in the first row\n",
    "                                        adnu_idx = data_rows[0].index(\"ADNU\")\n",
    "                                        # Replace \"ADNU\" with \"upgrade\" in that column for every row\n",
    "                                        data_rows[0][adnu_idx] = \"upgrade\"\n",
    "                            else:\n",
    "                                # General Handling for other tables\n",
    "                                if \"type of upgrade\" not in data_rows[0]:\n",
    "                                    # Insert 'type of upgrade' column at the beginning\n",
    "                                    print(\"Inserting 'type of upgrade' column with specific phrase in continuation table.\", file=log_file)\n",
    "                                    data_rows = [ [specific_phrase] + row for row in data_rows ]\n",
    "\n",
    "                                    if \"Type of Upgrade\" in data_rows[0]:\n",
    "                                        print(\"Handling continuation for non-ADNU table.\", file=log_file)\n",
    "                                        print(\"Renaming 'Type of Upgrade' to 'upgrade' in continuation table.\", file=log_file)\n",
    "                                        # Find the index where \"Type of Upgrade\" occurs in the first row\n",
    "                                        upgrade_idx = data_rows[0].index(\"Type of Upgrade\")\n",
    "                                        # Replace \"Type of Upgrade\" with \"upgrade\" in that column for every row\n",
    "                                        data_rows[0][upgrade_idx] = \"upgrade\"\n",
    "\n",
    "                        # Handle missing or extra columns\n",
    "                        if len(data_rows[0]) < expected_columns:\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                print(f\"Detected missing 'upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'upgrade' column.\", file=log_file)\n",
    "                                data_rows = [row[:7] + [specific_phrase] + row[7:] for row in data_rows]\n",
    "\n",
    "                            else:\n",
    "                                # For other tables, assume missing \"Type of Upgrade\" column\n",
    "                                print(f\"Detected missing 'Type of Upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'Type of Upgrade' column.\", file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                        elif len(data_rows[0]) > expected_columns:\n",
    "                            # Extra columns detected; adjust accordingly\n",
    "                            print(f\"Detected extra columns in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping extra columns.\", file=log_file)\n",
    "                            data_rows = [row[:expected_columns] for row in data_rows]\n",
    "\n",
    "                        # Create DataFrame for the continuation table\n",
    "                        if is_header_row:\n",
    "                            data_rows = data_rows[1:]\n",
    "                            print(f\"Dropped header row from data_rows after modifications for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue  # Skip this table due to error\n",
    "\n",
    "                        # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"type of upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        else:\n",
    "                            # General Handling for other tables\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\", file=log_file)\n",
    "                            df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)            \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 7 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After processing all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "def scrape_alternative_tables(pdf_path, log_file, specific_phrases):\n",
    "    \"\"\"\n",
    "    Scrapes all tables in the PDF that contain any of the specific phrases in their cells.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        specific_phrases (list): List of specific phrases to search for.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted data from tables containing the specific phrases.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for alternative table scraping...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    # Check if any cell contains any of the specific phrases\n",
    "                    match_found = False\n",
    "                    for row in tab:\n",
    "                        for cell in row:\n",
    "                            if cell and any(re.search(rf\"\\b{re.escape(phrase)}\\b\", cell, re.IGNORECASE) for phrase in specific_phrases):\n",
    "                                match_found = True\n",
    "                                break\n",
    "                        if match_found:\n",
    "                            break\n",
    "                    if match_found:\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        headers = make_unique_headers(headers)  # Ensure headers are unique\n",
    "                        data_rows = tab[1:]\n",
    "                        try:\n",
    "                            df = pd.DataFrame(data_rows, columns=headers)\n",
    "                            extracted_tables.append(df)\n",
    "                            print(f\"Extracted alternative table from page {page_number}, table {table_index}.\", file=log_file)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for alternative table on page {page_number}, table {table_index}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping alternative tables in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            # Ensure columns are unique before reindexing\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted alternative tables...\", file=log_file)\n",
    "        try:\n",
    "            alternative_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} alternative tables.\", file=log_file)\n",
    "            return alternative_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating alternative tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No alternative tables found containing specific phrases.\", file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 7 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        if is_addendum and original_has_table7.get(project_id, False):\n",
    "            # Scrape alternative tables based on specific phrases\n",
    "            specific_phrases = [\n",
    "                \"PTO\",\n",
    "                \"Reliability Network Upgrade\",\n",
    "                \"Area Delivery Network Upgrade\",\n",
    "                \"Local Delivery Network\",\n",
    "                \"Other Potential Network Upgrade\",\n",
    "                \"Area Delivery Network Upgrades\",\n",
    "                \"Conditionally Assigned Network Upgrades\",\n",
    "                \"ADNU\",\n",
    "                \"LDNU\",\n",
    "                \"RNU\"\n",
    "            ]\n",
    "            alternative_data = scrape_alternative_tables(pdf_path, log_file, specific_phrases)\n",
    "            if not alternative_data.empty:\n",
    "                table7_data = alternative_data\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 7 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 7 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 7-1 to 7-5.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*8[-.]([2-3])\\b\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def has_network_upgrade_type_column(pdf_path, log_file):\n",
    "    \"\"\"Checks if any table in the PDF has a column header 'Network Upgrade Type'.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables()\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    headers = clean_column_headers(tab[0])\n",
    "                    if \"network upgrade type\" in headers:\n",
    "                        print(f\"Found 'Network Upgrade Type' in PDF {pdf_path} on page {page_number}, table {table_index}.\", file=log_file)\n",
    "                        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking 'Network Upgrade Type' in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path, log_file):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' or 'Revision' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            print(f\"Extracted Text: {text}\", file= log_file)  # Debugging step\n",
    "            # Case-insensitive check for 'Addendum' or 'Revision'\n",
    "            text_lower = text.lower()\n",
    "            return \"addendum\" in text_lower or \"revision\" in text_lower\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    \"\"\"\n",
    "    Appends a suffix to duplicate headers to make them unique.\n",
    "\n",
    "    Args:\n",
    "        headers (list): List of column headers.\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique column headers.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in projects_to_process:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"03_phase_2_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            \n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Separate PDFs into originals and addendums\n",
    "            list_pdfs = [pdf for pdf in os.listdir(project_path) if pdf.endswith(\".pdf\")]\n",
    "            originals = []\n",
    "            addendums = []\n",
    "            for pdf_name in list_pdfs:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                if is_addendum(pdf_path, log_file):\n",
    "                    addendums.append(pdf_name)\n",
    "                else:\n",
    "                    originals.append(pdf_name)\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Process original PDFs first\n",
    "            for pdf_name in originals:\n",
    "                \n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    # Still check if original has table7\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                original_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "\n",
    "                    if not has_table7:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    # Extract Table 7 and merge\n",
    "                    df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False)\n",
    "                    if not df.empty:\n",
    "                        core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        project_scraped = True\n",
    "                        total_pdfs_scraped += 1\n",
    "                        print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                    else:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Then process addendum PDFs\n",
    "            for pdf_name in addendums:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                addendum_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "\n",
    "                    if not has_table7:\n",
    "                        if original_has_table7.get(project_id, False):\n",
    "                            # Attempt to scrape alternative tables\n",
    "                            table7_data = scrape_alternative_tables(pdf_path, log_file, [\n",
    "                                \"PTO\",\n",
    "                                \"Reliability Network Upgrade\",\n",
    "                                \"Area Delivery Network Upgrade\",\n",
    "                                \"Local Delivery Network\",\n",
    "                                \"Other Potential Network Upgrade\",\n",
    "                                \"Area Delivery Network Upgrades\",\n",
    "                                \"Conditionally Assigned Network Upgrades\",\n",
    "                                 \n",
    "                            ])\n",
    "                            if not table7_data.empty:\n",
    "                                # Merge base data with Table 7 data\n",
    "                                merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\", file=log_file)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7 and original does not have Table 7)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 7 and original does not have Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not is_add and not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    if is_add and base_data_extracted:\n",
    "                        # For addendums, use the extracted base data\n",
    "                        table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                        if table7_data.empty and original_has_table7.get(project_id, False):\n",
    "                            # Scrape alternative tables\n",
    "                            table7_data = scrape_alternative_tables(pdf_path, log_file, [\n",
    "                                \"PTO\",\n",
    "                                \"Reliability Network Upgrade\",\n",
    "                                \"Area Delivery Network Upgrade\",\n",
    "                                \"Local Delivery Network\",\n",
    "                                \"Other Potential Network Upgrade\",\n",
    "                                \"Area Delivery Network Upgrades\",\n",
    "                                \"Conditionally Assigned Network Upgrades\",\n",
    "                                \"ADNU\",\n",
    "                                \"LDNU\",\n",
    "                                \"RNU\"\n",
    "                            ])\n",
    "                        if not table7_data.empty:\n",
    "                            \n",
    "\n",
    "\n",
    "                            \n",
    "                            # Merge base data with Table 7 data\n",
    "                            merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                            merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                            core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                            scraped_pdfs.append(pdf_name)\n",
    "                            scraped_projects.add(project_id)\n",
    "                            project_scraped = True\n",
    "                            total_pdfs_scraped += 1\n",
    "                            print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                            # Optionally, print to ipynb\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    # Optionally, print to ipynb\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "    # Rest of the code remains unchanged...\n",
    "\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nList of Style N PDFs (Skipped due to 'Network Upgrade Type'):\")\n",
    "    print(style_n_pdfs)\n",
    "\n",
    "    print(\"\\nTotal Number of Style N PDFs:\", len(style_n_pdfs))\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.applymap(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total and Itemized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_2_cluster_12_style_Q_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_12_style_Q_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU' 'LOPNU' 'CANU']\n",
      "[1548 1550 1552 1553 1557 1558 1565 1568 1586 1587 1591 1593 1594 1596]\n",
      "[12]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/03_raw/ph2_rawdata_cluster12_style_Q_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\"\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"estimated cost x 1000\",\n",
    "            \"allocated_cost\",\n",
    "            \"sum of allocated constant cost\"\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "#df.drop('incremental deliverability', axis=1, inplace=True)\n",
    "#df.drop('dependent system upgrade', axis=1, inplace=True)\n",
    "#df.drop('upgrade_classification', axis=1, inplace=True)\n",
    "\n",
    "#STEP 2: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "required_columns = ['type_of_upgrade', 'cost_allocation_factor']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "\n",
    "df = df[\n",
    "    ~df['type_of_upgrade'].str.contains(r'Precursor Network Upgrades \\(PNU\\)|Estimated in Service Date', na=False)\n",
    "]\n",
    " \n",
    " \n",
    "# Step 3: Rename 'Grand Total' to 'Total' in total_estimated_cost_x_1000\n",
    "if 'total_estimated_cost_x_1000' in df.columns:\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Grand Total', 'Total')\n",
    "\n",
    "# Step 4: Move 'Total' from total_estimated_cost_x_1000 to cost_allocation_factor\n",
    "if 'total_estimated_cost_x_1000' in df.columns and 'cost_allocation_factor' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: 'Total' if str(row['total_estimated_cost_x_1000']) == 'Total' else row['cost_allocation_factor'],\n",
    "        axis=1\n",
    "    )\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Total', None)\n",
    "\n",
    "df = df[df[\"description\"] != \"Total Allocated\"]    \n",
    "\n",
    "if 'description' in df.columns and 'cost_allocation_factor' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: 'Total' if str(row['description']) == 'Total' else row['cost_allocation_factor'],\n",
    "        axis=1\n",
    "    )\n",
    "    df['description'] = df['description'].replace('Total', None)\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "   \n",
    "df.drop('upgrade_classification', axis=1, inplace=True)\n",
    "\n",
    "# Step 5: Clean the type of upgrade column\n",
    "   \n",
    " \n",
    "df['type_of_upgrade'] = (\n",
    "    df['type_of_upgrade']\n",
    "    .fillna('')  # Temporarily replace NaN with an empty string\n",
    "    .str.replace(r'\\(Note \\d+\\)', '', regex=True)  # Remove (Note digit)\n",
    "    .str.strip()  # Strip leading/trailing whitespace\n",
    "    .str.title()  # Capitalize the first letter of each word\n",
    "    .str.replace(r'Upgrades$', 'Upgrade', regex=True)  # Fix plural endings\n",
    "    .replace('', pd.NA)  # Convert empty strings back to NaN\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'Reliability Network Upgrade': 'RNU',\n",
    "    'Reliability Network Upgrades': 'RNU',\n",
    "    'Local Delivery Network Upgrade': 'LDNU',\n",
    "    'Local Delivery Network': 'LDNU',\n",
    "    'Potential Local Delivery Network Upgrade': 'LDNU',\n",
    "    \"Ptos Interconnection Facilities\": 'PTO_IF',\n",
    "    'Area Delivery Network Upgrade': 'ADNU',\n",
    "    'Reliability Network Upgrade To Physically Interconnect': 'RNU',\n",
    "    \"Reliability Network upgrade To Physically Interconnect\": \"RNU\",\n",
    "     'Pto': 'PTO_IF',\n",
    "    \"Other Potential Network Upgrade\": \"OPNU\",\n",
    "    \"Conditionally Assigned Network Upgrade\": \"CANU\",\n",
    "    \"Canus\": \"CANU\",\n",
    "        \"Escalated Cost And Time To Construct For Reliability Network Upgrade4\": \"RNU\",\n",
    "    \"Escalated Cost And Time To Construct For Reliability Network Upgrade3\": \"RNU\",\n",
    "    \"Local Off-Peak Network Upgrade\": \"LOPNU\",\n",
    "    'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    "  \n",
    " 'Total ADNU': 'ADNU',\n",
    "  'Ptos Interconnect Ion Facilities' : 'PTO_IF',\n",
    "  'Local Off- Peak Network Upgrade': 'LOPNU',\n",
    " 'P Os Interconnection Facilities': 'PTO_IF',\n",
    "  \n",
    "\n",
    " \n",
    "}\n",
    "\n",
    "\n",
    "#Step 6: Apply mapping and ffill type of upgrade column\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()  \n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# build a boolean mask: True for rows where cost contains \"()\"\n",
    "has_paren = df['estimated_cost_x_1000'].astype(str).str.contains(r'\\([^)]*\\)')\n",
    "# drop those rows\n",
    "df = df.loc[~has_paren].copy()\n",
    "\n",
    "def drop_rows_with_star_in_costs(df, cost_cols):\n",
    "    \"\"\"\n",
    "    Remove any row where any of the cost_cols contains a '*' character.\n",
    "    \"\"\"\n",
    "    mask = pd.Series(False, index=df.index)\n",
    "    for col in cost_cols:\n",
    "        if col in df.columns:\n",
    "            # star anywhere in the string\n",
    "            mask = mask | df[col].astype(str).str.contains(r\"\\*\", regex=True)\n",
    "    # keep only rows without a star\n",
    "    return df.loc[~mask].reset_index(drop=True)\n",
    "\n",
    "def drop_rows_with_dash_in_time(df, time_col):\n",
    "    \"\"\"\n",
    "    Remove any row where the time_col contains a dash '-' (e.g. '-').\n",
    "    \"\"\"\n",
    "    if time_col in df.columns:\n",
    "        mask = df[time_col].astype(str).str.contains(r\"^-+$\", regex=True)\n",
    "        return df.loc[~mask].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def keep_second_entry_in_cells(df, columns):\n",
    "    \"\"\"\n",
    "    For each column in columns, if the cell contains multiple spaceseparated entries,\n",
    "    keep only the second one.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        def pick_second(cell):\n",
    "            parts = re.findall(r\"[\\d\\.\\$%,]+\", str(cell))\n",
    "            return parts[1] if len(parts) > 1 else (parts[0] if parts else cell)\n",
    "        df[col] = df[col].apply(pick_second)\n",
    "    return df\n",
    "\n",
    "#  Integration \n",
    "# Place this just before your Step7 clean_currency block:\n",
    "\n",
    "# 1) drop any row where estimated or escalated cost has '*'\n",
    "df = drop_rows_with_star_in_costs(\n",
    "    df,\n",
    "    cost_cols=[\n",
    "        'estimated_cost_x_1000',\n",
    "        'escalated_cost_x_1000',\n",
    "        'total_estimated_cost_x_1000',\n",
    "        'total_estimated_cost_x_1000_escalated'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2) drop any row where estimated_time_to_construct is just a dash\n",
    "df = drop_rows_with_dash_in_time(df, 'estimated_time_to_construct')\n",
    "\n",
    "# 3) if multiple entries exist in a cell, keep only the second\n",
    "df = keep_second_entry_in_cells(\n",
    "    df,\n",
    "    columns=[\n",
    "        'cost_allocation_factor',\n",
    "        'estimated_cost_x_1000',\n",
    "        'escalated_cost_x_1000',\n",
    "        'total_estimated_cost_x_1000',\n",
    "        'total_estimated_cost_x_1000_escalated'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now proceed with your clean_currency step\n",
    "\n",
    "\n",
    "    \n",
    "# Step 7: Remove $ signs and convert to numeric\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000', 'adnu_cost_rate_x_1000_escalated']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated',   'adnu_cost_rate_x_1000_escalated']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "                # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = group[\n",
    "            (group['type_of_upgrade'] == upgrade) & (group['item'] == 'no')\n",
    "        ].shape[0] > 0\n",
    "        \n",
    "        if total_exists:\n",
    "             \n",
    "            continue\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "        if not total_exists:\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate specified columns from the existing row\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns (single row, so it remains the same)\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate the specified columns from the first row in the group\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 8: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "# Step 9: Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()\n",
    "\n",
    "if 'upgrade' in df.columns and 'type_of_upgrade' in df.columns and 'q_id' in df.columns:\n",
    "    df['upgrade'] = df.groupby(['q_id', 'type_of_upgrade'])['upgrade'].ffill()\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/02_intermediate/costs_phase_2_cluster_12_style_Q_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/02_intermediate/costs_phase_2_cluster_12_style_Q_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_12_style_Q_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_12_style_Q_total.csv'.\")\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Only two so made the change manually(one row in each) as was hard to scrape - 1593, 1596"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/02_intermediate/costs_phase_2_cluster_12_style_Q_itemized.csv')\n",
    "df2 = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/02_intermediate/costs_phase_2_cluster_12_style_Q_total.csv')\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/01_clean/costs_phase_2_cluster_12_style_Q_itemized_updated.csv', index=False)\n",
    "df2.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/01_clean/costs_phase_2_cluster_12_style_Q_total_updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
