{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/02_intermediate/costs_phase_2_cluster_7_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_N_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_G_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/02_intermediate/costs_phase_2_cluster_9_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/02_intermediate/costs_phase_2_cluster_9_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 8/02_intermediate/costs_phase_2_cluster_8_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 6/02_intermediate/costs_phase_2_cluster_6_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/02_intermediate/costs_phase_2_cluster_12_style_N_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/02_intermediate/costs_phase_2_cluster_12_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 3/02_intermediate/costs_phase_2_cluster_3_style_new_itemized_addendums.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '643W'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 309\u001b[0m\n\u001b[1;32m    306\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Run the function\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m create_addendum_list_and_mark_original(root_folder, output_folder)\n",
      "Cell \u001b[0;32mIn[9], line 157\u001b[0m, in \u001b[0;36mcreate_addendum_list_and_mark_original\u001b[0;34m(root_folder, output_folder)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m    156\u001b[0m                     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_id\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m--> 157\u001b[0m                         addendum_qids\u001b[38;5;241m.\u001b[39mextend(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39munique())  \u001b[38;5;66;03m# Convert to integers\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Remove duplicates from addendum_qids and sort them\u001b[39;00m\n\u001b[1;32m    160\u001b[0m addendum_qids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(addendum_qids))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6639\u001b[0m     ]\n\u001b[1;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    432\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    433\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    434\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    435\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[1;32m    436\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '643W'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"original\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_time_to_construct\",\n",
    "        \"estimated_cost\",\n",
    "        \"escalated_cost\",\n",
    "        \"total_estimated_cost\",\n",
    "        \"total_escalated_cost\",\n",
    "        \n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df  \n",
    "\n",
    "\n",
    "def replace_text_with_zero(value):\n",
    "    \"\"\"\n",
    "    Cleans a value by removing percentage symbols, converting numeric values, and replacing text with zero.\n",
    "    Ignores NA values and retains original values if conversion fails.\n",
    "\n",
    "    Args:\n",
    "        value (str/int/float): The value to be processed.\n",
    "\n",
    "    Returns:\n",
    "        int/float/str: Cleaned numeric value, original value if conversion fails, or NA if value is missing.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):  # Ignore NA values and return as is\n",
    "        return value  \n",
    "\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "\n",
    "        # Remove percentage symbols and convert to a numeric value\n",
    "        if value.endswith('%'):\n",
    "            value = value.replace('%', '')  # Remove % symbol\n",
    "\n",
    "        # If value contains alphabetic characters, consider it as non-numeric and return 0\n",
    "        if re.search(r'[a-zA-Z]', value):\n",
    "            return 0\n",
    "        \n",
    " \n",
    "\n",
    "    try:\n",
    "        return pd.to_numeric(value, errors='coerce') if value != '' else value\n",
    "    except (ValueError, TypeError):\n",
    "        return value  # Keep the original value if conversion fails\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "columns_to_clean = ['cost_allocation_factor', 'estimated_cost']        \n",
    "\n",
    "\n",
    "def clean_text(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing unwanted characters and patterns,\n",
    "    such as '$', '*', and text like '6months' while keeping numeric ranges (e.g., '6-24') intact.\n",
    "    \n",
    "    Args:\n",
    "        value (str): The value to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "        float/int/str: Cleaned numeric value or original string if numeric patterns are detected.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Remove unwanted characters like $, * and \"(Note 2)\" references\n",
    "        \n",
    "     \n",
    "        \n",
    "        # Replace \"6months\", \"12months\" etc., while preserving numeric ranges like \"6-24\"\n",
    "        value = re.sub(r'(\\d+)months', r'\\1', value, flags=re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        return pd.to_numeric(value)  # Convert to numeric type where possible\n",
    "    except ValueError:\n",
    "        return value  # Return the cleaned string if conversion fails\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        #value = re.sub(r'months','', value)\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "\n",
    "\n",
    "def create_addendum_list_and_mark_original(root_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Collects all q_ids from itemized_addendums files in each cluster folder, creates an addendum_projects_list.csv,\n",
    "    and adds an 'original' column to the combined itemized and total datasets based on the presence in the addendum list.\n",
    "\n",
    "    Args:\n",
    "        root_folder (str): Path to the root folder containing cluster folders.\n",
    "        output_folder (str): Path to save the addendum_projects_list.csv and combined datasets.\n",
    "    \"\"\"\n",
    "    addendum_qids = []\n",
    "\n",
    "    # Step 1: Collect all q_ids from itemized_addendums files\n",
    "    for cluster_folder in os.listdir(root_folder):\n",
    "        cluster_path = os.path.join(root_folder, cluster_folder)\n",
    "        if os.path.isdir(cluster_path):  # Ensure it's a directory\n",
    "            intermediate_folder = os.path.join(cluster_path, \"02_intermediate\")\n",
    "            if os.path.exists(intermediate_folder):  # Check if 02_intermediate exists\n",
    "                for file_name in os.listdir(intermediate_folder):\n",
    "                    if \"itemized_addendums.csv\" in file_name:\n",
    "                        file_path = os.path.join(intermediate_folder, file_name)\n",
    "                        print(f\"Processing: {file_path}\")\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        if 'q_id' in df.columns:\n",
    "                            addendum_qids.extend(df['q_id'].dropna().astype(int).unique())  # Convert to integers\n",
    "\n",
    "    # Remove duplicates from addendum_qids and sort them\n",
    "    addendum_qids = sorted(set(addendum_qids))\n",
    "\n",
    "    # Save addendum_projects_list.csv\n",
    "    addendum_list_df = pd.DataFrame({'q_id': addendum_qids})\n",
    "    addendum_list_file = os.path.join(output_folder, \"addendum_projects_list.csv\")\n",
    "    addendum_list_df.to_csv(addendum_list_file, index=False)\n",
    "    print(f\"Addendum projects list saved to: {addendum_list_file}\")\n",
    "    print(f\"Total number of projects in addendum_projects_list: {len(addendum_qids)}\")\n",
    "\n",
    "    # Step 2: Combine all itemized_updated and total_updated files\n",
    "    combined_itemized = []\n",
    "    combined_total = []\n",
    "\n",
    "    for cluster_folder in os.listdir(root_folder):\n",
    "        cluster_path = os.path.join(root_folder, cluster_folder)\n",
    "        if os.path.isdir(cluster_path):  # Ensure it's a directory\n",
    "            clean_folder = os.path.join(cluster_path, \"01_clean\")\n",
    "            if os.path.exists(clean_folder):  # Check if 01_clean exists\n",
    "                for file_name in os.listdir(clean_folder):\n",
    "                    file_path = os.path.join(clean_folder, file_name)\n",
    "                    if \"itemized_updated.csv\" in file_name:\n",
    "                        print(f\"Loading: {file_path}\")\n",
    "                        itemized_df = pd.read_csv(file_path)\n",
    "                        combined_itemized.append(itemized_df)\n",
    "                    elif \"total_updated.csv\" in file_name:\n",
    "                        print(f\"Loading: {file_path}\")\n",
    "                        total_df = pd.read_csv(file_path)\n",
    "                        combined_total.append(total_df)\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "    # Combine all itemized datasets\n",
    "    if combined_itemized:\n",
    "        combined_itemized_df = pd.concat(combined_itemized, ignore_index=True)\n",
    "\n",
    "        # Convert q_id to integers\n",
    "        combined_itemized_df['q_id'] = combined_itemized_df['q_id'].fillna(0).astype(str)\n",
    "        \n",
    "\n",
    "        # Add 'original' column to itemized dataset\n",
    "        combined_itemized_df['original'] = combined_itemized_df['q_id'].apply(\n",
    "            lambda qid: 'no' if qid in addendum_qids else 'yes'\n",
    "        )\n",
    "\n",
    "        # Reorder columns to place 'original' next to 'q_id'\n",
    "        if 'q_id' in combined_itemized_df.columns and 'original' in combined_itemized_df.columns:\n",
    "            cols = list(combined_itemized_df.columns)\n",
    "            cols.insert(cols.index('q_id') + 1, cols.pop(cols.index('original')))\n",
    "            combined_itemized_df = combined_itemized_df[cols]\n",
    "\n",
    "        # Sort by q_id\n",
    "        combined_itemized_df.drop(['estimate_d_time_to_construc_t','estimated_cost_x_1000_escalated_with_itcca' ,'adnu_cost_rate_x_1000_escalated',  'potential_duration_months',\n",
    "                                   'none_7', 'none_8', 'network_upgrade_type', 'adnu_cost_rate_escalated_x_1000','upgrade_classification', 'sum_of_reallocated_share',\t'sum_of_reallocated_cost_x_1000_constant_dollar_2022',\n",
    "                                       \t'sum_of_reallocated_costs_x_1000_escalated_constant_dollars_od_year',\n",
    "                                   'ttype_of_upgrade','adnu_cost_rate_x_1000'], axis=1, errors='ignore', inplace=True)\n",
    "        combined_itemized_df.rename(columns={'estimated_cost_x_1000': 'estimated_cost', 'escalated_cost_x_1000': 'escalated_cost', 'total_estimated_cost_x_1000': 'total_estimated_cost',\n",
    "                                             'total_estimated_cost_x_1000_escalated':'total_escalated_cost',}, inplace=True)\n",
    "        \n",
    "        for col in ['estimated_cost', 'escalated_cost', 'total_estimated_cost', 'total_escalated_cost', ]:\n",
    "            if col in combined_itemized_df.columns:\n",
    "                combined_itemized_df[col] = combined_itemized_df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "        for col in ['estimated_time_to_construct']:\n",
    "            if col in combined_itemized_df.columns:\n",
    "                combined_itemized_df[col]=combined_itemized_df[col].apply(clean_text)   \n",
    "\n",
    "        for col in columns_to_clean:\n",
    "            if col in combined_itemized_df.columns:\n",
    "                combined_itemized_df[col] = combined_itemized_df[col].apply(replace_text_with_zero)      \n",
    " \n",
    "        \n",
    "        combined_itemized_df=reorder_columns(combined_itemized_df)\n",
    "\n",
    "\n",
    "        combined_itemized_df = combined_itemized_df.sort_values(by=\"q_id\", kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "        # Save the combined itemized dataset\n",
    "        combined_itemized_file = os.path.join(output_folder, \"costs_phase_2_all_clusters_itemized.csv\")\n",
    "        combined_itemized_df.to_csv(combined_itemized_file, index=False)\n",
    "        print(f\"Combined itemized dataset with 'original' column saved to: {combined_itemized_file}\")\n",
    "\n",
    "    # Combine all total datasets\n",
    "    if combined_total:\n",
    "        combined_total_df = pd.concat(combined_total, ignore_index=True)\n",
    "\n",
    "        # Convert q_id to integers\n",
    "        combined_total_df['q_id'] = combined_total_df['q_id'].fillna(0).astype(str)\n",
    "\n",
    "        # Add 'original' column to total dataset\n",
    "        combined_total_df['original'] = combined_total_df['q_id'].apply(\n",
    "            lambda qid: 'no' if qid in addendum_qids else 'yes'\n",
    "        )\n",
    "\n",
    "        # Reorder columns to place 'original' next to 'q_id'\n",
    "        if 'q_id' in combined_total_df.columns and 'original' in combined_total_df.columns:\n",
    "            cols = list(combined_total_df.columns)\n",
    "            cols.insert(cols.index('q_id') + 1, cols.pop(cols.index('original')))\n",
    "            combined_total_df = combined_total_df[cols]\n",
    "\n",
    "        combined_total_df.drop(['estimate_d_time_to_construc_t','estimated_cost_x_1000_escalated_with_itcca' ,'adnu_cost_rate_x_1000_escalated',  'potential_duration_months',\n",
    "                                   'none_7', 'none_8', 'network_upgrade_type', 'adnu_cost_rate_escalated_x_1000','upgrade_classification', 'sum_of_reallocated_share',\t'sum_of_reallocated_cost_x_1000_constant_dollar_2022',\n",
    "                                       \t'sum_of_reallocated_costs_x_1000_escalated_constant_dollars_od_year',\n",
    "                                   'ttype_of_upgrade','adnu_cost_rate_x_1000'], errors='ignore', axis=1, inplace=True)\n",
    "        combined_total_df.rename(columns={'estimated_cost_x_1000': 'estimated_cost', 'escalated_cost_x_1000': 'escalated_cost', 'total_estimated_cost_x_1000': 'total_estimated_cost',\n",
    "                                             'total_estimated_cost_x_1000_escalated':'total_escalated_cost',}, inplace=True)\n",
    "        \n",
    "        for col in ['estimated_cost', 'escalated_cost', 'total_estimated_cost', 'total_escalated_cost', ]:\n",
    "            if col in combined_total_df.columns:\n",
    "                combined_total_df[col] = combined_total_df[col].apply(clean_currency)\n",
    "\n",
    "        for col in ['estimated_time_to_construct']:\n",
    "            if col in combined_total_df.columns:\n",
    "                combined_total_df[col]=combined_total_df[col].apply(clean_text)  \n",
    "                \n",
    "        for col in columns_to_clean:\n",
    "            if col in combined_total_df.columns:\n",
    "                combined_total_df[col] = combined_total_df[col].apply(replace_text_with_zero)      \n",
    "\n",
    "         \n",
    "\n",
    "        # Columns to clean by converting text to 0\n",
    "        #columns_to_clean = ['cost_allocation_factor', 'estimated_cost']\n",
    "\n",
    "        # Convert non-numeric values to 0\n",
    "        #combined_total_df[columns_to_clean] = combined_total_df[columns_to_clean].apply(pd.to_numeric, errors='coerce').fillna(0)        \n",
    "        \n",
    " \n",
    "        \n",
    "        combined_total_df=reorder_columns(combined_total_df)    \n",
    "\n",
    "        # Sort by q_id\n",
    "        combined_total_df = combined_total_df.sort_values(by=\"q_id\", kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "        # Save the combined total dataset\n",
    "        combined_total_file = os.path.join(output_folder, \"costs_phase_2_all_clusters_total.csv\")\n",
    "        combined_total_df.to_csv(combined_total_file, index=False)\n",
    "        print(f\"Combined total dataset with 'original' column saved to: {combined_total_file}\")\n",
    "\n",
    "# Define the root folder and output folder\n",
    "root_folder = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/\"  # Update with your root folder path\n",
    "output_folder = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters\"  # Update with your output folder path\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Run the function\n",
    "create_addendum_list_and_mark_original(root_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allows for non numeric q_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/02_intermediate/costs_phase_2_cluster_7_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_N_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/02_intermediate/costs_phase_2_cluster_14_style_G_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/02_intermediate/costs_phase_2_cluster_9_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/02_intermediate/costs_phase_2_cluster_9_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 8/02_intermediate/costs_phase_2_cluster_8_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 6/02_intermediate/costs_phase_2_cluster_6_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/02_intermediate/costs_phase_2_cluster_12_style_N_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/02_intermediate/costs_phase_2_cluster_12_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/02_intermediate/costs_phase_2_cluster_SGIP-TC_style_new_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 3/02_intermediate/costs_phase_2_cluster_3_style_new_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/02_intermediate/costs_phase_2_cluster_4_style_new_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 10/02_intermediate/costs_phase_2_cluster_10_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 10/02_intermediate/costs_phase_2_cluster_10_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 5/02_intermediate/costs_phase_2_cluster_5_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/02_intermediate/costs_phase_2_cluster_11_style_Q_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/02_intermediate/costs_phase_2_cluster_11_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_missed_itemized_addendums.csv\n",
      "Total in addendum list: 125\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/01_clean/costs_phase_2_cluster_7_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/01_clean/costs_phase_2_cluster_7_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/01_clean/costs_phase_2_cluster_7_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 7/01_clean/costs_phase_2_cluster_7_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_N_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_N_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/01_clean/costs_phase_2_cluster_14_style_G_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/01_clean/costs_phase_2_cluster_14_style_G_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/01_clean/costs_phase_2_cluster_14_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/01_clean/costs_phase_2_cluster_14_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/01_clean/costs_phase_2_cluster_14_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/01_clean/costs_phase_2_cluster_14_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/01_clean/costs_phase_2_cluster_9_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/01_clean/costs_phase_2_cluster_9_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/01_clean/costs_phase_2_cluster_9_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/01_clean/costs_phase_2_cluster_9_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 8/01_clean/costs_phase_2_cluster_8_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 8/01_clean/costs_phase_2_cluster_8_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 8/01_clean/costs_phase_2_cluster_8_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 8/01_clean/costs_phase_2_cluster_8_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 1/01_clean/costs_phase_2_cluster_1_style_R_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 1/01_clean/costs_phase_2_cluster_1_style_R_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 6/01_clean/costs_phase_2_cluster_6_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 6/01_clean/costs_phase_2_cluster_6_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 6/01_clean/costs_phase_2_cluster_6_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 6/01_clean/costs_phase_2_cluster_6_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/01_clean/costs_phase_2_cluster_12_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/01_clean/costs_phase_2_cluster_12_style_N_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/01_clean/costs_phase_2_cluster_12_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/01_clean/costs_phase_2_cluster_12_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/01_clean/costs_phase_2_cluster_12_style_N_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 12/01_clean/costs_phase_2_cluster_12_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/01_clean/costs_phase_2_cluster_1_style_SGIP-TC_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/01_clean/costs_phase_2_cluster_SGIP-TC_style_new_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/01_clean/costs_phase_2_cluster_SGIP-TC_style_new_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/01_clean/costs_phase_2_cluster_1_style_SGIP-TC_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 3/01_clean/costs_phase_2_cluster_3_style_new_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 3/01_clean/costs_phase_2_cluster_3_style_new_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/01_clean/costs_phase_2_cluster_4_style_new_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 4/01_clean/costs_phase_2_cluster_4_style_new_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 10/01_clean/costs_phase_2_cluster_10_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 10/01_clean/costs_phase_2_cluster_10_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 10/01_clean/costs_phase_2_cluster_10_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 10/01_clean/costs_phase_2_cluster_10_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 5/01_clean/costs_phase_2_cluster_5_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 5/01_clean/costs_phase_2_cluster_5_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 5/01_clean/costs_phase_2_cluster_5_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 5/01_clean/costs_phase_2_cluster_5_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/01_clean/costs_phase_2_cluster_11_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/01_clean/costs_phase_2_cluster_11_style_N_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/01_clean/costs_phase_2_cluster_11_style_N_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/01_clean/costs_phase_2_cluster_11_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/01_clean/costs_phase_2_cluster_11_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/01_clean/costs_phase_2_cluster_11_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/01_clean/costs_phase_2_cluster_2_style_R_missed_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/01_clean/costs_phase_2_cluster_2_style_R_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/01_clean/costs_phase_2_cluster_2_style_R_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/01_clean/costs_phase_2_cluster_2_style_R_missed_total_updated.csv\n",
      "Saved itemized to: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters/costs_phase_2_all_clusters_itemized.csv\n",
      "Saved total to: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters/costs_phase_2_all_clusters_total.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"original\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_time_to_construct\",\n",
    "        \"estimated_cost\",\n",
    "        \"escalated_cost\",\n",
    "        \"total_estimated_cost\",\n",
    "        \"total_escalated_cost\",\n",
    "    ]\n",
    "\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "    df = df[existing_desired + remaining]\n",
    "    return df  \n",
    "\n",
    "\n",
    "def replace_text_with_zero(value):\n",
    "    \"\"\"\n",
    "    Cleans a value by removing percentage symbols, converting numeric values,\n",
    "    and replacing text with zero.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return value  \n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if value.endswith('%'):\n",
    "            value = value.replace('%', '')\n",
    "        if re.search(r'[a-zA-Z]', value):\n",
    "            return 0\n",
    "    try:\n",
    "        return pd.to_numeric(value, errors='coerce') if value != '' else value\n",
    "    except (ValueError, TypeError):\n",
    "        return value\n",
    "\n",
    "\n",
    "columns_to_clean = ['cost_allocation_factor', 'estimated_cost']        \n",
    "\n",
    "\n",
    "def clean_text(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by removing unwanted patterns while preserving numeric ranges.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+)months', r'\\1', value, flags=re.IGNORECASE)\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return value\n",
    "\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Removes $, *, (Note X), commas, then converts to numeric.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)\n",
    "        value = value.replace(',', '').strip()\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA\n",
    "\n",
    "\n",
    "def create_addendum_list_and_mark_original(root_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Collects all q_ids from itemized_addendums, writes a master list,\n",
    "    then combines and cleans both itemized and total files per cluster.\n",
    "    \"\"\"\n",
    "    addendum_qids = []\n",
    "\n",
    "    # Step 1: collect addendum q_ids as strings (don't cast to int)\n",
    "    for cluster_folder in os.listdir(root_folder):\n",
    "        cluster_path = os.path.join(root_folder, cluster_folder)\n",
    "        if os.path.isdir(cluster_path):\n",
    "            interm = os.path.join(cluster_path, \"02_intermediate\")\n",
    "            if os.path.exists(interm):\n",
    "                for fn in os.listdir(interm):\n",
    "                    if \"itemized_addendums.csv\" in fn:\n",
    "                        path = os.path.join(interm, fn)\n",
    "                        print(f\"Processing: {path}\")\n",
    "                        df = pd.read_csv(path)\n",
    "                        if 'q_id' in df.columns:\n",
    "                            addendum_qids.extend(df['q_id'].dropna().astype(str).unique())\n",
    "\n",
    "    addendum_qids = sorted(set(addendum_qids))\n",
    "    pd.DataFrame({'q_id': addendum_qids})\\\n",
    "      .to_csv(os.path.join(output_folder, \"addendum_projects_list.csv\"), index=False)\n",
    "    print(f\"Total in addendum list: {len(addendum_qids)}\")\n",
    "\n",
    "    # Step 2: load itemized and total files\n",
    "    combined_itemized = []\n",
    "    combined_total = []\n",
    "    for cluster_folder in os.listdir(root_folder):\n",
    "        cluster_path = os.path.join(root_folder, cluster_folder)\n",
    "        if os.path.isdir(cluster_path):\n",
    "            clean_folder = os.path.join(cluster_path, \"01_clean\")\n",
    "            if os.path.exists(clean_folder):\n",
    "                for fn in os.listdir(clean_folder):\n",
    "                    path = os.path.join(clean_folder, fn)\n",
    "                    if \"itemized_updated.csv\" in fn:\n",
    "                        print(f\"Loading: {path}\")\n",
    "                        combined_itemized.append(pd.read_csv(path))\n",
    "                    elif \"total_updated.csv\" in fn:\n",
    "                        print(f\"Loading: {path}\")\n",
    "                        combined_total.append(pd.read_csv(path))\n",
    "\n",
    "    # Combine itemized\n",
    "    if combined_itemized:\n",
    "        df = pd.concat(combined_itemized, ignore_index=True)\n",
    "        df['q_id'] = df['q_id'].fillna(0).astype(str)\n",
    "        df['original'] = df['q_id'].apply(lambda q: 'no' if q in addendum_qids else 'yes')\n",
    "\n",
    "        cols = list(df.columns)\n",
    "        cols.insert(cols.index('q_id')+1, cols.pop(cols.index('original')))\n",
    "        df = df[cols]\n",
    "        df.drop([\n",
    "            'estimate_d_time_to_construc_t','estimated_cost_x_1000_escalated_with_itcca',\n",
    "            'adnu_cost_rate_x_1000_escalated','potential_duration_months',\n",
    "            'none_7','none_8','network_upgrade_type',\n",
    "            'adnu_cost_rate_escalated_x_1000','upgrade_classification',\n",
    "            'sum_of_reallocated_share','sum_of_reallocated_cost_x_1000_constant_dollar_2022',\n",
    "            'sum_of_reallocated_costs_x_1000_escalated_constant_dollars_od_year',\n",
    "            'ttype_of_upgrade','adnu_cost_rate_x_1000',\n",
    "            'project_size_mw',\n",
    "            'sum_of_reallocated_cost_x_1000_constant_dollar_2024',\t'sum_ofreallocated_costs_x_1000_escalated_constant_dollars_od_year',\t\n",
    "            'unnamed_15',\n",
    "            'other_potential_network_upgrades', 'upgrade_classification_grnu_irnu',\n",
    "        ], axis=1, errors='ignore', inplace=True)\n",
    "        df.rename(columns={\n",
    "            'estimated_cost_x_1000': 'estimated_cost',\n",
    "            'escalated_cost_x_1000': 'escalated_cost',\n",
    "            'total_estimated_cost_x_1000': 'total_estimated_cost',\n",
    "            'total_estimated_cost_x_1000_escalated':'total_escalated_cost',\n",
    "        }, inplace=True)\n",
    "\n",
    "        for c in ['estimated_cost','escalated_cost','total_estimated_cost','total_escalated_cost']:\n",
    "            if c in df: df[c] = df[c].apply(clean_currency)\n",
    "        if 'estimated_time_to_construct' in df:\n",
    "            df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_text)\n",
    "            df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(replace_text_with_zero)\n",
    "        for c in columns_to_clean:\n",
    "            if c in df: df[c] = df[c].apply(replace_text_with_zero)\n",
    "\n",
    "        df = reorder_columns(df)\n",
    "\n",
    "        #  sort by numeric part of q_id then full string\n",
    "        df['q_id_num'] = (\n",
    "            df['q_id']\n",
    "              .str.extract(r'(\\d+)', expand=False)\n",
    "              .astype(float)\n",
    "              .fillna(0)\n",
    "              .astype(int)\n",
    "        )\n",
    "        df = (\n",
    "            df\n",
    "              .sort_values(['q_id_num', 'q_id'], kind='stable')\n",
    "              .drop(columns='q_id_num')\n",
    "              .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        out1 = os.path.join(output_folder, \"costs_phase_2_all_clusters_itemized.csv\")\n",
    "        df.to_csv(out1, index=False)\n",
    "        print(f\"Saved itemized to: {out1}\")\n",
    "\n",
    "    # Combine total\n",
    "    if combined_total:\n",
    "        df = pd.concat(combined_total, ignore_index=True)\n",
    "        df['q_id'] = df['q_id'].fillna(0).astype(str)\n",
    "        df['original'] = df['q_id'].apply(lambda q: 'no' if q in addendum_qids else 'yes')\n",
    "\n",
    "        cols = list(df.columns)\n",
    "        cols.insert(cols.index('q_id')+1, cols.pop(cols.index('original')))\n",
    "        df = df[cols]\n",
    "        df.drop([\n",
    "            'estimate_d_time_to_construc_t','estimated_cost_x_1000_escalated_with_itcca',\n",
    "            'adnu_cost_rate_x_1000_escalated','potential_duration_months',\n",
    "            'none_7','none_8','network_upgrade_type',\n",
    "            'adnu_cost_rate_escalated_x_1000','upgrade_classification',\n",
    "            'sum_of_reallocated_share','sum_of_reallocated_cost_x_1000_constant_dollar_2022',\n",
    "            'sum_of_reallocated_costs_x_1000_escalated_constant_dollars_od_year',\n",
    "            'ttype_of_upgrade','adnu_cost_rate_x_1000',\n",
    "                        'project_size_mw',\n",
    "            'sum_of_reallocated_cost_x_1000_constant_dollar_2024',\t'sum_ofreallocated_costs_x_1000_escalated_constant_dollars_od_year',\t\n",
    "            'unnamed_15',\n",
    "            'other_potential_network_upgrades', 'upgrade_classification_grnu_irnu',\n",
    "        ], axis=1, errors='ignore', inplace=True)\n",
    "        df.rename(columns={\n",
    "            'estimated_cost_x_1000': 'estimated_cost',\n",
    "            'escalated_cost_x_1000': 'escalated_cost',\n",
    "            'total_estimated_cost_x_1000': 'total_estimated_cost',\n",
    "            'total_estimated_cost_x_1000_escalated':'total_escalated_cost',\n",
    "        }, inplace=True)\n",
    "\n",
    "        for c in ['estimated_cost','escalated_cost','total_estimated_cost','total_escalated_cost']:\n",
    "            if c in df: df[c] = df[c].apply(clean_currency)\n",
    "        if 'estimated_time_to_construct' in df:\n",
    "            df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_text)\n",
    "            df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(replace_text_with_zero)\n",
    "        for c in columns_to_clean:\n",
    "            if c in df: df[c] = df[c].apply(replace_text_with_zero)\n",
    "\n",
    "        df = reorder_columns(df)\n",
    "\n",
    "        #  sort by numeric part of q_id then full string\n",
    "        df['q_id_num'] = (\n",
    "            df['q_id']\n",
    "              .str.extract(r'(\\d+)', expand=False)\n",
    "              .astype(float)\n",
    "              .fillna(0)\n",
    "              .astype(int)\n",
    "        )\n",
    "        df = (\n",
    "            df\n",
    "              .sort_values(['q_id_num', 'q_id'], kind='stable')\n",
    "              .drop(columns='q_id_num')\n",
    "              .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        out2 = os.path.join(output_folder, \"costs_phase_2_all_clusters_total.csv\")\n",
    "        df.to_csv(out2, index=False)\n",
    "        print(f\"Saved total to: {out2}\")\n",
    "\n",
    "\n",
    "# Define folders & run\n",
    "root_folder = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "output_folder = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "create_addendum_list_and_mark_original(root_folder, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
