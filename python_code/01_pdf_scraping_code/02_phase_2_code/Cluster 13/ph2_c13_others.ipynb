{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Cluster 13 style other pdfs - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects to process: [1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828]\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1679\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1680\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1681\n",
      "\n",
      "--- Processing project 1682 ---\n",
      "No original Appendix A PDF found for project 1682.\n",
      "\n",
      "--- Processing project 1683 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1683/03_phase_2_study/Q1683-Buena Vista Storage-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1683\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('37.801094', '-121.641678')\n",
      "Base data extracted:\n",
      "{'q_id': ['1683'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['37.801094'], 'longitude': ['-121.641678'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1683/03_phase_2_study/Q1683-Buena Vista Storage-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1683/03_phase_2_study/Q1683-Buena Vista Storage-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1684 ---\n",
      "No original Appendix A PDF found for project 1684.\n",
      "\n",
      "--- Processing project 1685 ---\n",
      "No original Appendix A PDF found for project 1685.\n",
      "\n",
      "--- Processing project 1686 ---\n",
      "No original Appendix A PDF found for project 1686.\n",
      "\n",
      "--- Processing project 1687 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1687/03_phase_2_study/Q1687-Conaway Hybrid Power Plant-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1687\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('38.595665', '-121.664698')\n",
      "Base data extracted:\n",
      "{'q_id': ['1687'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['38.595665'], 'longitude': ['-121.664698'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1687/03_phase_2_study/Q1687-Conaway Hybrid Power Plant-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1687/03_phase_2_study/Q1687-Conaway Hybrid Power Plant-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1688 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1688/03_phase_2_study/Q1688-Crossroads-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1688\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('40.808931', '-121.931726')\n",
      "Base data extracted:\n",
      "{'q_id': ['1688'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['40.808931'], 'longitude': ['-121.931726'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1688/03_phase_2_study/Q1688CrossroadsAppendix_AC13PhIIRevision1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1688\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('40.808931', '-121.931726')\n",
      "Base data extracted:\n",
      "{'q_id': ['1688'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['40.808931'], 'longitude': ['-121.931726'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1688/03_phase_2_study/Q1688-Crossroads-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1688/03_phase_2_study/Q1688-Crossroads-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1688/03_phase_2_study/Q1688CrossroadsAppendix_AC13PhIIRevision1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1688/03_phase_2_study/Q1688CrossroadsAppendix_AC13PhIIRevision1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1689 ---\n",
      "No original Appendix A PDF found for project 1689.\n",
      "\n",
      "--- Processing project 1690 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1690/03_phase_2_study/Q1690-Denali Energy Storage-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1690\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('37.797067', '-121.2755')\n",
      "Base data extracted:\n",
      "{'q_id': ['1690'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['37.797067'], 'longitude': ['-121.2755'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1690/03_phase_2_study/Q1690-Denali Energy Storage-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1690/03_phase_2_study/Q1690-Denali Energy Storage-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1691 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1691/03_phase_2_study/Q1691-Fortis-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1691\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('38.215846', '-121.840441')\n",
      "Base data extracted:\n",
      "{'q_id': ['1691'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['38.215846'], 'longitude': ['-121.840441'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1691/03_phase_2_study/Q1691FortisAppendix_AC13PhIIRevision1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1691\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('38.215846', '-121.840441')\n",
      "Base data extracted:\n",
      "{'q_id': ['1691'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['38.215846'], 'longitude': ['-121.840441'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1691/03_phase_2_study/Q1691FortisAppendix_AC13PhIIRevision1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1691/03_phase_2_study/Q1691FortisAppendix_AC13PhIIRevision1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1691/03_phase_2_study/Q1691-Fortis-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1691/03_phase_2_study/Q1691-Fortis-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1692 ---\n",
      "No original Appendix A PDF found for project 1692.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1693\n",
      "\n",
      "--- Processing project 1694 ---\n",
      "No original Appendix A PDF found for project 1694.\n",
      "\n",
      "--- Processing project 1695 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1695/03_phase_2_study/Q1695-Meadows Energy Storage-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1695\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('38.760856', '-121.266223')\n",
      "Base data extracted:\n",
      "{'q_id': ['1695'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['38.760856'], 'longitude': ['-121.266223'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1695/03_phase_2_study/Q1695-Meadows Energy Storage-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1695/03_phase_2_study/Q1695-Meadows Energy Storage-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1696 ---\n",
      "No original Appendix A PDF found for project 1696.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1697\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1698\n",
      "\n",
      "--- Processing project 1699 ---\n",
      "No original Appendix A PDF found for project 1699.\n",
      "\n",
      "--- Processing project 1700 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1700/03_phase_2_study/Q1700-North Bay Energy Storage-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1700\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('38.249905', '-122.584439')\n",
      "Base data extracted:\n",
      "{'q_id': ['1700'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['38.249905'], 'longitude': ['-122.584439'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1700/03_phase_2_study/Q1700-North Bay Energy Storage-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1700/03_phase_2_study/Q1700-North Bay Energy Storage-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1701\n",
      "\n",
      "--- Processing project 1702 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1702/03_phase_2_study/Q1702-Potentia-Viridi-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1702\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('37.703617', '-121.587499')\n",
      "Base data extracted:\n",
      "{'q_id': ['1702'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['37.703617'], 'longitude': ['-121.587499'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1702/03_phase_2_study/Q1702-Potentia-Viridi-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1702/03_phase_2_study/Q1702-Potentia-Viridi-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1703 ---\n",
      "No original Appendix A PDF found for project 1703.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1704\n",
      "\n",
      "--- Processing project 1705 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1705/03_phase_2_study/Q1705-Steel City Battery Storage-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1705\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('38.038561', '-121.894036')\n",
      "Base data extracted:\n",
      "{'q_id': ['1705'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['38.038561'], 'longitude': ['-121.894036'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1705/03_phase_2_study/Q1705-Steel City Battery Storage-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1705/03_phase_2_study/Q1705-Steel City Battery Storage-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1706\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1707\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1708\n",
      "\n",
      "--- Processing project 1709 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1709/03_phase_2_study/Q1709-Rosemary-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1709\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1709'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1709/03_phase_2_study/P2RPT-Q1709RosemaryAppendix_AC13PhIIAddendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1709\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1709'], 'cluster': ['13'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1709/03_phase_2_study/P2RPT-Q1709RosemaryAppendix_AC13PhIIAddendum1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1709/03_phase_2_study/P2RPT-Q1709RosemaryAppendix_AC13PhIIAddendum1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1709/03_phase_2_study/C13Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1709.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1709/03_phase_2_study/C13Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1709.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1709/03_phase_2_study/Q1709-Rosemary-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1709/03_phase_2_study/Q1709-Rosemary-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1710 ---\n",
      "No original Appendix A PDF found for project 1710.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1711\n",
      "\n",
      "--- Processing project 1712 ---\n",
      "No original Appendix A PDF found for project 1712.\n",
      "\n",
      "--- Processing project 1713 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1713/03_phase_2_study/Q1713-Bia BESS 1-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1713\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('36.270212', '-119.648275')\n",
      "Base data extracted:\n",
      "{'q_id': ['1713'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['36.270212'], 'longitude': ['-119.648275'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1713/03_phase_2_study/Q1713Lead_Appendix_AAddendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1713\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1713'], 'cluster': ['13'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1713/03_phase_2_study/Q1713Lead_Appendix_AAddendum1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1713/03_phase_2_study/Q1713Lead_Appendix_AAddendum1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1713/03_phase_2_study/P2RPT-Q1713BiaBESS1Appendix_AC13PhIIAddendum1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1713/03_phase_2_study/P2RPT-Q1713BiaBESS1Appendix_AC13PhIIAddendum1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1713/03_phase_2_study/Q1713-Bia BESS 1-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1713/03_phase_2_study/Q1713-Bia BESS 1-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1714 ---\n",
      "No original Appendix A PDF found for project 1714.\n",
      "\n",
      "--- Processing project 1715 ---\n",
      "No original Appendix A PDF found for project 1715.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1716\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1717\n",
      "\n",
      "--- Processing project 1718 ---\n",
      "No original Appendix A PDF found for project 1718.\n",
      "\n",
      "--- Processing project 1719 ---\n",
      "No original Appendix A PDF found for project 1719.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1720\n",
      "\n",
      "--- Processing project 1721 ---\n",
      "No original Appendix A PDF found for project 1721.\n",
      "\n",
      "--- Processing project 1722 ---\n",
      "No original Appendix A PDF found for project 1722.\n",
      "\n",
      "--- Processing project 1723 ---\n",
      "No original Appendix A PDF found for project 1723.\n",
      "\n",
      "--- Processing project 1724 ---\n",
      "No original Appendix A PDF found for project 1724.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1725\n",
      "\n",
      "--- Processing project 1726 ---\n",
      "No original Appendix A PDF found for project 1726.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1727\n",
      "\n",
      "--- Processing project 1728 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1728/03_phase_2_study/Q1728-Zeta-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1728\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('36.921358', '-120.819683')\n",
      "Base data extracted:\n",
      "{'q_id': ['1728'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['36.921358'], 'longitude': ['-120.819683'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1728/03_phase_2_study/P2RPT-Q1728ZetaAppendix_AC13PhIIAddendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1728\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1728'], 'cluster': ['13'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1728/03_phase_2_study/Q1728-Zeta-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1728/03_phase_2_study/Q1728-Zeta-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1728/03_phase_2_study/P2RPT-Q1728ZetaAppendix_AC13PhIIAddendum1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1728/03_phase_2_study/P2RPT-Q1728ZetaAppendix_AC13PhIIAddendum1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1728/03_phase_2_study/P2RPT-Q1728ZetaAppendix_AC13PhIIAddendum2_.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1728/03_phase_2_study/P2RPT-Q1728ZetaAppendix_AC13PhIIAddendum2_.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1729 ---\n",
      "No original Appendix A PDF found for project 1729.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1730\n",
      "\n",
      "--- Processing project 1731 ---\n",
      "No original Appendix A PDF found for project 1731.\n",
      "\n",
      "--- Processing project 1732 ---\n",
      "No original Appendix A PDF found for project 1732.\n",
      "\n",
      "--- Processing project 1733 ---\n",
      "No original Appendix A PDF found for project 1733.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1734\n",
      "\n",
      "--- Processing project 1735 ---\n",
      "No original Appendix A PDF found for project 1735.\n",
      "\n",
      "--- Processing project 1736 ---\n",
      "No original Appendix A PDF found for project 1736.\n",
      "\n",
      "--- Processing project 1737 ---\n",
      "No original Appendix A PDF found for project 1737.\n",
      "\n",
      "--- Processing project 1738 ---\n",
      "No original Appendix A PDF found for project 1738.\n",
      "\n",
      "--- Processing project 1739 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1739/03_phase_2_study/Q1739-Pecho Energy Storage-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1739\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('35.370369', '-120.828165')\n",
      "Base data extracted:\n",
      "{'q_id': ['1739'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.370369'], 'longitude': ['-120.828165'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1739/03_phase_2_study/Q1739Pecho_Energy_Storage_Appendix_A_Addendum2.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1739\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1739'], 'cluster': ['13'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1739/03_phase_2_study/Q1739Pecho_Energy_Storage_Appendix_A_Addendum2.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1739/03_phase_2_study/Q1739Pecho_Energy_Storage_Appendix_A_Addendum2.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1739/03_phase_2_study/Q1739Pecho_Energy_Storage_Appendix_A_Addendum3.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1739/03_phase_2_study/Q1739Pecho_Energy_Storage_Appendix_A_Addendum3.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1739/03_phase_2_study/Q1739Pecho_Energy_StorageAppendix_AC13PhIIRevision1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1739/03_phase_2_study/Q1739Pecho_Energy_StorageAppendix_AC13PhIIRevision1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1739/03_phase_2_study/Q1739-Pecho Energy Storage-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1739/03_phase_2_study/Q1739-Pecho Energy Storage-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1740 ---\n",
      "No original Appendix A PDF found for project 1740.\n",
      "\n",
      "--- Processing project 1741 ---\n",
      "No original Appendix A PDF found for project 1741.\n",
      "\n",
      "--- Processing project 1742 ---\n",
      "No original Appendix A PDF found for project 1742.\n",
      "\n",
      "--- Processing project 1743 ---\n",
      "No original Appendix A PDF found for project 1743.\n",
      "\n",
      "--- Processing project 1744 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1744/03_phase_2_study/Q1744-Second Fiddle-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1744\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('35.2532', '-118.7884')\n",
      "Base data extracted:\n",
      "{'q_id': ['1744'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.2532'], 'longitude': ['-118.7884'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1744/03_phase_2_study/P2RPT-Q1744Second_FiddleAppendix_AC13PhIIAddendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1744\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1744'], 'cluster': ['13'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1744/03_phase_2_study/P2RPT-Q1744Second_FiddleAppendix_AC13PhIIAddendum1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1744/03_phase_2_study/P2RPT-Q1744Second_FiddleAppendix_AC13PhIIAddendum1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1744/03_phase_2_study/Q1744-Second Fiddle-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1744/03_phase_2_study/Q1744-Second Fiddle-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1745 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1745/03_phase_2_study/Q1745-Sunrise Power Improvement-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1745\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('35.2103', '-119.583753')\n",
      "Base data extracted:\n",
      "{'q_id': ['1745'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.2103'], 'longitude': ['-119.583753'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1745/03_phase_2_study/Q1745-Sunrise Power Improvement-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1745/03_phase_2_study/Q1745-Sunrise Power Improvement-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1746\n",
      "\n",
      "--- Processing project 1747 ---\n",
      "No original Appendix A PDF found for project 1747.\n",
      "\n",
      "--- Processing project 1748 ---\n",
      "No original Appendix A PDF found for project 1748.\n",
      "\n",
      "--- Processing project 1749 ---\n",
      "No original Appendix A PDF found for project 1749.\n",
      "\n",
      "--- Processing project 1750 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1750/03_phase_2_study/Q1750-Windwalker Offshore-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1750\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('35.216', '-120.848')\n",
      "Base data extracted:\n",
      "{'q_id': ['1750'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.216'], 'longitude': ['-120.848'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1750/03_phase_2_study/Q1750Windwalker_OffshoreAppendix_AC13PhIIAddendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1750\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1750'], 'cluster': ['13'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1750/03_phase_2_study/Q1750Windwalker_OffshoreAppendix_AC13PhIIAddendum1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1750/03_phase_2_study/Q1750Windwalker_OffshoreAppendix_AC13PhIIAddendum1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1750/03_phase_2_study/Q1750-Windwalker Offshore-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1750/03_phase_2_study/Q1750-Windwalker Offshore-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1751 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1751/03_phase_2_study/Q1751-Winston Hybrid PV and BESS-Appendix_A-C13PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1751\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found GPS coordinates: ('35.449591', '-119.448087')\n",
      "Base data extracted:\n",
      "{'q_id': ['1751'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.449591'], 'longitude': ['-119.448087'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1751/03_phase_2_study/P2RPT-Q1751Winston_Hybrid_PV_and_BESSAppendix_AC13PhIIAddendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1751\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1751'], 'cluster': ['13'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1751/03_phase_2_study/Q1751-Winston Hybrid PV and BESS-Appendix_A-C13PhII.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1751/03_phase_2_study/Q1751-Winston Hybrid PV and BESS-Appendix_A-C13PhII.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1751/03_phase_2_study/P2RPT-Q1751Winston_Hybrid_PV_and_BESSAppendix_AC13PhIIAddendum1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1751/03_phase_2_study/P2RPT-Q1751Winston_Hybrid_PV_and_BESSAppendix_AC13PhIIAddendum1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1752 ---\n",
      "No original Appendix A PDF found for project 1752.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1753\n",
      "\n",
      "--- Processing project 1754 ---\n",
      "No original Appendix A PDF found for project 1754.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1755\n",
      "\n",
      "--- Processing project 1756 ---\n",
      "No original Appendix A PDF found for project 1756.\n",
      "\n",
      "--- Processing project 1757 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1757\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.57676', '-114.82714')\n",
      "Base data extracted:\n",
      "{'q_id': ['1757'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.57676'], 'longitude': ['-114.82714'], 'capacity': [None], 'point_of_interconnection': ['Colorado River Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-revised.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1757\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.57676', '-114.82714')\n",
      "Base data extracted:\n",
      "{'q_id': ['1757'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.57676'], 'longitude': ['-114.82714'], 'capacity': [None], 'point_of_interconnection': ['Colorado River Substation 220 kV Bus']}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-Attachment 3-revised.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-Attachment 3-revised.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-revised.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-revised.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-Attachment 3.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-Attachment 3.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Attachment 2.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Attachment 3.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Attachment 3.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1757/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Attachment 3.pdf flagged as addendum.\n",
      "\n",
      "--- Processing project 1758 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1758/03_phase_2_study/QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1758\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.936331', '-116.578068')\n",
      "Base data extracted:\n",
      "{'q_id': ['1758'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.936331'], 'longitude': ['-116.578068'], 'capacity': [None], 'point_of_interconnection': ['Devers Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1758/03_phase_2_study/QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1758/03_phase_2_study/QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1758/03_phase_2_study/QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1758/03_phase_2_study/QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1758/03_phase_2_study/QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A-Attachment 2.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1758/03_phase_2_study/QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A-Attachment 2.pdf\n",
      "\n",
      "--- Processing project 1759 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1759/03_phase_2_study/QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1759\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.85', '-115.48')\n",
      "Base data extracted:\n",
      "{'q_id': ['1759'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.85'], 'longitude': ['-115.48'], 'capacity': [None], 'point_of_interconnection': ['Generating Facility Charging']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1759/03_phase_2_study/QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A-Attachment 2.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1759/03_phase_2_study/QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1759/03_phase_2_study/QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1759/03_phase_2_study/QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1759/03_phase_2_study/QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1759/03_phase_2_study/QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1760 ---\n",
      "No original Appendix A PDF found for project 1760.\n",
      "\n",
      "--- Processing project 1761 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1761\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.652683', '-114.675482')\n",
      "Base data extracted:\n",
      "{'q_id': ['1761'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.652683'], 'longitude': ['-114.675482'], 'capacity': [None], 'point_of_interconnection': ['Colorado River Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-revised.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1761\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: ull\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.652683', '-114.675482')\n",
      "Base data extracted:\n",
      "{'q_id': ['1761'], 'cluster': ['13'], 'req_deliverability': ['ull'], 'latitude': ['33.652683'], 'longitude': ['-114.675482'], 'capacity': [None], 'point_of_interconnection': ['Colorado River Substation 220 kV Bus']}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-Attachment 3-revised.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-Attachment 3-revised.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-revised.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-revised.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 2.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 3.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 3.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 3.pdf flagged as addendum.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-Attachment 3.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1761/03_phase_2_study/QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-Attachment 3.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1762 ---\n",
      "No original Appendix A PDF found for project 1762.\n",
      "\n",
      "--- Processing project 1763 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1763/03_phase_2_study/QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1763\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.951608', '-116.570178')\n",
      "Base data extracted:\n",
      "{'q_id': ['1763'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.951608'], 'longitude': ['-116.570178'], 'capacity': [None], 'point_of_interconnection': ['Devers Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1763/03_phase_2_study/QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A-Attachment 2.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1763/03_phase_2_study/QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1763/03_phase_2_study/QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1763/03_phase_2_study/QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1763/03_phase_2_study/QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1763/03_phase_2_study/QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1764 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1764/03_phase_2_study/QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1764\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.7664', '-115.3725')\n",
      "Base data extracted:\n",
      "{'q_id': ['1764'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.7664'], 'longitude': ['-115.3725'], 'capacity': [None], 'point_of_interconnection': ['Red Bluff Substation 220 kV Bus via Desert Harvest']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1764/03_phase_2_study/QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1764/03_phase_2_study/QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1764/03_phase_2_study/QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1764/03_phase_2_study/QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1764/03_phase_2_study/QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A-Attachment 2.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1764/03_phase_2_study/QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A-Attachment 2.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1765\n",
      "\n",
      "--- Processing project 1766 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1766/03_phase_2_study/C13.2-Metro-Q1766-TOT996-Commerce2-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1766\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.97416667', '-118.1461')\n",
      "Base data extracted:\n",
      "{'q_id': ['1766'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.97416667'], 'longitude': ['-118.1461'], 'capacity': [None], 'point_of_interconnection': ['Laguna Bell Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1766/03_phase_2_study/C13.2-Metro-Q1766-TOT996-Commerce2-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1766/03_phase_2_study/C13.2-Metro-Q1766-TOT996-Commerce2-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1766/03_phase_2_study/QC13 Ph2 Attachment 2 Q1766 TOT996.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1766/03_phase_2_study/QC13 Ph2 Attachment 2 Q1766 TOT996.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1767\n",
      "\n",
      "--- Processing project 1768 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/C13.2-Metro-Q1768-TOT1008-Roadhouse-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1768\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.008805', '-117.572009')\n",
      "Base data extracted:\n",
      "{'q_id': ['1768'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['34.008805'], 'longitude': ['-117.572009'], 'capacity': [None], 'point_of_interconnection': ['Mira Loma Substation 220 kV Bus (West Bus)']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/C13PII- Metro-Area Report_rev2_rs.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/C13PII- Metro-Area Report_rev2_rs.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/C13PII-Metro-Attachment1-TOT1008 Q1768.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/C13PII-Metro-Attachment1-TOT1008 Q1768.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/C13.2-Metro-Q1768-TOT1008-Roadhouse-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/C13.2-Metro-Q1768-TOT1008-Roadhouse-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/QC13 Ph2 Attachment 2 Q1768 TOT1008.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/QC13 Ph2 Attachment 2 Q1768 TOT1008.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/QC13 Ph2 Attachment 3 Q1768 TOT1008.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1768/03_phase_2_study/QC13 Ph2 Attachment 3 Q1768 TOT1008.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1769\n",
      "\n",
      "--- Processing project 1770 ---\n",
      "No original Appendix A PDF found for project 1770.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1771\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1772\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1773\n",
      "\n",
      "--- Processing project 1774 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1774/03_phase_2_study/C13.2-NOL-Q1774-TOT981-Overnight-ApndxA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1774\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1774'], 'cluster': ['13'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['Kramer Substation 220 kV Bus (via KramerSandlot']}\n",
      "Missing base data columns in first Appendix A PDF: ['req_deliverability', 'latitude', 'longitude', 'capacity']\n",
      "After update, still missing: ['req_deliverability', 'latitude', 'longitude', 'capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1774/03_phase_2_study/QC13 Ph2 Attachment 2 Q1774 TOT981.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1774/03_phase_2_study/QC13 Ph2 Attachment 2 Q1774 TOT981.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1774/03_phase_2_study/C13.2-NOL-Q1774-TOT981-Overnight-ApndxA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1774/03_phase_2_study/C13.2-NOL-Q1774-TOT981-Overnight-ApndxA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1774/03_phase_2_study/QC13 Ph2 Attachment 3 Q1774 TOT981.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1774/03_phase_2_study/QC13 Ph2 Attachment 3 Q1774 TOT981.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1774/03_phase_2_study/QC13 Ph2-Attachment 1 Q1774 TOT981-Overnight.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1774/03_phase_2_study/QC13 Ph2-Attachment 1 Q1774 TOT981-Overnight.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1775 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1775/03_phase_2_study/C13.2-NOL-Q1775-TOT989-SEGSEx2-ApndxA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1775\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.039921', '-117.347748')\n",
      "Base data extracted:\n",
      "{'q_id': ['1775'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.039921'], 'longitude': ['-117.347748'], 'capacity': [None], 'point_of_interconnection': ['Kramer Substation 220 kV Bus (via Kramer-LSP 220']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1775/03_phase_2_study/C13.2-NOL-Q1775-TOT989-SEGSEx2-ApndxA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1775/03_phase_2_study/C13.2-NOL-Q1775-TOT989-SEGSEx2-ApndxA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1775/03_phase_2_study/QC13 Ph2 Attachment 3 Q1775 TOT989.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1775/03_phase_2_study/QC13 Ph2 Attachment 3 Q1775 TOT989.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1775/03_phase_2_study/QC13 Ph2 Attachment 2 Q1775 TOT989.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1775/03_phase_2_study/QC13 Ph2 Attachment 2 Q1775 TOT989.pdf\n",
      "\n",
      "--- Processing project 1776 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1776/03_phase_2_study/C13.2-NOL-Q1776-TOT993-Ventoso-ApndxA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1776\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.851188', '-116.946242')\n",
      "Base data extracted:\n",
      "{'q_id': ['1776'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['34.851188'], 'longitude': ['-116.946242'], 'capacity': [None], 'point_of_interconnection': ['Coolwater Substation 115 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1776/03_phase_2_study/QC13 Ph2 Attachment 1 Q1776 TOT993-Ventoso.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1776/03_phase_2_study/QC13 Ph2 Attachment 1 Q1776 TOT993-Ventoso.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1776/03_phase_2_study/C13.2-NOL-Q1776-TOT993-Ventoso-ApndxA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1776/03_phase_2_study/C13.2-NOL-Q1776-TOT993-Ventoso-ApndxA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1776/03_phase_2_study/QC13 Ph2 Attachment 3 Q1776 TOT993.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1776/03_phase_2_study/QC13 Ph2 Attachment 3 Q1776 TOT993.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1776/03_phase_2_study/QC13 Ph2 Attachment 2 Q1776 TOT993.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1776/03_phase_2_study/QC13 Ph2 Attachment 2 Q1776 TOT993.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1777\n",
      "\n",
      "--- Processing project 1778 ---\n",
      "No original Appendix A PDF found for project 1778.\n",
      "\n",
      "--- Processing project 1779 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1779/03_phase_2_study/C13.2-North-Q1779-TOT966-Bellefiled3-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1779\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.029153', '-117.992561')\n",
      "Base data extracted:\n",
      "{'q_id': ['1779'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.029153'], 'longitude': ['-117.992561'], 'capacity': [None], 'point_of_interconnection': ['Windhub Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1779/03_phase_2_study/QC13PII-SCE-Northern-Q1779&TOT966-Bellefield3-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1779/03_phase_2_study/QC13PII-SCE-Northern-Q1779&TOT966-Bellefield3-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1779/03_phase_2_study/QC13 Ph2 Attachment 2 Q1779 TOT966.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1779/03_phase_2_study/QC13 Ph2 Attachment 2 Q1779 TOT966.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1779/03_phase_2_study/C13.2-North-Q1779-TOT966-Bellefiled3-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1779/03_phase_2_study/C13.2-North-Q1779-TOT966-Bellefiled3-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1780 ---\n",
      "No original Appendix A PDF found for project 1780.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1781\n",
      "\n",
      "--- Processing project 1782 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1782/03_phase_2_study/C13.2-North-Q1782-TOT1002-Gem-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1782\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: 9\n",
      "Found project coordinates: ('34.890843', '-118.285596')\n",
      "Base data extracted:\n",
      "{'q_id': ['1782'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['34.890843'], 'longitude': ['-118.285596'], 'capacity': [9], 'point_of_interconnection': ['Whirlwind Substation 220kV Bus']}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1782/03_phase_2_study/QC13PII-SCE-Northern-Q1782&TOT1002-Gem-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1782/03_phase_2_study/QC13PII-SCE-Northern-Q1782&TOT1002-Gem-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1782/03_phase_2_study/QC13 Ph2 Attachment 2 Q1782 TOT1002.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1782/03_phase_2_study/QC13 Ph2 Attachment 2 Q1782 TOT1002.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1782/03_phase_2_study/C13.2-North-Q1782-TOT1002-Gem-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1782/03_phase_2_study/C13.2-North-Q1782-TOT1002-Gem-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1783 ---\n",
      "No original Appendix A PDF found for project 1783.\n",
      "\n",
      "--- Processing project 1784 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1784/03_phase_2_study/C13.2-North-Q1784-TOT983-Keyhole-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1784\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.886769', '-118.482995')\n",
      "Base data extracted:\n",
      "{'q_id': ['1784'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['34.886769'], 'longitude': ['-118.482995'], 'capacity': [None], 'point_of_interconnection': ['100.00 MW']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1784/03_phase_2_study/QC13PII-SCE-Northern-Q1784&TOT983-Keyhole-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1784/03_phase_2_study/QC13PII-SCE-Northern-Q1784&TOT983-Keyhole-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1784/03_phase_2_study/C13.2-North-Q1784-TOT983-Keyhole-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1784/03_phase_2_study/C13.2-North-Q1784-TOT983-Keyhole-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1784/03_phase_2_study/QC13 Ph2 Attachment 2 Q1784 TOT983.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1784/03_phase_2_study/QC13 Ph2 Attachment 2 Q1784 TOT983.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1785\n",
      "\n",
      "--- Processing project 1786 ---\n",
      "No original Appendix A PDF found for project 1786.\n",
      "\n",
      "--- Processing project 1787 ---\n",
      "No original Appendix A PDF found for project 1787.\n",
      "\n",
      "--- Processing project 1788 ---\n",
      "No original Appendix A PDF found for project 1788.\n",
      "\n",
      "--- Processing project 1789 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/C13.2-North-Q1789-TOT965-Rexford2-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1789\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.898151', '-119.024414')\n",
      "Base data extracted:\n",
      "{'q_id': ['1789'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.898151'], 'longitude': ['-119.024414'], 'capacity': [None], 'point_of_interconnection': ['Vestal Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13 Ph2 Attachment 2 Q1789 TOT965.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13 Ph2 Attachment 2 Q1789 TOT965.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13PII-SCE-Northern-Q1789&TOT965-Rexford2-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13PII-SCE-Northern-Q1789&TOT965-Rexford2-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13 Ph2 Attachment 3 Q1789 TOT965.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13 Ph2 Attachment 3 Q1789 TOT965.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13 Ph2 Attachment 2 Q1789 TOT965 addendum.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13 Ph2 Attachment 2 Q1789 TOT965 addendum.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13 Ph2 Attachment 2 Q1789 TOT965 addendum.pdf flagged as addendum.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13PII-SCE-Northern-Q1789&TOT965-Rexford2-Appendix A-Attachment 1_v2.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/QC13PII-SCE-Northern-Q1789&TOT965-Rexford2-Appendix A-Attachment 1_v2.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/C13.2-North-Q1789-TOT965-Rexford2-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1789/03_phase_2_study/C13.2-North-Q1789-TOT965-Rexford2-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1790 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1790/03_phase_2_study/C13.2-North-Q1790-TOT987-Sanborn4-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1790\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.0252139', '-118.1131028')\n",
      "Base data extracted:\n",
      "{'q_id': ['1790'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.0252139'], 'longitude': ['-118.1131028'], 'capacity': [None], 'point_of_interconnection': ['Windhub Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1790/03_phase_2_study/C13.2-North-Q1790-TOT987-Sanborn4-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1790/03_phase_2_study/C13.2-North-Q1790-TOT987-Sanborn4-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1790/03_phase_2_study/QC13PII-SCE-Northern-Q1790&TOT987-Sanborn4-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1790/03_phase_2_study/QC13PII-SCE-Northern-Q1790&TOT987-Sanborn4-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1790/03_phase_2_study/QC13 Ph2 Attachment 2 Q1790 TOT987.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1790/03_phase_2_study/QC13 Ph2 Attachment 2 Q1790 TOT987.pdf\n",
      "\n",
      "--- Processing project 1791 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1791/03_phase_2_study/C13.2-North-Q1791-TOT985-Sanborn5-ApndxA-01.04.22.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1791\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.025890', '-118.76208')\n",
      "Base data extracted:\n",
      "{'q_id': ['1791'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.025890'], 'longitude': ['-118.76208'], 'capacity': [None], 'point_of_interconnection': ['Windhub Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1791/03_phase_2_study/QC13 Ph2 Attachment 3 Q1791 TOT985.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1791/03_phase_2_study/QC13 Ph2 Attachment 3 Q1791 TOT985.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1791/03_phase_2_study/QC13PII-SCE-Northern-Q1791&TOT985-Sanborn5-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1791/03_phase_2_study/QC13PII-SCE-Northern-Q1791&TOT985-Sanborn5-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1791/03_phase_2_study/C13.2-North-Q1791-TOT985-Sanborn5-ApndxA-01.04.22.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1791/03_phase_2_study/C13.2-North-Q1791-TOT985-Sanborn5-ApndxA-01.04.22.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1791/03_phase_2_study/QC13 Ph2 Attachment 2 Q1791 TOT985.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1791/03_phase_2_study/QC13 Ph2 Attachment 2 Q1791 TOT985.pdf\n",
      "\n",
      "--- Processing project 1792 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/C13.2-North-Q1792-TOT969-Sequoia-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1792\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.9209', '-119.0983')\n",
      "Base data extracted:\n",
      "{'q_id': ['1792'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.9209'], 'longitude': ['-119.0983'], 'capacity': [None], 'point_of_interconnection': ['Vestal Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13PII-SCE-Northern-Q1792&TOT969-Sequoia-Appendix A-Attachment 1.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13PII-SCE-Northern-Q1792&TOT969-Sequoia-Appendix A-Attachment 1.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13 Ph2 Attachment 2 Q1792 TOT969 addendum.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13 Ph2 Attachment 2 Q1792 TOT969 addendum.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13 Ph2 Attachment 2 Q1792 TOT969 addendum.pdf flagged as addendum.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13 Ph2 Attachment 2 Q1792 TOT969.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13 Ph2 Attachment 2 Q1792 TOT969.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/C13.2-North-Q1792-TOT969-Sequoia-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/C13.2-North-Q1792-TOT969-Sequoia-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13 Ph2 Attachment 3 Q1792 TOT969.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13 Ph2 Attachment 3 Q1792 TOT969.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13PII-SCE-Northern-Q1792&TOT969-Sequoia-Appendix A-Attachment 1_v2.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1792/03_phase_2_study/QC13PII-SCE-Northern-Q1792&TOT969-Sequoia-Appendix A-Attachment 1_v2.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1793 ---\n",
      "No original Appendix A PDF found for project 1793.\n",
      "\n",
      "--- Processing project 1794 ---\n",
      "No original Appendix A PDF found for project 1794.\n",
      "\n",
      "--- Processing project 1795 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1795/03_phase_2_study/C13.2-EOP-Q1795-TOT967-Arida3-ApndxA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1795\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.001228', '114.629128')\n",
      "Base data extracted:\n",
      "{'q_id': ['1795'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.001228'], 'longitude': ['114.629128'], 'capacity': [None], 'point_of_interconnection': ['Mohave Substation 500 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1795/03_phase_2_study/QC13 Ph2 Attachment 2 Q1795 TOT967.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1795/03_phase_2_study/QC13 Ph2 Attachment 2 Q1795 TOT967.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1795/03_phase_2_study/C13.2-EOP-Q1795-TOT967-Arida3-ApndxA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1795/03_phase_2_study/C13.2-EOP-Q1795-TOT967-Arida3-ApndxA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1795/03_phase_2_study/QC13 Ph2 Attachment 3 Q1795 TOT967.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1795/03_phase_2_study/QC13 Ph2 Attachment 3 Q1795 TOT967.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1795/03_phase_2_study/QC13PII EOP Attch1 TOT967-Q1795 Arida Solar Farm 3.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1795/03_phase_2_study/QC13PII EOP Attch1 TOT967-Q1795 Arida Solar Farm 3.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1796 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/C13.2-EOP-Q1796-TOT995-Delamar-ApndxA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1796\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.793331', '114.970969')\n",
      "Base data extracted:\n",
      "{'q_id': ['1796'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.793331'], 'longitude': ['114.970969'], 'capacity': [None], 'point_of_interconnection': ['SCE Owned Eldorado Substation 220 kV Bus']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13 Ph2 Attachment 2 Q1796 TOT995.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13 Ph2 Attachment 2 Q1796 TOT995.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13 Ph2 Attachment 2 Q1796 TOT995_CRAS Removal Addendum.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13 Ph2 Attachment 2 Q1796 TOT995_CRAS Removal Addendum.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13 Ph2 Attachment 2 Q1796 TOT995_CRAS Removal Addendum.pdf flagged as addendum.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13PII EOP Attch1 TOT995-Q1796 Delamar Energy Storage.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13PII EOP Attch1 TOT995-Q1796 Delamar Energy Storage.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13 Ph2 Attachment 3 Q1796 TOT995.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13 Ph2 Attachment 3 Q1796 TOT995.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/C13.2-EOP-Q1796-TOT995-Delamar-ApndxA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/C13.2-EOP-Q1796-TOT995-Delamar-ApndxA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13 Ph2 Attachment 3 Q1796 TOT995_CRAS Removal Addendum.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1796/03_phase_2_study/QC13 Ph2 Attachment 3 Q1796 TOT995_CRAS Removal Addendum.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1797 ---\n",
      "No original Appendix A PDF found for project 1797.\n",
      "\n",
      "--- Processing project 1798 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/QC13PII-EOP-GLW-Q1798&TOT972-CalvadaSprings-Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1798\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.957377', '-115.845073')\n",
      "Base data extracted:\n",
      "{'q_id': ['1798'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['35.957377'], 'longitude': ['-115.845073'], 'capacity': [None], 'point_of_interconnection': ['Trout Canyon 230 kV Switching Station']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "Attempting to update base data from: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/C13.2-EOP-Q1798-TOT972-Calvada-AFS_ApndxA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1798\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1798'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/QC13 Ph2 Attachment 2 Q1798 TOT972 AFS.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/QC13 Ph2 Attachment 2 Q1798 TOT972 AFS.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/QC13PII-EOP-GLW-Q1798&TOT972-CalvadaSprings-Appendix A.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/QC13PII-EOP-GLW-Q1798&TOT972-CalvadaSprings-Appendix A.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/QC13 Ph2 Attachment 2 Q1798 TOT972 AFS_CRAS Removal Addendum.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/QC13 Ph2 Attachment 2 Q1798 TOT972 AFS_CRAS Removal Addendum.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/QC13 Ph2 Attachment 2 Q1798 TOT972 AFS_CRAS Removal Addendum.pdf flagged as addendum.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/C13P2-GLW-Q1798-Appendix A-GLW-Attachment 2.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/C13P2-GLW-Q1798-Appendix A-GLW-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/C13.2-EOP-Q1798-TOT972-Calvada-AFS_ApndxA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1798/03_phase_2_study/C13.2-EOP-Q1798-TOT972-Calvada-AFS_ApndxA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1799 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1799/03_phase_2_study/QC13PII-EOP-GLW-Q1799&TOT979-RoughHat2-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1799\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('36.087', '-115.')\n",
      "Base data extracted:\n",
      "{'q_id': ['1799'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['36.087'], 'longitude': ['-115.'], 'capacity': [None], 'point_of_interconnection': ['Trout Canyon 230 kV Switching Station']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "Attempting to update base data from: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1799/03_phase_2_study/C13.2-EOP-Q1799-TOT979-RoughHat2-AFS_ApndxA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1799\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1799'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1799/03_phase_2_study/C13P2-GLW-Q1799-Appendix A-GLW-Attachment 2.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1799/03_phase_2_study/C13P2-GLW-Q1799-Appendix A-GLW-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1799/03_phase_2_study/QC13PII-EOP-GLW-Q1799&TOT979-RoughHat2-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1799/03_phase_2_study/QC13PII-EOP-GLW-Q1799&TOT979-RoughHat2-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1799/03_phase_2_study/QC13 Ph2 Attachment 2 Q1799 TOT979 AFS.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1799/03_phase_2_study/QC13 Ph2 Attachment 2 Q1799 TOT979 AFS.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1799/03_phase_2_study/C13.2-EOP-Q1799-TOT979-RoughHat2-AFS_ApndxA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1799/03_phase_2_study/C13.2-EOP-Q1799-TOT979-RoughHat2-AFS_ApndxA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1800 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/QC13PII-EOP-GLW-Q1800&TOT984-Sagittarius-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1800\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('36.028883', '-115.836632')\n",
      "Base data extracted:\n",
      "{'q_id': ['1800'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['36.028883'], 'longitude': ['-115.836632'], 'capacity': [None], 'point_of_interconnection': ['Trout Canyon 230 kV Switching Station']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "Attempting to update base data from: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/C13.2-EOP-Q1800-TOT984-Sagittarius-AFS_ApndxA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1800\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1800'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/QC13PII-EOP-GLW-Q1800&TOT984-Sagittarius-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/QC13PII-EOP-GLW-Q1800&TOT984-Sagittarius-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/C13P2-GLW-Q1800-Appendix A-GLW-Attachment 2.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/C13P2-GLW-Q1800-Appendix A-GLW-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/QC13 Ph2 Attachment 2 Q1800 TOT984 AFS.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/QC13 Ph2 Attachment 2 Q1800 TOT984 AFS.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/QC13 Ph2 Attachment 2 Q1800 TOT984 AFS_CRAS Removal Addendum.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/QC13 Ph2 Attachment 2 Q1800 TOT984 AFS_CRAS Removal Addendum.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/QC13 Ph2 Attachment 2 Q1800 TOT984 AFS_CRAS Removal Addendum.pdf flagged as addendum.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/C13.2-EOP-Q1800-TOT984-Sagittarius-AFS_ApndxA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1800/03_phase_2_study/C13.2-EOP-Q1800-TOT984-Sagittarius-AFS_ApndxA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1801 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/QC13PII-EOP-GLW-Q1801&TOT971-WaterRock-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1801\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('36.109', '-115.875')\n",
      "Base data extracted:\n",
      "{'q_id': ['1801'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['36.109'], 'longitude': ['-115.875'], 'capacity': [None], 'point_of_interconnection': ['Gamebird 230 kV Switching Station']}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity']\n",
      "Attempting to update base data from: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/C13.2-EOP-Q1801-TOT971-WaterRock-AFS_ApndxA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1801\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1801'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "After update, still missing: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/QC13 Ph2 Attachment 2 Q1801 TOT971 AFS.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/QC13 Ph2 Attachment 2 Q1801 TOT971 AFS.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/QC13 Ph2 Attachment 2 Q1801 TOT971 AFS_CRAS Removal Addendum.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/QC13 Ph2 Attachment 2 Q1801 TOT971 AFS_CRAS Removal Addendum.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/QC13 Ph2 Attachment 2 Q1801 TOT971 AFS_CRAS Removal Addendum.pdf flagged as addendum.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/QC13PII-EOP-GLW-Q1801&TOT971-WaterRock-AppendixA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/QC13PII-EOP-GLW-Q1801&TOT971-WaterRock-AppendixA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/C13.2-EOP-Q1801-TOT971-WaterRock-AFS_ApndxA.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/C13.2-EOP-Q1801-TOT971-WaterRock-AFS_ApndxA.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/C13P2-GLW-Q1801-Appendix A-GLW-Attachment 2.pdf\n",
      "Scraped this Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1801/03_phase_2_study/C13P2-GLW-Q1801-Appendix A-GLW-Attachment 2.pdf\n",
      "\n",
      "--- Processing project 1802 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1802/03_phase_2_study/QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1802\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.594889', '-117.057076')\n",
      "Base data extracted:\n",
      "{'q_id': ['1802'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['32.594889'], 'longitude': ['-117.057076'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1802/03_phase_2_study/P2RPT-QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_1_121721.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1802\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.594889', '-117.057076')\n",
      "Base data extracted:\n",
      "{'q_id': ['1802'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['32.594889'], 'longitude': ['-117.057076'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1802/03_phase_2_study/P2RPT-QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_1_121721.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1802/03_phase_2_study/P2RPT-QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_1_121721.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1802/03_phase_2_study/QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_2_12122.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1802/03_phase_2_study/QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_2_12122.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1802/03_phase_2_study/QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1802/03_phase_2_study/QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1803 ---\n",
      "No original Appendix A PDF found for project 1803.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1804\n",
      "\n",
      "--- Processing project 1805 ---\n",
      "No original Appendix A PDF found for project 1805.\n",
      "\n",
      "--- Processing project 1806 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1806/03_phase_2_study/QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1806\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.532816', '-117.677040')\n",
      "Base data extracted:\n",
      "{'q_id': ['1806'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.532816'], 'longitude': ['-117.677040'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1806/03_phase_2_study/QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_Addendum2_32822.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1806\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.532816', '-117.677040')\n",
      "Base data extracted:\n",
      "{'q_id': ['1806'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.532816'], 'longitude': ['-117.677040'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1806/03_phase_2_study/QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_Addendum2_32822.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1806/03_phase_2_study/QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_Addendum2_32822.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1806/03_phase_2_study/QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_Addendum1_012422.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1806/03_phase_2_study/QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_Addendum1_012422.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1806/03_phase_2_study/QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1806/03_phase_2_study/QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1807\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1808\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1809\n",
      "\n",
      "--- Processing project 1810 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1810/03_phase_2_study/QC13PhII_Q1810_Drop_Zone_Storage_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1810\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.569397', '-116.916559')\n",
      "Base data extracted:\n",
      "{'q_id': ['1810'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['32.569397'], 'longitude': ['-116.916559'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1810/03_phase_2_study/QC13PhII_Q1810_Drop_Zone_Storage_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1810/03_phase_2_study/QC13PhII_Q1810_Drop_Zone_Storage_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1811 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1811/03_phase_2_study/QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1811\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.85815', '-116.8945')\n",
      "Base data extracted:\n",
      "{'q_id': ['1811'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['32.85815'], 'longitude': ['-116.8945'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1811/03_phase_2_study/P2RPT-QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_Addendum1_121721.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1811\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.85815', '-116.8945')\n",
      "Base data extracted:\n",
      "{'q_id': ['1811'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['32.85815'], 'longitude': ['-116.8945'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1811/03_phase_2_study/QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1811/03_phase_2_study/QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1811/03_phase_2_study/P2RPT-QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_Addendum1_121721.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1811/03_phase_2_study/P2RPT-QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_Addendum1_121721.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1812 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1812/03_phase_2_study/QC13PhII_Q1812_Elisabeth_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1812\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.97', '-113.516')\n",
      "Base data extracted:\n",
      "{'q_id': ['1812'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['32.97'], 'longitude': ['-113.516'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "Attempting to update base data from: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1812/03_phase_2_study/QC13PhII_Q1812_Elisabeth_Appendix_A_112221_v2.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1812\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.97', '-113.516')\n",
      "Base data extracted:\n",
      "{'q_id': ['1812'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['32.97'], 'longitude': ['-113.516'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1812/03_phase_2_study/QC13PhII_Q1812_Elisabeth_Appendix_A_v2_Addendum1_12122.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1812\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.97', '-113.516')\n",
      "Base data extracted:\n",
      "{'q_id': ['1812'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['32.97'], 'longitude': ['-113.516'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1812/03_phase_2_study/QC13PhII_Q1812_Elisabeth_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1812/03_phase_2_study/QC13PhII_Q1812_Elisabeth_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1812/03_phase_2_study/QC13PhII_Q1812_Elisabeth_Appendix_A_v2_Addendum1_12122.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1812/03_phase_2_study/QC13PhII_Q1812_Elisabeth_Appendix_A_v2_Addendum1_12122.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1812/03_phase_2_study/QC13PhII_Q1812_Elisabeth_Appendix_A_112221_v2.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1812/03_phase_2_study/QC13PhII_Q1812_Elisabeth_Appendix_A_112221_v2.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1813\n",
      "\n",
      "--- Processing project 1814 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1814/03_phase_2_study/QC13PhII_Q1814_Hedionda_Energy_Storage_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1814\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.1354', '-117.333342')\n",
      "Base data extracted:\n",
      "{'q_id': ['1814'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.1354'], 'longitude': ['-117.333342'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1814/03_phase_2_study/QC13PhII_Q1814_Hedionda_Energy_Storage_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1814/03_phase_2_study/QC13PhII_Q1814_Hedionda_Energy_Storage_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1815 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1815/03_phase_2_study/QC13PhII_Q1815_Hinton_Energy_Storage_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1815\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.01854', '-')\n",
      "Base data extracted:\n",
      "{'q_id': ['1815'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.01854'], 'longitude': ['-'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1815/03_phase_2_study/QC13PhII_Q1815_Hinton_Energy_Storage_Appendix_A_Rev1_12122.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1815\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.01854', '-')\n",
      "Base data extracted:\n",
      "{'q_id': ['1815'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.01854'], 'longitude': ['-'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1815/03_phase_2_study/QC13PhII_Q1815_Hinton_Energy_Storage_Appendix_A_Rev1_12122.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1815/03_phase_2_study/QC13PhII_Q1815_Hinton_Energy_Storage_Appendix_A_Rev1_12122.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1815/03_phase_2_study/QC13PhII_Q1815_Hinton_Energy_Storage_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1815/03_phase_2_study/QC13PhII_Q1815_Hinton_Energy_Storage_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1816\n",
      "\n",
      "--- Processing project 1817 ---\n",
      "No original Appendix A PDF found for project 1817.\n",
      "\n",
      "--- Processing project 1818 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1818/03_phase_2_study/QC13PhII_Q1818_Saddle Mountain_Solar_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1818\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.324703', '-')\n",
      "Base data extracted:\n",
      "{'q_id': ['1818'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.324703'], 'longitude': ['-'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1818/03_phase_2_study/QC13PhII_Q1818_Saddle Mountain_Solar_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1818/03_phase_2_study/QC13PhII_Q1818_Saddle Mountain_Solar_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1819\n",
      "\n",
      "--- Processing project 1820 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1820/03_phase_2_study/QC13PhII_Q1820_Scafell_Storage_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1820\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.018184', '-116.856026')\n",
      "Base data extracted:\n",
      "{'q_id': ['1820'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.018184'], 'longitude': ['-116.856026'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1820/03_phase_2_study/QC13PhII_Q1820_Scafell_Storage_Appendix_A_Rev1_12122.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1820\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.018184', '-116.856026')\n",
      "Base data extracted:\n",
      "{'q_id': ['1820'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.018184'], 'longitude': ['-116.856026'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1820/03_phase_2_study/QC13PhII_Q1820_Scafell_Storage_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1820/03_phase_2_study/QC13PhII_Q1820_Scafell_Storage_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1820/03_phase_2_study/QC13PhII_Q1820_Scafell_Storage_Appendix_A_Rev1_12122.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1820/03_phase_2_study/QC13PhII_Q1820_Scafell_Storage_Appendix_A_Rev1_12122.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1821 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1821/03_phase_2_study/QC13PhII_Q1821_Seguro_Storage_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1821\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.12711', '-117.11715')\n",
      "Base data extracted:\n",
      "{'q_id': ['1821'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.12711'], 'longitude': ['-117.11715'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1821/03_phase_2_study/QC13PhII_Q1821_Seguro_Storage_Appendix_A_Addendum1_12122.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1821\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.12711', '-117.11715')\n",
      "Base data extracted:\n",
      "{'q_id': ['1821'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.12711'], 'longitude': ['-117.11715'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1821/03_phase_2_study/QC13PhII_Q1821_Seguro_Storage_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1821/03_phase_2_study/QC13PhII_Q1821_Seguro_Storage_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1821/03_phase_2_study/QC13PhII_Q1821_Seguro_Storage_Appendix_A_Addendum1_12122.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1821/03_phase_2_study/QC13PhII_Q1821_Seguro_Storage_Appendix_A_Addendum1_12122.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1822 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1822/03_phase_2_study/QC13PhII_Q1822_Sun_Streams_4_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1822\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.347', '-112.845')\n",
      "Base data extracted:\n",
      "{'q_id': ['1822'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.347'], 'longitude': ['-112.845'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1822/03_phase_2_study/QC13PhII_Q1822_Sun_Streams_4_Appendix_A_Addendum1_12122.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1822\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.347', '-112.845')\n",
      "Base data extracted:\n",
      "{'q_id': ['1822'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.347'], 'longitude': ['-112.845'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1822/03_phase_2_study/QC13PhII_Q1822_Sun_Streams_4_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1822/03_phase_2_study/QC13PhII_Q1822_Sun_Streams_4_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1822/03_phase_2_study/QC13PhII_Q1822_Sun_Streams_4_Appendix_A_Addendum1_12122.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1822/03_phase_2_study/QC13PhII_Q1822_Sun_Streams_4_Appendix_A_Addendum1_12122.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1823 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1823/03_phase_2_study/QC13PhII_Q1823_Sun_Streams_5_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1823\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.347', '-112.845')\n",
      "Base data extracted:\n",
      "{'q_id': ['1823'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.347'], 'longitude': ['-112.845'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1823/03_phase_2_study/QC13PhII_Q1823_Sun_Streams_5_Appendix_A_Addendum1_12122.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1823\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.347', '-112.845')\n",
      "Base data extracted:\n",
      "{'q_id': ['1823'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.347'], 'longitude': ['-112.845'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1823/03_phase_2_study/QC13PhII_Q1823_Sun_Streams_5_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1823/03_phase_2_study/QC13PhII_Q1823_Sun_Streams_5_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1823/03_phase_2_study/QC13PhII_Q1823_Sun_Streams_5_Appendix_A_Addendum1_12122.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1823/03_phase_2_study/QC13PhII_Q1823_Sun_Streams_5_Appendix_A_Addendum1_12122.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1824 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1824/03_phase_2_study/QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1824\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.5467970', '-116.1613804')\n",
      "Base data extracted:\n",
      "{'q_id': ['1824'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['32.5467970'], 'longitude': ['-116.1613804'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1824/03_phase_2_study/QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_Addendum1_12122.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1824\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.5467970', '-116.1613804')\n",
      "Base data extracted:\n",
      "{'q_id': ['1824'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['32.5467970'], 'longitude': ['-116.1613804'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1824/03_phase_2_study/QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_Addendum1_12122.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1824/03_phase_2_study/QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_Addendum1_12122.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1824/03_phase_2_study/P2RPT-QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_121721.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1824/03_phase_2_study/P2RPT-QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_121721.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1824/03_phase_2_study/P2RPT-QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev2_22924.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1824/03_phase_2_study/P2RPT-QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev2_22924.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1824/03_phase_2_study/QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1824/03_phase_2_study/QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "\n",
      "--- Processing project 1825 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1825/03_phase_2_study/QC13PhII_Q1825_Viking_Energy_Storage_Appendix_A_11-22-21.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1825\n",
      "Extracted Cluster Number: 13\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.209793', '-117.295907')\n",
      "Base data extracted:\n",
      "{'q_id': ['1825'], 'cluster': ['13'], 'req_deliverability': ['Full'], 'latitude': ['33.209793'], 'longitude': ['-117.295907'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base data columns in first Appendix A PDF: ['capacity', 'point_of_interconnection']\n",
      "After update, still missing: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1825/03_phase_2_study/QC13PhII_Q1825_Viking_Energy_Storage_Appendix_A_11-22-21.pdf\n",
      "--> /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1825/03_phase_2_study/QC13PhII_Q1825_Viking_Energy_Storage_Appendix_A_11-22-21.pdf is not an Attachment 2 PDF. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1826\n",
      "\n",
      "--- Processing project 1827 ---\n",
      "No original Appendix A PDF found for project 1827.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1828\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/rawdata_cluster13_style_others_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_others_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 55\n",
      "Total Projects Scraped: 24\n",
      "Total Projects Skipped: 31\n",
      "Total Projects Missing: 40\n",
      "Total PDFs Accessed: 162\n",
      "Total PDFs Scraped: 36\n",
      "Total PDFs Skipped: 126\n",
      "\n",
      "List of Scraped Projects: [1757, 1758, 1759, 1761, 1763, 1764, 1766, 1768, 1774, 1775, 1776, 1779, 1782, 1784, 1789, 1790, 1791, 1792, 1795, 1796, 1798, 1799, 1800, 1801]\n",
      "\n",
      "List of Skipped Projects: [1683, 1687, 1688, 1690, 1691, 1695, 1700, 1702, 1705, 1709, 1713, 1728, 1739, 1744, 1745, 1750, 1751, 1802, 1806, 1810, 1811, 1812, 1814, 1815, 1818, 1820, 1821, 1822, 1823, 1824, 1825]\n",
      "\n",
      "List of Missing Projects: [1679, 1680, 1681, 1693, 1697, 1698, 1701, 1704, 1706, 1707, 1708, 1711, 1716, 1717, 1720, 1725, 1727, 1730, 1734, 1746, 1753, 1755, 1765, 1767, 1769, 1771, 1772, 1773, 1777, 1781, 1785, 1804, 1807, 1808, 1809, 1813, 1816, 1819, 1826, 1828]\n",
      "\n",
      "List of Scraped PDFs: ['Revision2-QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Attachment 2.pdf', 'Revision2-QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Attachment 3.pdf', 'QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A-Attachment 2.pdf', 'QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A-Attachment 2.pdf', 'Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 2.pdf', 'Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 3.pdf', 'QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A-Attachment 2.pdf', 'QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A-Attachment 2.pdf', 'QC13 Ph2 Attachment 2 Q1766 TOT996.pdf', 'QC13 Ph2 Attachment 2 Q1768 TOT1008.pdf', 'QC13 Ph2 Attachment 2 Q1774 TOT981.pdf', 'QC13 Ph2 Attachment 2 Q1775 TOT989.pdf', 'QC13 Ph2 Attachment 2 Q1776 TOT993.pdf', 'QC13 Ph2 Attachment 2 Q1779 TOT966.pdf', 'QC13 Ph2 Attachment 2 Q1782 TOT1002.pdf', 'QC13 Ph2 Attachment 2 Q1784 TOT983.pdf', 'QC13 Ph2 Attachment 2 Q1789 TOT965.pdf', 'QC13 Ph2 Attachment 2 Q1789 TOT965 addendum.pdf', 'QC13 Ph2 Attachment 2 Q1790 TOT987.pdf', 'QC13 Ph2 Attachment 2 Q1791 TOT985.pdf', 'QC13 Ph2 Attachment 2 Q1792 TOT969 addendum.pdf', 'QC13 Ph2 Attachment 2 Q1792 TOT969.pdf', 'QC13 Ph2 Attachment 2 Q1795 TOT967.pdf', 'QC13 Ph2 Attachment 2 Q1796 TOT995.pdf', 'QC13 Ph2 Attachment 2 Q1796 TOT995_CRAS Removal Addendum.pdf', 'QC13 Ph2 Attachment 2 Q1798 TOT972 AFS.pdf', 'QC13 Ph2 Attachment 2 Q1798 TOT972 AFS_CRAS Removal Addendum.pdf', 'C13P2-GLW-Q1798-Appendix A-GLW-Attachment 2.pdf', 'C13P2-GLW-Q1799-Appendix A-GLW-Attachment 2.pdf', 'QC13 Ph2 Attachment 2 Q1799 TOT979 AFS.pdf', 'C13P2-GLW-Q1800-Appendix A-GLW-Attachment 2.pdf', 'QC13 Ph2 Attachment 2 Q1800 TOT984 AFS.pdf', 'QC13 Ph2 Attachment 2 Q1800 TOT984 AFS_CRAS Removal Addendum.pdf', 'QC13 Ph2 Attachment 2 Q1801 TOT971 AFS.pdf', 'QC13 Ph2 Attachment 2 Q1801 TOT971 AFS_CRAS Removal Addendum.pdf', 'C13P2-GLW-Q1801-Appendix A-GLW-Attachment 2.pdf']\n",
      "\n",
      "List of Skipped PDFs: ['Q1683-Buena Vista Storage-Appendix_A-C13PhII.pdf', 'Q1687-Conaway Hybrid Power Plant-Appendix_A-C13PhII.pdf', 'Q1688-Crossroads-Appendix_A-C13PhII.pdf', 'Q1688CrossroadsAppendix_AC13PhIIRevision1.pdf', 'Q1690-Denali Energy Storage-Appendix_A-C13PhII.pdf', 'Q1691FortisAppendix_AC13PhIIRevision1.pdf', 'Q1691-Fortis-Appendix_A-C13PhII.pdf', 'Q1695-Meadows Energy Storage-Appendix_A-C13PhII.pdf', 'Q1700-North Bay Energy Storage-Appendix_A-C13PhII.pdf', 'Q1702-Potentia-Viridi-Appendix_A-C13PhII.pdf', 'Q1705-Steel City Battery Storage-Appendix_A-C13PhII.pdf', 'P2RPT-Q1709RosemaryAppendix_AC13PhIIAddendum1.pdf', 'C13Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1709.pdf', 'Q1709-Rosemary-Appendix_A-C13PhII.pdf', 'Q1713Lead_Appendix_AAddendum1.pdf', 'P2RPT-Q1713BiaBESS1Appendix_AC13PhIIAddendum1.pdf', 'Q1713-Bia BESS 1-Appendix_A-C13PhII.pdf', 'Q1728-Zeta-Appendix_A-C13PhII.pdf', 'P2RPT-Q1728ZetaAppendix_AC13PhIIAddendum1.pdf', 'P2RPT-Q1728ZetaAppendix_AC13PhIIAddendum2_.pdf', 'Q1739Pecho_Energy_Storage_Appendix_A_Addendum2.pdf', 'Q1739Pecho_Energy_Storage_Appendix_A_Addendum3.pdf', 'Q1739Pecho_Energy_StorageAppendix_AC13PhIIRevision1.pdf', 'Q1739-Pecho Energy Storage-Appendix_A-C13PhII.pdf', 'P2RPT-Q1744Second_FiddleAppendix_AC13PhIIAddendum1.pdf', 'Q1744-Second Fiddle-Appendix_A-C13PhII.pdf', 'Q1745-Sunrise Power Improvement-Appendix_A-C13PhII.pdf', 'Q1750Windwalker_OffshoreAppendix_AC13PhIIAddendum1.pdf', 'Q1750-Windwalker Offshore-Appendix_A-C13PhII.pdf', 'Q1751-Winston Hybrid PV and BESS-Appendix_A-C13PhII.pdf', 'P2RPT-Q1751Winston_Hybrid_PV_and_BESSAppendix_AC13PhIIAddendum1.pdf', 'QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-Attachment 3-revised.pdf', 'QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-revised.pdf', 'QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-Attachment 1.pdf', 'QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A.pdf', 'QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Appendix A-Attachment 3.pdf', 'QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A-Attachment 1.pdf', 'QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A.pdf', 'QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A-Attachment 1.pdf', 'QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A.pdf', 'QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-Attachment 3-revised.pdf', 'QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-revised.pdf', 'Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 1.pdf', 'QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A.pdf', 'QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-Attachment 1.pdf', 'QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Appendix A-Attachment 3.pdf', 'QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A-Attachment 1.pdf', 'QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A.pdf', 'QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A.pdf', 'QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A-Attachment 1.pdf', 'C13.2-Metro-Q1766-TOT996-Commerce2-AppendixA.pdf', 'C13PII- Metro-Area Report_rev2_rs.pdf', 'C13PII-Metro-Attachment1-TOT1008 Q1768.pdf', 'C13.2-Metro-Q1768-TOT1008-Roadhouse-AppendixA.pdf', 'QC13 Ph2 Attachment 3 Q1768 TOT1008.pdf', 'C13.2-NOL-Q1774-TOT981-Overnight-ApndxA.pdf', 'QC13 Ph2 Attachment 3 Q1774 TOT981.pdf', 'QC13 Ph2-Attachment 1 Q1774 TOT981-Overnight.pdf', 'C13.2-NOL-Q1775-TOT989-SEGSEx2-ApndxA.pdf', 'QC13 Ph2 Attachment 3 Q1775 TOT989.pdf', 'QC13 Ph2 Attachment 1 Q1776 TOT993-Ventoso.pdf', 'C13.2-NOL-Q1776-TOT993-Ventoso-ApndxA.pdf', 'QC13 Ph2 Attachment 3 Q1776 TOT993.pdf', 'QC13PII-SCE-Northern-Q1779&TOT966-Bellefield3-Appendix A-Attachment 1.pdf', 'C13.2-North-Q1779-TOT966-Bellefiled3-AppendixA.pdf', 'QC13PII-SCE-Northern-Q1782&TOT1002-Gem-Appendix A-Attachment 1.pdf', 'C13.2-North-Q1782-TOT1002-Gem-AppendixA.pdf', 'QC13PII-SCE-Northern-Q1784&TOT983-Keyhole-Appendix A-Attachment 1.pdf', 'C13.2-North-Q1784-TOT983-Keyhole-AppendixA.pdf', 'QC13PII-SCE-Northern-Q1789&TOT965-Rexford2-Appendix A-Attachment 1.pdf', 'QC13 Ph2 Attachment 3 Q1789 TOT965.pdf', 'QC13PII-SCE-Northern-Q1789&TOT965-Rexford2-Appendix A-Attachment 1_v2.pdf', 'C13.2-North-Q1789-TOT965-Rexford2-AppendixA.pdf', 'C13.2-North-Q1790-TOT987-Sanborn4-AppendixA.pdf', 'QC13PII-SCE-Northern-Q1790&TOT987-Sanborn4-Appendix A-Attachment 1.pdf', 'QC13 Ph2 Attachment 3 Q1791 TOT985.pdf', 'QC13PII-SCE-Northern-Q1791&TOT985-Sanborn5-Appendix A-Attachment 1.pdf', 'C13.2-North-Q1791-TOT985-Sanborn5-ApndxA-01.04.22.pdf', 'QC13PII-SCE-Northern-Q1792&TOT969-Sequoia-Appendix A-Attachment 1.pdf', 'C13.2-North-Q1792-TOT969-Sequoia-AppendixA.pdf', 'QC13 Ph2 Attachment 3 Q1792 TOT969.pdf', 'QC13PII-SCE-Northern-Q1792&TOT969-Sequoia-Appendix A-Attachment 1_v2.pdf', 'C13.2-EOP-Q1795-TOT967-Arida3-ApndxA.pdf', 'QC13 Ph2 Attachment 3 Q1795 TOT967.pdf', 'QC13PII EOP Attch1 TOT967-Q1795 Arida Solar Farm 3.pdf', 'QC13PII EOP Attch1 TOT995-Q1796 Delamar Energy Storage.pdf', 'QC13 Ph2 Attachment 3 Q1796 TOT995.pdf', 'C13.2-EOP-Q1796-TOT995-Delamar-ApndxA.pdf', 'QC13 Ph2 Attachment 3 Q1796 TOT995_CRAS Removal Addendum.pdf', 'QC13PII-EOP-GLW-Q1798&TOT972-CalvadaSprings-Appendix A.pdf', 'C13.2-EOP-Q1798-TOT972-Calvada-AFS_ApndxA.pdf', 'QC13PII-EOP-GLW-Q1799&TOT979-RoughHat2-AppendixA.pdf', 'C13.2-EOP-Q1799-TOT979-RoughHat2-AFS_ApndxA.pdf', 'QC13PII-EOP-GLW-Q1800&TOT984-Sagittarius-AppendixA.pdf', 'C13.2-EOP-Q1800-TOT984-Sagittarius-AFS_ApndxA.pdf', 'QC13PII-EOP-GLW-Q1801&TOT971-WaterRock-AppendixA.pdf', 'C13.2-EOP-Q1801-TOT971-WaterRock-AFS_ApndxA.pdf', 'P2RPT-QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_1_121721.pdf', 'QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_2_12122.pdf', 'QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_Addendum2_32822.pdf', 'QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_Addendum1_012422.pdf', 'QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1810_Drop_Zone_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_11-22-21.pdf', 'P2RPT-QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_Addendum1_121721.pdf', 'QC13PhII_Q1812_Elisabeth_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1812_Elisabeth_Appendix_A_v2_Addendum1_12122.pdf', 'QC13PhII_Q1812_Elisabeth_Appendix_A_112221_v2.pdf', 'QC13PhII_Q1814_Hedionda_Energy_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1815_Hinton_Energy_Storage_Appendix_A_Rev1_12122.pdf', 'QC13PhII_Q1815_Hinton_Energy_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1818_Saddle Mountain_Solar_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1820_Scafell_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1820_Scafell_Storage_Appendix_A_Rev1_12122.pdf', 'QC13PhII_Q1821_Seguro_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1821_Seguro_Storage_Appendix_A_Addendum1_12122.pdf', 'QC13PhII_Q1822_Sun_Streams_4_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1822_Sun_Streams_4_Appendix_A_Addendum1_12122.pdf', 'QC13PhII_Q1823_Sun_Streams_5_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1823_Sun_Streams_5_Appendix_A_Addendum1_12122.pdf', 'QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_Addendum1_12122.pdf', 'P2RPT-QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_121721.pdf', 'P2RPT-QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev2_22924.pdf', 'QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1825_Viking_Energy_Storage_Appendix_A_11-22-21.pdf']\n",
      "\n",
      "List of Addendum PDFs: ['Revision2-QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Attachment 3.pdf', 'Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 3.pdf', 'QC13 Ph2 Attachment 2 Q1789 TOT965 addendum.pdf', 'QC13 Ph2 Attachment 2 Q1792 TOT969 addendum.pdf', 'QC13 Ph2 Attachment 2 Q1796 TOT995_CRAS Removal Addendum.pdf', 'QC13 Ph2 Attachment 2 Q1798 TOT972 AFS_CRAS Removal Addendum.pdf', 'QC13 Ph2 Attachment 2 Q1800 TOT984 AFS_CRAS Removal Addendum.pdf', 'QC13 Ph2 Attachment 2 Q1801 TOT971 AFS_CRAS Removal Addendum.pdf']\n",
      "\n",
      "List of Original PDFs: ['Revision2-QC13PII-SCE-Eastern-Q1757 TOT1013-Cobalt-Attachment 2.pdf', 'QC13PII-SCE-Eastern-Q1758 TOT1005-DesertSands-Appendix A-Attachment 2.pdf', 'QC13PII-SCE-Eastern-Q1759 TOT1016-Eternal-Appendix A-Attachment 2.pdf', 'Revision2-QC13PII-SCE-Eastern-Q1761 TOT1006-Grace-Attachment 2.pdf', 'QC13PII-SCE-Eastern-Q1763 TOT988-Porta-Appendix A-Attachment 2.pdf', 'QC13PII-SCE-Eastern-Q1764 TOT976-Sapphire-Appendix A-Attachment 2.pdf', 'QC13 Ph2 Attachment 2 Q1766 TOT996.pdf', 'QC13 Ph2 Attachment 2 Q1768 TOT1008.pdf', 'QC13 Ph2 Attachment 2 Q1774 TOT981.pdf', 'QC13 Ph2 Attachment 2 Q1775 TOT989.pdf', 'QC13 Ph2 Attachment 2 Q1776 TOT993.pdf', 'QC13 Ph2 Attachment 2 Q1779 TOT966.pdf', 'QC13 Ph2 Attachment 2 Q1782 TOT1002.pdf', 'QC13 Ph2 Attachment 2 Q1784 TOT983.pdf', 'QC13 Ph2 Attachment 2 Q1789 TOT965.pdf', 'QC13 Ph2 Attachment 2 Q1790 TOT987.pdf', 'QC13 Ph2 Attachment 2 Q1791 TOT985.pdf', 'QC13 Ph2 Attachment 2 Q1792 TOT969.pdf', 'QC13 Ph2 Attachment 2 Q1795 TOT967.pdf', 'QC13 Ph2 Attachment 2 Q1796 TOT995.pdf', 'QC13 Ph2 Attachment 2 Q1798 TOT972 AFS.pdf', 'C13P2-GLW-Q1798-Appendix A-GLW-Attachment 2.pdf', 'C13P2-GLW-Q1799-Appendix A-GLW-Attachment 2.pdf', 'QC13 Ph2 Attachment 2 Q1799 TOT979 AFS.pdf', 'C13P2-GLW-Q1800-Appendix A-GLW-Attachment 2.pdf', 'QC13 Ph2 Attachment 2 Q1800 TOT984 AFS.pdf', 'QC13 Ph2 Attachment 2 Q1801 TOT971 AFS.pdf', 'C13P2-GLW-Q1801-Appendix A-GLW-Attachment 2.pdf']\n",
      "\n",
      "List of Style N PDFs (Skipped due to 'Network Upgrade Type'): []\n",
      "\n",
      "Total Number of Style N PDFs: 0\n",
      "\n",
      "Number of Original PDFs Scraped: 28\n",
      "Number of Addendum PDFs Scraped: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3337052921.py:738: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3337052921.py:738: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import traceback\n",
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------- Configuration -------------------\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/rawdata_cluster13_style_others_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_others_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_scraping_cluster13_style_others_log.txt\"\n",
    "PROJECT_RANGE = range(1679, 1829)  # Original range #(1831, 2193)\n",
    "\n",
    "# Read the CSV file containing processed projects (with q_id column)\n",
    "processed_csv_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters/costs_phase_2_all_clusters_total.csv\"  # UPDATE THIS PATH\n",
    "processed_df = pd.read_csv(processed_csv_path)\n",
    "# Convert q_id values to numeric then to int for filtering\n",
    "processed_q_ids = pd.to_numeric(processed_df['q_id'], errors='coerce').dropna().astype(int).unique()\n",
    "projects_to_process = sorted([q_id for q_id in PROJECT_RANGE if q_id not in processed_q_ids])\n",
    "\n",
    "# ------------------- Global Tracking Variables -------------------\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "style_n_pdfs = []  # Not used in this version but kept for consistency\n",
    "\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "# ------------------- Helper Function for Logging -------------------\n",
    "def log_msg(msg, log_file):\n",
    "    \"\"\"Prints a message to both the log file and console.\"\"\"\n",
    "    print(msg, file=log_file)\n",
    "    print(msg)\n",
    "\n",
    "# ------------------- Other Helper Functions -------------------\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    elif value is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(value).replace('\\n', ' ').strip()\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Conditionally Assigned Network Upgrades\",\n",
    "        \"Local Off-Peak Network Upgrade\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b(?=\\d|\\W|$)\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\"\n",
    "    ]\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "    new_order = existing_desired + remaining\n",
    "    return df[new_order]\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        log_msg(f\"Found GPS coordinates: {gps_coords.groups()}\", log_file)\n",
    "        return gps_coords.groups()\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        log_msg(f\"Found project coordinates: {project_coords.groups()}\", log_file)\n",
    "        return project_coords.groups()\n",
    "    gps_coords_directional = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"\n",
    "        log_msg(f\"Found directional GPS coordinates: {(latitude, longitude)}\", log_file)\n",
    "        return (latitude, longitude)\n",
    "    log_msg(\"GPS coordinates not found.\", log_file)\n",
    "    return (None, None)\n",
    "\n",
    "# ------------------- Appendix PDF Check -------------------\n",
    "def is_appendix_pdf(pdf_path):\n",
    "    \"\"\"Returns True if the first page of the PDF contains 'Appendix A'.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "            first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "            return \"Appendix A\" in first_page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# ------------------- Base Data & Table 1 Extraction (Appendix A Only) -------------------\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    This function is intended to run only on the Appendix A PDF.\n",
    "    Now it searches for pages containing \"Table A.2\", \"Table B.2\" or \"Table C.2\"\n",
    "    and within the tables it looks for either \"Point of Interconnection\" or \"POI\".\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "    # Modified to match either \"Point of Interconnection\" or \"POI\"\n",
    "    poi_pattern = re.compile(r\"(Point\\s+of\\s+Interconnection|POI)\", re.IGNORECASE)\n",
    "    table_settings_list = [\n",
    "        {\"horizontal_strategy\": \"text\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 1},\n",
    "        {\"horizontal_strategy\": \"lines\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 2}\n",
    "    ]\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            table1_pages = []\n",
    "            # Modified regex: look for \"Table A.2\", \"Table B.2\" or \"Table C.2\"\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*[ABC]\\.2\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "            extraction_successful = False\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"Attempt {attempt} with settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1}\", file=log_file)\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty.\", file=log_file)\n",
    "                            continue\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"Found POI: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            print(f\"POI label found but adjacent value empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            poi_value_parts = []\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                            if poi_value_parts:\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"Concatenated POI: '{point_of_interconnection}'\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break\n",
    "                                    else:\n",
    "                                        print(f\"POI label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                            if extraction_successful:\n",
    "                                break\n",
    "                        if extraction_successful:\n",
    "                            break\n",
    "                    if extraction_successful:\n",
    "                        break\n",
    "                if extraction_successful:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "    if not extraction_successful:\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            print(\"POI label found but no value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            print(\"POI not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "    return point_of_interconnection\n",
    "\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"\n",
    "    Extracts base data from the Appendix A PDF.\n",
    "    (This function is meant to run only on a PDF verified as an Appendix A PDF.)\n",
    "    \"\"\"\n",
    "    if not is_appendix_pdf(pdf_path):\n",
    "        log_msg(f\"Skipping base extraction because {pdf_path} is not an Appendix A PDF.\", log_file)\n",
    "        return pd.DataFrame()\n",
    "    log_msg(\"Extracting base data from Appendix A PDF...\", log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "        text = clean_string_cell(text)\n",
    "        #queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        #queue_id = queue_id.group(1) if queue_id else str(project_id)\n",
    "        queue_id = str(project_id)\n",
    "        log_msg(f\"Extracted Queue ID: {queue_id}\", log_file)\n",
    "        #clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        #if '13' in clusters:\n",
    "        #    cluster_number = '13'\n",
    "        #elif clusters:\n",
    "        #    cluster_number = max(clusters, key=lambda x: int(x))\n",
    "        #else:\n",
    "        #   cluster_number = '13'\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = '13'    \n",
    "        log_msg(f\"Extracted Cluster Number: {cluster_number}\", log_file)\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        log_msg(f\"Extracted Deliverability Status: {deliverability_status}\", log_file)\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        log_msg(f\"Extracted Capacity: {capacity}\", log_file)\n",
    "        poi_value = extract_table1(pdf_path, log_file)\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [poi_value]\n",
    "        }\n",
    "        log_msg(\"Base data extracted:\", log_file)\n",
    "        log_msg(str(base_data), log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "    except Exception as e:\n",
    "        log_msg(f\"Error extracting base data from {pdf_path}: {e}\", log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------------- Attachment 2 Processing & Merging -------------------\n",
    "def is_attachment2_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Returns True if the first page of the PDF contains \"Attachment 2\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "            first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "            return \"Attachment 2\" in first_page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def is_addendum_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Returns True if the first page of the PDF contains \"Addendum\" or \"Revision\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "            first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "            return (\"Addendum\" in first_page_text) or (\"Revision\" in first_page_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    \"\"\"\n",
    "    Appends a suffix to duplicate headers to make them unique.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def extract_first_table(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Iterates over all pages and returns the first non-empty table found as a DataFrame,\n",
    "    treating the first row as the column headers and ensuring unique header names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    if table and any(any(cell is not None and cell.strip() for cell in row) for row in table):\n",
    "                        df = pd.DataFrame(table)\n",
    "                        if not df.empty:\n",
    "                            # Use the first row as headers\n",
    "                            headers = df.iloc[0].tolist()\n",
    "                            headers = make_unique_headers(headers)\n",
    "                            df.columns = headers\n",
    "                            df = df[1:].reset_index(drop=True)\n",
    "                            # Remove any duplicate column names (safeguard)\n",
    "                            df = df.loc[:, ~df.columns.duplicated()]\n",
    "                            return df\n",
    "    except Exception as e:\n",
    "        log_msg(f\"Error extracting table from {pdf_path}: {e}\",log_file)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "'''\n",
    "\n",
    "def extract_first_table(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the *first* non-empty table from the entire PDF (just like before).\n",
    "    Then, on the *same page* where that first table was found, searches for another\n",
    "    table that contains the phrase \"Other Potential\" in any cell. If found, merges\n",
    "    that second table with the first. Returns the resulting DataFrame.\n",
    "\n",
    "    NOTE:\n",
    "      - Currently, this merges the two tables *row-wise* (one below the other).\n",
    "      - If you prefer side-by-side merging (assuming same number of rows), replace\n",
    "        the pd.concat(..., axis=0) with pd.concat(..., axis=1).\n",
    "      - If you want to handle more complex scenarios (e.g., multiple \"Other Potential\"\n",
    "        tables or table on a subsequent page), you will need to expand the logic.\n",
    "    \"\"\"\n",
    "    import pdfplumber\n",
    "    import pandas as pd\n",
    "    \n",
    "    first_table_df = pd.DataFrame()\n",
    "    found_page_index = None\n",
    "\n",
    "    try:\n",
    "        # 1) Find the *first* non-empty table in the PDF\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_index, page in enumerate(pdf.pages):\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    if table and any(any(cell and cell.strip() for cell in row) for row in table):\n",
    "                        # Convert list-of-lists to a DataFrame\n",
    "                        df = pd.DataFrame(table)\n",
    "                        if not df.empty:\n",
    "                            # Use the first row as headers\n",
    "\n",
    "                            headers = df.iloc[0].tolist()\n",
    "                            headers = make_unique_headers(headers)\n",
    "                            df.columns = headers\n",
    "                            df = df[1:].reset_index(drop=True)\n",
    "                            # Remove any duplicate column names (safeguard)\n",
    "                            df = df.loc[:, ~df.columns.duplicated()]\n",
    "                             \n",
    "                             \n",
    "                            \n",
    "                            first_table_df = df\n",
    "                            found_page_index = page_index\n",
    "                            break  # Stop looking at more tables on this page\n",
    "                if found_page_index is not None:\n",
    "                    # We found our first table; stop searching further pages\n",
    "                    break\n",
    "\n",
    "            # If no table was found at all, just return empty DataFrame\n",
    "            if first_table_df.empty:\n",
    "                return first_table_df\n",
    "\n",
    "            # 2) On that *same page*, look for a table containing \"Other Potential\"\n",
    "            other_table_df = pd.DataFrame()\n",
    "            if found_page_index is not None:\n",
    "                page = pdf.pages[found_page_index]\n",
    "                tables = page.extract_tables()\n",
    "                \n",
    "                for table in tables:\n",
    "                    if table and any(any(cell and cell.strip() for cell in row) for row in table):\n",
    "                        # Check if \"Other Potential\" appears in any cell\n",
    "                        found_phrase = False\n",
    "                        for row in table:\n",
    "                            for cell in row:\n",
    "                                if cell and \"other potential\" in cell.lower():\n",
    "                                    found_phrase = True\n",
    "                                    break\n",
    "                            if found_phrase:\n",
    "                                break\n",
    "\n",
    "                        # If found, convert to DataFrame\n",
    "                        if found_phrase:\n",
    "                            df = pd.DataFrame(table)\n",
    "                            if not df.empty:\n",
    "                               \n",
    "                                headers = df.iloc[0].tolist()\n",
    "                                headers = make_unique_headers(headers)\n",
    "                                df.columns = headers\n",
    "                                df = df[1:].reset_index(drop=True)\n",
    "                                # Remove any duplicate column names (safeguard)\n",
    "                                df = df.loc[:, ~df.columns.duplicated()]\n",
    "                                other_table_df = df\n",
    "                                break  # Stop at the first \"Other Potential\" table\n",
    "\n",
    "            # 3) Merge the two tables if the second one was found\n",
    "            if other_table_df.empty:\n",
    "                # No \"Other Potential\" table found; return only the first table\n",
    "                return first_table_df\n",
    "            else:\n",
    "                # Example: Merge *row-wise* (append below). \n",
    "                # If you prefer side-by-side (and row counts match), use axis=1.\n",
    "                # ---- Merge duplicate columns before final concatenation ----\n",
    "                common_cols = set(first_table_df.columns).intersection(other_table_df.columns)\n",
    "                for col in common_cols:\n",
    "                    # Combine (fill missing in first_table_df with data from other_table_df)\n",
    "                    first_table_df[col] = first_table_df[col].combine_first(other_table_df[col])\n",
    "                    # Remove the duplicate column from the second table\n",
    "                    other_table_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "                merged_df = pd.concat([first_table_df, other_table_df], ignore_index=True)\n",
    "                return merged_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting table from {pdf_path}: {e}\", file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def extract_first_table(pdf_path,log_file):\n",
    "    \"\"\"\n",
    "    Iterates over all pages and returns the first non-empty table found as a DataFrame.\n",
    "    It first extracts the table normally (using the first row as header).\n",
    "    Then it scans the raw table rows for a row that, after replacing newline characters with a space,\n",
    "    has a contiguous block of cells (starting at index 1) exactly equal to:\n",
    "        [\"Costs per Category w/o ITCC (A)\",\n",
    "         \"One Time Costs (B)\",\n",
    "         \"Total Costs w/o ITCC (C=A+B)\",\n",
    "         \"Total Escalated Costs w/o ITCC\",\n",
    "         \"Estimated Time for Licensing, Permitting, & Construction (Months)\",\n",
    "         \"Maximum Escalation Duration (Months)\"]\n",
    "    If found, it discards all rows above that row, uses that row as the header (taking columns 1 to 6),\n",
    "    and returns the resulting DataFrame. Otherwise, it returns the table as originally extracted.\n",
    "    \"\"\"\n",
    "    import pdfplumber, pandas as pd\n",
    "\n",
    "    # Define the expected header row exactly as you want it.\n",
    "    expected_header = [\n",
    "        \"Costs per Category w/o ITCC (A)\",\n",
    "        \"One Time Costs (B)\",\n",
    "        \"Total Costs w/o ITCC (C=A+B)\",\n",
    "        \"Total Escalated Costs w/o ITCC\",\n",
    "        \"Estimated Time for Licensing, Permitting, & Construction (Months)\",\n",
    "        \"Maximum Escalation Duration (Months)\"\n",
    "    ]\n",
    "    \n",
    "    # First, extract the raw table from the PDF.\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            raw_table = None\n",
    "            for page in pdf.pages:\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    if table and any(any(cell is not None and cell.strip() for cell in row) for row in table):\n",
    "                        raw_table = table\n",
    "                        break\n",
    "                if raw_table is not None:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting table from {pdf_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if raw_table is None:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Build a DataFrame from the raw table (without assigning header)\n",
    "    df_raw = pd.DataFrame(raw_table)\n",
    "    if df_raw.empty or len(df_raw) < 2:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Also build a fallback DataFrame using the first row as header\n",
    "    df_fallback = df_raw.copy()\n",
    "    fallback_header = df_fallback.iloc[0].tolist()\n",
    "    df_fallback.columns = fallback_header\n",
    "    df_fallback = df_fallback.iloc[1:].reset_index(drop=True)\n",
    "    \n",
    "    # Now, scan each row of the raw table to see if it contains the expected header.\n",
    "    # We replace newline characters with a space for the comparison.\n",
    "    special_idx = None\n",
    "    for i in range(len(df_raw)):\n",
    "        row_vals = df_raw.iloc[i].tolist()\n",
    "        cleaned = [cell.replace(\"\\n\", \" \") if cell is not None else \"\" for cell in row_vals]\n",
    "        # Check if the row has at least 1 + len(expected_header) cells\n",
    "        if len(cleaned) >= 1 + len(expected_header):\n",
    "            # Compare cells 1 to 1+len(expected_header)\n",
    "            if cleaned[1:1+len(expected_header)] == expected_header:\n",
    "                special_idx = i\n",
    "                break\n",
    "\n",
    "    if special_idx is not None:\n",
    "        # Found the special header row.\n",
    "        # Remove all rows above that row.\n",
    "        df_new = df_raw.iloc[special_idx:].reset_index(drop=True)\n",
    "        # Use columns 1 to 1+len(expected_header) from that row as header.\n",
    "        new_header = df_new.iloc[0].tolist()[1:1+len(expected_header)]\n",
    "        log_msg(f\"Special header found at row {special_idx}. New header: {new_header}\", log_file)\n",
    "        # Now, select only those columns.\n",
    "        df_new = df_new.iloc[:, 1:1+len(expected_header)]\n",
    "        # Drop the header row from data.\n",
    "        df_new = df_new.iloc[1:].reset_index(drop=True)\n",
    "        df_new.columns = new_header\n",
    "        return df_new\n",
    "    else:\n",
    "        log_msg(\"No special header row found. Using fallback header.\", log_file)\n",
    "        return df_fallback\n",
    "'''\n",
    "\n",
    "\n",
    "def update_base_data(existing_df, new_df):\n",
    "    \"\"\"\n",
    "    For each column in existing_df (assumed to be a single-row DataFrame),\n",
    "    if the value is missing (empty string, \"None\", or NA), update it with the corresponding\n",
    "    value from new_df (if provided and not missing).\n",
    "    Returns the updated DataFrame.\n",
    "    \"\"\"\n",
    "    for col in existing_df.columns:\n",
    "        existing_val = existing_df.at[0, col]\n",
    "        # Use pd.isna() to check for NA values.\n",
    "        if pd.isna(existing_val) or existing_val == \"\" or existing_val == \"None\":\n",
    "            if col in new_df.columns:\n",
    "                new_val = new_df.at[0, col]\n",
    "                if not (pd.isna(new_val) or new_val == \"\" or new_val == \"None\"):\n",
    "                    existing_df.at[0, col] = new_val\n",
    "    return existing_df\n",
    "\n",
    "\n",
    "\n",
    "def process_attachment2_for_project(project_id, log_file):\n",
    "    \"\"\"\n",
    "    For the given project:\n",
    "      1. Identify all original Appendix A PDFs (nonrevision) in the project's \"02_phase_2_study\" folder.\n",
    "      2. Extract base data from the first one.\n",
    "      3. If any base data column is missing a value, iterate through additional original Appendix A PDFs\n",
    "         (if available) and update the missing columns.\n",
    "      4. Then proceed to scrape Attachment 2 PDFs and merge the base data (duplicated for each row) with\n",
    "         the extracted table (using the first row as header).\n",
    "      5. Also handles addendum PDFs separately.\n",
    "    \"\"\"\n",
    "    global total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    project_folder = os.path.join(BASE_DIRECTORY, str(project_id), \"03_phase_2_study\")\n",
    "    if not os.path.exists(project_folder):\n",
    "        log_msg(f\"Project folder not found: {project_folder}\", log_file)\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # Gather all original (nonrevision) Appendix A PDFs\n",
    "    original_appendix_pdfs = []\n",
    "    addendum_appendix_pdf = None\n",
    "    for f in os.listdir(project_folder):\n",
    "        if not f.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_path = os.path.join(project_folder, f)\n",
    "        if is_appendix_pdf(pdf_path):\n",
    "            if is_addendum_pdf(pdf_path):\n",
    "                if not addendum_appendix_pdf:\n",
    "                    addendum_appendix_pdf = f\n",
    "            else:\n",
    "                original_appendix_pdfs.append(f)\n",
    "    \n",
    "    if not original_appendix_pdfs:\n",
    "        log_msg(f\"No original Appendix A PDF found for project {project_id}.\", log_file)\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Use the first original Appendix A PDF to extract base data\n",
    "    base_pdf = original_appendix_pdfs[0]\n",
    "    original_base_pdf_path = os.path.join(project_folder, base_pdf)\n",
    "    log_msg(f\"Scraped base data from original Appendix A PDF: {original_base_pdf_path}\", log_file)\n",
    "    base_data_df = extract_base_data(original_base_pdf_path, project_id, log_file)\n",
    "    \n",
    "    # Check for missing values in base_data_df (assumed single-row DataFrame)\n",
    "    missing = [col for col in base_data_df.columns if pd.isna(base_data_df.at[0, col]) or base_data_df.at[0, col] in [\"\", \"None\"]]\n",
    "    if missing:\n",
    "        log_msg(f\"Missing base data columns in first Appendix A PDF: {missing}\", log_file)\n",
    "        # Iterate over any additional original Appendix A PDFs to update missing values.\n",
    "        for other_pdf in original_appendix_pdfs[1:]:\n",
    "            other_pdf_path = os.path.join(project_folder, other_pdf)\n",
    "            log_msg(f\"Attempting to update base data from: {other_pdf_path}\", log_file)\n",
    "            new_base_df = extract_base_data(other_pdf_path, project_id, log_file)\n",
    "            base_data_df = update_base_data(base_data_df, new_base_df)\n",
    "            # Recalculate missing columns.\n",
    "            missing = [col for col in base_data_df.columns if pd.isna(base_data_df.at[0, col]) or base_data_df.at[0, col] in [\"\", \"None\"]]\n",
    "            if not missing:\n",
    "                break\n",
    "        if missing:\n",
    "            log_msg(f\"After update, still missing: {missing}\", log_file)\n",
    "        else:\n",
    "            log_msg(\"Successfully updated all missing base data.\", log_file)\n",
    "\n",
    "    # Process addendum base data similarly.\n",
    "    if addendum_appendix_pdf:\n",
    "        addendum_base_pdf_path = os.path.join(project_folder, addendum_appendix_pdf)\n",
    "        log_msg(f\"Scraped base data from addendum Appendix A PDF: {addendum_base_pdf_path}\", log_file)\n",
    "        addendum_base_data_df = extract_base_data(addendum_base_pdf_path, project_id, log_file)\n",
    "        if addendum_base_data_df.empty:\n",
    "            addendum_base_data_df = base_data_df.copy()\n",
    "    else:\n",
    "        addendum_base_data_df = base_data_df.copy()\n",
    "\n",
    "    # Now process Attachment 2 PDFs.\n",
    "    attachment_data_list = []      # Regular Attachment 2 PDFs\n",
    "    attachment_addendum_list = []  # Addendum Attachment 2 PDFs\n",
    "\n",
    "    for f in os.listdir(project_folder):\n",
    "        if not f.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_path = os.path.join(project_folder, f)\n",
    "        log_msg(f\"Accessing PDF: {pdf_path}\", log_file)\n",
    "        total_pdfs_accessed += 1\n",
    "        if is_attachment2_pdf(pdf_path):\n",
    "            log_msg(f\"Scraped this Attachment 2 PDF: {pdf_path}\", log_file)\n",
    "            table_df = extract_first_table(pdf_path,log_file)\n",
    "            if table_df.empty:\n",
    "                log_msg(f\"--> No table found in {pdf_path}. Skipping.\", log_file)\n",
    "                skipped_pdfs.append(f)\n",
    "                total_pdfs_skipped += 1\n",
    "                continue\n",
    "\n",
    "            # (If needed, rename common columns; if not, comment this block out)\n",
    "            common_cols = set(base_data_df.columns) & set(table_df.columns)\n",
    "            for col in common_cols:\n",
    "                table_df.rename(columns={col: f\"{col}_table\"}, inplace=True)\n",
    "\n",
    "            if is_addendum_pdf(pdf_path):\n",
    "                log_msg(f\"--> {pdf_path} flagged as addendum.\", log_file)\n",
    "                base_df = addendum_base_data_df\n",
    "                addendum_pdfs.append(f)\n",
    "            else:\n",
    "                base_df = base_data_df\n",
    "                original_pdfs.append(f)\n",
    "\n",
    "            # Ensure unique column names\n",
    "            base_df = base_df.loc[:, ~base_df.columns.duplicated()]\n",
    "            table_df = table_df.loc[:, ~table_df.columns.duplicated()]\n",
    "\n",
    "            # Duplicate base data for each row of the table and merge side-by-side.\n",
    "            repeated_base = pd.concat([base_df] * len(table_df), ignore_index=True)\n",
    "            merged_df = pd.concat([repeated_base, table_df], axis=1)\n",
    "            if is_addendum_pdf(pdf_path):\n",
    "                attachment_addendum_list.append(merged_df)\n",
    "            else:\n",
    "                attachment_data_list.append(merged_df)\n",
    "            scraped_pdfs.append(f)\n",
    "            total_pdfs_scraped += 1\n",
    "        else:\n",
    "            log_msg(f\"--> {pdf_path} is not an Attachment 2 PDF. Skipping.\", log_file)\n",
    "            skipped_pdfs.append(f)\n",
    "            total_pdfs_skipped += 1\n",
    "\n",
    "    if not attachment_data_list and not attachment_addendum_list:\n",
    "        skipped_projects.add(project_id)\n",
    "    else:\n",
    "        scraped_projects.add(project_id)\n",
    "\n",
    "    project_attachment_df = pd.concat(attachment_data_list, ignore_index=True) if attachment_data_list else pd.DataFrame()\n",
    "    project_attachment_addendum_df = pd.concat(attachment_addendum_list, ignore_index=True) if attachment_addendum_list else pd.DataFrame()\n",
    "    return project_attachment_df, project_attachment_addendum_df\n",
    "\n",
    "\n",
    "# ------------------- CSV Saving & Summary Functions -------------------\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "    df = df.applymap(clean_string_cell)\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "# ------------------- Main Processing Function -------------------\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"\n",
    "    Processes all projects in ascending order (filtered via projects_to_process).\n",
    "    For each project, it:\n",
    "      - Checks for the project folder.\n",
    "      - Processes Attachment 2 PDFs by merging base data (from Appendix A PDFs) with\n",
    "        scraped table data (only the first nonempty table).\n",
    "      - Aggregates results across projects.\n",
    "    After processing, it saves the combined results to CSV files and prints a summary.\n",
    "    \"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    all_attachment_data = []\n",
    "    all_attachment_addendum_data = []\n",
    "\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        log_msg(f\"Projects to process: {projects_to_process}\", log_file)\n",
    "        for project_id in projects_to_process:\n",
    "            project_folder = os.path.join(BASE_DIRECTORY, str(project_id))\n",
    "            if not os.path.exists(project_folder):\n",
    "                missing_projects.add(project_id)\n",
    "                log_msg(f\"Project folder not found: {project_folder}\", log_file)\n",
    "                continue\n",
    "            log_msg(f\"\\n--- Processing project {project_id} ---\", log_file)\n",
    "            proj_attach_df, proj_attach_add_df = process_attachment2_for_project(project_id, log_file)\n",
    "            if not proj_attach_df.empty:\n",
    "                all_attachment_data.append(proj_attach_df)\n",
    "            if not proj_attach_add_df.empty:\n",
    "                all_attachment_addendum_data.append(proj_attach_add_df)\n",
    "        \n",
    "        if all_attachment_data:\n",
    "            core_originals = pd.concat(all_attachment_data, ignore_index=True)\n",
    "            save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "        else:\n",
    "            log_msg(\"\\nNo Attachment 2 data processed for regular PDFs.\", log_file)\n",
    "        \n",
    "        if all_attachment_addendum_data:\n",
    "            core_addendums = pd.concat(all_attachment_addendum_data, ignore_index=True)\n",
    "            save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "        else:\n",
    "            log_msg(\"\\nNo Attachment 2 data processed for addendum PDFs.\", log_file)\n",
    "\n",
    "        total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "        log_msg(\"\\n=== Scraping Summary ===\", log_file)\n",
    "        log_msg(f\"Total Projects Processed: {total_projects_processed}\", log_file)\n",
    "        log_msg(f\"Total Projects Scraped: {len(scraped_projects)}\", log_file)\n",
    "        log_msg(f\"Total Projects Skipped: {len(skipped_projects)}\", log_file)\n",
    "        log_msg(f\"Total Projects Missing: {len(missing_projects)}\", log_file)\n",
    "        log_msg(f\"Total PDFs Accessed: {total_pdfs_accessed}\", log_file)\n",
    "        log_msg(f\"Total PDFs Scraped: {total_pdfs_scraped}\", log_file)\n",
    "        log_msg(f\"Total PDFs Skipped: {total_pdfs_skipped}\", log_file)\n",
    "        log_msg(\"\\nList of Scraped Projects: \" + str(sorted(scraped_projects)), log_file)\n",
    "        log_msg(\"\\nList of Skipped Projects: \" + str(sorted(skipped_projects)), log_file)\n",
    "        log_msg(\"\\nList of Missing Projects: \" + str(sorted(missing_projects)), log_file)\n",
    "        log_msg(\"\\nList of Scraped PDFs: \" + str(scraped_pdfs), log_file)\n",
    "        log_msg(\"\\nList of Skipped PDFs: \" + str(skipped_pdfs), log_file)\n",
    "        log_msg(\"\\nList of Addendum PDFs: \" + str(addendum_pdfs), log_file)\n",
    "        log_msg(\"\\nList of Original PDFs: \" + str(original_pdfs), log_file)\n",
    "        log_msg(\"\\nList of Style N PDFs (Skipped due to 'Network Upgrade Type'): \" + str(style_n_pdfs), log_file)\n",
    "        log_msg(\"\\nTotal Number of Style N PDFs: \" + str(len(style_n_pdfs)), log_file)\n",
    "        log_msg(\"\\nNumber of Original PDFs Scraped: \" + str(len([pdf for pdf in scraped_pdfs if pdf in original_pdfs])), log_file)\n",
    "        log_msg(\"Number of Addendum PDFs Scraped: \" + str(len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs])), log_file)\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "def main():\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote shifted CSV to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_others_originals.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def shift_block_right(\n",
    "    df: pd.DataFrame,\n",
    "    qid_list: list,\n",
    "    start_col: str,\n",
    "    end_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For rows where df['q_id'] is in qid_list, shift the values in the\n",
    "    contiguous column block [start_col ... end_col] one position to the right.\n",
    "    \"\"\"\n",
    "    # locate column indices\n",
    "    cols = list(df.columns)\n",
    "    try:\n",
    "        i0 = cols.index(start_col)\n",
    "        i1 = cols.index(end_col)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Could not find column: {e}\")\n",
    "\n",
    "    block = cols[i0:i1+1]\n",
    "\n",
    "    # mask of rows to shift\n",
    "    mask = df['q_id'].isin(qid_list)\n",
    "\n",
    "    # perform the shift for those rows only\n",
    "    # df.loc[mask, block] is a sub-DataFrame: shift its values along axis=1\n",
    "    df_shifted = df.copy()\n",
    "    df_shifted.loc[mask, block] = (\n",
    "        df.loc[mask, block]\n",
    "          .shift(periods=1, axis=1)   # move everything right by 1\n",
    "          .values                     # ensure assignment aligns\n",
    "    )\n",
    "    return df_shifted\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_CSV  = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/rawdata_cluster13_style_others_originals.csv\"    # path to your input file\n",
    "    OUTPUT_CSV = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_others_originals.csv\"   # path for the shifted result\n",
    "\n",
    "    # define the q_ids you want to adjust\n",
    "    target_qids = [1758, 1764, 1766, 1768, 1774, 1775, 1779, 1790, 1786, 1799, 1800, 1801,]   #  replace with your list\n",
    "\n",
    "    # load\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "    df['q_id'] = df['q_id'].astype('Int64')\n",
    "    df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "    def clean_column_headers(headers):\n",
    "        \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "        cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "        for header in headers:  # Iterate over each header in the input.\n",
    "            if header is None:\n",
    "                header = \"\"  # If the header is None, set it to an empty string.\n",
    "            elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "                #header = header.lower()  # Convert the header to lowercase.\n",
    "                header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "                #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "                header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "                header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "            cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "        return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "\n",
    "\n",
    "    df.columns = clean_column_headers(df.columns)\n",
    "\n",
    "    def convert_to_snake_case(column_name):\n",
    "        column_name = column_name.strip().lower()\n",
    "        column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "        column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "        return column_name\n",
    "\n",
    "    def clean_string_cell(value):\n",
    "        if isinstance(value, str):\n",
    "            value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "            value = value.replace('\\n', ' ').strip()\n",
    "        return value\n",
    "\n",
    "    df = df.map(clean_string_cell)\n",
    "    df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "    # shift the block from 'Unnamed_8' through 'None_6'\n",
    "    df2 = shift_block_right(\n",
    "        df,\n",
    "        qid_list   = target_qids,\n",
    "        start_col  = 'unnamed_7',\n",
    "        end_col    = 'none_6'\n",
    "    )\n",
    "\n",
    "    # save out\n",
    "    df2.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Wrote shifted CSV to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized and Total Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Originals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### had to remove the data of 1678 to match the rest of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: ['q_id', 'cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection', 'unnamed_7', 'none_2', 'none_3', 'none_4', 'none_5', 'none_6', 'none_7', 'none_8', 'unnamed_15', 'other_potential_cost', 'total_allocated_costs_constant_2020_dollar_in_1000s', 'total_escalated_costs_in_1000s', 'upgrade_duration_months', 'maximum_escalation_duration_months', 'project_q1758tot1005_updated_as_of_10282021_eastern', 'project_q1764tot976_updated_as_of_10282021_eastern', 'project_q1766tot996_updated_as_of_10282021_metro', 'project_q1768tot1008_updated_as_of_10282021_metro', 'project_q1774tot981_updated_as_of_10282021_north_of_lugo', 'project_q1775tot989_updated_as_of_10282021_north_of_lugo', 'project_q1779tot966_updated_as_of_10282021_northern', 'project_q1790tot987_updated_as_of_10282021_northern', 'project_q1798tot972_updated_as_of_10282021_east_of_pisgah', 'cost_category_notes_1a_to_1f', 'total_estimated_costs_x_1000_constant_dollar_2020', 'total_estimated_costs_x_1000_escalated_constant_dollars_od_year', 'estimated_time_to_construct_months_note_1g', 'od_dollar_escalation_duration_months_note1g', 'project_q1799tot979_updated_as_of_10282021_east_of_pisgah', 'project_q1800tot984_updated_as_of_10282021_east_of_pisgah', 'project_q1801tot971_updated_as_of_10282021_east_of_pisgah']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_others_originals.csv\", dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "\n",
    "\n",
    "df.columns = clean_column_headers(df.columns)\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "print(\"After cleaning:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_2_cluster_14_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_14_total.csv'.\n",
      "['PTO_IF' 'RNU' 'CANU' 'Distribution Upgrades' 'LDNU']\n",
      "[1757 1758 1759 1761 1763 1764 1766 1768 1774 1775 1776 1779 1782 1784\n",
      " 1789 1790 1791 1792 1795 1796 1798 1799 1800 1801]\n",
      "[13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/1011195534.py:214: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/1011195534.py:214: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/1011195534.py:659: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_others_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "#df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "df.columns = clean_column_headers(df.columns)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#STEP 2: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def move_dollar_values(df, source_column, target_column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in `source_column` starts with a '$',\n",
    "    move that value to `target_column` and clear the value in the source column.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      source_column (str): The column to check for values starting with '$'.\n",
    "      target_column (str): The column to move the values into.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure target_column exists; if not, create it filled with empty strings.\n",
    "    if target_column not in df.columns:\n",
    "        df[target_column] = \"\"\n",
    "    \n",
    "    # Create a boolean mask for rows where the source column starts with '$'\n",
    "    mask = df[source_column].astype(str).str.startswith('$', na=False)\n",
    "    \n",
    "    # Move the values: assign the source values to the target column where the mask is True.\n",
    "    df.loc[mask, target_column] = df.loc[mask, source_column]\n",
    "    \n",
    "    # Clear the source column values for those rows (set to empty string)\n",
    "    df.loc[mask, source_column] = \"\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Move values from 'unnamed_8' to a new column 'moved_value'\n",
    "#df = move_dollar_values(df, 'none_5', 'total_estimated_costs_x_1000_escalated_constant_dollars_od_year')\n",
    "\n",
    "\n",
    "#df = move_dollar_values(df, 'none_3','total_estimated_costs_x_1000_constant_dollar_2020')\n",
    "\n",
    "def remove_dollar_values_and_fill_nan(df, column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in the specified column starts with '$',\n",
    "    set that value to NaN. Also, replace any empty strings in that column with NaN.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      column (str): The column to check and clean.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the column is treated as string\n",
    "    df[column] = df[column].astype(str)\n",
    "    \n",
    "    # Set values starting with '$' to NaN\n",
    "    mask = df[column].str.startswith('$', na=False)\n",
    "    df.loc[mask, column] = np.nan\n",
    "    \n",
    "    # Replace any remaining empty strings with NaN\n",
    "    df[column] = df[column].replace(\"\", np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = remove_dollar_values_and_fill_nan(df, 'unnamed_7')\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def merge_columns(df):\n",
    "\n",
    "    merge_columns_dict = {\n",
    "\n",
    " \n",
    "\n",
    "        \"one_time_costs_b\": [\n",
    "            \"none_2\",\n",
    "        ],\n",
    "\n",
    " \n",
    "    \n",
    "        \"type_of_upgrade\": [\n",
    "            \"cost_ca_tegory\",\n",
    "            \"cost_category\",\n",
    "            \"cost_category_notes_1a_to_1f\",\n",
    "           \n",
    "            'other_potential_cost', \n",
    "            'other_potential_network_cost',\n",
    "           'project_q1678tot964_updated_as_of_10282021_eastern',\n",
    "           \n",
    "           'project_q1758tot1005_updated_as_of_10282021_eastern', 'project_q1764tot976_updated_as_of_10282021_eastern', 'project_q1766tot996_updated_as_of_10282021_metro', 'project_q1768tot1008_updated_as_of_10282021_metro', 'project_q1774tot981_updated_as_of_10282021_north_of_lugo', 'project_q1775tot989_updated_as_of_10282021_north_of_lugo',\n",
    "             'project_q1779tot966_updated_as_of_10282021_northern', 'project_q1790tot987_updated_as_of_10282021_northern', 'project_q1798tot972_updated_as_of_10282021_east_of_pisgah', \n",
    "           'project_q1774tot981_updated_as_of_10282021_north_of_lugo', 'project_q1775tot989_updated_as_of_10282021_north_of_lugo', 'project_q1779tot966_updated_as_of_10282021_northern',\n",
    "             'project_q1790tot987_updated_as_of_10282021_northern', 'project_q1798tot972_updated_as_of_10282021_east_of_pisgah', 'cost_category_notes_1a_to_1f',\n",
    "              \n",
    "           'project_q1799tot979_updated_as_of_10282021_east_of_pisgah', 'project_q1800tot984_updated_as_of_10282021_east_of_pisgah', 'project_q1801tot971_updated_as_of_10282021_east_of_pisgah',\n",
    "           \n",
    "           \n",
    "             \"unnamed_7\",\n",
    "        ],\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"total_escalated_costs_wo_itcc\",\n",
    "            \n",
    "            \"total_estimated_costs_x_1000_escalated_constant_dollars_od_year\",\n",
    "            \"none_5\",\n",
    "            'total_escalated_costs_in_1000s',\n",
    "            'escalated_cost_in_1000s_note_8',\n",
    "            'total_estimated_costs_x_1000_escalated_constant_dollars_od_year',\n",
    "\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"none_4\",\n",
    "            'total_costs_wo_itcc_cab',\n",
    "       'total_allocated_costs_constant_2020_dollar_in_1000s',\n",
    "       'total_estimated_costs_x_1000_constant_dollar_2020',\n",
    "\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            'estimated_time_for_licensing_permitting_construction_months',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months2',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months_note_1g',\n",
    "            'estimated_time_to_construct_months_note_1g',\n",
    "            'estimated_time_to_construct_months',\n",
    "            \"none_6\",\n",
    "            'estimated_time_to_construct_months',\n",
    "            'upgrade_duration_months',\n",
    "            'estimated_time_to_construct_months_note_12', \n",
    "             'estimated_time_to_construct_months_note_1g', \n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    " \n",
    "        \"max_time_to_construct\": [\n",
    "            'maximum_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months_note1g',\n",
    "            'od_dollar_escalation_duration_months_note1g',\n",
    "            \"none_7\",\n",
    "            'maximum_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months_note1g', \n",
    "            \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "        # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"type_of_upgrade\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "df.drop([ 'none_8', 'unnamed_15' , \"none_3\",\n",
    "         'one_time_costs_b'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    " \n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/rawdata_cluster13_style_other.csv', index=False)\n",
    "\n",
    "df = df[~df.apply(lambda row: row.astype(str).isin([\"Total Escalated Costs w/o ITCC\", \"Constant 2020 Dollar in $1000s (Estimate)\", \"Eastern\"]).any(), axis=1)]\n",
    "df = df[~df.apply(lambda row: any(str(cell).startswith(\"Project #:\") for cell in row), axis=1)]\n",
    "df = df[df['type_of_upgrade'].notna() & (df['type_of_upgrade'].astype(str).str.strip() != \"\")]\n",
    "\n",
    " \n",
    "def process_upgrade_columns(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame df with a column \"type_of_upgrade\" that contains both group headers and upgrade data,\n",
    "    this function:\n",
    "      1. Inserts a new column \"upgrade\" as a duplicate of \"type_of_upgrade\" (placed immediately after it).\n",
    "      2. Renames rows in \"type_of_upgrade\" that contain specific phrases as follows:\n",
    "           - If it contains \"Interconnection Facilities\", rename to \"PTO_IF\" (or \"PTO_IF Total\" if \"Total\" is present)\n",
    "           - If it contains \"Reliability Network Upgrade\", rename to \"RNU\" (or \"RNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Local Delivery Network Upgrades\", rename to \"LDNU\" (or \"LDNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Area Deliverability Network Upgrades\", rename to \"ADNU\" (or \"ADNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Distribution Upgrades\", leave it as is.\n",
    "      3. Creates a temporary column that only holds the header values (from rows that were detected as header rows) and forward-fills it downward.\n",
    "         The forward fill stops (i.e. does not fill into a row) if that rows original \"type_of_upgrade\" contains any of the \"total\" indicators.\n",
    "      4. Replaces \"type_of_upgrade\" with the forward-filled header values.\n",
    "      5. Drops the rows that originally were header rows.\n",
    "      6. This deletes any rows which are either Total or Subtotal or Total cost assigned, the reason is some proejcts have multiple pdfs thus we rather calculate the total in the end.\n",
    "      \n",
    "    Returns the updated DataFrame.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 1. Create a new column \"upgrade\" immediately after \"type_of_upgrade\"\n",
    "    loc = df.columns.get_loc(\"type_of_upgrade\")\n",
    "    df.insert(loc+1, \"upgrade\", df[\"type_of_upgrade\"])\n",
    "    \n",
    "    # 2. Define a helper to rename header rows.\n",
    "    def rename_header(val):\n",
    "        # If the cell contains any of these phrases, rename accordingly.\n",
    "        # We'll check using the substring test (case-sensitive) per your request.\n",
    "        if \"Interconnection Facilities\" in val:\n",
    "            return \"PTO_IF\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Reliability Network Upgrade\" in val:\n",
    "            return \"RNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Local Delivery Network Upgrades\" in val:\n",
    "            return \"LDNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Area Deliverability Network Upgrades\" in val:\n",
    "            return \"ADNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Distribution Upgrades\" in val:\n",
    "            return val  # leave unchanged\n",
    "        elif \"Conditional Assigned Network Upgrades\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"CANU\" \n",
    "        elif \"Non-Allocated IRNU\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"Non-Allocated IRNU\"\n",
    "        elif \"Non-allocated IRNU\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"Non-Allocated IRNU\"\n",
    "        \n",
    "        elif \"IRNU-NA\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"IRNU-NA\"\n",
    "        \n",
    "\n",
    "\n",
    "        elif \"Area Delivery Network Upgrades\" in val:\n",
    "            return \"ADNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    # 3. Identify header rows. We consider a row to be a header row if its \"type_of_upgrade\" cell \n",
    "    # contains any of the target phrases.\n",
    "    target_phrases = [\n",
    "        \"Interconnection Facilities\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Local Delivery Network Upgrades\",\n",
    "        \"Area Deliverability Network Upgrades\",\n",
    "        \"Distribution Upgrades\",\n",
    "        \"Conditional Assigned Network Upgrades\",\n",
    "        \"Non-Allocated IRNU\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Non-allocated IRNU\",\n",
    "        \"IRNU-NA\",\n",
    "\n",
    "    ]\n",
    "    # Create a boolean mask for header rows.\n",
    "    header_mask = df[\"type_of_upgrade\"].apply(lambda x: any(phrase in x for phrase in target_phrases))\n",
    "    \n",
    "    # Apply renaming to the header rows.\n",
    "    df.loc[header_mask, \"type_of_upgrade\"] = df.loc[header_mask, \"type_of_upgrade\"].apply(rename_header)\n",
    "    \n",
    "    # 4. Create a temporary column 'header_temp' that holds only the header rows, then forward fill it.\n",
    "    df[\"header_temp\"] = df[\"type_of_upgrade\"].where(header_mask)\n",
    "    df[\"header_temp\"] = df[\"header_temp\"].ffill()\n",
    "    \n",
    "    # We want to stop the forward fill if we encounter a row that indicates totals.\n",
    "    # Define a simple function that returns True if a cell contains \"Total\" or \"Subtotal\" or \"Total cost assigned\".\n",
    "    def is_total_indicator(val):\n",
    "        return (\"Total\" in val) or (\"Subtotal\" in val) or (\"Total cost assigned\" in val)\n",
    "    \n",
    "    # For rows that themselves are total indicators in the \"upgrade\" column, do not forward-fill (set header_temp to NaN)\n",
    "    df.loc[df[\"upgrade\"].apply(lambda x: is_total_indicator(x)), \"header_temp\"] = None\n",
    "    \n",
    "    # Now, replace the \"type_of_upgrade\" column with the forward-filled header\n",
    "    df[\"type_of_upgrade\"] = df[\"header_temp\"]\n",
    "    df.drop(\"header_temp\", axis=1, inplace=True)\n",
    "    \n",
    "    # 5. Finally, drop the rows that were header rows (i.e. where header_mask is True)\n",
    "    df = df[~header_mask].reset_index(drop=True)\n",
    "    \n",
    "    # Also, drop any rows that have an empty \"type_of_upgrade\"\n",
    "    df = df[df[\"type_of_upgrade\"].notna() & (df[\"type_of_upgrade\"].str.strip() != \"\")]\n",
    "    \n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    "df = process_upgrade_columns(df)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    " \"Non-Allocated IRNU\": \"RNU\",\n",
    " \"Non-allocated IRNU\": \"RNU\",\n",
    "    \"IRNU-NA\": \"RNU\",\n",
    "    \"Total IRNU-NA\": \"Total RNU\",\n",
    " \"Total Non-allocated IRNU\": \"Total RNU\",\n",
    " \"Total Non-Allocated IRNU\": \"Total RNU\",\n",
    " }\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "   \n",
    "\n",
    "\n",
    " \n",
    "   \n",
    " \n",
    " \n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "# Step 7: Remove $ signs and convert to numeric\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "                # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = group[\n",
    "            (group['type_of_upgrade'] == upgrade) & (group['item'] == 'no')\n",
    "        ].shape[0] > 0\n",
    "        \n",
    "        if total_exists:\n",
    "             \n",
    "            continue\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "        if not total_exists:\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate specified columns from the existing row\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns (single row, so it remains the same)\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate the specified columns from the first row in the group\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 8: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    "'PTO_IF Total': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'RNU Total': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    " 'Total Distribution Upgrades': 'Distribution Upgrades',\n",
    " 'Distribution Upgrades Total': 'Distribution Upgrades',\n",
    " \"Non-allocated IRNU\": \"RNU\",\n",
    "    \"IRNU-NA\": \"RNU\",\n",
    "    \"Total IRNU-NA\": \"Total RNU\",\n",
    " \"Total Non-allocated IRNU\": \"Total RNU\",\n",
    "}\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    \n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "     \n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "     \n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    " \n",
    "     \n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    # Define the cost columns.\n",
    "    cost_cols = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "\n",
    "    # Build an aggregation dictionary:\n",
    "    # For columns not in grouping or cost_cols, we assume they are identical and take the first value.\n",
    "    agg_dict = {col: 'first' for col in totals_df.columns \n",
    "                if col not in ['q_id', 'type_of_upgrade'] + cost_cols}\n",
    "\n",
    "    # For the cost columns, we want to sum them.\n",
    "    agg_dict.update({col: 'sum' for col in cost_cols})\n",
    "\n",
    "    \n",
    "\n",
    "    # Group by both q_id and type_of_upgrade using the aggregation dictionary.\n",
    "    totals_df = totals_df.groupby(['q_id', 'type_of_upgrade'], as_index=False).agg(agg_dict)\n",
    "    totals_df = reorder_columns(totals_df)\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_14_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_14_total.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addendums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: ['q_id', 'cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection', 'allocated_cost_escalated_k_see_attachment_2_for_total_nu_cost_2021_k_project_allocation_allocated_cost_2021_k_od_year', 'unnamed_8', 'none_2', 'none_3', 'none_4', 'none_5', 'none_6', 'none_7', 'none_8', 'unnamed_16', 'other_potential_cost', 'total_allocated_costs_constant_2020_dollar_in_1000s', 'total_escalated_costs_in_1000s', 'upgrade_duration_months', 'maximum_escalation_duration_months']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_others_addendums.csv\", dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "\n",
    "\n",
    "df.columns = clean_column_headers(df.columns)\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "print(\"After cleaning:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_2_cluster_14_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_14_total.csv'.\n",
      "['PTO_IF' 'RNU' 'CANU']\n",
      "[1789 1792 1796 1798 1800 1801]\n",
      "[13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/1744960067.py:214: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/1744960067.py:214: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/1744960067.py:659: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_others_addendums.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "#df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "df.columns = clean_column_headers(df.columns)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#STEP 2: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def move_dollar_values(df, source_column, target_column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in `source_column` starts with a '$',\n",
    "    move that value to `target_column` and clear the value in the source column.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      source_column (str): The column to check for values starting with '$'.\n",
    "      target_column (str): The column to move the values into.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure target_column exists; if not, create it filled with empty strings.\n",
    "    if target_column not in df.columns:\n",
    "        df[target_column] = \"\"\n",
    "    \n",
    "    # Create a boolean mask for rows where the source column starts with '$'\n",
    "    mask = df[source_column].astype(str).str.startswith('$', na=False)\n",
    "    \n",
    "    # Move the values: assign the source values to the target column where the mask is True.\n",
    "    df.loc[mask, target_column] = df.loc[mask, source_column]\n",
    "    \n",
    "    # Clear the source column values for those rows (set to empty string)\n",
    "    df.loc[mask, source_column] = \"\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Move values from 'unnamed_8' to a new column 'moved_value'\n",
    "#df = move_dollar_values(df, 'none_5', 'total_estimated_costs_x_1000_escalated_constant_dollars_od_year')\n",
    "\n",
    "\n",
    "#df = move_dollar_values(df, 'none_3','total_estimated_costs_x_1000_constant_dollar_2020')\n",
    "\n",
    "def remove_dollar_values_and_fill_nan(df, column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in the specified column starts with '$',\n",
    "    set that value to NaN. Also, replace any empty strings in that column with NaN.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      column (str): The column to check and clean.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the column is treated as string\n",
    "    df[column] = df[column].astype(str)\n",
    "    \n",
    "    # Set values starting with '$' to NaN\n",
    "    mask = df[column].str.startswith('$', na=False)\n",
    "    df.loc[mask, column] = np.nan\n",
    "    \n",
    "    # Replace any remaining empty strings with NaN\n",
    "    df[column] = df[column].replace(\"\", np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#df = remove_dollar_values_and_fill_nan(df, 'unnamed_8')\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def merge_columns(df):\n",
    "\n",
    "    merge_columns_dict = {\n",
    "\n",
    " \n",
    "\n",
    "        \"one_time_costs_b\": [\n",
    "            \"none_2\",\n",
    "        ],\n",
    "\n",
    " \n",
    "    \n",
    "        \"type_of_upgrade\": [\n",
    "            \"cost_ca_tegory\",\n",
    "            \"cost_category\",\n",
    "            \"cost_category_notes_1a_to_1f\",\n",
    "           \n",
    "            'other_potential_cost', \n",
    "            'other_potential_network_cost',\n",
    "           'project_q1678tot964_updated_as_of_10282021_eastern',\n",
    "           \n",
    "           'project_q1758tot1005_updated_as_of_10282021_eastern', 'project_q1764tot976_updated_as_of_10282021_eastern', 'project_q1766tot996_updated_as_of_10282021_metro', 'project_q1768tot1008_updated_as_of_10282021_metro', 'project_q1774tot981_updated_as_of_10282021_north_of_lugo', 'project_q1775tot989_updated_as_of_10282021_north_of_lugo',\n",
    "             'project_q1779tot966_updated_as_of_10282021_northern', 'project_q1790tot987_updated_as_of_10282021_northern', 'project_q1798tot972_updated_as_of_10282021_east_of_pisgah', \n",
    "           'project_q1774tot981_updated_as_of_10282021_north_of_lugo', 'project_q1775tot989_updated_as_of_10282021_north_of_lugo', 'project_q1779tot966_updated_as_of_10282021_northern',\n",
    "             'project_q1790tot987_updated_as_of_10282021_northern', 'project_q1798tot972_updated_as_of_10282021_east_of_pisgah', 'cost_category_notes_1a_to_1f',\n",
    "              \n",
    "           'project_q1799tot979_updated_as_of_10282021_east_of_pisgah', 'project_q1800tot984_updated_as_of_10282021_east_of_pisgah', 'project_q1801tot971_updated_as_of_10282021_east_of_pisgah',\n",
    "           \n",
    "           \n",
    "             \"unnamed_8\",\n",
    "        ],\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"total_escalated_costs_wo_itcc\",\n",
    "            \n",
    "            \"total_estimated_costs_x_1000_escalated_constant_dollars_od_year\",\n",
    "            \"none_5\",\n",
    "            'total_escalated_costs_in_1000s',\n",
    "            'escalated_cost_in_1000s_note_8',\n",
    "            'total_estimated_costs_x_1000_escalated_constant_dollars_od_year',\n",
    "\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"none_4\",\n",
    "            'total_costs_wo_itcc_cab',\n",
    "       'total_allocated_costs_constant_2020_dollar_in_1000s',\n",
    "       'total_estimated_costs_x_1000_constant_dollar_2020',\n",
    "\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            'estimated_time_for_licensing_permitting_construction_months',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months2',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months_note_1g',\n",
    "            'estimated_time_to_construct_months_note_1g',\n",
    "            'estimated_time_to_construct_months',\n",
    "            \"none_6\",\n",
    "            'estimated_time_to_construct_months',\n",
    "            'upgrade_duration_months',\n",
    "            'estimated_time_to_construct_months_note_12', \n",
    "             'estimated_time_to_construct_months_note_1g', \n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    " \n",
    "        \"max_time_to_construct\": [\n",
    "            'maximum_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months_note1g',\n",
    "            'od_dollar_escalation_duration_months_note1g',\n",
    "            \"none_7\",\n",
    "            'maximum_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months_note1g', \n",
    "            \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "        # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"type_of_upgrade\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "df.drop([ 'none_8', 'unnamed_16' , \"none_3\", 'allocated_cost_escalated_k_see_attachment_2_for_total_nu_cost_2021_k_project_allocation_allocated_cost_2021_k_od_year', \n",
    "         'one_time_costs_b'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    " \n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/rawdata_cluster13_style_other.csv', index=False)\n",
    "\n",
    "df = df[~df.apply(lambda row: row.astype(str).isin([\"Total Escalated Costs w/o ITCC\", \"Constant 2020 Dollar in $1000s (Estimate)\", \"Eastern\"]).any(), axis=1)]\n",
    "df = df[~df.apply(lambda row: any(str(cell).startswith(\"Project #:\") for cell in row), axis=1)]\n",
    "df = df[df['type_of_upgrade'].notna() & (df['type_of_upgrade'].astype(str).str.strip() != \"\")]\n",
    "\n",
    " \n",
    "def process_upgrade_columns(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame df with a column \"type_of_upgrade\" that contains both group headers and upgrade data,\n",
    "    this function:\n",
    "      1. Inserts a new column \"upgrade\" as a duplicate of \"type_of_upgrade\" (placed immediately after it).\n",
    "      2. Renames rows in \"type_of_upgrade\" that contain specific phrases as follows:\n",
    "           - If it contains \"Interconnection Facilities\", rename to \"PTO_IF\" (or \"PTO_IF Total\" if \"Total\" is present)\n",
    "           - If it contains \"Reliability Network Upgrade\", rename to \"RNU\" (or \"RNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Local Delivery Network Upgrades\", rename to \"LDNU\" (or \"LDNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Area Deliverability Network Upgrades\", rename to \"ADNU\" (or \"ADNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Distribution Upgrades\", leave it as is.\n",
    "      3. Creates a temporary column that only holds the header values (from rows that were detected as header rows) and forward-fills it downward.\n",
    "         The forward fill stops (i.e. does not fill into a row) if that rows original \"type_of_upgrade\" contains any of the \"total\" indicators.\n",
    "      4. Replaces \"type_of_upgrade\" with the forward-filled header values.\n",
    "      5. Drops the rows that originally were header rows.\n",
    "      6. This deletes any rows which are either Total or Subtotal or Total cost assigned, the reason is some proejcts have multiple pdfs thus we rather calculate the total in the end.\n",
    "      \n",
    "    Returns the updated DataFrame.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 1. Create a new column \"upgrade\" immediately after \"type_of_upgrade\"\n",
    "    loc = df.columns.get_loc(\"type_of_upgrade\")\n",
    "    df.insert(loc+1, \"upgrade\", df[\"type_of_upgrade\"])\n",
    "    \n",
    "    # 2. Define a helper to rename header rows.\n",
    "    def rename_header(val):\n",
    "        # If the cell contains any of these phrases, rename accordingly.\n",
    "        # We'll check using the substring test (case-sensitive) per your request.\n",
    "        if \"Interconnection Facilities\" in val:\n",
    "            return \"PTO_IF\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Reliability Network Upgrade\" in val:\n",
    "            return \"RNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Local Delivery Network Upgrades\" in val:\n",
    "            return \"LDNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Area Deliverability Network Upgrades\" in val:\n",
    "            return \"ADNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Distribution Upgrades\" in val:\n",
    "            return val  # leave unchanged\n",
    "        elif \"Conditional Assigned Network Upgrades\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"CANU\" \n",
    "        elif \"Non-Allocated IRNU\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"Non-Allocated IRNU\"\n",
    "        elif \"Non-allocated IRNU\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"Non-Allocated IRNU\"\n",
    "        \n",
    "        elif \"IRNU-NA\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"IRNU-NA\"\n",
    "        \n",
    "\n",
    "\n",
    "        elif \"Area Delivery Network Upgrades\" in val:\n",
    "            return \"ADNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    # 3. Identify header rows. We consider a row to be a header row if its \"type_of_upgrade\" cell \n",
    "    # contains any of the target phrases.\n",
    "    target_phrases = [\n",
    "        \"Interconnection Facilities\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Local Delivery Network Upgrades\",\n",
    "        \"Area Deliverability Network Upgrades\",\n",
    "        \"Distribution Upgrades\",\n",
    "        \"Conditional Assigned Network Upgrades\",\n",
    "        \"Non-Allocated IRNU\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Non-allocated IRNU\",\n",
    "        \"IRNU-NA\",\n",
    "\n",
    "    ]\n",
    "    # Create a boolean mask for header rows.\n",
    "    header_mask = df[\"type_of_upgrade\"].apply(lambda x: any(phrase in x for phrase in target_phrases))\n",
    "    \n",
    "    # Apply renaming to the header rows.\n",
    "    df.loc[header_mask, \"type_of_upgrade\"] = df.loc[header_mask, \"type_of_upgrade\"].apply(rename_header)\n",
    "    \n",
    "    # 4. Create a temporary column 'header_temp' that holds only the header rows, then forward fill it.\n",
    "    df[\"header_temp\"] = df[\"type_of_upgrade\"].where(header_mask)\n",
    "    df[\"header_temp\"] = df[\"header_temp\"].ffill()\n",
    "    \n",
    "    # We want to stop the forward fill if we encounter a row that indicates totals.\n",
    "    # Define a simple function that returns True if a cell contains \"Total\" or \"Subtotal\" or \"Total cost assigned\".\n",
    "    def is_total_indicator(val):\n",
    "        return (\"Total\" in val) or (\"Subtotal\" in val) or (\"Total cost assigned\" in val)\n",
    "    \n",
    "    # For rows that themselves are total indicators in the \"upgrade\" column, do not forward-fill (set header_temp to NaN)\n",
    "    df.loc[df[\"upgrade\"].apply(lambda x: is_total_indicator(x)), \"header_temp\"] = None\n",
    "    \n",
    "    # Now, replace the \"type_of_upgrade\" column with the forward-filled header\n",
    "    df[\"type_of_upgrade\"] = df[\"header_temp\"]\n",
    "    df.drop(\"header_temp\", axis=1, inplace=True)\n",
    "    \n",
    "    # 5. Finally, drop the rows that were header rows (i.e. where header_mask is True)\n",
    "    df = df[~header_mask].reset_index(drop=True)\n",
    "    \n",
    "    # Also, drop any rows that have an empty \"type_of_upgrade\"\n",
    "    df = df[df[\"type_of_upgrade\"].notna() & (df[\"type_of_upgrade\"].str.strip() != \"\")]\n",
    "    \n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    "df = process_upgrade_columns(df)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    " \"Non-Allocated IRNU\": \"RNU\",\n",
    " \"Non-allocated IRNU\": \"RNU\",\n",
    "    \"IRNU-NA\": \"RNU\",\n",
    "    \"Total IRNU-NA\": \"Total RNU\",\n",
    " \"Total Non-allocated IRNU\": \"Total RNU\",\n",
    " \"Total Non-Allocated IRNU\": \"Total RNU\",\n",
    " }\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "   \n",
    "\n",
    "\n",
    " \n",
    "   \n",
    " \n",
    " \n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "# Step 7: Remove $ signs and convert to numeric\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "                # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = group[\n",
    "            (group['type_of_upgrade'] == upgrade) & (group['item'] == 'no')\n",
    "        ].shape[0] > 0\n",
    "        \n",
    "        if total_exists:\n",
    "             \n",
    "            continue\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "        if not total_exists:\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate specified columns from the existing row\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns (single row, so it remains the same)\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate the specified columns from the first row in the group\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 8: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    "'PTO_IF Total': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'RNU Total': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    " 'Total Distribution Upgrades': 'Distribution Upgrades',\n",
    " 'Distribution Upgrades Total': 'Distribution Upgrades',\n",
    " \"Non-allocated IRNU\": \"RNU\",\n",
    "    \"IRNU-NA\": \"RNU\",\n",
    "    \"Total IRNU-NA\": \"Total RNU\",\n",
    " \"Total Non-allocated IRNU\": \"Total RNU\",\n",
    "}\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    \n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "     \n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "     \n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    " \n",
    "     \n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_itemized_addendums.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    # Define the cost columns.\n",
    "    cost_cols = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "\n",
    "    # Build an aggregation dictionary:\n",
    "    # For columns not in grouping or cost_cols, we assume they are identical and take the first value.\n",
    "    agg_dict = {col: 'first' for col in totals_df.columns \n",
    "                if col not in ['q_id', 'type_of_upgrade'] + cost_cols}\n",
    "\n",
    "    # For the cost columns, we want to sum them.\n",
    "    agg_dict.update({col: 'sum' for col in cost_cols})\n",
    "\n",
    "    \n",
    "\n",
    "    # Group by both q_id and type_of_upgrade using the aggregation dictionary.\n",
    "    totals_df = totals_df.groupby(['q_id', 'type_of_upgrade'], as_index=False).agg(agg_dict)\n",
    "    totals_df = reorder_columns(totals_df)\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_total_addendums.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_14_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_14_total.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge - Conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing itemized: q_id=1789, type_of_upgrade=PTO_IF\n",
      "Length of addendum_rows: 12\n",
      "Length of original_rows: 13\n",
      "Processing itemized: q_id=1789, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 4\n",
      "Length of original_rows: 4\n",
      "Processing itemized: q_id=1789, type_of_upgrade=CANU\n",
      "Length of addendum_rows: 2\n",
      "Length of original_rows: 2\n",
      "Processing itemized: q_id=1792, type_of_upgrade=PTO_IF\n",
      "Length of addendum_rows: 13\n",
      "Length of original_rows: 14\n",
      "Processing itemized: q_id=1792, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 6\n",
      "Length of original_rows: 6\n",
      "Processing itemized: q_id=1792, type_of_upgrade=CANU\n",
      "Length of addendum_rows: 2\n",
      "Length of original_rows: 2\n",
      "Processing itemized: q_id=1796, type_of_upgrade=PTO_IF\n",
      "Length of addendum_rows: 13\n",
      "Length of original_rows: 14\n",
      "Processing itemized: q_id=1796, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 3\n",
      "Length of original_rows: 4\n",
      "Processing itemized: q_id=1798, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 1\n",
      "Length of original_rows: 9\n",
      "Processing itemized: q_id=1800, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 1\n",
      "Length of original_rows: 9\n",
      "Processing itemized: q_id=1801, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 1\n",
      "Length of original_rows: 9\n",
      "Processing total: q_id=1789, type_of_upgrade=CANU\n",
      "Processing total: q_id=1789, type_of_upgrade=PTO_IF\n",
      "Processing total: q_id=1789, type_of_upgrade=RNU\n",
      "Processing total: q_id=1792, type_of_upgrade=CANU\n",
      "Processing total: q_id=1792, type_of_upgrade=PTO_IF\n",
      "Processing total: q_id=1792, type_of_upgrade=RNU\n",
      "Processing total: q_id=1796, type_of_upgrade=CANU\n",
      "Processing total: q_id=1796, type_of_upgrade=PTO_IF\n",
      "Processing total: q_id=1796, type_of_upgrade=RNU\n",
      "Processing total: q_id=1798, type_of_upgrade=RNU\n",
      "Processing total: q_id=1800, type_of_upgrade=RNU\n",
      "Processing total: q_id=1801, type_of_upgrade=RNU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:127: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:203: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:208: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:203: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:208: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:203: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:208: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:203: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/3536515259.py:208: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Load a CSV file and ensure specific columns are treated as character, others as numeric.\n",
    "    \"\"\"\n",
    " # Get columns available in the dataset\n",
    "    available_columns = pd.read_csv(file_path, nrows=0).columns\n",
    "    \n",
    "    # Restrict to char_columns that are present in the dataset\n",
    "    char_columns_in_dataset = [col for col in char_columns if col in available_columns]\n",
    "    \n",
    "    # Load the dataset, treating char_columns_in_dataset as strings\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype={col: str for col in char_columns_in_dataset},\n",
    "        na_values=[],  # Disable automatic NaN interpretation\n",
    "        keep_default_na=False  # Prevent treating \"None\" as NaN\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert all other columns to numeric\n",
    "    #for col in df.columns:\n",
    "    #    if col not in char_columns:\n",
    "    #        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_data(df, file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Save a dataframe to a CSV file, ensuring specific columns are treated as character.\n",
    "    \"\"\"\n",
    "    for col in char_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def merge_with_addendums(itemized, itemized_addendums, total, total_addendums):\n",
    "    # Add an 'original' column to the datasets\n",
    "    itemized['original'] = \"yes\"\n",
    "    total['original'] = \"yes\"\n",
    "    \n",
    "    # Preserve the original row order\n",
    "    itemized['row_order'] = pd.to_numeric(itemized.index, errors=\"coerce\")\n",
    "    total['row_order'] = pd.to_numeric(total.index, errors=\"coerce\")\n",
    "    \n",
    "    # Ensure q_id is numeric for comparison\n",
    "    itemized['q_id'] = pd.to_numeric(itemized['q_id'], errors=\"coerce\")\n",
    "    itemized_addendums['q_id'] = pd.to_numeric(itemized_addendums['q_id'], errors=\"coerce\")\n",
    "    total['q_id'] = pd.to_numeric(total['q_id'], errors=\"coerce\")\n",
    "    total_addendums['q_id'] = pd.to_numeric(total_addendums['q_id'], errors=\"coerce\")\n",
    "    \n",
    "    # Columns for conditional replacement\n",
    "    conditional_columns = [\"req_deliverability\", \"latitude\", \"longitude\", \"capacity\", \"point_of_interconnection\"]\n",
    "    \n",
    "    # --- Process itemized data (unchanged) ---\n",
    "    updated_itemized_rows = []\n",
    "    for q_id in itemized_addendums['q_id'].unique():\n",
    "        for upgrade_type in itemized_addendums['type_of_upgrade'].unique():\n",
    "            addendum_rows = itemized_addendums[\n",
    "                (itemized_addendums['q_id'] == q_id) &\n",
    "                (itemized_addendums['type_of_upgrade'] == upgrade_type)\n",
    "            ]\n",
    "            if not addendum_rows.empty:\n",
    "                mask = (itemized['q_id'] == q_id) & (itemized['type_of_upgrade'] == upgrade_type)\n",
    "                original_rows = itemized[mask]\n",
    "                print(f\"Processing itemized: q_id={q_id}, type_of_upgrade={upgrade_type}\")\n",
    "                print(f\"Length of addendum_rows: {len(addendum_rows)}\")\n",
    "                print(f\"Length of original_rows: {len(original_rows)}\")\n",
    "                # For specified columns, replace if addendum values are non-empty\n",
    "                for col in conditional_columns:\n",
    "                    if col in addendum_rows.columns and col in original_rows.columns:\n",
    "                        addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
    "                        addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
    "                        addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
    "                # Align lengths\n",
    "                original_rows = original_rows.reset_index(drop=True)\n",
    "                addendum_rows = addendum_rows.reset_index(drop=True)\n",
    "                if len(addendum_rows) > len(original_rows):\n",
    "                    extra_rows = pd.DataFrame({col: pd.NA for col in original_rows.columns},\n",
    "                                              index=range(len(addendum_rows) - len(original_rows)))\n",
    "                    original_rows = pd.concat([original_rows, extra_rows], ignore_index=True)\n",
    "                elif len(addendum_rows) < len(original_rows):\n",
    "                    original_rows = original_rows.iloc[:len(addendum_rows)].reset_index(drop=True)\n",
    "                itemized.loc[mask, 'original'] = \"no\"\n",
    "                updated_itemized_rows.append(\n",
    "                    addendum_rows.assign(original=\"no\", row_order=original_rows['row_order'].values[:len(addendum_rows)])\n",
    "                )\n",
    "                itemized = itemized[~mask]\n",
    "    if updated_itemized_rows:\n",
    "        updated_itemized = pd.concat([itemized] + updated_itemized_rows, ignore_index=True)\n",
    "    else:\n",
    "        updated_itemized = itemized.copy()\n",
    "    updated_itemized[\"row_order\"] = pd.to_numeric(updated_itemized[\"row_order\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "    updated_itemized = updated_itemized.sort_values(by=\"row_order\").drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    \n",
    "    # --- Process total data ---\n",
    "    updated_total_rows = []\n",
    "    for q_id in total_addendums['q_id'].unique():\n",
    "        for upgrade_type in total_addendums['type_of_upgrade'].unique():\n",
    "            addendum_row = total_addendums[\n",
    "                (total_addendums['q_id'] == q_id) &\n",
    "                (total_addendums['type_of_upgrade'] == upgrade_type)\n",
    "            ]\n",
    "            if not addendum_row.empty:\n",
    "                mask = (total['q_id'] == q_id) & (total['type_of_upgrade'] == upgrade_type)\n",
    "                original_row = total[mask]\n",
    "                print(f\"Processing total: q_id={q_id}, type_of_upgrade={upgrade_type}\")\n",
    "                # If no matching original row exists, create a default row_order column\n",
    "                if original_row.empty:\n",
    "                    original_row = pd.DataFrame({'row_order': [pd.NA] * len(addendum_row)}, index=addendum_row.index)\n",
    "                else:\n",
    "                    original_row = original_row.reset_index(drop=True)\n",
    "                addendum_row = addendum_row.reset_index(drop=True)\n",
    "                if len(addendum_row) > len(original_row):\n",
    "                    extra_rows = pd.DataFrame({col: pd.NA for col in original_row.columns},\n",
    "                                              index=range(len(addendum_row) - len(original_row)))\n",
    "                    original_row = pd.concat([original_row, extra_rows], ignore_index=True)\n",
    "                elif len(addendum_row) < len(original_row):\n",
    "                    original_row = original_row.iloc[:len(addendum_row)].reset_index(drop=True)\n",
    "                for col in conditional_columns:\n",
    "                    if col in addendum_row.columns and col in original_row.columns:\n",
    "                        addendum_row[col] = addendum_row[col].replace(\"\", pd.NA)\n",
    "                        addendum_row[col] = addendum_row[col].combine_first(original_row[col].reset_index(drop=True))\n",
    "                        addendum_row[col] = addendum_row[col].fillna(\"\")\n",
    "                total.loc[mask, 'original'] = \"no\"\n",
    "                updated_total_rows.append(\n",
    "                    addendum_row.assign(original=\"no\", row_order=original_row['row_order'].values[:len(addendum_row)])\n",
    "                )\n",
    "                total = total[~mask]\n",
    "    if updated_total_rows:\n",
    "        updated_total = pd.concat([total] + updated_total_rows, ignore_index=True)\n",
    "    else:\n",
    "        updated_total = total.copy()\n",
    "    updated_total[\"row_order\"] = pd.to_numeric(updated_total[\"row_order\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "    updated_total = updated_total.sort_values(by=\"row_order\").drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Fill missing columns with zeros in the updated datasets\n",
    "    for col in set(itemized.columns) - set(updated_itemized.columns):\n",
    "        updated_itemized[col] = 0\n",
    "    for col in set(total.columns) - set(updated_total.columns):\n",
    "        updated_total[col] = 0\n",
    "\n",
    "    # Move the 'original' column to the last position\n",
    "    updated_itemized = updated_itemized[[col for col in updated_itemized.columns if col != 'original'] + ['original']]\n",
    "    updated_total = updated_total[[col for col in updated_total.columns if col != 'original'] + ['original']]\n",
    "    \n",
    "    if \"row_order\" in updated_itemized.columns:\n",
    "        updated_itemized = updated_itemized.drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    if \"row_order\" in updated_total.columns:\n",
    "        updated_total = updated_total.drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    \n",
    "    return updated_itemized, updated_total\n",
    "\n",
    "\n",
    "# Define the character columns\n",
    "char_columns = [\n",
    "    \"req_deliverability\", \"point_of_interconnection\", \"type_of_upgrade\",\n",
    "    \"upgrade\", \"description\", \"estimated_time_to_construct\", \"original\", \"item\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "itemized = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_itemized.csv\", char_columns)\n",
    "itemized_addendums = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_itemized_addendums.csv\", char_columns)\n",
    "total = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_total.csv\", char_columns)\n",
    "total_addendums = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_total_addendums.csv\", char_columns)\n",
    "\n",
    "\n",
    "updated_itemized, updated_total = merge_with_addendums(itemized, itemized_addendums, total, total_addendums)\n",
    "\n",
    "# Drop the specified columns from the updated datasets\n",
    "columns_to_drop = [ \"upgrade_classification\",\"estimated\", \"caiso_queue\", \"project_type\", \"dependent_system_upgrade\"]\n",
    "\n",
    "# For the itemized dataset\n",
    "updated_itemized = updated_itemized.drop(columns=[col for col in columns_to_drop if col in updated_itemized.columns], errors='ignore')\n",
    "\n",
    "# For the total dataset\n",
    "updated_total = updated_total.drop(columns=[col for col in columns_to_drop if col in updated_total.columns], errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "# List of columns to process with ffill and bfill\n",
    "columns_to_fill = [\"point_of_interconnection\", \"latitude\", \"longitude\", \"req_deliverability\", \"capacity\"]\n",
    "\n",
    "# Replace empty strings with NaN for the specified columns\n",
    "for col in columns_to_fill:\n",
    "    updated_itemized[col] = updated_itemized[col].replace('', np.nan)\n",
    "    updated_total[col] = updated_total[col].replace('', np.nan)\n",
    "\n",
    "# Sort by q_id while maintaining other column order (stable sorting)\n",
    "updated_itemized = updated_itemized.sort_values(by=[\"q_id\"], kind=\"stable\").reset_index(drop=True)\n",
    "updated_total = updated_total.sort_values(by=[\"q_id\"], kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "# Apply forward-fill and backward-fill for the specified columns within each q_id group\n",
    "for col in columns_to_fill:\n",
    "    updated_itemized[col] = (\n",
    "        updated_itemized.groupby(\"q_id\")[col]\n",
    "        .apply(lambda group: group.ffill().bfill())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    updated_total[col] = (\n",
    "        updated_total.groupby(\"q_id\")[col]\n",
    "        .apply(lambda group: group.ffill().bfill())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# Replace NaN back with empty strings for consistency\n",
    "for col in columns_to_fill:\n",
    "    updated_itemized[col] = updated_itemized[col].replace(np.nan, '')\n",
    "    updated_total[col] = updated_total[col].replace(np.nan, '')\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the updated datasets\n",
    "save_data(updated_itemized, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_others_itemized_updated.csv\", char_columns)\n",
    "save_data(updated_total, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_others_total_updated.csv\", char_columns)\n",
    "\n",
    "\n",
    "\n",
    "# Save the results\n",
    "save_data(updated_itemized, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_others_itemized_updated.csv\", char_columns)\n",
    "save_data(updated_total, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_others_total_updated.csv\", char_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Original and addendum - Complete replace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:107: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  .combine_first(orig[col])\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:191: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:191: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:191: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:191: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:193: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].replace(np.nan, '', inplace=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:191: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:191: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:191: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:191: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51478/390860820.py:193: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].replace(np.nan, '', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Load a CSV file and ensure specific columns are treated as character.\n",
    "    \"\"\"\n",
    "    available_columns = pd.read_csv(file_path, nrows=0).columns\n",
    "    char_cols = [c for c in char_columns if c in available_columns]\n",
    "    return pd.read_csv(\n",
    "        file_path,\n",
    "        dtype={c: str for c in char_cols},\n",
    "        na_values=[], \n",
    "        keep_default_na=False\n",
    "    )\n",
    "\n",
    "def save_data(df, file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Save a dataframe to CSV, forcing certain columns to string.\n",
    "    \"\"\"\n",
    "    for col in char_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def merge_with_addendums(itemized, itemized_addendums, total, total_addendums):\n",
    "    # mark originals & keep row order\n",
    "    itemized['original'] = \"yes\"\n",
    "    total['original']    = \"yes\"\n",
    "    itemized['row_order'] = itemized.index\n",
    "    total['row_order']    = total.index\n",
    "\n",
    "    # ensure numeric q_id\n",
    "    for df in (itemized, itemized_addendums, total, total_addendums):\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors=\"coerce\")\n",
    "\n",
    "    conditional_columns = [\n",
    "        \"req_deliverability\",\"latitude\",\"longitude\",\n",
    "        \"capacity\",\"point_of_interconnection\"\n",
    "    ]\n",
    "\n",
    "    # --- ITEMIZED: replace all rows by q_id ---\n",
    "    updated_itemized_rows = []\n",
    "    for q in itemized_addendums['q_id'].unique():\n",
    "        adds = itemized_addendums[itemized_addendums['q_id']==q].reset_index(drop=True)\n",
    "        orig = itemized[itemized['q_id']==q].reset_index(drop=True)\n",
    "\n",
    "        if not orig.empty:\n",
    "            # combine conditional columns\n",
    "            for col in conditional_columns:\n",
    "                if col in adds.columns and col in orig.columns:\n",
    "                    adds[col] = (\n",
    "                        adds[col].replace(\"\", pd.NA)\n",
    "                                .combine_first(orig[col])\n",
    "                                .fillna(\"\")\n",
    "                    )\n",
    "            ro = orig['row_order'].tolist()\n",
    "            # pad if addendum has more rows\n",
    "            if len(ro) < len(adds):\n",
    "                ro += [pd.NA] * (len(adds) - len(ro))\n",
    "        else:\n",
    "            ro = [pd.NA] * len(adds)\n",
    "\n",
    "        adds = adds.assign(original=\"no\", row_order=ro[:len(adds)])\n",
    "        # drop original q_id rows\n",
    "        itemized = itemized[itemized['q_id'] != q]\n",
    "        updated_itemized_rows.append(adds)\n",
    "\n",
    "    updated_itemized = pd.concat([itemized] + updated_itemized_rows, ignore_index=True) \\\n",
    "                        if updated_itemized_rows else itemized.copy()\n",
    "    updated_itemized['row_order'] = updated_itemized['row_order'].fillna(-1).astype(int)\n",
    "    updated_itemized = (\n",
    "        updated_itemized\n",
    "        .sort_values('row_order')\n",
    "        .drop(columns='row_order')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # --- TOTAL: per type_of_upgrade (unchanged logic) ---\n",
    "    updated_total_rows = []\n",
    "    for q in total_addendums['q_id'].unique():\n",
    "        for t in total_addendums['type_of_upgrade'].unique():\n",
    "            adds = total_addendums[\n",
    "                (total_addendums['q_id']==q)&\n",
    "                (total_addendums['type_of_upgrade']==t)\n",
    "            ].reset_index(drop=True)\n",
    "            if adds.empty:\n",
    "                continue\n",
    "\n",
    "            mask = (total['q_id']==q)&(total['type_of_upgrade']==t)\n",
    "            orig = total[mask].reset_index(drop=True)\n",
    "            if orig.empty:\n",
    "                orig = pd.DataFrame({'row_order':[pd.NA]*len(adds)}, index=adds.index)\n",
    "\n",
    "            # align lengths\n",
    "            if len(adds) > len(orig):\n",
    "                extra = pd.DataFrame({c: pd.NA for c in orig.columns},\n",
    "                                     index=range(len(adds)-len(orig)))\n",
    "                orig = pd.concat([orig, extra], ignore_index=True)\n",
    "            elif len(adds) < len(orig):\n",
    "                orig = orig.iloc[:len(adds)].reset_index(drop=True)\n",
    "\n",
    "            for col in conditional_columns:\n",
    "                if col in adds.columns and col in orig.columns:\n",
    "                    adds[col] = (\n",
    "                        adds[col].replace(\"\", pd.NA)\n",
    "                                  .combine_first(orig[col])\n",
    "                                  .fillna(\"\")\n",
    "                    )\n",
    "\n",
    "            total.loc[mask, 'original'] = \"no\"\n",
    "            updated_total_rows.append(\n",
    "                adds.assign(original=\"no\", row_order=orig['row_order'].tolist()[:len(adds)])\n",
    "            )\n",
    "            total = total[~mask]\n",
    "\n",
    "    updated_total = pd.concat([total] + updated_total_rows, ignore_index=True) \\\n",
    "                    if updated_total_rows else total.copy()\n",
    "    updated_total['row_order'] = updated_total['row_order'].fillna(-1).astype(int)\n",
    "    updated_total = (\n",
    "        updated_total\n",
    "        .sort_values('row_order')\n",
    "        .drop(columns='row_order')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # move 'original' to end\n",
    "    def move_last(df):\n",
    "        cols = [c for c in df.columns if c!='original'] + ['original']\n",
    "        return df[cols]\n",
    "\n",
    "    return move_last(updated_itemized), move_last(updated_total)\n",
    "\n",
    "\n",
    "#  main script \n",
    "\n",
    "char_columns = [\n",
    "    \"req_deliverability\",\"point_of_interconnection\",\"type_of_upgrade\",\n",
    "    \"upgrade\",\"description\",\"estimated_time_to_construct\",\"original\",\"item\"\n",
    "]\n",
    "\n",
    "itemized = load_data(\n",
    "'/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_others_itemized.csv',\n",
    "    char_columns\n",
    ")\n",
    "itemized_addendums = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 13/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_13_style_others_itemized_addendums.csv\",\n",
    "    char_columns\n",
    ")\n",
    "total = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 13/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_13_style_others_total.csv\",\n",
    "    char_columns\n",
    ")\n",
    "total_addendums = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 13/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_13_style_others_total_addendums.csv\",\n",
    "    char_columns\n",
    ")\n",
    "\n",
    "updated_itemized, updated_total = merge_with_addendums(\n",
    "    itemized, itemized_addendums, total, total_addendums\n",
    ")\n",
    "\n",
    "# drop unwanted columns\n",
    "to_drop = [\n",
    "    \"upgrade_classification\",\"estimated\",\"caiso_queue\",\n",
    "    \"project_type\",\"dependent_system_upgrade\"\n",
    "]\n",
    "updated_itemized = updated_itemized.drop(columns=[c for c in to_drop if c in updated_itemized], errors='ignore')\n",
    "updated_total   = updated_total.drop(columns=[c for c in to_drop if c in updated_total],   errors='ignore')\n",
    "\n",
    "# fill & sort\n",
    "fill_cols = [\n",
    "    \"point_of_interconnection\",\"latitude\",\"longitude\",\n",
    "    \"req_deliverability\",\"capacity\"\n",
    "]\n",
    "for df in (updated_itemized, updated_total):\n",
    "    for c in fill_cols:\n",
    "        df[c] = df[c].replace('', np.nan)\n",
    "    df.sort_values('q_id', kind='stable', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    for c in fill_cols:\n",
    "        df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
    "    for c in fill_cols:\n",
    "        df[c].replace(np.nan, '', inplace=True)\n",
    "\n",
    "# save\n",
    "save_data(\n",
    "    updated_itemized,\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 13/01_clean/\"\n",
    "    \"costs_phase_2_cluster_13_style_others_itemized_updated.csv\",\n",
    "    char_columns\n",
    ")\n",
    "save_data(\n",
    "    updated_total,\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 13/01_clean/\"\n",
    "    \"costs_phase_2_cluster_13_style_others_total_updated.csv\",\n",
    "    char_columns\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
