{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style N - Cluster 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: P2RPT-QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_1_121721.pdf from Project 1802\n",
      "Scraped PDF: QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_2_12122.pdf from Project 1802\n",
      "Scraped PDF: QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_11-22-21.pdf from Project 1802\n",
      "Scraped PDF: QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_Addendum2_32822.pdf from Project 1806\n",
      "Scraped PDF: QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_11-22-21.pdf from Project 1806\n",
      "Scraped PDF: QC13PhII_Q1810_Drop_Zone_Storage_Appendix_A_11-22-21.pdf from Project 1810\n",
      "Scraped PDF: QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_11-22-21.pdf from Project 1811\n",
      "Scraped PDF: P2RPT-QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_Addendum1_121721.pdf from Project 1811\n",
      "Scraped PDF: QC13PhII_Q1812_Elisabeth_Appendix_A_v2_Addendum1_12122.pdf from Project 1812\n",
      "Scraped PDF: QC13PhII_Q1812_Elisabeth_Appendix_A_112221_v2.pdf from Project 1812\n",
      "Scraped PDF: QC13PhII_Q1814_Hedionda_Energy_Storage_Appendix_A_11-22-21.pdf from Project 1814\n",
      "Scraped PDF: QC13PhII_Q1815_Hinton_Energy_Storage_Appendix_A_Rev1_12122.pdf from Project 1815\n",
      "Scraped PDF: QC13PhII_Q1820_Scafell_Storage_Appendix_A_Rev1_12122.pdf from Project 1820\n",
      "Scraped PDF: QC13PhII_Q1821_Seguro_Storage_Appendix_A_11-22-21.pdf from Project 1821\n",
      "Scraped PDF: QC13PhII_Q1821_Seguro_Storage_Appendix_A_Addendum1_12122.pdf from Project 1821\n",
      "Scraped PDF: QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_Addendum1_12122.pdf from Project 1824\n",
      "Scraped PDF: P2RPT-QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_121721.pdf from Project 1824\n",
      "Scraped PDF: QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_11-22-21.pdf from Project 1824\n",
      "Scraped PDF: QC13PhII_Q1825_Viking_Energy_Storage_Appendix_A_11-22-21.pdf from Project 1825\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_N_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_N_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 17\n",
      "Total Projects Scraped: 14\n",
      "Total Projects Skipped: 3\n",
      "Total Projects Missing: 0\n",
      "Total PDFs Accessed: 25\n",
      "Total PDFs Scraped: 25\n",
      "Total PDFs Skipped: 0\n",
      "\n",
      "List of Scraped Projects:\n",
      "[1802, 1806, 1810, 1811, 1812, 1814, 1815, 1818, 1820, 1821, 1822, 1823, 1824, 1825]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[1818, 1822, 1823]\n",
      "\n",
      "List of Missing Projects:\n",
      "[]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['P2RPT-QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_1_121721.pdf', 'QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_Addendum_2_12122.pdf', 'QC13PhII_Q1802_AmpChamp_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_Addendum2_32822.pdf', 'QC13PhII_Q1806_Captiva_Energy_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1810_Drop_Zone_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_11-22-21.pdf', 'P2RPT-QC13PhII_Q1811_Duck_Pilot_Storage_Appendix_A_Addendum1_121721.pdf', 'QC13PhII_Q1812_Elisabeth_Appendix_A_v2_Addendum1_12122.pdf', 'QC13PhII_Q1812_Elisabeth_Appendix_A_112221_v2.pdf', 'QC13PhII_Q1814_Hedionda_Energy_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1815_Hinton_Energy_Storage_Appendix_A_Rev1_12122.pdf', 'QC13PhII_Q1818_Saddle Mountain_Solar_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1820_Scafell_Storage_Appendix_A_Rev1_12122.pdf', 'QC13PhII_Q1821_Seguro_Storage_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1821_Seguro_Storage_Appendix_A_Addendum1_12122.pdf', 'QC13PhII_Q1822_Sun_Streams_4_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1822_Sun_Streams_4_Appendix_A_Addendum1_12122.pdf', 'QC13PhII_Q1823_Sun_Streams_5_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1823_Sun_Streams_5_Appendix_A_Addendum1_12122.pdf', 'QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_Addendum1_12122.pdf', 'P2RPT-QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev1_121721.pdf', 'P2RPT-QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev2_22924.pdf', 'QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1825_Viking_Energy_Storage_Appendix_A_11-22-21.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "[]\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['QC13PhII_Q1822_Sun_Streams_4_Appendix_A_Addendum1_12122.pdf', 'QC13PhII_Q1823_Sun_Streams_5_Appendix_A_Addendum1_12122.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['QC13PhII_Q1818_Saddle Mountain_Solar_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1822_Sun_Streams_4_Appendix_A_11-22-21.pdf', 'QC13PhII_Q1823_Sun_Streams_5_Appendix_A_11-22-21.pdf', 'P2RPT-QC13PhII_Q1824_Viento_Fronterizo_Appendix_A_Rev2_22924.pdf']\n",
      "\n",
      "List of Style N PDFs (Skipped due to 'Network Upgrade Type'):\n",
      "[]\n",
      "\n",
      "Total Number of Style N PDFs: 0\n",
      "\n",
      "Number of Original PDFs Scraped: 4\n",
      "Number of Addendum PDFs Scraped: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_62678/478634874.py:1178: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_62678/478634874.py:1178: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project list\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_N_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_N_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_scraping_cluster13_style_N_log.txt\"\n",
    "\n",
    "# List of Style N PDFs\n",
    "\n",
    "\n",
    "# List of Style N PDFs\n",
    "STYLE_N_PDF_NAMES = [\n",
    "     'Q1802.pdf', 'Q1806.pdf', 'Q1810.pdf', 'Q1811.pdf', 'Q1812.pdf', 'Q1814.pdf', 'Q1815.pdf', 'Q1818.pdf', 'Q1820.pdf', 'Q1821.pdf', 'Q1822.pdf', 'Q1823.pdf', 'Q1824.pdf', 'Q1825.pdf'\n",
    " \n",
    "\n",
    "]\n",
    "\n",
    "# Extract q_ids from Style N PDF filenames\n",
    "def extract_q_id(pdf_name):\n",
    "    match = re.search(r'_Q(\\d+)_', pdf_name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "STYLE_N_Q_IDS = [extract_q_id(name) for name in STYLE_N_PDF_NAMES if extract_q_id(name) is not None]\n",
    " \n",
    "\n",
    "STYLE_N_Q_IDS = [1802, 1806, 1810, 1811, 1812, 1814, 1815, 1818, 1820, 1821, 1822, 1823, 1824, 1825]\n",
    "\n",
    "# Set PROJECT_LIST to the list of Style N q_ids\n",
    "PROJECT_LIST = STYLE_N_Q_IDS\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "style_n_pdfs = []  # List to track style N PDFs\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "original_has_table8 = {}  # Dictionary to track if original PDFs have table8\n",
    "\n",
    "# Define the list of specific phrases 2\n",
    "SPECIFIC_PHRASES_2 = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"CANU-GR\"]\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"\n",
    "    Cleans column headers by normalizing and removing unwanted characters.\n",
    "\n",
    "    Args:\n",
    "        headers (list): List of raw column headers from the table.\n",
    "\n",
    "    Returns:\n",
    "        list: List of cleaned and normalized column headers.\n",
    "    \"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Normalize whitespace\n",
    "            \n",
    "            # Extract 'Escalated' if present inside parentheses\n",
    "            match = re.search(r'\\(.*?(escalated).*?\\)', header, re.IGNORECASE)\n",
    "            escalated_text = match.group(1).strip() if match else \"\"\n",
    "\n",
    "            # Remove everything inside parentheses but retain 'escalated' if found\n",
    "            header = re.sub(r'\\(.*?\\)', '', header).strip()\n",
    "            \n",
    "            # If 'Escalated' was found, append it to the cleaned header\n",
    "            if escalated_text:\n",
    "                header += f\" {escalated_text}\"\n",
    "            \n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)  # Remove special characters\n",
    "            header = header.strip()\n",
    "\n",
    "        cleaned_headers.append(header)\n",
    "\n",
    "    # Handle specific header issues (e.g., 'Max of Estimated Time to Construct')\n",
    "#    for i, col in enumerate(cleaned_headers):\n",
    "#        if col.startswith(\"max of\"):\n",
    "#            cleaned_headers[i] = \"max of estimated time to construct\"\n",
    "    return cleaned_headers\n",
    "\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Conditionally Assigned Network Upgrades\",\n",
    "        \"PTO's Interconnection Facilities\",\n",
    "        \"Network Upgrades\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\",\n",
    "        \n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus two to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        # Updated Cluster Extraction\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        if '13' in clusters:\n",
    "            cluster_number = '13'\n",
    "        elif clusters:\n",
    "            cluster_number = max(clusters, key=lambda x: int(x))  # Choose the highest cluster number found\n",
    "        else:\n",
    "            cluster_number = '13'  # Default to 13 if not found\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    " \n",
    "def extract_table8(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 8-1 to 8-3 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 8 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 8 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None  # Holds the last extracted table title\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 8-1\" to \"Table 8-3\" or \"8.1\" to \"8.3\"\n",
    "            table8_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*8[-.]([1-2])\\b\", text, re.IGNORECASE):\n",
    "                    table8_pages.append(i)\n",
    "\n",
    "            if not table8_pages:\n",
    "                print(\"No Table 8-1 to 8-3 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table8_pages[0]\n",
    "            last_page = table8_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                            \"snap_tolerance\":6,  # Optimal value to group close elements\n",
    "                    \"join_tolerance\": 6,  # Helps combine fragmented text within cells\n",
    "                    \"intersection_x_tolerance\": 1,  # Fine-tune vertical alignment\n",
    "                    \"intersection_y_tolerance\": 8,  # Adjust for row merging\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    # Extract table title from the region above the table\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "\n",
    "                    if re.search(r\"Network Upgrade Cost Responsibility\", title_text, re.IGNORECASE):\n",
    "                        print(\n",
    "                            f\"Skipping table {table_index} on page {page_number + 1} in pdf {pdf_path} due to 'Network Upgrade Cost Responsibility' in the bounding box above.\",\n",
    "                        \n",
    "                        )\n",
    "                        continue  # Skip this table and move to the next\n",
    "\n",
    "                    # If no skip condition, proceed with processing the table\n",
    "                    print(f\"Processing table {table_index} on page {page_number + 1}...\", file=log_file)\n",
    "\n",
    "                    if title_text.strip():  # If title is found in the bounding box above the table\n",
    "                        title_lines = title_text.split(\"\\n\")[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*8[-.]([1-2])[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(3).strip()\n",
    "                                break\n",
    "                    else:\n",
    "                        # If no title is found, use the last table title\n",
    "                        print(f\"No title found above table on page {page_number + 1}, table {table_index}. Using last specific phrase.\", file=log_file)\n",
    "                        table_title = specific_phrase\n",
    "\n",
    "                    if table_title:\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "\n",
    "                    # Clean headers and data\n",
    "                    headers = tab[0]\n",
    "                    data_rows = tab[1:]\n",
    "\n",
    "                     \n",
    "\n",
    "                    # Clean headers\n",
    "                                        # Clean headers and replace empty ones\n",
    "                    headers = clean_column_headers(headers)\n",
    "                    headers = [f\"unnamed_{i+1}\" if not header else header for i, header in enumerate(headers)]\n",
    "                    \n",
    "                    print(f\"Table {table_index} on page {page_number + 1} headers: {headers}\", file=log_file)\n",
    "\n",
    "                    # Check for required columns\n",
    "                    if not any(col in headers for col in [\"upgrade\", \"network upgrade type\"]):\n",
    "                        print(f\"Required columns not found on page {page_number + 1}, table {table_index}. Retrying extraction with basic settings...\", file=log_file)\n",
    "                        \n",
    "                        # Retry extraction with basic settings\n",
    "                        tables_basic = page.find_tables(table_settings={\n",
    "                            \"horizontal_strategy\": \"lines\",\n",
    "                            \"vertical_strategy\": \"lines\",\n",
    "                        })\n",
    "\n",
    "                        if not tables_basic:\n",
    "                            print(f\"No tables found on retry. Skipping page {page_number + 1}.\", file=log_file)\n",
    "                            continue  # Move to the next page\n",
    "\n",
    "                        # Re-extract with basic settings\n",
    "                        tab_basic = tables_basic[0].extract()\n",
    "                        \n",
    "                        headers = clean_column_headers(tab_basic[0])\n",
    "                        headers = [f\"unnamed_{i+1}\" if not header else header for i, header in enumerate(headers)]\n",
    "                        data_rows = tab_basic[1:]\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "                     \n",
    "\n",
    "                    \n",
    "\n",
    "                     \n",
    "\n",
    "                    # Create DataFrame\n",
    "                    try:\n",
    "                        df = pd.DataFrame(data_rows, columns=headers)\n",
    "\n",
    "                        if \"max of\" in df.columns:\n",
    "                            df.drop(columns=[\"max of\"], inplace=True)\n",
    "                            print(f\"Dropped 'Max of' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "\n",
    "                        if \"estimated\" in df.columns:\n",
    "                            df.drop(columns = [\"estimated\"], inplace=True)\n",
    "                            print(f\"Dropped 'estimated' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                           \n",
    "\n",
    "                        if \"nu estimated\" in df.columns:\n",
    "                            df.drop(columns = [\"nu estimated\"], inplace=True)\n",
    "                            print(f\"Dropped 'nu estimated' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"nu estimated time\" in df.columns:\n",
    "                            df.drop(columns = [\"nu estimated time\"], inplace=True)\n",
    "                            print(f\"Dropped 'nu estimated time' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)    \n",
    "\n",
    "                        if \"nu\" in df.columns:\n",
    "                            df.drop(columns = [\"nu\"], inplace=True)\n",
    "                            print(f\"Dropped 'nu' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)    \n",
    "\n",
    "                        if \"project\" in df.columns:\n",
    "                            df.drop(columns = [\"project\"], inplace=True)\n",
    "                            print(f\"Dropped 'project' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"assigned\" in df.columns:\n",
    "                            df.drop(columns = [\"assigned\"], inplace=True)\n",
    "                            print(f\"Dropped 'assigned' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "                             \n",
    "\n",
    "                        #required_columns = [\"type of upgrade\", \"upgrade\", \"network upgrade type\", \"cost rate\"]\n",
    "\n",
    "                        #if not any(\n",
    "                        #    any(req_col.lower() == col.lower() for col in df.columns) for req_col in required_columns\n",
    "                        #):\n",
    "                        #    print(\n",
    "                        #        f\"Skipping table {table_index} on page {page_number + 1} in PDF {pdf_path}, \"\n",
    "                        #        f\"as it does not have any of the required columns (case-insensitive): {', '.join(required_columns)}\"\n",
    "                        #    )\n",
    "                        #   continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # Replace \"None\" in \"type of upgrade\" for tables without \"network upgrade type\"\n",
    "                        if \"type of upgrade\" in df.columns and \"network upgrade type\" not in df.columns:\n",
    "                            first_row = df.iloc[0]\n",
    "                            if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                df[\"type of upgrade\"] = specific_phrase\n",
    "\n",
    "\n",
    "                                \n",
    "\n",
    "                        # Special logic for ADNU tables\n",
    "                        if re.search(r\"(Area\\s*Delivery\\s*Network\\s*Upgrades)?ADNU\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"adnu\" in df.columns:\n",
    "                                if \"type of upgrade\" not in df.columns:\n",
    "                                    # Group all adnu rows into one \"upgrade\" row\n",
    "                                    adnu_values = df[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    df = df_grouped\n",
    "                                else:\n",
    "                                    # Rename \"adnu\" if necessary\n",
    "                                    if \"upgrade\" in df.columns:\n",
    "                                        df.drop(columns=[\"adnu\"], inplace=True)\n",
    "                                    else:\n",
    "                                        df.rename(columns={\"adnu\": \"upgrade\"}, inplace=True)\n",
    "\n",
    "                            if \"type of upgrade\" not in df.columns:\n",
    "                                df[\"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                first_row = df.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        elif re.search(r\"Network\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" not in df.columns:\n",
    "                                df[\"type of upgrade\"] = \"RNU\"    \n",
    "\n",
    "\n",
    "                        elif re.search(r\"PTO’s Interconnection Facilities\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" not in df.columns:\n",
    "                                df[\"type of upgrade\"] = \"PTO's Interconnection Facilities\"\n",
    "\n",
    "                            elif pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df.at[0, \"type of upgrade\"] = \"PTO's Interconnection Facilities\"\n",
    "\n",
    "\n",
    "                        if \"type of upgrade\" not in df.columns:\n",
    "                                df[\"type of upgrade\"] = specific_phrase            \n",
    "\n",
    "\n",
    "                                    \n",
    "\n",
    "                                    \n",
    "\n",
    "                        base_columns = [\n",
    "                             \"type of upgrade\"\n",
    "                        ]\n",
    "                        remaining_columns = [col for col in df.columns if col not in base_columns]\n",
    "\n",
    "                        df = df.dropna(subset=remaining_columns, how='all')\n",
    "\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df.columns.duplicated().any():\n",
    "                            df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df)\n",
    "                        print(f\"Extracted table from page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                    except ValueError as ve:\n",
    "                        print(f\"Error creating DataFrame for table on page {page_number + 1}, table {table_index}: {ve}\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing page {page_number + 1}: {e}\", file=log_file)\n",
    "                        continue  # Skip the page if an error occurs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 8 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Concatenate all extracted tables\n",
    "    if extracted_tables:\n",
    "        try:\n",
    "            combined_df = pd.concat(extracted_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully combined {len(extracted_tables)} tables.\", file=log_file)\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating extracted tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 8 data extracted.\", file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def scrape_alternative_tables(pdf_path, log_file, specific_phrases):\n",
    "    \"\"\"\n",
    "    Scrapes all tables in the PDF that contain any of the specific phrases in their cells.\n",
    "    For Style N tables with 'Network Upgrade Type' column, handles 'type of upgrade' accordingly.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        specific_phrases (list): List of specific phrases to search for.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted data from alternative tables containing the specific phrases.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for alternative table scraping...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    # Check if any cell contains any of the specific phrases\n",
    "                    match_found = False\n",
    "                    for row in tab:\n",
    "                        for cell in row:\n",
    "                            if cell and any(re.search(rf\"\\b{re.escape(phrase)}\\b\", cell, re.IGNORECASE) for phrase in specific_phrases):\n",
    "                                match_found = True\n",
    "                                break\n",
    "                        if match_found:\n",
    "                            break\n",
    "                    if match_found:\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        headers = make_unique_headers(headers)  # Ensure headers are unique\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Extra Check: Only process tables with 'Upgrade' or 'Network Upgrade Type' columns\n",
    "                        if not any(col in headers for col in [\"upgrade\", \"network upgrade type\"]):\n",
    "                            print(f\"Skipping alternative Table {table_index} on page {page_number} as it lacks 'Upgrade' or 'Network Upgrade Type' columns.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        has_network_upgrade_type = \"network upgrade type\" in headers\n",
    "\n",
    "                        if has_network_upgrade_type:\n",
    "                            print(f\"'Network Upgrade Type' column found in alternative Table {table_index} on page {page_number}.\", file=log_file)\n",
    "                            # Create 'type of upgrade' column if it doesn't exist\n",
    "                            if \"type of upgrade\" not in headers:\n",
    "                                headers.append(\"type of upgrade\")\n",
    "                                tab[0].append(\"type of upgrade\")  # Add header to the table data\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "\n",
    "                            # Initialize 'type of upgrade' column with None\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = None\n",
    "\n",
    "                            current_upgrade = None\n",
    "                            for idx, row in df_new.iterrows():\n",
    "                                network_upgrade = str(row.get(\"network upgrade type\", \"\")).strip()\n",
    "                                if network_upgrade.lower() == \"total\":\n",
    "                                    current_upgrade = None\n",
    "                                elif network_upgrade in SPECIFIC_PHRASES_2:\n",
    "                                    current_upgrade = network_upgrade\n",
    "                                    df_new.at[idx, \"type of upgrade\"] = current_upgrade\n",
    "                                elif current_upgrade:\n",
    "                                    df_new.at[idx, \"type of upgrade\"] = current_upgrade\n",
    "                            print(f\"Populated 'type of upgrade' column based on 'Network Upgrade Type' in alternative Table {table_index}.\", file=log_file)\n",
    "                        else:\n",
    "                            # Handle tables without 'Network Upgrade Type'\n",
    "                            try:\n",
    "                                df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for alternative Table {table_index} on page {page_number}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "\n",
    "                            # Continue with existing specific phrase checks\n",
    "                            # Assuming you have a specific_phrase variable from elsewhere; adjust as needed\n",
    "                            if specific_phrase:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"Added 'type of upgrade' to all rows in alternative Table {table_index}.\", file=log_file)\n",
    "                                else:\n",
    "                                    first_row = df_new.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                        print(f\"Replaced None in 'type of upgrade' for alternative Table {table_index} on page {page_number}.\", file=log_file)\n",
    "\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in alternative Table {table_index} on page {page_number}. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping alternative tables in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted alternative tables...\", file=log_file)\n",
    "        try:\n",
    "            alternative_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} alternative tables.\", file=log_file)\n",
    "            return alternative_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating alternative tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No alternative tables found containing specific phrases.\", file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def extract_table8_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 8 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table8_data = extract_table8(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table8_data.empty:\n",
    "        if is_addendum and original_has_table8.get(project_id, False):\n",
    "            # Scrape alternative tables based on specific phrases\n",
    "            specific_phrases = [\n",
    "                \"PTO\",\n",
    "                \"Reliability Network Upgrade\",\n",
    "                \"Area Delivery Network Upgrade\",\n",
    "                \"Local Delivery Network\",\n",
    "                \"Other Potential Network Upgrade\",\n",
    "                \"Area Delivery Network Upgrades\",\n",
    "                \"Conditionally Assigned Network Upgrades\",\n",
    "                \"ADNU\",\n",
    "                \"LDNU\",\n",
    "                \"RNU\"\n",
    "            ]\n",
    "            alternative_data = scrape_alternative_tables(pdf_path, log_file, specific_phrases)\n",
    "            if not alternative_data.empty:\n",
    "                table8_data = alternative_data\n",
    "    if table8_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table8_data.columns).difference(['point_of_interconnection'])\n",
    "        table8_data = table8_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "\n",
    "        # Repeat base data for each row in table8_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table8_data), ignore_index=True)\n",
    "\n",
    "        try:\n",
    "            # Concatenate base data with Table 8 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table8_data], axis=1, sort=False)\n",
    "           # if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "           #     merged_df[\"is_duplicate\"] = merged_df.duplicated(subset=[\"q_id\", \"type of upgrade\", \"upgrade\"], keep=\"first\")\n",
    "            #    merged_df = merged_df[merged_df[\"is_duplicate\"] == False].drop(columns=[\"is_duplicate\"])\n",
    "            #    print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade'.\", file=log_file)\n",
    "\n",
    "\n",
    "            if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "                # Identify rows where 'type of upgrade' and 'upgrade' are not empty\n",
    "                non_empty_rows = merged_df[\n",
    "                    merged_df[\"type of upgrade\"].notna() & merged_df[\"upgrade\"].notna() &\n",
    "                    (merged_df[\"type of upgrade\"].str.strip() != \"\") & (merged_df[\"upgrade\"].str.strip() != \"\")\n",
    "                ]\n",
    "\n",
    "                # Group by q_id, type of upgrade, and upgrade, keeping the first occurrence\n",
    "                grouped_df = non_empty_rows.groupby([\"q_id\", \"type of upgrade\", \"upgrade\"], as_index=False).first()\n",
    "\n",
    "                # Get the original order of the rows in merged_df before filtering\n",
    "                merged_df[\"original_index\"] = merged_df.index\n",
    "\n",
    "                # Combine unique grouped rows with originally empty rows\n",
    "                final_df = pd.concat([\n",
    "                    grouped_df,\n",
    "                    merged_df[merged_df[\"type of upgrade\"].isna() | (merged_df[\"type of upgrade\"].str.strip() == \"\") |\n",
    "                            merged_df[\"upgrade\"].isna() | (merged_df[\"upgrade\"].str.strip() == \"\")]\n",
    "                ], ignore_index=True, sort=False)\n",
    "\n",
    "                # Restore the original order of the rows based on the saved index\n",
    "                final_df.sort_values(by=\"original_index\", inplace=True)\n",
    "                final_df.drop(columns=[\"original_index\"], inplace=True)\n",
    "                merged_df = final_df\n",
    "\n",
    "                print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade', excluding empty rows while preserving order.\", file=log_file)\n",
    "\n",
    "            \n",
    "\n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "\n",
    "            print(f\"Merged base data with Table 8 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 8 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "def check_has_table8(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 8-1 to 8-3.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*8[-.]([1-3])\\b\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def has_network_upgrade_type_column(pdf_path, log_file):\n",
    "    \"\"\"Checks if any table in the PDF has a column header 'Network Upgrade Type'.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables()\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    headers = clean_column_headers(tab[0])\n",
    "                    if \"network upgrade type\" in headers:\n",
    "                        print(f\"Found 'Network Upgrade Type' in PDF {pdf_path} on page {page_number}, table {table_index}.\", file=log_file)\n",
    "                        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking 'Network Upgrade Type' in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path, log_file):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' or 'Revision' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            print(f\"Extracted Text: {text}\", file= log_file)  # Debugging step\n",
    "            # Case-insensitive check for 'Addendum' or 'Revision'\n",
    "            text_lower = text.lower()\n",
    "            return \"addendum\" in text_lower in text_lower\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    \"\"\"\n",
    "    Appends a suffix to duplicate headers to make them unique.\n",
    "\n",
    "    Args:\n",
    "        headers (list): List of column headers.\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique column headers.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all Style N PDFs in the specified project list and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "\n",
    "    SKIP_PROJECTS = {0} #1811\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    \n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "\n",
    "        for project_id in PROJECT_LIST:\n",
    "            # Skip the projects in the SKIP_PROJECTS set\n",
    "            if project_id in SKIP_PROJECTS:\n",
    "                print(f\"Skipping Project {project_id} (marked to skip)\", file=log_file)\n",
    "                continue\n",
    "        \n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"03_phase_2_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            # Instead of filtering by name, take every PDF in this project folder\n",
    "            list_pdfs = [f for f in os.listdir(project_path) if f.lower().endswith(\".pdf\")]\n",
    "            print(f\"Project {project_id} — found PDFs: {list_pdfs}\", file=log_file)\n",
    "\n",
    "            # Treat all of these as Style N candidates\n",
    "            style_n_pdfs_in_project = list_pdfs.copy()\n",
    "\n",
    "\n",
    "            if not style_n_pdfs_in_project:\n",
    "                skipped_projects.add(project_id)\n",
    "                print(f\"No Style N PDFs found in Project {project_id}.\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            for pdf_name in style_n_pdfs_in_project:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                has_network_upgrade = has_network_upgrade_type_column(pdf_path, log_file)\n",
    "\n",
    "                if has_network_upgrade:\n",
    "                    print(f\"Processing Style N PDF: {pdf_name} from Project {project_id} with 'Network Upgrade Type' column.\", file=log_file)\n",
    "                else:\n",
    "                    print(f\"Processing Style N PDF: {pdf_name} from Project {project_id} without 'Network Upgrade Type' column.\", file=log_file)\n",
    "\n",
    "                try:\n",
    "                    has_table8 = check_has_table8(pdf_path)\n",
    "                    original_has_table8[project_id] = has_table8\n",
    "\n",
    "                    if not has_table8:\n",
    "                        print(f\"No Table 8 in {pdf_name}: extracting base data only\", file=log_file)\n",
    "                        base_df = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_df['table_8_missing'] = True\n",
    "\n",
    "                        if is_add:\n",
    "                            core_addendums = pd.concat([core_addendums, base_df], ignore_index=True)\n",
    "                            addendum_pdfs.append(pdf_name)\n",
    "                        else:\n",
    "                            core_originals  = pd.concat([core_originals,  base_df], ignore_index=True)\n",
    "                            original_pdfs.append(pdf_name)\n",
    "\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        total_pdfs_scraped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from Style N PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    # Extract Table 8 and merge\n",
    "                    df = extract_table8_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                    if not df.empty:\n",
    "                        if is_add:\n",
    "                            core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                        else:\n",
    "                            core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        project_scraped = True\n",
    "                        total_pdfs_scraped += 1\n",
    "                        print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                    else:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nList of Style N PDFs (Skipped due to 'Network Upgrade Type'):\")\n",
    "    print(style_n_pdfs)\n",
    "\n",
    "    print(\"\\nTotal Number of Style N PDFs:\", len(style_n_pdfs))\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.applymap(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized and Total Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 1802\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1806\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1810\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1811\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1812\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1818\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 1820\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1821\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1822\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 1823\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 1824\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1825\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "     q_id  cluster req_deliverability   latitude     longitude  capacity  \\\n",
      "0   1802       13               Full  32.594889   -117.057076       NaN   \n",
      "1   1802       13               Full  32.594889   -117.057076       NaN   \n",
      "2   1806       13               Full  33.532816   -117.677040       NaN   \n",
      "3   1806       13               Full  33.532816   -117.677040       NaN   \n",
      "4   1810       13               Full  32.569397   -116.916559       NaN   \n",
      "5   1810       13               Full  32.569397   -116.916559       NaN   \n",
      "6   1811       13               Full  32.858150     -116.8945       NaN   \n",
      "7   1811       13               Full  32.858150     -116.8945       NaN   \n",
      "8   1812       13               Full  32.970000      -113.516       NaN   \n",
      "9   1812       13               Full  32.970000      -113.516       NaN   \n",
      "10  1818       13               Full  33.324703             -       NaN   \n",
      "11  1818       13               Full  33.324703             -       NaN   \n",
      "12  1818       13               Full  33.324703             -       NaN   \n",
      "13  1820       13               Full  33.018184   -116.856026       NaN   \n",
      "14  1820       13               Full  33.018184   -116.856026       NaN   \n",
      "15  1821       13               Full  33.127110    -117.11715       NaN   \n",
      "16  1821       13               Full  33.127110    -117.11715       NaN   \n",
      "17  1822       13               Full  33.347000      -112.845       NaN   \n",
      "18  1822       13               Full  33.347000      -112.845       NaN   \n",
      "19  1822       13               Full  33.347000      -112.845       NaN   \n",
      "20  1823       13               Full  33.347000      -112.845       NaN   \n",
      "21  1823       13               Full  33.347000      -112.845       NaN   \n",
      "22  1823       13               Full  33.347000      -112.845       NaN   \n",
      "23  1824       13               Full  32.546797  -116.1613804       NaN   \n",
      "24  1824       13               Full  32.546797  -116.1613804       NaN   \n",
      "25  1825       13               Full  33.209793   -117.295907       NaN   \n",
      "26  1825       13               Full  33.209793   -117.295907       NaN   \n",
      "\n",
      "                             point_of_interconnection type_of_upgrade upgrade  \\\n",
      "0                        69 kV bus at Otay Substation    Total PTO_IF           \n",
      "1                        69 kV bus at Otay Substation       Total RNU           \n",
      "2   Switchyard Loop-In at the Trabuco - Capistrano...    Total PTO_IF           \n",
      "3   Switchyard Loop-In at the Trabuco - Capistrano...       Total RNU           \n",
      "4                  230 kV bus at Otay Mesa Substation    Total PTO_IF           \n",
      "5                  230 kV bus at Otay Mesa Substation       Total RNU           \n",
      "6                 138 kV bus at Los Coches Substation    Total PTO_IF           \n",
      "7                 138 kV bus at Los Coches Substation       Total RNU           \n",
      "8                500 kV bus at Hoodoo Wash Switchyard    Total PTO_IF           \n",
      "9                500 kV bus at Hoodoo Wash Switchyard       Total RNU           \n",
      "10         500 kV common bus at Hassayampa Switchyard    Total PTO_IF           \n",
      "11         500 kV common bus at Hassayampa Switchyard       Total RNU           \n",
      "12         500 kV common bus at Hassayampa Switchyard      Total LDNU           \n",
      "13                   69 kV bus at Creelman Substation    Total PTO_IF           \n",
      "14                   69 kV bus at Creelman Substation       Total RNU           \n",
      "15                 230 kV bus at Escondido Substation    Total PTO_IF           \n",
      "16                 230 kV bus at Escondido Substation       Total RNU           \n",
      "17         500 kV common bus at Hassayampa Switchyard    Total PTO_IF           \n",
      "18         500 kV common bus at Hassayampa Switchyard       Total RNU           \n",
      "19         500 kV common bus at Hassayampa Switchyard      Total LDNU           \n",
      "20         500 kV common bus at Hassayampa Switchyard    Total PTO_IF           \n",
      "21         500 kV common bus at Hassayampa Switchyard       Total RNU           \n",
      "22         500 kV common bus at Hassayampa Switchyard      Total LDNU           \n",
      "23               500 kV bus at East County Substation    Total PTO_IF           \n",
      "24               500 kV bus at East County Substation       Total RNU           \n",
      "25                69 kV bus at Ocean Ranch Substation    Total PTO_IF           \n",
      "26                69 kV bus at Ocean Ranch Substation       Total RNU           \n",
      "\n",
      "   description cost_allocation_factor  estimated_cost_x_1000  \\\n",
      "0                                                      626.0   \n",
      "1                                                      430.0   \n",
      "2                                                      687.0   \n",
      "3                                                    32104.0   \n",
      "4                                                      746.0   \n",
      "5                                                     4632.0   \n",
      "6                                                      626.0   \n",
      "7                                                     2003.0   \n",
      "8                                                        0.0   \n",
      "9                                                      936.0   \n",
      "10                                                       0.0   \n",
      "11                                                       0.0   \n",
      "12                                                       0.0   \n",
      "13                                                     445.0   \n",
      "14                                                     450.0   \n",
      "15                                                    3860.0   \n",
      "16                                                    3861.0   \n",
      "17                                                       0.0   \n",
      "18                                                       0.0   \n",
      "19                                                       0.0   \n",
      "20                                                       0.0   \n",
      "21                                                       0.0   \n",
      "22                                                       0.0   \n",
      "23                                                    2670.0   \n",
      "24                                                   19309.0   \n",
      "25                                                    2481.0   \n",
      "26                                                     430.0   \n",
      "\n",
      "    escalated_cost_x_1000 total_estimated_cost_x_1000  \\\n",
      "0                   665.0                               \n",
      "1                   430.0                               \n",
      "2                   715.0                               \n",
      "3                 33368.0                               \n",
      "4                   844.0                               \n",
      "5                  4897.0                               \n",
      "6                   665.0                               \n",
      "7                  2100.0                               \n",
      "8                     0.0                               \n",
      "9                   936.0                               \n",
      "10                    0.0                               \n",
      "11                    0.0                               \n",
      "12                    0.0                               \n",
      "13                  473.0                               \n",
      "14                  450.0                               \n",
      "15                 4103.0                               \n",
      "16                 4000.0                               \n",
      "17                    0.0                               \n",
      "18                    0.0                               \n",
      "19                    0.0                               \n",
      "20                    0.0                               \n",
      "21                    0.0                               \n",
      "22                    0.0                               \n",
      "23                 2897.0                               \n",
      "24                20164.0                               \n",
      "25                 2637.0                               \n",
      "26                  430.0                               \n",
      "\n",
      "   estimated_time_to_construct network_upgrade_type item  \\\n",
      "0                                                     no   \n",
      "1                                                     no   \n",
      "2                                                     no   \n",
      "3                                                     no   \n",
      "4                                                     no   \n",
      "5                                                     no   \n",
      "6                                                     no   \n",
      "7                                                     no   \n",
      "8                                                     no   \n",
      "9                                                     no   \n",
      "10                                                    no   \n",
      "11                                                    no   \n",
      "12                                                    no   \n",
      "13                                                    no   \n",
      "14                                                    no   \n",
      "15                                                    no   \n",
      "16                                                    no   \n",
      "17                                                    no   \n",
      "18                                                    no   \n",
      "19                                                    no   \n",
      "20                                                    no   \n",
      "21                                                    no   \n",
      "22                                                    no   \n",
      "23                                                    no   \n",
      "24                                                    no   \n",
      "25                                                    no   \n",
      "26                                                    no   \n",
      "\n",
      "    total_estimated_cost_x_1000_escalated  adnu_cost_rate_x_1000_escalated  \n",
      "0                                       0                                0  \n",
      "1                                       0                                0  \n",
      "2                                       0                                0  \n",
      "3                                       0                                0  \n",
      "4                                       0                                0  \n",
      "5                                       0                                0  \n",
      "6                                       0                                0  \n",
      "7                                       0                                0  \n",
      "8                                       0                                0  \n",
      "9                                       0                                0  \n",
      "10                                      0                                0  \n",
      "11                                      0                                0  \n",
      "12                                      0                                0  \n",
      "13                                      0                                0  \n",
      "14                                      0                                0  \n",
      "15                                      0                                0  \n",
      "16                                      0                                0  \n",
      "17                                      0                                0  \n",
      "18                                      0                                0  \n",
      "19                                      0                                0  \n",
      "20                                      0                                0  \n",
      "21                                      0                                0  \n",
      "22                                      0                                0  \n",
      "23                                      0                                0  \n",
      "24                                      0                                0  \n",
      "25                                      0                                0  \n",
      "26                                      0                                0  \n",
      "Itemized rows saved to 'costs_phase_2_cluster_13_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_13_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU']\n",
      "[1802 1806 1810 1811 1812 1818 1820 1821 1822 1823 1824 1825]\n",
      "[13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_8639/2063714096.py:188: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_8639/2063714096.py:188: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_N_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'cost allocation factor' in df.columns:\n",
    "#    df['description'] = df['cost allocation factor'].apply(extract_non_numeric_text)\n",
    " #   df['cost_allocation_factor'] = df['cost allocation factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "#\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "            \"unnamed_1\",\n",
    "             ],\n",
    "\n",
    "        \n",
    "        \"estimated_cost_x_1000\": [\n",
    "\n",
    "            \"estimated cost x 1000\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"allocated_cost\",\n",
    "            \"assigned cost\",\n",
    "            \"allocated cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"project allocated cost\",\n",
    "            \"assigned cost\",\n",
    "            \"unnamed_14\",\n",
    "            \"unnamed_4\",\n",
    "            \"unnamed_13\",\n",
    "            \n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"assigned cost escalated\",\n",
    "            \"assigned escalated cost\",\n",
    "            \n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"project allocated cost escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"unnamed_17\",\n",
    "            \"unnamed_16\",\n",
    "            \"allocat ed cost\",\n",
    "            \n",
    "           \n",
    "\n",
    "        ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "            \"max of estimated time to construct\",\n",
    "            \n",
    "            \"unnamed_6\",\n",
    "            \"unnamed_8\",\n",
    "            \"unnamed_9\",\n",
    "            \"nu estimated time to construct\",\n",
    "            \"unnamed_7\",\n",
    "            \"unnamed_18\",\n",
    "            \"unnamed_19\",\n",
    "            \"unnamed_10\",\n",
    "            \"time to construct\",\n",
    "             \n",
    "\n",
    "\n",
    "        ],\n",
    "\n",
    "        \"network upgrade type\": [\n",
    "            \"network upgrade type\",\n",
    "            \"unnamed_2\",\n",
    "        ],\n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"nu total cost\",\n",
    "            \"total cost constant\",\n",
    "            \"sum of total cost constant\",\n",
    "            \"total cost\",\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\",\n",
    "            \"nu total cost escalated\",\n",
    "            \"total cost escalated\",\n",
    "\n",
    "        ],\n",
    "       \n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "\n",
    "        \"adnu_cost_rate_escalated_x_1000\": [\n",
    "        \"cost rate escalated\",\n",
    "        \"escalated cost rate\",\n",
    "        ],\n",
    "\n",
    "        \"description\": [\"description\", \"unnamed_3\"],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "            \"project allocation\",\n",
    "            \n",
    "\n",
    "        ],\n",
    "        \"estimated cost x 1000 escalated with itcca\": [\n",
    "            \"estimated cost x 1000 escalated with itcca\",\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "#if 'estimated_time_to_construct' in df.columns:\n",
    "#    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "#if 'upgrade' in df.columns:\n",
    " #   df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    "#if 'cost_allocation_factor' in df.columns:\n",
    "#    df['description'] = df['cost_allocation_factor'].apply(extract_non_numeric_text)\n",
    " #   df['cost_allocation_factor'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values    \n",
    "\n",
    "\n",
    "# Clear cost_allocation_factor for rows where upgrade is \"Total\" (case-insensitive)\n",
    "#df.loc[df['upgrade'].str.lower() == 'total', 'cost_allocation_factor'] = None  # or use \"\" if you prefer an empty string\n",
    "\n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    "\n",
    "#df.drop(['unnamed_3', 'unnamed_15', 'unnamed_18', 'unnamed_16', 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "df.drop([   'table_8_missing',\t \t'scafell storage',\t'q1820', 'estimated cost x 1000 escalated with itcca', 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def move_description_to_upgrade(df):\n",
    "    \"\"\"Moves description to upgrade if type_of_upgrade is 'PTO' and upgrade is empty.\"\"\"\n",
    "    \n",
    "    # Ensure columns are strings and replace NaNs with empty strings for processing\n",
    "    df['upgrade'] = df['upgrade'].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
    "    df['description'] = df['description'].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
    "\n",
    "    # Debug: Print before update\n",
    "    #print(\"Before update (only PTO rows):\")\n",
    "    #print(df[df['type_of_upgrade'] == 'PTO'][['type_of_upgrade', 'upgrade', 'description']])\n",
    "\n",
    "    # Apply row-wise transformation\n",
    "    def move_if_empty(row):\n",
    "        if row['type_of_upgrade'] == 'PTO' and row['upgrade'].strip() == \"\" and row['description'].strip() != \"\":\n",
    "            row['upgrade'] = row['description']  # Move description to upgrade\n",
    "            row['description'] = None # Clear description\n",
    "        return row\n",
    "\n",
    "    df = df.apply(move_if_empty, axis=1)\n",
    "\n",
    "    # Debug: Print after update\n",
    "    #print(\"\\nAfter update (only PTO rows):\")\n",
    "    #print(df[df['type_of_upgrade'] == 'PTO'][['type_of_upgrade', 'upgrade', 'description']])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply function\n",
    "#df = move_description_to_upgrade(df)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "df['tot'] = df.apply(\n",
    "    lambda row: 'yes' if (\n",
    "        (pd.notna(row.get('upgrade')) and 'Total' in str(row['upgrade']))  \n",
    "    ) else 'no',\n",
    "    axis=1\n",
    ") \n",
    "\n",
    "# Now extract ONLY \"Total\" rows with a foolproof match\n",
    "total_rows_df = df[df['tot'] == 'yes']\n",
    "\n",
    "total_rows_df = total_rows_df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "total_rows_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_N_total_network.csv', index=False) \n",
    "df = df[df['upgrade'].str.strip().str.lower() != 'total']\n",
    "df = df[df['network_upgrade_type'].str.strip().str.lower() != 'total']\n",
    " \n",
    "\n",
    " \n",
    "df.drop('tot', axis=1, inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)  \n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"PTO's Interconnection Facilities (Note 1)\": \"PTO_IF\",\n",
    " \"PTO's Interconnection Facilities (Note 4)\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'CANU-GR': 'CANU-GR',\n",
    " 'Total CANU-GR': 'CANU-GR',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    "}\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# STEP 8½: For certain q_ids with no cost data, insert placeholder rows\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# list of q_ids that need 3 “empty” upgrade rows\n",
    "placeholder_qids = [ 1818, 1822, 1823 ]   # ← your set of missing‐data q_ids\n",
    "\n",
    "# columns that carry only the base data\n",
    "base_cols = [\n",
    "    \"q_id\",\n",
    "    \"cluster\",\n",
    "    \"req_deliverability\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"capacity\",\n",
    "    \"point_of_interconnection\",\n",
    "]\n",
    "\n",
    "# the three upgrade codes to inject\n",
    "placeholder_upgrades = [\"PTO_IF\", \"RNU\", \"LDNU\"]\n",
    "\n",
    "new_rows = []\n",
    "for qid in placeholder_qids:\n",
    "    # find base‐data rows for this qid\n",
    "    base_mask = df[\"q_id\"] == qid\n",
    "    if not base_mask.any():\n",
    "        print(f\"Warning: no row at all for q_id {qid}, skipping placeholders\")\n",
    "        continue\n",
    "\n",
    "    # extract the one base row\n",
    "    base = df.loc[base_mask, base_cols].iloc[0]\n",
    "\n",
    "    # build three empty‐detail rows\n",
    "    for code in placeholder_upgrades:\n",
    "        row = {c: \"\" for c in df.columns}\n",
    "        # copy only the base columns\n",
    "        for c in base_cols:\n",
    "            row[c] = base[c]\n",
    "        # assign the placeholder upgrade code\n",
    "        row[\"type_of_upgrade\"] = code\n",
    "        new_rows.append(row)\n",
    "\n",
    "# append placeholders if any\n",
    "if new_rows:\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    # re‐order columns so the new rows fit your desired layout\n",
    "    df = reorder_columns(df)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# now continue with STEP 9: Create Total rows…\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'escalated_cost_rate','total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000',  ]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "import re\n",
    "\n",
    "# Define the list of upgrade phrases\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    "def classify_and_clean_upgrades(df):\n",
    "    \"\"\"\n",
    "    For rows with type_of_upgrade equal to \"RNU\", moves the value from \n",
    "    'network_upgrade_type' to a new 'upgrade' column, and then removes rows where:\n",
    "      - 'network_upgrade_type' exactly matches an upgrade phrase, or\n",
    "      - 'network_upgrade_type' matches the pattern 'Total <upgrade_phrase>'.\n",
    "    Other rows remain unchanged.\n",
    "    \"\"\"\n",
    "    # Ensure 'network_upgrade_type' is a string and fill missing values with an empty string\n",
    "    df['network_upgrade_type'] = df['network_upgrade_type'].astype(str).fillna(\"\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # Process only rows where type_of_upgrade is \"RNU\"\n",
    "    rnu_mask = df['type_of_upgrade'] == \"RNU\"\n",
    "    \n",
    "    # For these rows, copy the network_upgrade_type to a new column called 'upgrade'\n",
    "    df.loc[rnu_mask, 'upgrade'] = df.loc[rnu_mask, 'network_upgrade_type']\n",
    "\n",
    "    canugr = df['network_upgrade_type'].str.strip().str.upper() == 'CANU-GR'\n",
    "    df.loc[canugr, 'type_of_upgrade'] = 'CANU'\n",
    "    \n",
    "    # Build a regex pattern to match strings like \"Total IRNU\", \"Total GRNU\", etc.\n",
    "    pattern = r\"^Total\\s+(\" + \"|\".join(re.escape(phrase) for phrase in upgrade_phrases) + r\")$\"\n",
    "    \n",
    "    # Create a condition that checks for rows (within type_of_upgrade==\"RNU\") that should be removed:\n",
    "    # - The network_upgrade_type is exactly one of the upgrade phrases.\n",
    "    # - OR the network_upgrade_type matches the pattern \"Total <upgrade_phrase>\".\n",
    "    remove_condition = rnu_mask & (\n",
    "        df['network_upgrade_type'].isin(upgrade_phrases) |\n",
    "        df['network_upgrade_type'].str.match(pattern)\n",
    "    )\n",
    "    \n",
    "    # Remove the rows meeting the remove_condition\n",
    "    df = df[~remove_condition]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the updated function\n",
    "df = classify_and_clean_upgrades(df)\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "# only drop rows whose upgrade is exactly \"total\"\n",
    "df = df[df['upgrade']\n",
    "          .astype(str)            # ensure it's string dtype\n",
    "          .str.strip()\n",
    "          .str.lower()\n",
    "          .fillna('')             # turn NaN → ''\n",
    "          .ne('total')]           # keep all rows not equal to 'total'\n",
    "\n",
    "\n",
    " \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5 , \"CANU\": 6, \"CANU-GR\": 7, \"LOPNU\": 8}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)  \n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/cluster_13_style_N.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'adnu_cost_rate_x_1000_escalated']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ((group['type_of_upgrade'] == f\"Total {upgrade}\") & (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['type_of_upgrade'] == f\"Total {upgrade}\") & (group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    " \n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "     \n",
    "     \n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_N_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    \n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_N_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_13_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_13_total.csv'.\")\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 1802\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1806\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1811\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1812\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1821\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1822\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 1823\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 1824\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "     q_id  cluster req_deliverability   latitude   longitude  capacity  \\\n",
      "0   1802       13               Full  32.594889 -117.057076       NaN   \n",
      "1   1802       13               Full  32.594889 -117.057076       NaN   \n",
      "2   1806       13               Full  33.532816 -117.677040       NaN   \n",
      "3   1806       13               Full  33.532816 -117.677040       NaN   \n",
      "4   1811       13               Full  32.858150 -116.894500       NaN   \n",
      "5   1811       13               Full  32.858150 -116.894500       NaN   \n",
      "6   1812       13               Full  32.970000 -113.516000       NaN   \n",
      "7   1812       13               Full  32.970000 -113.516000       NaN   \n",
      "8   1821       13               Full  33.127110 -117.117150       NaN   \n",
      "9   1821       13               Full  33.127110 -117.117150       NaN   \n",
      "10  1822       13               Full  33.347000 -112.845000       NaN   \n",
      "11  1822       13               Full  33.347000 -112.845000       NaN   \n",
      "12  1822       13               Full  33.347000 -112.845000       NaN   \n",
      "13  1823       13               Full  33.347000 -112.845000       NaN   \n",
      "14  1823       13               Full  33.347000 -112.845000       NaN   \n",
      "15  1823       13               Full  33.347000 -112.845000       NaN   \n",
      "16  1824       13               Full  32.546797 -116.161380       NaN   \n",
      "17  1824       13               Full  32.546797 -116.161380       NaN   \n",
      "\n",
      "                             point_of_interconnection type_of_upgrade upgrade  \\\n",
      "0                        69 kV bus at Otay Substation    Total PTO_IF           \n",
      "1                        69 kV bus at Otay Substation       Total RNU           \n",
      "2   Switchyard Loop-In at the Trabuco - Capistrano...    Total PTO_IF           \n",
      "3   Switchyard Loop-In at the Trabuco - Capistrano...       Total RNU           \n",
      "4                 138 kV bus at Los Coches Substation    Total PTO_IF           \n",
      "5                 138 kV bus at Los Coches Substation       Total RNU           \n",
      "6                500 kV bus at Hoodoo Wash Switchyard    Total PTO_IF           \n",
      "7                500 kV bus at Hoodoo Wash Switchyard       Total RNU           \n",
      "8                  230 kV bus at Escondido Substation    Total PTO_IF           \n",
      "9                  230 kV bus at Escondido Substation       Total RNU           \n",
      "10         500 kV common bus at Hassayampa Switchyard    Total PTO_IF           \n",
      "11         500 kV common bus at Hassayampa Switchyard       Total RNU           \n",
      "12         500 kV common bus at Hassayampa Switchyard      Total LDNU           \n",
      "13         500 kV common bus at Hassayampa Switchyard    Total PTO_IF           \n",
      "14         500 kV common bus at Hassayampa Switchyard       Total RNU           \n",
      "15         500 kV common bus at Hassayampa Switchyard      Total LDNU           \n",
      "16               500 kV bus at East County Substation    Total PTO_IF           \n",
      "17               500 kV bus at East County Substation       Total RNU           \n",
      "\n",
      "   description cost_allocation_factor  estimated_cost_x_1000  \\\n",
      "0                                                      626.0   \n",
      "1                                                      430.0   \n",
      "2                                                      687.0   \n",
      "3                                                    31854.0   \n",
      "4                                                      626.0   \n",
      "5                                                     2003.0   \n",
      "6                                                        0.0   \n",
      "7                                                      936.0   \n",
      "8                                                     3860.0   \n",
      "9                                                     3861.0   \n",
      "10                                                       0.0   \n",
      "11                                                       0.0   \n",
      "12                                                       0.0   \n",
      "13                                                       0.0   \n",
      "14                                                       0.0   \n",
      "15                                                       0.0   \n",
      "16                                                    2670.0   \n",
      "17                                                   19309.0   \n",
      "\n",
      "    escalated_cost_x_1000 total_estimated_cost_x_1000  \\\n",
      "0                   665.0                               \n",
      "1                   430.0                               \n",
      "2                   715.0                               \n",
      "3                 33118.0                               \n",
      "4                   665.0                               \n",
      "5                  2100.0                               \n",
      "6                     0.0                               \n",
      "7                   936.0                               \n",
      "8                  4103.0                               \n",
      "9                  4000.0                               \n",
      "10                    0.0                               \n",
      "11                    0.0                               \n",
      "12                    0.0                               \n",
      "13                    0.0                               \n",
      "14                    0.0                               \n",
      "15                    0.0                               \n",
      "16                 2897.0                               \n",
      "17                20164.0                               \n",
      "\n",
      "   estimated_time_to_construct network_upgrade_type item  \\\n",
      "0                                                     no   \n",
      "1                                                     no   \n",
      "2                                                     no   \n",
      "3                                                     no   \n",
      "4                                                     no   \n",
      "5                                                     no   \n",
      "6                                                     no   \n",
      "7                                                     no   \n",
      "8                                                     no   \n",
      "9                                                     no   \n",
      "10                                                    no   \n",
      "11                                                    no   \n",
      "12                                                    no   \n",
      "13                                                    no   \n",
      "14                                                    no   \n",
      "15                                                    no   \n",
      "16                                                    no   \n",
      "17                                                    no   \n",
      "\n",
      "    total_estimated_cost_x_1000_escalated  adnu_cost_rate_x_1000_escalated  \n",
      "0                                       0                                0  \n",
      "1                                       0                                0  \n",
      "2                                       0                                0  \n",
      "3                                       0                                0  \n",
      "4                                       0                                0  \n",
      "5                                       0                                0  \n",
      "6                                       0                                0  \n",
      "7                                       0                                0  \n",
      "8                                       0                                0  \n",
      "9                                       0                                0  \n",
      "10                                      0                                0  \n",
      "11                                      0                                0  \n",
      "12                                      0                                0  \n",
      "13                                      0                                0  \n",
      "14                                      0                                0  \n",
      "15                                      0                                0  \n",
      "16                                      0                                0  \n",
      "17                                      0                                0  \n",
      "Itemized rows saved to 'costs_phase_2_cluster_13_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_13_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU']\n",
      "[1802 1806 1811 1812 1821 1822 1823 1824]\n",
      "[13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_62678/3368979100.py:187: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_62678/3368979100.py:187: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_62678/3368979100.py:187: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/ph2_rawdata_cluster13_style_N_addendums.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'cost allocation factor' in df.columns:\n",
    "#    df['description'] = df['cost allocation factor'].apply(extract_non_numeric_text)\n",
    " #   df['cost_allocation_factor'] = df['cost allocation factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "#\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "            \"unnamed_1\",\n",
    "             ],\n",
    "\n",
    "        \n",
    "        \"estimated_cost_x_1000\": [\n",
    "\n",
    "            \"estimated cost x 1000\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"allocated_cost\",\n",
    "            \"assigned cost\",\n",
    "            \"allocated cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"project allocated cost\",\n",
    "            \"assigned cost\",\n",
    "            \"unnamed_14\",\n",
    "            \"unnamed_4\",\n",
    "            \"unnamed_13\",\n",
    "            \"allocat ed cost\",\n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"assigned cost escalated\",\n",
    "            \"assigned escalated cost\",\n",
    "            \n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"project allocated cost escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"unnamed_5\",\n",
    "            \"unnamed_16\",\n",
    "            \n",
    "           \n",
    "\n",
    "        ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "            \"max of estimated time to construct\",\n",
    "            \n",
    "            \"unnamed_6\",\n",
    "            \"unnamed_8\",\n",
    "            \"unnamed_9\",\n",
    "            \"nu estimated time to construct\",\n",
    "           \n",
    "            \"unnamed_18\",\n",
    "            \"unnamed_19\",\n",
    "             \n",
    "            \"time to construct\",\n",
    "             \n",
    "\n",
    "\n",
    "        ],\n",
    "\n",
    "        \"network upgrade type\": [\n",
    "            \"network upgrade type\",\n",
    "            \"unnamed_2\",\n",
    "        ],\n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"nu total cost\",\n",
    "            \"total cost constant\",\n",
    "            \"sum of total cost constant\",\n",
    "            \"total cost\",\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\",\n",
    "            \"nu total cost escalated\",\n",
    "            \"total cost escalated\",\n",
    "\n",
    "        ],\n",
    "       \n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "\n",
    "        \"adnu_cost_rate_escalated_x_1000\": [\n",
    "        \"cost rate escalated\",\n",
    "        \"escalated cost rate\",\n",
    "        ],\n",
    "\n",
    "        \"description\": [\"description\", \"unnamed_3\"],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "            \"project allocation\",\n",
    "            \n",
    "\n",
    "        ],\n",
    "        \"estimated cost x 1000 escalated with itcca\": [\n",
    "            \"estimated cost x 1000 escalated with itcca\",\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#if 'estimated_time_to_construct' in df.columns:\n",
    "#    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "#if 'upgrade' in df.columns:\n",
    " #   df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    "#if 'cost_allocation_factor' in df.columns:\n",
    "#    df['description'] = df['cost_allocation_factor'].apply(extract_non_numeric_text)\n",
    " #   df['cost_allocation_factor'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values    \n",
    "\n",
    "\n",
    "# Clear cost_allocation_factor for rows where upgrade is \"Total\" (case-insensitive)\n",
    "#df.loc[df['upgrade'].str.lower() == 'total', 'cost_allocation_factor'] = None  # or use \"\" if you prefer an empty string\n",
    "\n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    "\n",
    "#df.drop(['unnamed_3', 'unnamed_15', 'unnamed_18', 'unnamed_16', 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "df.drop([   'table_8_missing',\t \"allocated\", \"time to\",\t\"unnamed_10\",\t \"unnamed_7\", 'estimated cost x 1000 escalated with itcca', 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    " \n",
    "\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def move_description_to_upgrade(df):\n",
    "    \"\"\"Moves description to upgrade if type_of_upgrade is 'PTO' and upgrade is empty.\"\"\"\n",
    "    \n",
    "    # Ensure columns are strings and replace NaNs with empty strings for processing\n",
    "    df['upgrade'] = df['upgrade'].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
    "    df['description'] = df['description'].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
    "\n",
    "    # Debug: Print before update\n",
    "    #print(\"Before update (only PTO rows):\")\n",
    "    #print(df[df['type_of_upgrade'] == 'PTO'][['type_of_upgrade', 'upgrade', 'description']])\n",
    "\n",
    "    # Apply row-wise transformation\n",
    "    def move_if_empty(row):\n",
    "        if row['type_of_upgrade'] == 'PTO' and row['upgrade'].strip() == \"\" and row['description'].strip() != \"\":\n",
    "            row['upgrade'] = row['description']  # Move description to upgrade\n",
    "            row['description'] = None # Clear description\n",
    "        return row\n",
    "\n",
    "    df = df.apply(move_if_empty, axis=1)\n",
    "\n",
    "    # Debug: Print after update\n",
    "    #print(\"\\nAfter update (only PTO rows):\")\n",
    "    #print(df[df['type_of_upgrade'] == 'PTO'][['type_of_upgrade', 'upgrade', 'description']])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply function\n",
    "#df = move_description_to_upgrade(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# STEP 8½: For certain q_ids with no cost data, insert placeholder rows\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# list of q_ids that need 3 “empty” upgrade rows\n",
    "placeholder_qids = [ 1822, 1823 ]   # ← your set of missing‐data q_ids\n",
    "\n",
    "# columns that carry only the base data\n",
    "base_cols = [\n",
    "    \"q_id\",\n",
    "    \"cluster\",\n",
    "    \"req_deliverability\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"capacity\",\n",
    "    \"point_of_interconnection\",\n",
    "]\n",
    "\n",
    "# the three upgrade codes to inject\n",
    "placeholder_upgrades = [\"PTO_IF\", \"RNU\", \"LDNU\"]\n",
    "\n",
    "new_rows = []\n",
    "for qid in placeholder_qids:\n",
    "    # find base‐data rows for this qid\n",
    "    base_mask = df[\"q_id\"] == qid\n",
    "    if not base_mask.any():\n",
    "        print(f\"Warning: no row at all for q_id {qid}, skipping placeholders\")\n",
    "        continue\n",
    "\n",
    "    # extract the one base row\n",
    "    base = df.loc[base_mask, base_cols].iloc[0]\n",
    "\n",
    "    # build three empty‐detail rows\n",
    "    for code in placeholder_upgrades:\n",
    "        row = {c: \"\" for c in df.columns}\n",
    "        # copy only the base columns\n",
    "        for c in base_cols:\n",
    "            row[c] = base[c]\n",
    "        # assign the placeholder upgrade code\n",
    "        row[\"type_of_upgrade\"] = code\n",
    "        new_rows.append(row)\n",
    "\n",
    "# append placeholders if any\n",
    "if new_rows:\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    # re‐order columns so the new rows fit your desired layout\n",
    "    df = reorder_columns(df)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# now continue with STEP 9: Create Total rows…\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "df['tot'] = df.apply(\n",
    "    lambda row: 'yes' if (\n",
    "        (pd.notna(row.get('upgrade')) and 'Total' in str(row['upgrade']))  \n",
    "    ) else 'no',\n",
    "    axis=1\n",
    ") \n",
    "\n",
    "# Now extract ONLY \"Total\" rows with a foolproof match\n",
    "total_rows_df = df[df['tot'] == 'yes']\n",
    "\n",
    "total_rows_df = total_rows_df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "total_rows_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_N_total_network.csv', index=False) \n",
    "\n",
    "df = df[df['network_upgrade_type']\n",
    "          .astype(str)\n",
    "          .str.strip()\n",
    "          .str.lower()\n",
    "          .fillna('')\n",
    "          .ne('total')]\n",
    "\n",
    "df = df[df['upgrade'].str.strip().str.lower() != 'total']\n",
    "#df = df[df['network_upgrade_type'].str.strip().str.lower() != 'total']\n",
    " \n",
    "\n",
    " \n",
    "df.drop('tot', axis=1, inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)  \n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"PTO's Interconnection Facilities (Note 1)\": \"PTO_IF\",\n",
    " \"PTO's Interconnection Facilities (Note 4)\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    "}\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'escalated_cost_rate','total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000',  ]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    " \n",
    "\n",
    "import re\n",
    "\n",
    "# Define the list of upgrade phrases\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    "def classify_and_clean_upgrades(df):\n",
    "    \"\"\"\n",
    "    For rows with type_of_upgrade equal to \"RNU\", moves the value from \n",
    "    'network_upgrade_type' to a new 'upgrade' column, and then removes rows where:\n",
    "      - 'network_upgrade_type' exactly matches an upgrade phrase, or\n",
    "      - 'network_upgrade_type' matches the pattern 'Total <upgrade_phrase>'.\n",
    "    Other rows remain unchanged.\n",
    "    \"\"\"\n",
    "    # Ensure 'network_upgrade_type' is a string and fill missing values with an empty string\n",
    "    df['network_upgrade_type'] = df['network_upgrade_type'].astype(str).fillna(\"\")\n",
    "    \n",
    "    # Process only rows where type_of_upgrade is \"RNU\"\n",
    "    rnu_mask = df['type_of_upgrade'] == \"RNU\"\n",
    "    \n",
    "    # For these rows, copy the network_upgrade_type to a new column called 'upgrade'\n",
    "    df.loc[rnu_mask, 'upgrade'] = df.loc[rnu_mask, 'network_upgrade_type']\n",
    "    \n",
    "    # Build a regex pattern to match strings like \"Total IRNU\", \"Total GRNU\", etc.\n",
    "    pattern = r\"^Total\\s+(\" + \"|\".join(re.escape(phrase) for phrase in upgrade_phrases) + r\")$\"\n",
    "    \n",
    "    # Create a condition that checks for rows (within type_of_upgrade==\"RNU\") that should be removed:\n",
    "    # - The network_upgrade_type is exactly one of the upgrade phrases.\n",
    "    # - OR the network_upgrade_type matches the pattern \"Total <upgrade_phrase>\".\n",
    "    remove_condition = rnu_mask & (\n",
    "        df['network_upgrade_type'].isin(upgrade_phrases) |\n",
    "        df['network_upgrade_type'].str.match(pattern)\n",
    "    )\n",
    "    \n",
    "    # Remove the rows meeting the remove_condition\n",
    "    df = df[~remove_condition]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the updated function\n",
    "df = classify_and_clean_upgrades(df)\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "# only drop rows whose upgrade is exactly \"total\"\n",
    "df = df[df['upgrade']\n",
    "          .astype(str)            # ensure it's string dtype\n",
    "          .str.strip()\n",
    "          .str.lower()\n",
    "          .fillna('')             # turn NaN → ''\n",
    "          .ne('total')]           # keep all rows not equal to 'total'\n",
    "\n",
    "\n",
    " \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)  \n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/03_raw/cluster_13_style_N.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'adnu_cost_rate_x_1000_escalated']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ((group['type_of_upgrade'] == f\"Total {upgrade}\") & (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['type_of_upgrade'] == f\"Total {upgrade}\") & (group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    " \n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "     \n",
    "     \n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_N_itemized_addendums.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    \n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/costs_phase_2_cluster_13_style_N_total_addendums.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_13_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_13_total.csv'.\")\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_N_itemized_updated.csv\n",
      "Saved → /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_N_total_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) I/O helpers\n",
    "# -------------------------------------------------------------------\n",
    "def load_data(file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Load a CSV file and ensure specific columns are treated as character.\n",
    "    \"\"\"\n",
    "    # Read only header to get available columns\n",
    "    available = pd.read_csv(file_path, nrows=0).columns\n",
    "    # Only enforce dtype for those char_columns present\n",
    "    dtypes = {c: str for c in char_columns if c in available}\n",
    "    return pd.read_csv(\n",
    "        file_path,\n",
    "        dtype=dtypes,\n",
    "        na_values=[],            # don’t auto‑convert blanks to NaN\n",
    "        keep_default_na=False\n",
    "    )\n",
    "\n",
    "def save_data(df, file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Save df to CSV, casting char_columns back to strings.\n",
    "    \"\"\"\n",
    "    for c in char_columns:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved → {file_path}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Merge logic\n",
    "# -------------------------------------------------------------------\n",
    "def merge_with_addendums(orig, add, on=[\"q_id\",\"type_of_upgrade\"]):\n",
    "    \"\"\"\n",
    "    For any group defined by `on` columns, if it appears in `add`, drop it from `orig`\n",
    "    and then concatenate all of `add`.  Otherwise keep orig.\n",
    "    \"\"\"\n",
    "    add_idx = add.set_index(on).index\n",
    "    orig_idxed = orig.set_index(on, drop=False)\n",
    "    mask = ~orig_idxed.index.isin(add_idx)\n",
    "    orig_kept = orig_idxed[mask].reset_index(drop=True)\n",
    "    merged = pd.concat([orig_kept, add], ignore_index=True, sort=False)\n",
    "    return merged.sort_values(by=on, kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Main\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # character columns you never want auto‑converted\n",
    "    char_columns = [\n",
    "        \"req_deliverability\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"estimated_time_to_construct\",\n",
    "        \"original\",\n",
    "        \"item\"\n",
    "    ]\n",
    "\n",
    "    # file‑paths (Cluster 13, Style Q)\n",
    "    BASE = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate\"\n",
    "    ITEM    = f\"{BASE}/costs_phase_2_cluster_13_style_N_itemized.csv\"\n",
    "    ITEM_AD = f\"{BASE}/costs_phase_2_cluster_13_style_N_itemized_addendums.csv\"\n",
    "    TOT     = f\"{BASE}/costs_phase_2_cluster_13_style_N_total.csv\"\n",
    "    TOT_AD  = f\"{BASE}/costs_phase_2_cluster_13_style_N_total_addendums.csv\"\n",
    "\n",
    "    CLEAN_BASE = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean\"\n",
    "    ITEM_OUT = f\"{CLEAN_BASE}/costs_phase_2_cluster_13_style_N_itemized_updated.csv\"\n",
    "    TOT_OUT  = f\"{CLEAN_BASE}/costs_phase_2_cluster_13_style_N_total_updated.csv\"\n",
    "\n",
    "    # load\n",
    "    itemized           = load_data(ITEM,    char_columns)\n",
    "    itemized_addendums = load_data(ITEM_AD, char_columns)\n",
    "    total              = load_data(TOT,     char_columns)\n",
    "    total_addendums    = load_data(TOT_AD,  char_columns)\n",
    "\n",
    "    # merge\n",
    "    updated_itemized = merge_with_addendums(itemized,           itemized_addendums)\n",
    "    updated_total    = merge_with_addendums(total,              total_addendums)\n",
    "\n",
    "    # save\n",
    "    save_data(updated_itemized, ITEM_OUT, char_columns)\n",
    "    save_data(updated_total,    TOT_OUT,  char_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded itemized data from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_N_itemized_updated.csv\n",
      "Loaded totals data from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_N_total_updated.csv\n",
      "\n",
      "Q_ids with missing upgrades:\n",
      "  Q_id 1802 is missing upgrades: LDNU, ADNU, CANU, LOPNU\n",
      "  Q_id 1806 is missing upgrades: LDNU, ADNU, CANU, LOPNU\n",
      "  Q_id 1810 is missing upgrades: LDNU, ADNU, CANU, LOPNU\n",
      "  Q_id 1811 is missing upgrades: LDNU, ADNU, CANU, LOPNU\n",
      "  Q_id 1812 is missing upgrades: LDNU, ADNU, CANU, LOPNU\n",
      "  Q_id 1815 is missing upgrades: LDNU, ADNU, CANU, LOPNU\n",
      "  Q_id 1818 is missing upgrades: ADNU, CANU, LOPNU\n",
      "  Q_id 1820 is missing upgrades: LDNU, ADNU, CANU, LOPNU\n",
      "  Q_id 1821 is missing upgrades: LDNU, ADNU, CANU, LOPNU\n",
      "  Q_id 1822 is missing upgrades: ADNU, CANU, LOPNU\n",
      "  Q_id 1823 is missing upgrades: ADNU, CANU, LOPNU\n",
      "  Q_id 1824 is missing upgrades: LDNU, ADNU, CANU, LOPNU\n",
      "  Q_id 1825 is missing upgrades: LDNU, ADNU, CANU, LOPNU\n",
      "\n",
      "No type of upgrade is repeated for any Q_id.\n",
      "\n",
      "Mismatches saved to '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/ph2_mismatches_N.csv'.\n",
      "\n",
      "Direct Matches (Exact Point of Interconnection Names):\n",
      "                     point_of_interconnection                q_id\n",
      "5  500 KV COMMON BUS AT HASSAYAMPA SWITCHYARD  [1818, 1822, 1823]\n",
      "6            69 KV BUS AT CREELMAN SUBSTATION        [1815, 1820]\n",
      "\n",
      "Fuzzy Matches (>=80% Similarity in Point of Interconnection):\n",
      "             point_of_interconnection_1       q_ids_1  \\\n",
      "0          69 KV BUS AT OTAY SUBSTATION        [1802]   \n",
      "1          69 KV BUS AT OTAY SUBSTATION        [1802]   \n",
      "2          69 KV BUS AT OTAY SUBSTATION        [1802]   \n",
      "3          69 KV BUS AT OTAY SUBSTATION        [1802]   \n",
      "4          69 KV BUS AT OTAY SUBSTATION        [1802]   \n",
      "5    230 KV BUS AT OTAY MESA SUBSTATION        [1810]   \n",
      "6  500 KV BUS AT HOODOO WASH SWITCHYARD        [1812]   \n",
      "7      69 KV BUS AT CREELMAN SUBSTATION  [1815, 1820]   \n",
      "\n",
      "                   point_of_interconnection_2             q_ids_2  \\\n",
      "0          230 KV BUS AT OTAY MESA SUBSTATION              [1810]   \n",
      "1            69 KV BUS AT CREELMAN SUBSTATION        [1815, 1820]   \n",
      "2         69 KV BUS AT OCEAN RANCH SUBSTATION              [1825]   \n",
      "3         138 KV BUS AT LOS COCHES SUBSTATION              [1811]   \n",
      "4          230 KV BUS AT ESCONDIDO SUBSTATION              [1821]   \n",
      "5          230 KV BUS AT ESCONDIDO SUBSTATION              [1821]   \n",
      "6  500 KV COMMON BUS AT HASSAYAMPA SWITCHYARD  [1818, 1822, 1823]   \n",
      "7         69 KV BUS AT OCEAN RANCH SUBSTATION              [1825]   \n",
      "\n",
      "   similarity_score  \n",
      "0                94  \n",
      "1                90  \n",
      "2                90  \n",
      "3                83  \n",
      "4                83  \n",
      "5                83  \n",
      "6                80  \n",
      "7                84  \n",
      "Matched Q_ids saved to '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/ph2_matched_qids_N.csv'.\n",
      "\n",
      "Total checks performed: 29\n",
      "Total mismatches found: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "# Paths to the CSV files\n",
    "ITEMIZED_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_N_itemized_updated.csv'\n",
    "TOTALS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/01_clean/costs_phase_2_cluster_13_style_N_total_updated.csv'\n",
    "\n",
    "# Columns in totals_df that hold the reported total costs\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "# Upgrade types to check\n",
    "REQUIRED_UPGRADES = ['PTO_IF', 'RNU', 'LDNU', 'ADNU', 'CANU', \"LOPNU\"]\n",
    "\n",
    "# Output paths\n",
    "MISMATCHES_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/ph2_mismatches_N.csv'\n",
    "MATCHED_QIDS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 13/02_intermediate/ph2_matched_qids_N.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "def load_csv(path, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {dataset_name} from {path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Load datasets\n",
    "itemized_df = load_csv(ITEMIZED_CSV_PATH, \"itemized data\")\n",
    "totals_df = load_csv(TOTALS_CSV_PATH, \"totals data\")\n",
    "\n",
    "# ---------------------- Data Cleaning ---------------------- #\n",
    "\n",
    "def clean_text(df, column):\n",
    "    \"\"\"\n",
    "    Cleans text data by stripping leading/trailing spaces and converting to uppercase.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        df[column] = df[column].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        print(f\"Warning: '{column}' column is missing in the dataset. Filling with 'UNKNOWN'.\")\n",
    "        df[column] = 'UNKNOWN'\n",
    "    return df\n",
    "\n",
    "# Clean 'type_of_upgrade' and 'point_of_interconnection' in both datasets\n",
    "itemized_df = clean_text(itemized_df, 'type_of_upgrade')\n",
    "itemized_df = clean_text(itemized_df, 'point_of_interconnection')\n",
    "\n",
    "totals_df = clean_text(totals_df, 'type_of_upgrade')\n",
    "totals_df = clean_text(totals_df, 'point_of_interconnection')\n",
    "\n",
    "# ---------------------- Data Preparation ---------------------- #\n",
    "\n",
    "# Ensure necessary columns exist in itemized_df\n",
    "required_itemized_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', 'estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in required_itemized_columns:\n",
    "    if col not in itemized_df.columns:\n",
    "        print(f\"Warning: '{col}' column is missing in the itemized dataset.\")\n",
    "        if col in ['q_id', 'type_of_upgrade', 'point_of_interconnection']:\n",
    "            itemized_df[col] = 'UNKNOWN'\n",
    "        else:\n",
    "            itemized_df[col] = 0\n",
    "\n",
    "# Ensure necessary columns exist in totals_df\n",
    "required_totals_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in required_totals_columns:\n",
    "    if col not in totals_df.columns:\n",
    "        print(f\"Error: '{col}' column is missing in the totals dataset. Cannot proceed.\")\n",
    "        exit(1)\n",
    "\n",
    "# Convert cost columns to numeric, coercing errors to NaN and filling with 0\n",
    "cost_columns_itemized = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in cost_columns_itemized:\n",
    "    itemized_df[col] = pd.to_numeric(itemized_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "cost_columns_totals = [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in cost_columns_totals:\n",
    "    totals_df[col] = pd.to_numeric(totals_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# ---------------------- Calculate Manual Totals ---------------------- #\n",
    "\n",
    "# Group itemized data by q_id and type_of_upgrade and calculate sums\n",
    "itemized_grouped = itemized_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    'estimated_cost_x_1000': 'sum',\n",
    "    'escalated_cost_x_1000': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "itemized_grouped['manual_total'] = itemized_grouped.apply(\n",
    "    lambda row: row['estimated_cost_x_1000'] if row['estimated_cost_x_1000'] > 0 else row['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Prepare Totals Data ---------------------- #\n",
    "\n",
    "# Group totals data by q_id and type_of_upgrade and calculate sums\n",
    "totals_grouped = totals_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "    TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "totals_grouped['reported_total'] = totals_grouped.apply(\n",
    "    lambda row: row[TOTALS_ESTIMATED_COLUMN] if row[TOTALS_ESTIMATED_COLUMN] > 0 else row[TOTALS_ESCALATED_COLUMN],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Merge Data ---------------------- #\n",
    "\n",
    "# Merge the itemized and totals data on q_id and type_of_upgrade\n",
    "comparison_df = pd.merge(\n",
    "    itemized_grouped,\n",
    "    totals_grouped[['q_id', 'type_of_upgrade', 'reported_total']],\n",
    "    on=['q_id', 'type_of_upgrade'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---------------------- Check for Missing Upgrades ---------------------- #\n",
    "\n",
    "# Identify q_ids that are missing any of the required upgrades\n",
    "missing_upgrades_report = []\n",
    "for q_id in comparison_df['q_id'].unique():\n",
    "    upgrades_present = comparison_df[comparison_df['q_id'] == q_id]['type_of_upgrade'].unique()\n",
    "    missing_upgrades = [upgrade for upgrade in REQUIRED_UPGRADES if upgrade not in upgrades_present]\n",
    "    if missing_upgrades:\n",
    "        missing_upgrades_report.append((q_id, missing_upgrades))\n",
    "\n",
    "# Report missing upgrades\n",
    "if missing_upgrades_report:\n",
    "    print(\"\\nQ_ids with missing upgrades:\")\n",
    "    for q_id, missing in missing_upgrades_report:\n",
    "        print(f\"  Q_id {q_id} is missing upgrades: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nAll q_ids have all required upgrades.\")\n",
    "\n",
    "\n",
    "# ------------------------ Check for no duplicates in type of upgrade in total data ------------------------ #\n",
    "\n",
    " \n",
    "\n",
    "# Identify duplicates by grouping by q_id and type_of_upgrade\n",
    "duplicates = totals_df[totals_df.duplicated(subset=['q_id', 'type_of_upgrade'], keep=False)]\n",
    "\n",
    "if not duplicates.empty:\n",
    "    print(\"\\nDuplicate upgrade types detected:\")\n",
    "    for q_id, group in duplicates.groupby('q_id'):\n",
    "        upgrade_types = group['type_of_upgrade'].unique()\n",
    "        print(f\"  Q_id {q_id} has repeated upgrade types: {', '.join(upgrade_types)}\")\n",
    "else:\n",
    "    print(\"\\nNo type of upgrade is repeated for any Q_id.\")        \n",
    "\n",
    "# ---------------------- Compare Totals and Identify Mismatches ---------------------- #\n",
    "\n",
    "# Initialize list to store mismatches\n",
    "mismatches = []\n",
    "\n",
    "# Iterate through each row to compare manual_total with reported_total\n",
    "for index, row in comparison_df.iterrows():\n",
    "    q_id = row['q_id']\n",
    "    upgrade = row['type_of_upgrade']\n",
    "    manual_total = row['manual_total']\n",
    "    reported_total = row['reported_total']\n",
    "    \n",
    "    # Determine if both manual_total and reported_total are zero\n",
    "    if manual_total == 0.0 and reported_total == 0.0:\n",
    "        continue  # No mismatch\n",
    "    # Determine if manual_total is zero and reported_total is missing or zero\n",
    "    elif manual_total == 0.0 and (pd.isna(row['reported_total']) or reported_total == 0.0):\n",
    "        continue  # No mismatch\n",
    "    # If reported_total is missing (NaN) and manual_total is not zero\n",
    "    elif pd.isna(row['reported_total']) and manual_total != 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: Missing\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': 'Missing'\n",
    "        })\n",
    "    # If manual_total is not zero and reported_total is zero\n",
    "    elif manual_total != 0.0 and reported_total == 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: 0.0\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # If both totals are non-zero but differ beyond tolerance\n",
    "    elif abs(manual_total - reported_total) > 1e+2:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: {reported_total}\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # Else, totals match; do nothing\n",
    "\n",
    "# Create a DataFrame for mismatches\n",
    "mismatches_df = pd.DataFrame(mismatches, columns=['q_id', 'type_of_upgrade', 'manual_total', 'reported_total'])\n",
    "\n",
    "# Save mismatches to a CSV file\n",
    "try:\n",
    "    mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "    print(f\"\\nMismatches saved to '{MISMATCHES_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving mismatches CSV: {e}\")\n",
    "\n",
    "# ---------------------- Point of Interconnection Matching ---------------------- #\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from itemized dataset\n",
    "itemized_poi = itemized_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from totals dataset\n",
    "totals_poi = totals_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Merge both to have a complete list of q_id and point_of_interconnection\n",
    "all_poi = pd.concat([itemized_poi, totals_poi]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ---------------------- Direct Match Identification ---------------------- #\n",
    "\n",
    "# Group by point_of_interconnection to find q_ids sharing the same point_of_interconnection\n",
    "direct_matches = all_poi.groupby('point_of_interconnection')['q_id'].apply(list).reset_index()\n",
    "\n",
    "# Filter groups with more than one q_id (i.e., shared points_of_interconnection)\n",
    "direct_matches = direct_matches[direct_matches['q_id'].apply(len) > 1]\n",
    "\n",
    "print(\"\\nDirect Matches (Exact Point of Interconnection Names):\")\n",
    "if not direct_matches.empty:\n",
    "    print(direct_matches)\n",
    "else:\n",
    "    print(\"No direct matches found.\")\n",
    "\n",
    "# ---------------------- Fuzzy Match Identification ---------------------- #\n",
    "\n",
    "# Prepare list of points_of_interconnection for fuzzy matching\n",
    "poi_list = all_poi['point_of_interconnection'].unique().tolist()\n",
    "\n",
    "# Initialize list to store fuzzy matches\n",
    "fuzzy_matches = []\n",
    "\n",
    "# Iterate through each point_of_interconnection to find similar ones\n",
    "for i, poi in enumerate(poi_list):\n",
    "    # Compare with the rest of the points to avoid redundant comparisons\n",
    "    similar_pois = process.extract(poi, poi_list[i+1:], scorer=fuzz.token_set_ratio)\n",
    "    \n",
    "    # Filter matches with similarity >= 80%\n",
    "    for match_poi, score in similar_pois:\n",
    "        if score >= 80:\n",
    "            # Retrieve q_ids for both points_of_interconnection\n",
    "            qids_poi1 = all_poi[all_poi['point_of_interconnection'] == poi]['q_id'].tolist()\n",
    "            qids_poi2 = all_poi[all_poi['point_of_interconnection'] == match_poi]['q_id'].tolist()\n",
    "            \n",
    "            # Append the matched pairs with their points_of_interconnection and similarity score\n",
    "            fuzzy_matches.append({\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_ids_1': qids_poi1,\n",
    "                'point_of_interconnection_2': match_poi,\n",
    "                'q_ids_2': qids_poi2,\n",
    "                'similarity_score': score\n",
    "            })\n",
    "\n",
    "# Convert fuzzy matches to DataFrame\n",
    "fuzzy_matches_df = pd.DataFrame(fuzzy_matches)\n",
    "\n",
    "print(\"\\nFuzzy Matches (>=80% Similarity in Point of Interconnection):\")\n",
    "if not fuzzy_matches_df.empty:\n",
    "    print(fuzzy_matches_df)\n",
    "else:\n",
    "    print(\"No fuzzy matches found.\")\n",
    "\n",
    "# ---------------------- Save Matched Q_ids to CSV ---------------------- #\n",
    "\n",
    "# For clarity, create a combined DataFrame for direct and fuzzy matches\n",
    "\n",
    "# Direct matches: list each pair of q_ids sharing the same point_of_interconnection\n",
    "direct_matches_expanded = []\n",
    "for _, row in direct_matches.iterrows():\n",
    "    qids = row['q_id']\n",
    "    poi = row['point_of_interconnection']\n",
    "    # Generate all possible unique pairs\n",
    "    for i in range(len(qids)):\n",
    "        for j in range(i+1, len(qids)):\n",
    "            direct_matches_expanded.append({\n",
    "                'match_type': 'Direct',\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_id_1': qids[i],\n",
    "                'point_of_interconnection_2': poi,\n",
    "                'q_id_2': qids[j],\n",
    "                'similarity_score': 100\n",
    "            })\n",
    "\n",
    "# Fuzzy matches: already have pairs\n",
    "fuzzy_matches_expanded = []\n",
    "for _, row in fuzzy_matches_df.iterrows():\n",
    "    fuzzy_matches_expanded.append({\n",
    "        'match_type': 'Fuzzy',\n",
    "        'point_of_interconnection_1': row['point_of_interconnection_1'],\n",
    "        'q_id_1': row['q_ids_1'],\n",
    "        'point_of_interconnection_2': row['point_of_interconnection_2'],\n",
    "        'q_id_2': row['q_ids_2'],\n",
    "        'similarity_score': row['similarity_score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "matched_qids_df = pd.DataFrame(direct_matches_expanded + fuzzy_matches_expanded)\n",
    "\n",
    "# Save matched q_ids to CSV\n",
    "try:\n",
    "    matched_qids_df.to_csv(MATCHED_QIDS_CSV_PATH, index=False)\n",
    "    print(f\"Matched Q_ids saved to '{MATCHED_QIDS_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving matched Q_ids CSV: {e}\")\n",
    "\n",
    "# ---------------------- Summary ---------------------- #\n",
    "\n",
    "# Print a summary\n",
    "total_checked = len(comparison_df)\n",
    "total_mismatches = len(mismatches_df)\n",
    "print(f\"\\nTotal checks performed: {total_checked}\")\n",
    "print(f\"Total mismatches found: {total_mismatches}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
