{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSGIP Style R- table 11.1 and/or 11.SGIP-TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process these project‐folders:\n",
      "['521', '522C', '522', '541', '555', '568', '576', '579', '585', '586', '628', '632C', '632AA', '640', '642', '643', '643AS', '643AA', '643AF', '643AH', '643AI', '643I', '643G', '643R', '643F', '643O', '643Z', '643S', '643T', '643AE', '643AB', '643AK', '643AP', '643AM', '643AJ', '643AC', '643X', '643J', '643D', '643W', '643E', '644A', '644', '645A', '647', '649A', '649', '649C', '649B', '650AA', '650A', '650AC', '651', '651A', '653EA', '653D', '653E', '653B', '653EB', '653ED', '653H', '653A', '653F', '653']\n",
      "Table Title: Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: QC1QC2PII_Northern_Appendix A_Q521_Columbia 1.pdf from Project 521\n",
      "Table Title: Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q521 Attachment 1.pdf from Project 521\n",
      "Skipped PDF: 09AS692646-SCE_Recurrent_Projects_Ph_II_RM_Min__Final.pdf from Project 522 (No Table 11)\n",
      "Table Title: Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: QC1QC2PII_Northern_Appendix A_Q522_Columbia 2.pdf from Project 522\n",
      "Table Title: Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q522 Attachment 1.pdf from Project 522\n",
      "Table Title: Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: QC1QC2PII_Northern_Appendix A_Q628_FRV Mojave Solar 4.pdf from Project 628\n",
      "Skipped PDF: Appendix A - Q632C C1C2 Phase II report-final.pdf from Project 632C (No Table 11)\n",
      "Table Title: Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: QC1QC2PII_Eastern_Q632AA_MV Solar_Final.pdf from Project 632AA\n",
      "Skipped PDF: C3C4P2-Northern-Q643AA-Blue Sky Wind Energy Center-AppendixA.pdf from Project 643AA (No Table 11)\n",
      "Skipped Addendum PDF: 10AS665623-QC34PIINorthernQ643AAAddendum_20121227.pdf from Project 643AA (No Table 11 and original does not have Table 11)\n",
      "Skipped PDF: C3C4P2-SCE EOP-Q643AI-AppendixA.pdf from Project 643AI (No Table 11)\n",
      "Skipped Addendum PDF: 10AS670281-C3C4PII_Q643AI_Addendum.pdf from Project 643AI (No Table 11 and original does not have Table 11)\n",
      "Skipped PDF: 10AS666507-QC34PII_Appendix_A_PGE_Q643I_.pdf from Project 643I (No Table 11)\n",
      "Skipped PDF: 10AS664297-C3C4P2NorthernQ643RWillow_Spring_2AppendixA.pdf from Project 643R (No Table 11)\n",
      "Skipped Addendum PDF: 10AS664297-QC34PIINorthernQ643RAddendum_20130104.pdf from Project 643R (No Table 11 and original does not have Table 11)\n",
      "Skipped PDF: 10AS669159-QC_34_Phase_II_PGE_North_Group_Report.pdf from Project 643F (Table 11 found but extraction failed)\n",
      "Skipped PDF: 10AS666745-QC_34_Phase_II_PGE_North_Group_Report.pdf from Project 643O (Table 11 found but extraction failed)\n",
      "Skipped PDF: C3C4P2-Eastern-Q643AEAppendixA-Final.pdf from Project 643AE (No Table 11)\n",
      "Skipped PDF: 10AS670009-C3C4P2EastQ643AEAppndxAReissued.pdf from Project 643AE (No Table 11)\n",
      "Skipped PDF: 10AS664331-C3C4P2NorthernQ643AJNorth_RosamondAppendixA.pdf from Project 643AJ (No Table 11)\n",
      "Skipped Addendum PDF: 10AS664331-QC34PIINorthernQ643AJAddendum_20130104.pdf from Project 643AJ (No Table 11 and original does not have Table 11)\n",
      "Skipped PDF: C3C4P2-Eastern-Q643ACAppendixA-Final.pdf from Project 643AC (No Table 11)\n",
      "Skipped Addendum PDF: 10AS684173-QC34PIIQ643ACAddendum.pdf from Project 643AC (No Table 11 and original does not have Table 11)\n",
      "Skipped PDF: 10AS669533-C3C4Phase_II_AppendixAStudyFinal_05NOV2012.pdf from Project 643W (No Table 11)\n",
      "Skipped Addendum PDF: 10AS669533-C3C4Phase_II_Addendum2AppendixAFinal_06MARCH2013.pdf from Project 643W (No Table 11 and original does not have Table 11)\n",
      "Skipped Addendum PDF: 10AS669533-C3C4Phase_II_Addendum3AppendixAFinal_17APRIL2013.pdf from Project 643W (No Table 11 and original does not have Table 11)\n",
      "Skipped Addendum PDF: 10AS669533-C3C4Phase_II__Addendum1AppendixAFinal_17JAN2013.pdf from Project 643W (No Table 11 and original does not have Table 11)\n",
      "Table Title: SDG&E Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Table Title: SCE Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: 10AS693794-Jacumba_Solar_Farm_Q644A_Appendix_A___11092012.pdf from Project 644A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_43972/101970297.py:802: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'SDG&E Upgrades, Estimated Costs, and Estimated Time to Construct Summary' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_43972/101970297.py:802: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'SCE Upgrades, Estimated Costs, and Estimated Time to Construct Summary' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped PDF: Appendix A - Q644 C1C2 Phase II report-final.pdf from Project 644 (No Table 11)\n",
      "Skipped PDF: QC1&2P2_PG&E_Q644_revision.pdf from Project 644 (No Table 11)\n",
      "Skipped Addendum PDF: Appendix A - Q644 C1C2 Phase II report-final_r1.pdf from Project 644 (No Table 11 and original does not have Table 11)\n",
      "Skipped PDF: Appendix A - Q645A C1C2 Phase II report-final.pdf from Project 645A (No Table 11)\n",
      "Skipped PDF: QC1QC2PII_Northern Group Report.pdf from Project 649C (Table 11 found but extraction failed)\n",
      "Table Title: Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: QC1QC2PII_Northern_Appendix A_Q650AA_American Solar Greenworks.pdf from Project 650AA\n",
      "Skipped Addendum PDF: Appendix A - Q650AC C1C2 Phase II report-final_r1.pdf from Project 650AC (No Table 11 and original does not have Table 11)\n",
      "Table Title: Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: QC1QC2PII_Northern_Appendix A_Q651A_Acacia.pdf from Project 651A\n",
      "Skipped PDF: Appendix A - Q653EA C1C2 Phase II report-final.pdf from Project 653EA (No Table 11)\n",
      "Skipped PDF: Appendix A - Q653E C1C2 Phase II report-final.pdf from Project 653E (No Table 11)\n",
      "Skipped Addendum PDF: Appendix A - Q653E C1C2 Phase II report-final_r1.pdf from Project 653E (No Table 11 and original does not have Table 11)\n",
      "Skipped Addendum PDF: 10AS689072-Appendix_A__Q653E_C1C2_Phase_II_reportfinal_r11.pdf from Project 653E (No Table 11 and original does not have Table 11)\n",
      "Table Title: below.\n",
      "Table Title: below.\n",
      "Scraped PDF: 10AS689152-Appendix_A__Q653B_Pumpjack_Solar_I_QC34_Ph_II_Study_ReportFinal_CMB_5NOV2012.pdf from Project 653B\n",
      "Skipped PDF: Appendix A - Q653B C1C2 Phase II report-final.pdf from Project 653B (No Table 11)\n",
      "Skipped PDF: 10AS686189-Soitec_PhII_RM_Min__Final.pdf from Project 653ED (No Table 11)\n",
      "Table Title: SDG&E Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: Appendix A - S653ED_08-24-2011_final.pdf from Project 653ED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_43972/101970297.py:802: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'SDG&E Upgrades, Estimated Costs, and Estimated Time to Construct Summary' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_43972/101970297.py:802: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'SDG&E Upgrades, Estimated Costs, and Estimated Time to Construct Summary' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Title: Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Scraped PDF: QC1QC2PII_Northern_Appendix A_Q653H_Western Antelope Dry Ranch.pdf from Project 653H\n",
      "Table Title: Upgrades, Estimated Costs, and Estimated Time to Construct Summary\n",
      "Table Title: PTO Interconnection Facilities Cost Estimate Summary\n",
      "Scraped PDF: Appendix A - Q653F SGIP C1C2 Phase II Final Study Report_CMB 22AUG2011.pdf from Project 653F\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/03_raw/ph2_rawdata_clusterSGIP-TC_style_R_originals.csv\n",
      "No data to save for addendums.\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 64\n",
      "Total Projects Scraped: 11\n",
      "Total Projects Skipped due to failed extraction of Table: 3\n",
      "Total Projects Skipped: 53\n",
      "Total Projects Missing: 0\n",
      "Total PDFs Accessed: 47\n",
      "Total PDFs Scraped: 13\n",
      "Total PDFs Skipped: 30\n",
      "\n",
      "List of Scraped Projects:\n",
      "['521', '522', '628', '632AA', '644A', '650AA', '651A', '653B', '653ED', '653F', '653H']\n",
      "\n",
      "List of Skipped Projects:\n",
      "['522C', '541', '555', '568', '576', '579', '585', '586', '632C', '640', '642', '643', '643AA', '643AB', '643AC', '643AE', '643AF', '643AH', '643AI', '643AJ', '643AK', '643AM', '643AP', '643AS', '643D', '643E', '643F', '643G', '643I', '643J', '643O', '643R', '643S', '643T', '643W', '643X', '643Z', '644', '645A', '647', '649', '649A', '649B', '649C', '650A', '650AC', '651', '653', '653A', '653D', '653E', '653EA', '653EB']\n",
      "\n",
      "List of Missing Projects:\n",
      "[]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['QC1QC2PII_Northern_Appendix A_Q521_Columbia 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q521 Attachment 1.pdf', 'QC1QC2PII_Northern_Appendix A_Q522_Columbia 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q522 Attachment 1.pdf', 'QC1QC2PII_Northern_Appendix A_Q628_FRV Mojave Solar 4.pdf', 'QC1QC2PII_Eastern_Q632AA_MV Solar_Final.pdf', '10AS693794-Jacumba_Solar_Farm_Q644A_Appendix_A___11092012.pdf', 'QC1QC2PII_Northern_Appendix A_Q650AA_American Solar Greenworks.pdf', 'QC1QC2PII_Northern_Appendix A_Q651A_Acacia.pdf', '10AS689152-Appendix_A__Q653B_Pumpjack_Solar_I_QC34_Ph_II_Study_ReportFinal_CMB_5NOV2012.pdf', 'Appendix A - S653ED_08-24-2011_final.pdf', 'QC1QC2PII_Northern_Appendix A_Q653H_Western Antelope Dry Ranch.pdf', 'Appendix A - Q653F SGIP C1C2 Phase II Final Study Report_CMB 22AUG2011.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['09AS692646-SCE_Recurrent_Projects_Ph_II_RM_Min__Final.pdf', 'Appendix A - Q632C C1C2 Phase II report-final.pdf', 'C3C4P2-Northern-Q643AA-Blue Sky Wind Energy Center-AppendixA.pdf', '10AS665623-QC34PIINorthernQ643AAAddendum_20121227.pdf', 'C3C4P2-SCE EOP-Q643AI-AppendixA.pdf', '10AS670281-C3C4PII_Q643AI_Addendum.pdf', '10AS666507-QC34PII_Appendix_A_PGE_Q643I_.pdf', '10AS664297-C3C4P2NorthernQ643RWillow_Spring_2AppendixA.pdf', '10AS664297-QC34PIINorthernQ643RAddendum_20130104.pdf', '10AS669159-QC_34_Phase_II_PGE_North_Group_Report.pdf', '10AS666745-QC_34_Phase_II_PGE_North_Group_Report.pdf', 'C3C4P2-Eastern-Q643AEAppendixA-Final.pdf', '10AS670009-C3C4P2EastQ643AEAppndxAReissued.pdf', '10AS664331-C3C4P2NorthernQ643AJNorth_RosamondAppendixA.pdf', '10AS664331-QC34PIINorthernQ643AJAddendum_20130104.pdf', 'C3C4P2-Eastern-Q643ACAppendixA-Final.pdf', '10AS684173-QC34PIIQ643ACAddendum.pdf', '10AS669533-C3C4Phase_II_AppendixAStudyFinal_05NOV2012.pdf', '10AS669533-C3C4Phase_II_Addendum2AppendixAFinal_06MARCH2013.pdf', '10AS669533-C3C4Phase_II_Addendum3AppendixAFinal_17APRIL2013.pdf', '10AS669533-C3C4Phase_II__Addendum1AppendixAFinal_17JAN2013.pdf', 'Appendix A - Q644 C1C2 Phase II report-final.pdf', 'QC1&2P2_PG&E_Q644_revision.pdf', 'Appendix A - Q644 C1C2 Phase II report-final_r1.pdf', 'Appendix A - Q645A C1C2 Phase II report-final.pdf', 'QC1QC2PII_Northern Group Report.pdf', 'Appendix A - Q650AC C1C2 Phase II report-final_r1.pdf', 'Appendix A - Q653EA C1C2 Phase II report-final.pdf', 'Appendix A - Q653E C1C2 Phase II report-final.pdf', 'Appendix A - Q653E C1C2 Phase II report-final_r1.pdf', '10AS689072-Appendix_A__Q653E_C1C2_Phase_II_reportfinal_r11.pdf', 'Appendix A - Q653B C1C2 Phase II report-final.pdf', '10AS686189-Soitec_PhII_RM_Min__Final.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['10AS665623-QC34PIINorthernQ643AAAddendum_20121227.pdf', '10AS670281-C3C4PII_Q643AI_Addendum.pdf', '10AS664297-QC34PIINorthernQ643RAddendum_20130104.pdf', '10AS664331-QC34PIINorthernQ643AJAddendum_20130104.pdf', '10AS684173-QC34PIIQ643ACAddendum.pdf', '10AS669533-C3C4Phase_II_Addendum2AppendixAFinal_06MARCH2013.pdf', '10AS669533-C3C4Phase_II_Addendum3AppendixAFinal_17APRIL2013.pdf', '10AS669533-C3C4Phase_II__Addendum1AppendixAFinal_17JAN2013.pdf', 'Appendix A - Q644 C1C2 Phase II report-final_r1.pdf', 'QC1QC2PII_Northern_Q649B_Central Antelope Dry Ranch C_Addendum.pdf', 'Appendix A - Q650AC C1C2 Phase II report-final_r1.pdf', 'Appendix A - Q653E C1C2 Phase II report-final_r1.pdf', '10AS689072-Appendix_A__Q653E_C1C2_Phase_II_reportfinal_r11.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['QC1QC2PII_Northern_Appendix A_Q521_Columbia 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q521 Attachment 1.pdf', '09AS692646-SCE_Recurrent_Projects_Ph_II_RM_Min__Final.pdf', 'QC1QC2PII_Northern_Appendix A_Q522_Columbia 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q522 Attachment 1.pdf', 'QC1QC2PII_Northern_Appendix A_Q628_FRV Mojave Solar 4.pdf', 'Appendix A - Q632C C1C2 Phase II report-final.pdf', 'QC1QC2PII_Eastern_Q632AA_MV Solar_Final.pdf', 'C3C4P2-Northern-Q643AA-Blue Sky Wind Energy Center-AppendixA.pdf', 'C3C4P2-SCE EOP-Q643AI-AppendixA.pdf', '10AS666507-QC34PII_Appendix_A_PGE_Q643I_.pdf', '10AS664297-C3C4P2NorthernQ643RWillow_Spring_2AppendixA.pdf', '10AS669159-QC_34_Phase_II_PGE_North_Group_Report.pdf', '10AS666745-QC_34_Phase_II_PGE_North_Group_Report.pdf', 'C3C4P2-Eastern-Q643AEAppendixA-Final.pdf', '10AS670009-C3C4P2EastQ643AEAppndxAReissued.pdf', '10AS664331-C3C4P2NorthernQ643AJNorth_RosamondAppendixA.pdf', 'C3C4P2-Eastern-Q643ACAppendixA-Final.pdf', '10AS669533-C3C4Phase_II_AppendixAStudyFinal_05NOV2012.pdf', '10AS693794-Jacumba_Solar_Farm_Q644A_Appendix_A___11092012.pdf', 'Appendix A - Q644 C1C2 Phase II report-final.pdf', 'QC1&2P2_PG&E_Q644_revision.pdf', 'Appendix A - Q645A C1C2 Phase II report-final.pdf', 'QC1QC2PII_Northern Group Report.pdf', 'QC1QC2PII_Northern_Appendix A_Q650AA_American Solar Greenworks.pdf', 'QC1QC2PII_Northern_Appendix A_Q651A_Acacia.pdf', 'Appendix A - Q653EA C1C2 Phase II report-final.pdf', 'Appendix A - Q653E C1C2 Phase II report-final.pdf', '10AS689152-Appendix_A__Q653B_Pumpjack_Solar_I_QC34_Ph_II_Study_ReportFinal_CMB_5NOV2012.pdf', 'Appendix A - Q653B C1C2 Phase II report-final.pdf', '10AS686189-Soitec_PhII_RM_Min__Final.pdf', 'Appendix A - S653ED_08-24-2011_final.pdf', 'QC1QC2PII_Northern_Appendix A_Q653H_Western Antelope Dry Ranch.pdf', 'Appendix A - Q653F SGIP C1C2 Phase II Final Study Report_CMB 22AUG2011.pdf']\n",
      "\n",
      "List of Style N PDFs (Skipped due to 'Network Upgrade Type'):\n",
      "[]\n",
      "\n",
      "Total Number of Style N PDFs: 0\n",
      "\n",
      "Number of Original PDFs Scraped: 13\n",
      "Number of Addendum PDFs Scraped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_43972/101970297.py:1287: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY =\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/03_raw/ph2_rawdata_clusterSGIP-TC_style_R_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/03_raw/ph2_rawdata_clusterSGIP-TC_style_R_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/03_raw/ph2_scraping_clusterSGIP-TC_style_R_log.txt\"\n",
    "PROJECT_RANGE = range(521, 654)  # Inclusive range for q_ids in Clusters SGIP-TC range(667, 860)\n",
    "\n",
    "\n",
    "# Read the CSV file containing processed projects (with q_id column)\n",
    "processed_csv_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters/costs_phase_2_all_clusters_total.csv\"  # UPDATE THIS PATH\n",
    "processed_df = pd.read_csv(processed_csv_path)\n",
    "# Convert q_id values to numeric then to int for filtering\n",
    "processed_q_ids = pd.to_numeric(processed_df['q_id'], errors='coerce').dropna().astype(int).unique()\n",
    "# Now build the list of folders to process:\n",
    "projects_to_process = []\n",
    "for folder in os.listdir(BASE_DIRECTORY):\n",
    "    full = os.path.join(BASE_DIRECTORY, folder)\n",
    "    if not os.path.isdir(full):\n",
    "        continue\n",
    "    m = re.match(r'^(\\d+)', folder)\n",
    "    if not m:\n",
    "        continue\n",
    "    num = int(m.group(1))\n",
    "    if num in PROJECT_RANGE and num not in processed_q_ids:\n",
    "        projects_to_process.append(folder)\n",
    "\n",
    "# Sort by the integer prefix, not lexicographically as strings\n",
    "projects_to_process = sorted(projects_to_process, key=lambda f: int(re.match(r'^(\\d+)', f).group(1)))\n",
    "\n",
    "\n",
    "print(\"Will process these project‐folders:\")\n",
    "print(projects_to_process)\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "style_n_pdfs = []  # List to track style N PDFs\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "total_pdfs_skipped_extraction = 0\n",
    "original_has_table7 = {}  # Dictionary to track if original PDFs have table7\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters, but keeps parentheses.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            # collapse internal whitespace\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            # strip out everything except letters, digits, spaces, and parentheses\n",
    "            header = re.sub(r'[^a-z0-9\\s\\(\\)]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    elif value is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(value).replace('\\n', ' ').strip()\n",
    "     \n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "         \n",
    "        \n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Conditionally Assigned Network Upgrades\",\n",
    "        \"Local Off-Peak Network Upgrade\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if  re.search(rf\"\\b{re.escape(phrase)}\\b(?=\\d|\\W|$)\", title, re.IGNORECASE):\n",
    "        \n",
    "         #re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*2[\\.-]1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 2)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id =  str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        # Updated Cluster Extraction\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        #if 'SGIP-TC' in clusters:\n",
    "        cluster_number = 'SGIP-TC'\n",
    "        #elif clusters:\n",
    "        #    cluster_number = max(clusters, key=lambda x: int(x))  # Choose the highest cluster number found\n",
    "        #else:\n",
    "        #    cluster_number = 'SGIP-TC'  # Default to SGIP-TC if not found\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 11 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 11 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 11 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 11-1\" to \"Table 11-SGIP-TC\" with hyphen or dot\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*11\\s*[\\.-]\\s*[1-2]\\s*:\", text, re.IGNORECASE): # the \\s* is to match any whitespace between the table number and the colon\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 11-1 to 11-SGIP-TC found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus two to include possible continuation\n",
    "\n",
    "            print(f\"Table 11 starts on page {scrape_start} and ends on page {scrape_end}\", file=log_file)\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                    \n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*11[\\.-]([1-2])[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                try:\n",
    "                                    table_title = match.group(3).strip()  # match.group(2) is just the sub-table number (\"1\" or \"2\")   # match.group(3) is everything after \"Table 11-X:\", e.g. \"Upgrades, Estimated Costs,\n",
    "                                except IndexError:\n",
    "                                    table_title = match.group(0).strip()\n",
    "                                    print(\"Fallback to whole match for table title\", file=log_file)\n",
    "\n",
    " \n",
    "                \n",
    "\n",
    "\n",
    "                    if table_title:\n",
    "                        print(f\"Table Title: {table_title}\")\n",
    "                        if re.search(r\"PTO Interconnection Facilities Cost Estimate Summary\", table_title, re.IGNORECASE):\n",
    "                            print(f\"Skipping Table 11-1 PTO on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    " \n",
    "\n",
    "                        \n",
    "\n",
    "                        # New Table 11 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New Table 11 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        headers = clean_string_cell(tab[0])\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                            # ← INSERT this block:\n",
    "                        if df_new.empty:\n",
    "                            # store an empty DF with the right columns,\n",
    "                            # so that continuation blocks can append to it\n",
    "                            extracted_tables.append(pd.DataFrame(columns=headers))\n",
    "                            print(f\"Header-only Table 11 (‘{specific_phrase}’) detected on page {page_number+1}; waiting for continuation…\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        # Handle ADNU-specific grouping\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    # Group all adnu rows into one 'upgrade' row\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    # If 'type of upgrade' exists, just rename adnu if needed\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row is none, replace only first row if needed\n",
    "                                if df_new.empty:\n",
    "                                    # should never happen once you’ve done step 1, but safe to check\n",
    "                                    continue\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' first row for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            # Non-ADNU new tables\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row is none, replace only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for the first row in new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        #if df_new.columns.duplicated().any():\n",
    "                        #    print(\"Duplicate columns detected in new table. Dropping duplicates.\", file=log_file)\n",
    "                        #    df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(\"Duplicate columns detected in new table. Renaming instead of dropping.\", file=log_file)\n",
    "\n",
    "\n",
    "                            \n",
    "\n",
    "                            # Build a new list of column names, appending _1, _2, … to repeats\n",
    "                            new_cols = []\n",
    "                            counts = {}  # keep track of how many times we've seen each base name\n",
    "                            for orig in df_new.columns:\n",
    "                                # 1) Decide on a non‐blank base name:\n",
    "                                #    If `orig` is blank/None/whitespace, use \"column\" instead.\n",
    "                                if pd.isna(orig) or str(orig).strip() == \"\":\n",
    "                                    base = \"column\"\n",
    "                                else:\n",
    "                                    base = str(orig).strip()\n",
    "\n",
    "                                # SGIP-TC) Increment a counter for that base‐name:\n",
    "                                if base not in counts:\n",
    "                                    counts[base] = 0\n",
    "                                    new_cols.append(base)\n",
    "                                else:\n",
    "                                    counts[base] += 1\n",
    "                                    new_cols.append(f\"{base}_{counts[base]}\")\n",
    " \n",
    "                            df_new.columns = new_cols\n",
    "\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table\n",
    "                        if not extracted_tables:\n",
    "                            print(f\"No previous Table 11 detected to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        last_table = extracted_tables[-1]\n",
    "                        expected_columns = last_table.columns.tolist()\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "\n",
    "\n",
    "\n",
    "                        # 1) Define the phrases that, if they appear anywhere in the first row, force a skip:\n",
    "                        skip_keywords = [\"Q522 RECURRENT COLUMBIA 2\"]\n",
    "\n",
    "                        # 2) Grab the first row (if it exists)\n",
    "                        first_row = data_rows[0] if data_rows else []\n",
    "\n",
    "                        # 3) Check if any skip_keyword appears in any cell of that first row\n",
    "                        is_skip_row = any(\n",
    "                            any(\n",
    "                                re.search(rf\"\\b{re.escape(kw.lower())}\\b\", str(cell).lower())\n",
    "                                for kw in skip_keywords\n",
    "                            )\n",
    "                            for cell in first_row\n",
    "                        )\n",
    "\n",
    "                        if is_skip_row:\n",
    "                            print(\n",
    "                                f\"Skipping continuation table on page {page_number+1}, table {table_index+1} \"\n",
    "                                f\"because skip‐keyword found in first row.\",\n",
    "                                file=log_file\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        # Check if the first row is a header row\n",
    "                        #  we will treat all continuation table rows as data points\n",
    "                        # without any header detection\n",
    "                        # However,  checking if there is a header row first,  \n",
    "                        # Detect if first row is a header\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\", \"MW at POI\", \"upgrade\"]\n",
    "                        first_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            any(re.search(rf\"\\b{kw}\\b\", clean_string_cell(cell).lower()) for kw in header_keywords)\n",
    "                            for cell in first_row\n",
    "                        )\n",
    "\n",
    "\n",
    "                        if is_header_row:\n",
    "                            # Handle header row in continuation table\n",
    "                            headers = clean_string_cell(first_row)\n",
    "                            headers = clean_column_headers(first_row)\n",
    "                            data_rows = data_rows[1:]  # Exclude header row\n",
    "\n",
    "                            # Update expected_columns by adding new columns if any\n",
    "                            new_columns = [col for col in headers if col not in expected_columns]\n",
    "                            if new_columns:\n",
    "                                expected_columns.extend(new_columns)\n",
    "                                print(f\"Added new columns from continuation table: {new_columns}\", file=log_file)\n",
    "\n",
    "                            # Create a mapping of new columns to add with default NaN\n",
    "                            for new_col in new_columns:\n",
    "                                last_table[new_col] = pd.NA\n",
    "\n",
    "                            # Reindex last_table to include new columns\n",
    "                            last_table = last_table.reindex(columns=expected_columns)\n",
    "                            extracted_tables[-1] = last_table\n",
    "\n",
    "                            # Update 'type of upgrade' column in the first row if needed\n",
    "                            if \"type of upgrade\" in headers:\n",
    "                                type_upgrade_idx = headers.index(\"type of upgrade\")\n",
    "                                if pd.isna(data_rows[0][type_upgrade_idx]) or data_rows[0][type_upgrade_idx] == \"\":\n",
    "                                    data_rows[0][type_upgrade_idx] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'type of upgrade' first row for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            elif \"upgrade\" in headers:\n",
    "                                upgrade_idx = headers.index(\"upgrade\")\n",
    "                                if pd.isna(data_rows[0][upgrade_idx]) or data_rows[0][upgrade_idx] == \"\":\n",
    "                                    data_rows[0][upgrade_idx] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'upgrade' first row for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' or 'upgrade' does not exist, add it\n",
    "                                headers.append(\"type of upgrade\")\n",
    "                                expected_columns.append(\"type of upgrade\")\n",
    "                                for idx, row in enumerate(data_rows):\n",
    "                                    data_rows[idx].append(specific_phrase)\n",
    "                                print(f\"Added 'type of upgrade' column and filled with '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                            # Handle ADNU-specific logic if applicable\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                if \"adnu\" in headers:\n",
    "                                    if \"upgrade\" not in headers:\n",
    "                                        # Rename 'adnu' to 'upgrade'\n",
    "                                        adnu_idx = headers.index(\"adnu\")\n",
    "                                        headers[adnu_idx] = \"upgrade\"\n",
    "                                        for row in data_rows:\n",
    "                                            row[adnu_idx] = \" \".join([str(cell) for cell in row[adnu_idx] if pd.notna(cell)])\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in continuation ADNU table.\", file=log_file)\n",
    "                                # Ensure 'type of upgrade' column is filled\n",
    "                                if \"type of upgrade\" not in headers:\n",
    "                                    headers.append(\"type of upgrade\")\n",
    "                                    expected_columns.append(\"type of upgrade\")\n",
    "                                    for row in data_rows:\n",
    "                                        row.append(specific_phrase)\n",
    "                                    print(\"Added 'type of upgrade' column with specific phrase for continuation ADNU table.\", file=log_file)\n",
    "\n",
    "                        else:\n",
    "                            # No header row detected, treat all rows as data points\n",
    "                            print(f\"No header row detected in continuation table on page {page_number + 1}, table {table_index + 1}. Treating all rows as data.\", file=log_file)\n",
    "\n",
    "                        # Create DataFrame for continuation table\n",
    "                        if is_header_row:\n",
    "                            try:\n",
    "                                df_continuation = pd.DataFrame(data_rows, columns=headers)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "                        else:\n",
    "                            # Create DataFrame with expected_columns\n",
    "                            # Handle cases where continuation table has more columns\n",
    "                            standardized_data = []\n",
    "                            for row in data_rows:\n",
    "                                if len(row) < len(expected_columns):\n",
    "                                    # Insert 'type of upgrade' or 'upgrade' with specific_phrase\n",
    "                                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                        # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                        missing_cols = len(expected_columns) - len(row)\n",
    "                                        #row += [specific_phrase] * missing_cols\n",
    "                                        data_rows = [row[:2] + [specific_phrase] + row[2:] for row in data_rows]\n",
    "                                        print(f\"Inserted '{specific_phrase}' for missing columns in ADNU continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                    else:\n",
    "                                        # For non-ADNU tables, assume missing \"type of upgrade\" column\n",
    "                                        missing_cols = len(expected_columns) - len(row)\n",
    "                                        #row += [specific_phrase] * missing_cols\n",
    "                                        data_rows = [ [specific_phrase]  for row in data_rows]\n",
    "                                        print(f\"Inserted '{specific_phrase}' for missing columns in non-ADNU continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                elif len(row) > len(expected_columns):\n",
    "                                    # Add new columns with default names\n",
    "                                    extra_cols = len(row) - len(expected_columns)\n",
    "                                    for i in range(extra_cols):\n",
    "                                        new_col_name = f\"column{len(expected_columns) + 1 + i}\"\n",
    "                                        expected_columns.append(new_col_name)\n",
    "                                        last_table[new_col_name] = pd.NA\n",
    "                                        print(f\"Added new column '{new_col_name}' for extra data in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                    row = row[:len(expected_columns)]\n",
    "\n",
    "                                row_dict = dict(zip(expected_columns, [clean_string_cell(cell) for cell in row]))\n",
    "\n",
    "                                # Handle 'type of upgrade' column\n",
    "                                if \"type of upgrade\" in row_dict and (pd.isna(row_dict[\"type of upgrade\"]) or row_dict[\"type of upgrade\"] == \"\"):\n",
    "                                    row_dict[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'type of upgrade' for a row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                                standardized_data.append(row_dict)\n",
    "\n",
    "                            try:\n",
    "                                df_continuation = pd.DataFrame(standardized_data, columns=expected_columns)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "\n",
    "\n",
    "                             # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                if \"type of upgrade\" in df_continuation.columns:\n",
    "                                    first_row = df_continuation.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                        df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                else:\n",
    "                                    # If \"type of upgrade\" column does not exist, add it\n",
    "                                    df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                            else:\n",
    "                                # General Handling for other tables\n",
    "                                if \"type of upgrade\" in df_continuation.columns:\n",
    "                                    first_row = df_continuation.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                        df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                else:\n",
    "                                    # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                    df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. renaming duplicates.\", file=log_file)\n",
    "                                                        # Build a new list of column names, appending _1, _2, … to repeats\n",
    "                            new_cols = []\n",
    "                            counts = {}  # keep track of how many times we've seen each base name\n",
    "                            for orig in df_continuation.columns:\n",
    "                                # 1) Decide on a non‐blank base name:\n",
    "                                #    If `orig` is blank/None/whitespace, use \"column\" instead.\n",
    "                                if pd.isna(orig) or str(orig).strip() == \"\":\n",
    "                                    base = \"column\"\n",
    "                                else:\n",
    "                                    base = str(orig).strip()\n",
    "\n",
    "                                # SGIP-TC) Increment a counter for that base‐name:\n",
    "                                if base not in counts:\n",
    "                                    counts[base] = 0\n",
    "                                    new_cols.append(base)\n",
    "                                else:\n",
    "                                    counts[base] += 1\n",
    "                                    new_cols.append(f\"{base}_{counts[base]}\")\n",
    " \n",
    "                            df_continuation.columns = new_cols\n",
    "                            #df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                        print(f\"Appended continuation table data to the last extracted table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 11 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After processing all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 11 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 11 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "\n",
    "'''\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 11 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 11 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 11 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 11 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "'''\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 11 data and merges with base data.\n",
    "    Returns:\n",
    "      df       – either base_data or base_data×Table4 rows merged\n",
    "      status   – one of \"no_marker\", \"failed\", or \"success\"\n",
    "    \"\"\"\n",
    "    # 1) Pull out base data\n",
    "    base_data   = extract_base_data(pdf_path, project_id, log_file)\n",
    "    # SGIP-TC) Did we even see a Table 11 marker in the text?\n",
    "    has_marker  = check_has_table7(pdf_path)\n",
    "    if not has_marker:\n",
    "        print(f\"No Table 11 marker found in {os.path.basename(pdf_path)}; skipping extraction.\", \n",
    "              file=log_file)\n",
    "        return base_data, \"no_marker\"\n",
    "\n",
    "    # SGIP-TC) Try to scrape Table 11\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "    if table7_data.empty:\n",
    "        print(f\"Table 11 marker found in {os.path.basename(pdf_path)}, \"\n",
    "              f\"but extraction returned empty DataFrame.\", file=log_file)\n",
    "        return base_data, \"failed\"\n",
    "\n",
    "    # SGIP-TC) We got actual rows → merge and return\n",
    "    #    Drop any overlapping columns first\n",
    "    overlapping = base_data.columns.intersection(table7_data.columns)\n",
    "    if not overlapping.empty:\n",
    "        table7_data = table7_data.drop(columns=overlapping, errors=\"ignore\")\n",
    "\n",
    "    #    Repeat base_data for each row of table7_data\n",
    "    base_rep   = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "    merged_df  = pd.concat([base_rep, table7_data.reset_index(drop=True)], axis=1, sort=False)\n",
    "\n",
    "    print(f\"Merged base data with {len(table7_data)} row(s) of Table 11 for \"\n",
    "          f\"{os.path.basename(pdf_path)}.\", file=log_file)\n",
    "    return merged_df, \"success\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 11-1 to SGIP-TC-SGIP-TC.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*11[\\.-][1-2]\\b\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def has_network_upgrade_type_column(pdf_path, log_file):\n",
    "    \"\"\"Checks if any table in the PDF has a column header 'Network Upgrade Type'.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables()\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    headers = clean_column_headers(tab[0])\n",
    "                    if \"network upgrade type\" in headers:\n",
    "                        print(f\"Found 'Network Upgrade Type' in PDF {pdf_path} on page {page_number}, table {table_index}.\", file=log_file)\n",
    "                        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking 'Network Upgrade Type' in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "    return False\n",
    "\n",
    " \n",
    "\n",
    "def is_addendum(pdf_path, log_file):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching for 'Addendum', 'Addendum #3', or 'Revision' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            print(f\"Extracted Text: {text}\", file=log_file)  # Debug\n",
    "\n",
    "            # Compile a pattern that matches:\n",
    "            #   • “Addendum” or “ADDENDUM”\n",
    "            #   • optionally followed by whitespace, a ‘#’, then digits (e.g. “Addendum #3”)\n",
    "            #   • OR the word “Revision”\n",
    "            pattern = re.compile(\n",
    "                r\"\\b(?:addendum(?:\\s*#\\s*\\d+)?|revision)\\b\",\n",
    "                re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            return bool(pattern.search(text))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    \"\"\"\n",
    "    Appends a suffix to duplicate headers to make them unique.\n",
    "\n",
    "    Args:\n",
    "        headers (list): List of column headers.\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique column headers.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped, total_pdfs_skipped_extraction\n",
    "\n",
    "    SKIP_PROJECTS = {1860, 2003, 2006}\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "\n",
    "\n",
    "        for project_id in projects_to_process:\n",
    "            \n",
    "            # Skip the projects in the SKIP_PROJECTS set\n",
    "            if project_id in SKIP_PROJECTS:\n",
    "                print(f\"Skipping Project {project_id} (marked to skip)\", file=log_file)\n",
    "                continue\n",
    "\n",
    "         \n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"03_phase_2_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Separate PDFs into originals and addendums\n",
    "            list_pdfs = [pdf for pdf in os.listdir(project_path) if pdf.endswith(\".pdf\")]\n",
    "            originals = []\n",
    "            addendums = []\n",
    "            for pdf_name in list_pdfs:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                if is_addendum(pdf_path, log_file):\n",
    "                    addendums.append(pdf_name)\n",
    "                else:\n",
    "                    originals.append(pdf_name)\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Process original PDFs first\n",
    "            for pdf_name in originals:\n",
    "                \n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    # Still check if original has table7\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                original_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "\n",
    "                    if not has_table7:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 11)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 11)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    # Extract Table 11 and merge\n",
    "                    '''\n",
    "                    df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False)\n",
    "                    if not df.empty:\n",
    "                        core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        project_scraped = True\n",
    "                        total_pdfs_scraped += 1\n",
    "                        print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                    else:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "                    '''\n",
    "                        # Extract Table 11 and merge\n",
    "                    df, status = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False)\n",
    "\n",
    "                    if status == \"success\":\n",
    "                        core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        project_scraped = True\n",
    "                        total_pdfs_scraped += 1\n",
    "                        print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "\n",
    "                    elif status == \"failed\":\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        total_pdfs_skipped_extraction += 1\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Table 11 found but extraction failed)\"\n",
    "                             )\n",
    "\n",
    "                    else:  # status == \"no_marker\"\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        total_pdfs_skipped += 1\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 11 present)\" )\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Then process addendum PDFs\n",
    "            for pdf_name in addendums:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                addendum_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "\n",
    "                    if not has_table7:\n",
    "                        if original_has_table7.get(project_id, False):\n",
    "                            # Attempt to scrape alternative tables is no longer needed\n",
    "                            # According to the latest request, alternative table scraping is removed\n",
    "                            # Therefore, we skip addendum PDFs that do not have Table 11\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 11)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 11)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 11 and original does not have Table 11)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 11 and original does not have Table 11)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not is_add and not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    if is_add and base_data_extracted:\n",
    "                        # For addendums, use the extracted base data\n",
    "                        table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                        if table7_data.empty and original_has_table7.get(project_id, False):\n",
    "                            # Scrape alternative tables is removed, so skip if no data\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        if not table7_data.empty:\n",
    "                            # Merge base data with Table 11 data\n",
    "                            merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                            merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                            core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                            scraped_pdfs.append(pdf_name)\n",
    "                            scraped_projects.add(project_id)\n",
    "                            project_scraped = True\n",
    "                            total_pdfs_scraped += 1\n",
    "                            print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    # Optionally, print to ipynb\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "    # Rest of the code remains unchanged...\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped due to failed extraction of Table: {total_pdfs_skipped_extraction}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nList of Style N PDFs (Skipped due to 'Network Upgrade Type'):\")\n",
    "    print(style_n_pdfs)\n",
    "\n",
    "    print(\"\\nTotal Number of Style N PDFs:\", len(style_n_pdfs))\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.applymap(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    " \n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized and Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: ['q_id', 'cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection', 'type of upgrade', 'upgrade', 'description', 'cost allocation factor', 'upgrade (may include the following)', 'estimated cost x 1000 constant dollar (od year) (note 4)', 'estimated cost x 1000 constant dollar (2011) (note 4)', 'estimated time to construct (note 3)', 'Unnamed: 15', 'column_14', 'column_1', 'estimated', 'column_8', 'column_17', 'column_3', 'column_7', 'column_9', 'column_18', 'column_5', 'column_2', 'column_12', 'column_4', 'column', 'column_6', 'column_11', 'column_10', 'column_13', 'column_16', 'column_15', 'estimated_1', 'estimated time to construct (note 1)', 'estimated cost (x 1000)']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/03_raw/ph2_rawdata_clusterSGIP-TC_style_R_originals.csv\", dtype={'estimated_time_to_construct': str})\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "\n",
    "\n",
    "#df.columns = clean_column_headers(df.columns)\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "#df = df.map(clean_string_cell)\n",
    "#df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "print(\"After cleaning:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 521\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 522\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 628\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 632AA\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 644A\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 650AA\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 651A\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 653ED\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 653F\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 653H\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "      q_id  cluster req_deliverability  latitude  longitude  capacity  \\\n",
      "0     521  SGIP-TC               None       NaN        NaN      20.0   \n",
      "1     521  SGIP-TC               None       NaN        NaN      20.0   \n",
      "2     521  SGIP-TC               None       NaN        NaN      20.0   \n",
      "3     522  SGIP-TC               Full       NaN        NaN      20.0   \n",
      "4     522  SGIP-TC               Full       NaN        NaN      20.0   \n",
      "5     522  SGIP-TC               Full       NaN        NaN      20.0   \n",
      "6     628  SGIP-TC               Full       NaN        NaN      20.0   \n",
      "7     628  SGIP-TC               Full       NaN        NaN      20.0   \n",
      "8     628  SGIP-TC               Full       NaN        NaN      20.0   \n",
      "9   632AA  SGIP-TC               None       NaN        NaN      13.0   \n",
      "10  632AA  SGIP-TC               None       NaN        NaN      13.0   \n",
      "11  632AA  SGIP-TC               None       NaN        NaN      13.0   \n",
      "12   644A  SGIP-TC               Full       NaN        NaN       NaN   \n",
      "13   644A  SGIP-TC               Full       NaN        NaN       NaN   \n",
      "14   644A  SGIP-TC               Full       NaN        NaN       NaN   \n",
      "15  650AA  SGIP-TC               Full       NaN        NaN      15.0   \n",
      "16  650AA  SGIP-TC               Full       NaN        NaN      15.0   \n",
      "17  650AA  SGIP-TC               Full       NaN        NaN      15.0   \n",
      "18   651A  SGIP-TC               Full       NaN        NaN      20.0   \n",
      "19   651A  SGIP-TC               Full       NaN        NaN      20.0   \n",
      "20   651A  SGIP-TC               Full       NaN        NaN      20.0   \n",
      "21  653ED  SGIP-TC               None       NaN        NaN       NaN   \n",
      "22  653ED  SGIP-TC               None       NaN        NaN       NaN   \n",
      "23  653ED  SGIP-TC               None       NaN        NaN       NaN   \n",
      "24   653F  SGIP-TC               None       NaN        NaN       NaN   \n",
      "25   653F  SGIP-TC               None       NaN        NaN       NaN   \n",
      "26   653H  SGIP-TC               Full       NaN        NaN      10.0   \n",
      "27   653H  SGIP-TC               Full       NaN        NaN      10.0   \n",
      "28   653H  SGIP-TC               Full       NaN        NaN      10.0   \n",
      "\n",
      "                             point_of_interconnection type_of_upgrade  \\\n",
      "0                           Goldtown-Corum 66 kV Line      Total LDNU   \n",
      "1                           Goldtown-Corum 66 kV Line    Total PTO_IF   \n",
      "2                           Goldtown-Corum 66 kV Line       Total RNU   \n",
      "3                  Corum-Goldtown-Rosamond 66 kV Line      Total LDNU   \n",
      "4                  Corum-Goldtown-Rosamond 66 kV Line    Total PTO_IF   \n",
      "5                  Corum-Goldtown-Rosamond 66 kV Line       Total RNU   \n",
      "6             Antelope-Cal Cement-Rosamond 66 kV Line      Total LDNU   \n",
      "7             Antelope-Cal Cement-Rosamond 66 kV Line    Total PTO_IF   \n",
      "8             Antelope-Cal Cement-Rosamond 66 kV Line       Total RNU   \n",
      "9   The SCE Mountwind 115 kV substation connected ...      Total LDNU   \n",
      "10  The SCE Mountwind 115 kV substation connected ...    Total PTO_IF   \n",
      "11  The SCE Mountwind 115 kV substation connected ...       Total RNU   \n",
      "12               138 kV bus at East County Substation      Total LDNU   \n",
      "13               138 kV bus at East County Substation    Total PTO_IF   \n",
      "14               138 kV bus at East County Substation       Total RNU   \n",
      "15               Antelope-Del Sur-Rosamond 66 kV Line      Total LDNU   \n",
      "16               Antelope-Del Sur-Rosamond 66 kV Line    Total PTO_IF   \n",
      "17               Antelope-Del Sur-Rosamond 66 kV Line       Total RNU   \n",
      "18                                 Antelope 66 kV Bus      Total LDNU   \n",
      "19                                 Antelope 66 kV Bus    Total PTO_IF   \n",
      "20                                 Antelope 66 kV Bus       Total RNU   \n",
      "21                                Boulevard 69 kV Bus      Total LDNU   \n",
      "22                                Boulevard 69 kV Bus    Total PTO_IF   \n",
      "23                                Boulevard 69 kV Bus       Total RNU   \n",
      "24                        Woodland  Davis 115 kV Line    Total PTO_IF   \n",
      "25                        Woodland  Davis 115 kV Line       Total RNU   \n",
      "26                                 Antelope 66 kV Bus      Total LDNU   \n",
      "27                                 Antelope 66 kV Bus    Total PTO_IF   \n",
      "28                                 Antelope 66 kV Bus       Total RNU   \n",
      "\n",
      "   type_of_upgrade_2 upgrade description cost_allocation_factor  \\\n",
      "0                                                                 \n",
      "1                                                                 \n",
      "2                                                                 \n",
      "3                                                                 \n",
      "4                                                                 \n",
      "5                                                                 \n",
      "6                                                                 \n",
      "7                                                                 \n",
      "8                                                                 \n",
      "9                                                                 \n",
      "10                                                                \n",
      "11                                                                \n",
      "12                                                                \n",
      "13                                                                \n",
      "14                                                                \n",
      "15                                                                \n",
      "16                                                                \n",
      "17                                                                \n",
      "18                                                                \n",
      "19                                                                \n",
      "20                                                                \n",
      "21                                                                \n",
      "22                                                                \n",
      "23                                                                \n",
      "24                                                                \n",
      "25                                                                \n",
      "26                                                                \n",
      "27                                                                \n",
      "28                                                                \n",
      "\n",
      "    estimated_cost_x_1000  escalated_cost_x_1000 estimated_time_to_construct  \\\n",
      "0                   903.0                  987.0                               \n",
      "1                  2911.0                 3183.0                               \n",
      "2                 39233.0                44178.0                               \n",
      "3                   506.0                  553.0                               \n",
      "4                  2819.0                 3082.0                               \n",
      "5                 25797.0                29176.0                               \n",
      "6                    14.0                   15.0                               \n",
      "7                  2793.0                 3054.0                               \n",
      "8                 14312.0                15647.0                               \n",
      "9                    86.0                   91.0                               \n",
      "10                    3.0                    3.0                               \n",
      "11                    0.0                    0.0                               \n",
      "12                 1109.0                  427.0                               \n",
      "13                   45.0                    0.0                               \n",
      "14                    2.0                    0.0                               \n",
      "15                   14.0                   15.0                               \n",
      "16                 2805.0                 3067.0                               \n",
      "17                12841.0                14039.0                               \n",
      "18                   14.0                   15.0                               \n",
      "19                 6892.0                 7535.0                               \n",
      "20                 1064.0                 1163.0                               \n",
      "21                    0.0                    0.0                               \n",
      "22                  721.0                    0.0                               \n",
      "23                  695.0                    0.0                               \n",
      "24                  884.0                    0.0                               \n",
      "25                 2031.0                    0.0                               \n",
      "26                    0.0                    0.0                               \n",
      "27                 6262.0                 6847.0                               \n",
      "28                 1701.0                 1860.0                               \n",
      "\n",
      "   item  \n",
      "0    no  \n",
      "1    no  \n",
      "2    no  \n",
      "3    no  \n",
      "4    no  \n",
      "5    no  \n",
      "6    no  \n",
      "7    no  \n",
      "8    no  \n",
      "9    no  \n",
      "10   no  \n",
      "11   no  \n",
      "12   no  \n",
      "13   no  \n",
      "14   no  \n",
      "15   no  \n",
      "16   no  \n",
      "17   no  \n",
      "18   no  \n",
      "19   no  \n",
      "20   no  \n",
      "21   no  \n",
      "22   no  \n",
      "23   no  \n",
      "24   no  \n",
      "25   no  \n",
      "26   no  \n",
      "27   no  \n",
      "28   no  \n",
      "👉 Duplicate column names in df: []\n",
      "Itemized rows saved to 'costs_phase_2_cluster_SGIP-TC_style_R_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_SGIP-TC_style_R_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU']\n",
      "['521' '522' '628' '632AA' '644A' '650AA' '651A' '653ED' '653F' '653H']\n",
      "['SGIP-TC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_43972/2751462853.py:472: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean[required_cols] = df_clean[required_cols].applymap(lambda x: str(x).strip())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_43972/2751462853.py:988: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/03_raw/ph2_rawdata_clusterSGIP-TC_style_R_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_cost_allocation(df, source_col, target_col=\"cost_allocation_factor\"):\n",
    "    \"\"\"\n",
    "    Extracts percentage values from a specified source column and moves them into a target column.\n",
    "    \n",
    "    - A percentage value is defined as a string that, when stripped of whitespace,\n",
    "      fully matches a pattern of digits (with optional commas or periods) followed by a percent sign.\n",
    "    - If a cell in the source column matches this pattern, its value is placed into the target column,\n",
    "      and the source column cell is cleared (set to an empty string).\n",
    "    - If the cell does not match a percentage pattern, it is left untouched in the source column.\n",
    "    \n",
    "    Parameters:\n",
    "      df         : pandas DataFrame.\n",
    "      source_col : string, the name of the column to scan for percentage values.\n",
    "      target_col : string, the name of the column to store the extracted percentage values.\n",
    "                   Defaults to \"cost_allocation_factor\".\n",
    "    \n",
    "    Returns:\n",
    "      The DataFrame with the updated columns.\n",
    "    \"\"\"\n",
    "    # Define a regex pattern to match a percentage value (e.g., \"78.25%\").\n",
    "    # The pattern allows digits, commas, and periods, followed immediately by a \"%\" (ignoring leading/trailing spaces).\n",
    "    pattern = r\"^\\s*[\\d,\\.]+%\\s*$\"\n",
    "    \n",
    "    def extract_percentage(text):\n",
    "        # If text matches the percentage pattern, return the stripped text; otherwise, return None.\n",
    "        if isinstance(text, str) and re.fullmatch(pattern, text):\n",
    "            return text.strip()\n",
    "        return None\n",
    "\n",
    "    def clear_percentage(text):\n",
    "        # If text matches the percentage pattern, clear it (return an empty string).\n",
    "        # Otherwise, return the text stripped of surrounding whitespace.\n",
    "        if isinstance(text, str) and re.fullmatch(pattern, text):\n",
    "            return \"\"\n",
    "        if isinstance(text, str):\n",
    "            return text.strip()\n",
    "        return text\n",
    "\n",
    "    # Create (or overwrite) the target column with extracted percentage values from the source column.\n",
    "    df[target_col] = df[source_col].apply(extract_percentage)\n",
    "    # In the source column, remove any percentage values (leaving other text intact).\n",
    "    df[source_col] = df[source_col].apply(clear_percentage)\n",
    "    \n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "def filter_numeric_costs(df, col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame and column name, this function extracts the numeric cost from each cell,\n",
    "    converting values with an optional '$' sign (and possible commas) to floats.\n",
    "    If a valid numeric cost cannot be extracted, the cell is set to NaN.\n",
    "    \n",
    "    Parameters:\n",
    "      df  : pandas DataFrame.\n",
    "      col : string, the name of the column to process.\n",
    "      \n",
    "    Returns:\n",
    "      The original DataFrame with the specified column converted to numeric values (or NaN if conversion fails).\n",
    "    \"\"\"\n",
    "    def extract_numeric(value):\n",
    "        value_str = str(value)\n",
    "        # This regex matches an optional '$', optional spaces, and a number with commas and an optional decimal part.\n",
    "        match = re.search(r'\\$?\\s*([\\d,]+(?:\\.\\d+)?)', value_str)\n",
    "        if match:\n",
    "            num_str = match.group(1).replace(',', '')\n",
    "            try:\n",
    "                return float(num_str)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    # Apply the extraction function to the specified column.\n",
    "    df[col] = df[col].apply(extract_numeric)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def extract_months_values(df, col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame and column name, this function extracts text patterns matching\n",
    "    durations expressed in months (e.g., \"32 months\", \"43 Month\", \"23 Months\", \"22Months\", or \"21-29months\").\n",
    "    If a valid pattern is found, it returns the matched text; otherwise, it returns an empty string.\n",
    "    \n",
    "    Parameters:\n",
    "        df  : pandas DataFrame.\n",
    "        col : string, the name of the column to process.\n",
    "        \n",
    "    Returns:\n",
    "        The DataFrame with the specified column updated.\n",
    "    \"\"\"\n",
    "    def extract_months(text):\n",
    "        text = str(text)\n",
    "        # Pattern explanation:\n",
    "        #   \\d+          : one or more digits\n",
    "        #   (?:-\\d+)?    : optionally, a hyphen followed by one or more digits (to capture ranges like 21-29)\n",
    "        #   \\s*          : optional whitespace\n",
    "        #   [Mm]onths?   : \"month\" or \"months\" (case insensitive for the first letter)\n",
    "        pattern = r'(\\d+(?:-\\d+)?\\s*[Mm]onths?)'\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group(1) if match else \"\"\n",
    "    \n",
    "    df[col] = df[col].apply(extract_months)\n",
    "    return df\n",
    "\n",
    "def move_months_values(df, source_col, target_col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame, this function extracts text patterns matching durations expressed in months\n",
    "    (e.g., \"32 months\", \"43 Month\", \"23 Months\", \"22Months\", or \"21-29months\") from the source column,\n",
    "    moves the extracted text to the target column, and removes it from the source column.\n",
    "    \n",
    "    Parameters:\n",
    "        df         : pandas DataFrame.\n",
    "        source_col : string, the name of the column to extract the month text from.\n",
    "        target_col : string, the name of the column where the extracted month text will be moved.\n",
    "        \n",
    "    Returns:\n",
    "        The updated DataFrame with the month values moved.\n",
    "    \"\"\"\n",
    "    # Pattern explanation:\n",
    "    #   \\d+          : one or more digits\n",
    "    #   (?:-\\d+)?    : optionally, a hyphen and one or more digits (to capture ranges like 21-29)\n",
    "    #   \\s*          : optional whitespace\n",
    "    #   [Mm]onths?   : \"month\" or \"months\" (case insensitive for the first letter)\n",
    "    pattern = r'(\\d+(?:-\\d+)?\\s*[Mm]onths?)'\n",
    "    \n",
    "    def process_text(text):\n",
    "        text = str(text)\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            extracted = match.group(1)\n",
    "            # Remove the extracted text from the source text and clean up extra spaces\n",
    "            updated_text = re.sub(pattern, \"\", text).strip()\n",
    "            return extracted, updated_text\n",
    "        else:\n",
    "            return \"\", text\n",
    "\n",
    "    # Prepare lists to store the extracted month text and the updated source text\n",
    "    extracted_vals = []\n",
    "    updated_source_vals = []\n",
    "    \n",
    "    for val in df[source_col]:\n",
    "        ext, updated = process_text(val)\n",
    "        extracted_vals.append(ext)\n",
    "        updated_source_vals.append(updated)\n",
    "    \n",
    "    # Create/update the target column with the extracted month text\n",
    "    df[target_col] = extracted_vals\n",
    "    # Replace the source column values with the text after removal of the month text\n",
    "    df[source_col] = updated_source_vals\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Filter numeric costs in 'estimated_cost_x_1000' and 'escalated_cost_x_1000' columns\n",
    " \n",
    "\n",
    "df = filter_numeric_costs(df, 'column_9')\n",
    "df = filter_numeric_costs(df, 'column_10')\n",
    "\n",
    "df = filter_numeric_costs(df, 'estimated')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    " \n",
    "df = extract_months_values(df, 'estimated_1')\n",
    "df = extract_months_values(df, 'column_17')\n",
    "df = extract_months_values(df, 'column_13')\n",
    "df = extract_months_values(df, 'column_16')\n",
    "df = move_months_values(df, 'column_12', 'estimated time to construct (note 1)')\n",
    "df = move_months_values(df, 'column_13', 'estimated time to construct (note 1)')\n",
    "\n",
    "df = filter_numeric_costs(df, 'column_13')\n",
    "df = filter_numeric_costs(df, 'column_12')\n",
    "\n",
    "\n",
    "df= df[df['column_1']!= 'Type of']\n",
    "df= df[df['column_1']!= 'Upgrade']\n",
    "\n",
    "\n",
    "df['cost_allocation_factor'] = None\n",
    "\n",
    "\n",
    "df = extract_cost_allocation(df, \"column_7\", \"cost_allocation_factor\")\n",
    "\n",
    "#df = extract_cost_allocation(df, \"unnamed_9\", \"cost_allocation_factor\")\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'unnamed_9' in df.columns:\n",
    " #  df['unnamed_9'] = df['unnamed_9'].apply(extract_non_numeric_text)\n",
    "  # df['unnamed_9'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "\n",
    "\n",
    "#if 'unnamed_8' in df.columns:\n",
    " #  df['unnamed_8'] = df['unnamed_8'].apply(extract_non_numeric_text)\n",
    "  # df['unnamed_8'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "            \"column_3\",\n",
    "            \n",
    "            'upgrade (may include the following)',\n",
    " \n",
    "            ],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"MW\",\n",
    "            \n",
    "        ],   \n",
    "\n",
    "        \"description\": [\"description\",\n",
    "                         \"column_5\" ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [ \n",
    "            \"column_12\", 'estimated_1', \"column_17\", \"estimated time to construct (note 1)\",\n",
    "            'estimated time to construct (note 3)',   'column_16',\n",
    " \n",
    "                                         ],\n",
    "\n",
    "        \"type_of_upgrade\": [ \"type of upgrade\", ],\n",
    "\n",
    "        \"type_of_upgrade_2\": [ \"column\",   \"column_1\",  ],\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \"estimated_cost_x_1000\": [ 'column_10', \n",
    "             \"estimated cost (x 1000)\" ,  'estimated cost x 1000 constant dollar (2011) (note 4)', \"estimated\",\n",
    " \n",
    "           \n",
    "\n",
    "             \n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\", \"estimated cost x 1000 constant dollar (od year) (note 2)\", \n",
    "            'estimated cost x 1000 constant dollar (od year) (note 4)', 'column_9', \"column_12\", \"column_10\", 'column_13',\n",
    "            \n",
    " \n",
    "            \n",
    "             \n",
    "\n",
    "        ],\n",
    "\n",
    "         \n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "       \n",
    "         \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \"cost_allocation_factor\": [\n",
    "            \n",
    "            'cost allocation factor', \n",
    "            \"column_6\",\n",
    " \n",
    "            \n",
    "           \n",
    "\n",
    "        ],\n",
    "       \n",
    "    }\n",
    "\n",
    "   # 1) If there are any truly “unnamed” columns (blank names or starting with \"Unnamed\"),\n",
    "    #    tack them onto the \"description\" group so they also get merged under \"description\".\n",
    "    unnamed_columns = [\n",
    "        col for col in df.columns\n",
    "        if (pd.isna(col) or str(col).strip() == \"\" or str(col).lower().startswith(\"nnamed\"))\n",
    "    ]\n",
    "    if unnamed_columns:\n",
    "        # Only add those that aren’t already listed\n",
    "        for uc in unnamed_columns:\n",
    "            if uc not in merge_columns_dict[\"description\"]:\n",
    "                merge_columns_dict[\"description\"].append(uc)\n",
    "\n",
    "    # SGIP-TC) For each (new_col → list_of_old_cols), build new_col by picking\n",
    "    #    the first non‐missing value in row‐order. Then drop only the old columns\n",
    "    #    (but keep new_col).\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        # (a) Restrict to columns that actually exist in df\n",
    "        existing = [c for c in old_cols if c in df.columns]\n",
    "        if not existing:\n",
    "            continue\n",
    "\n",
    "        # (b) Define a helper that returns the first non‐missing, non‐blank value\n",
    "        def first_non_missing(row):\n",
    "            for val in row:\n",
    "                # treat \"\" or whitespace‐only strings as missing, too\n",
    "                if pd.notna(val) and not (isinstance(val, str) and val.strip() == \"\"):\n",
    "                    return val\n",
    "            return pd.NA\n",
    "\n",
    "        # (c) Apply it row‐wise to df[existing]\n",
    "        df[new_col] = df[existing].apply(first_non_missing, axis=1)\n",
    "\n",
    "        # (d) Drop only those source columns that are NOT equal to new_col.\n",
    "        #     That way, if “upgrade” was already a column name, we don’t drop the newly created “upgrade” column,\n",
    "        #     but _do_ drop “column4” (and any others in existing except new_col itself).\n",
    "        to_drop = [c for c in existing if c != new_col]\n",
    "        if to_drop:\n",
    "            df.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP SGIP-TC: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,SGIP-TC})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "#if 'upgrade' in df.columns:\n",
    " #   df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    " \n",
    "\n",
    "df.drop(['column_2', 'column_8',\"column_11\", \"Unnamed: 15\", \"column_7\",  \"column_14\", \"column_9\", \"column_4\", 'column_18', 'column_15'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "#df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "\n",
    " \n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Processes the DataFrame as follows:\n",
    "    \n",
    "    1. Drops any rows where any of these columns are empty or blank:\n",
    "       - 'upgrade', 'description', 'cost_allocation_factor',\n",
    "         'estimated_time_to_construct', 'type_of_upgrade_2', 'estimated_cost_x_1000'\n",
    "    \n",
    "    SGIP-TC. For each remaining row, if the value in 'type_of_upgrade' starts with\n",
    "       'SCE', 'SDG&E', or 'PG&E' (or is empty after stripping),\n",
    "       then the value in 'type_of_upgrade_2' is replaced with the value from 'type_of_upgrade'.\n",
    "       \n",
    "    Parameters:\n",
    "        df: pandas DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        A cleaned DataFrame with the above processing applied.\n",
    "    \"\"\"\n",
    "    # Define the required columns\n",
    "    required_cols = [\n",
    "        \"upgrade\", \"description\", \"cost_allocation_factor\",\n",
    "        \"estimated_time_to_construct\", \"type_of_upgrade_2\", \"estimated_cost_x_1000\"\n",
    "    ]\n",
    "    \n",
    "       # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Replace NaN with empty strings for checking emptiness\n",
    "    df_clean[required_cols] = df_clean[required_cols].fillna(\"\")\n",
    "\n",
    "    # Convert all required columns to strings and strip whitespace\n",
    "    df_clean[required_cols] = df_clean[required_cols].applymap(lambda x: str(x).strip())\n",
    "    \n",
    "    \n",
    " # Drop rows where all required columns are empty\n",
    "    df_clean = df_clean[~(df_clean[required_cols].apply(lambda row: all(row == \"\"), axis=1))]\n",
    "    \n",
    " \n",
    "    \n",
    "    # Define a function to update type_of_upgrade_2 if needed.\n",
    "    def update_type(row):\n",
    "        # Get the value from type_of_upgrade (converted to string and stripped)\n",
    "        val = str(row.get(\"type_of_upgrade\", \"\")).strip()\n",
    "        # If the value is empty or starts with SCE, SDG&E, or PG&E, then update type_of_upgrade_2\n",
    "        if val == \"\" or re.match(r'^(SCE|SDG&E|PG&E)', val):\n",
    "            row[\"type_of_upgrade\"] = row[\"type_of_upgrade_2\"]\n",
    "        return row\n",
    "\n",
    "\n",
    "    # Apply the function row-wise\n",
    "    df_clean = df_clean.apply(update_type, axis=1)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "    df_clean = df_clean.apply(update_type, axis=1)\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "\n",
    "df = process_dataframe(df)\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"type_of_upgrade_2\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "\n",
    "df = reorder_columns(df)\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].replace(\"\", np.nan).ffill() \n",
    "\n",
    "\n",
    "df= df[df['type_of_upgrade']!= '12. Local Furnishing Bonds']\n",
    "df= df[df['type_of_upgrade']!= '(when applicable):']\n",
    "df= df[df['type_of_upgrade']!= '12. Items Not Covered In This Study']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/03_raw/cluster_SGIP-TC_style_R.csv', index=False)\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "\n",
    "\n",
    "df= df[df['type_of_upgrade']!= '12. Local Furnishing Bonds']\n",
    "df= df[df['type_of_upgrade']!= '(when applicable):']\n",
    "df= df[df['type_of_upgrade']!= '12. Items Not Covered In This Study']\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/03_raw/cluster_SGIP-TC_style_R.csv', index=False)\n",
    "\n",
    "mask_agg = (\n",
    "    df['type_of_upgrade'].fillna('').eq('Total') |\n",
    "     df['type_of_upgrade'].fillna('').eq('Total Cost') |\n",
    "    df['cost_allocation_factor'].fillna('').eq('Total')\n",
    ")\n",
    "\n",
    "# SGIP-TC) Extract them\n",
    "aggregate_total = df.loc[mask_agg].copy()\n",
    "\n",
    "# 3) Tag them in the original df\n",
    "df['is_aggregate_total'] = mask_agg\n",
    "\n",
    "\n",
    "agg_data = df[df['is_aggregate_total']].copy()\n",
    "agg_data.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/02_intermediate/costs_phase_2_cluster_SGIP-TC_style_R_aggregate.csv', index=False) \n",
    "\n",
    "# 3) Then drop them from your main itemized set\n",
    "df = df.loc[~mask_agg].reset_index(drop=True)\n",
    "\n",
    "df.drop(columns=['is_aggregate_total'], inplace=True, errors='ignore')\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)  \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    \"PTO’s Interconnection Facilities (Note SGIP-TC)\": \"PTO_IF\",\n",
    "    \"PTO’s Interconnectio n Facilities (Note SGIP-TC)\": \"PTO_IF\",\n",
    "    \"PTOs Interconnection Facilities\": \"PTO_IF\",\n",
    "    \"PTOs Interconnectio n Facilities\": \"PTO_IF\",\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Delivery Network\": \"ADNU\",\n",
    " \"Plan of Service Reliability Network Upgrades\": \"RNU\",\n",
    " \"Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Reliability Network Upgrades\": \"RNU\",\n",
    " \"SDG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Upgrades\": \"LDNU\",\n",
    " \"SCE Distribution Upgrades\": \"LDNU\",\n",
    " \"SCE Reliability Network Upgrades for Short Circuit duty\": \"RNU\",\n",
    " \"SCE Network Upgrades\": \"RNU\",\n",
    " \"Plan of Service Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Upgrades, Estimated Costs, and Estimated Time to Construct Summary for C565 - Continued\": \"LDNU\",\n",
    " \"Upgrades, Estimated Costs, and Estimated Time to Construct Summary for C565 -\": \"LDNU\",\n",
    " \"Reliability Network Upgrades to Physically Interconnect\": \"RNU\",\n",
    " 'Reliability Network Upgrade': \"RNU\",\n",
    " \"Reliability Network Upgrades\": \"RNU\",\n",
    "    \"Local Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Area Deliverability Upgrades\": \"ADNU\",\n",
    "    \"Escalated Cost and Time to Construct for Interconnection Facilities, Reliability Network Upgrades, and Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Distribution\": \"ADNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "#df = stable_sort_by_type_of_upgrade(df)  \n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    "\n",
    "\n",
    "def extract_first_value(df, column_name):\n",
    "    \"\"\"\n",
    "    For each entry in `df[column_name]`, extract everything before the first \"(\".\n",
    "    E.g. \"$3,183 (2014)\" -> \"$3,183\". If there's no \"(\", the entire string is kept.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # 1. Ensure the column is treated as string\n",
    "    # 2. Split at the first \"(\" (allowing optional spaces before it)\n",
    "    # 3. Take the left side ([0]) and re‐assign into that column\n",
    "    df[column_name] = (\n",
    "        df[column_name]\n",
    "        .astype(str)\n",
    "        .str.split(r'\\s*\\(', n=1)\n",
    "        .str[0]\n",
    "        .str.strip()  # remove any stray whitespace\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = extract_first_value(df, 'escalated_cost_x_1000')\n",
    " \n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note SGIP-TC), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note SGIP-TC)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = df[df[\"type_of_upgrade\"] != \"may\"]    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ( (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    # 1) Diagnose\n",
    "    dups = df.columns[df.columns.duplicated()]\n",
    "    print(\"👉 Duplicate column names in df:\", dups.tolist())\n",
    "\n",
    "    # SGIP-TC) Drop perfect duplicates\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    \"\"\"\n",
    "    Removes the word 'month' or 'months' (case insensitive) from the value.\n",
    "    Leaves behind any numbers or number ranges (e.g. \"6\", \"6-12\").\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Remove 'month' or 'months' (case-insensitive), optionally with spaces around them.\n",
    "        cleaned_value = re.sub(r'(?i)\\s*months?\\s*', '', value)\n",
    "        \n",
    "        return cleaned_value.strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Then apply it to your column, for example with Pandas:\n",
    "df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "         \n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note SGIP-TC)\"\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "\n",
    "def pick_max_from_range(val):\n",
    "    \"\"\"\n",
    "    Given a value like \"12-24\" or \" 6 - 18 \" (or even \"20\"), return the larger number.\n",
    "    If nothing can be parsed, returns np.nan.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "\n",
    "    s = str(val).strip()\n",
    "    # Split on hyphen (either ASCII \"-\" or any unicode dash)\n",
    "    parts = re.split(r'\\s*[-–—]\\s*', s)\n",
    "    nums = []\n",
    "    for part in parts:\n",
    "        try:\n",
    "            # Convert each piece to float (or int)\n",
    "            nums.append(float(part))\n",
    "        except ValueError:\n",
    "            # If it isn’t purely a number, skip it\n",
    "            continue\n",
    "\n",
    "    if not nums:\n",
    "        return np.nan\n",
    "    return max(nums)\n",
    "\n",
    "# Then apply it:\n",
    "df[\"estimated_time_to_construct\"] = df[\"estimated_time_to_construct\"]\\\n",
    "    .apply(pick_max_from_range)    \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "if 'upgrade' in df.columns:\n",
    "    df['upgrade'] = df['upgrade'].ffill()      \n",
    "\n",
    "\n",
    "df.drop('type_of_upgrade_2', axis=1, inplace=True, errors='ignore') \n",
    "\n",
    "#df= reorder_columns(df)\n",
    "\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/02_intermediate/costs_phase_2_cluster_SGIP-TC_style_R_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/02_intermediate/costs_phase_2_cluster_SGIP-TC_style_R_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_SGIP-TC_style_R_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_SGIP-TC_style_R_total.csv'.\")\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge- Complete replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Scraped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orignals only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to compare the total cost across all types of upgrade as that is given in the pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing required upgrades in totals dataset ===\n",
      "Q_id 521 missing: ['ADNU']\n",
      "Q_id 522 missing: ['ADNU']\n",
      "Q_id 628 missing: ['ADNU']\n",
      "Q_id 632AA missing: ['ADNU']\n",
      "Q_id 644A missing: ['ADNU']\n",
      "Q_id 650AA missing: ['ADNU']\n",
      "Q_id 651A missing: ['ADNU']\n",
      "Q_id 653ED missing: ['ADNU']\n",
      "Q_id 653F missing: ['LDNU', 'ADNU']\n",
      "Q_id 653H missing: ['ADNU']\n",
      "\n",
      "=== Duplicate upgrades in totals dataset ===\n",
      "No duplicates found in totals dataset.\n",
      "\n",
      "⚠️  Found 2 mismatches:\n",
      "    q_id  itemized_total  aggregate_total  difference\n",
      "0   644A          1156.0            428.0       728.0\n",
      "1  653ED          1416.0           2417.0     -1001.0\n",
      "\n",
      "Mismatches written to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/mismatches.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "ITEMIZED_CSV_PATH       = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/02_intermediate/costs_phase_2_cluster_SGIP-TC_style_R_itemized.csv'\n",
    "TOTALS_CSV_PATH         = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/02_intermediate/costs_phase_2_cluster_SGIP-TC_style_R_total.csv'\n",
    "AGGREGATE_CSV_PATH      = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/02_intermediate/costs_phase_2_cluster_SGIP-TC_style_R_aggregate.csv'\n",
    "\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "REQUIRED_UPGRADES       = ['PTO_IF', 'RNU', 'LDNU', 'ADNU']\n",
    "\n",
    "MISMATCHES_CSV_PATH     = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/mismatches.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "itemized_df = pd.read_csv(ITEMIZED_CSV_PATH, dtype={'type_of_upgrade': str})\n",
    "totals_df   = pd.read_csv(TOTALS_CSV_PATH, dtype={'type_of_upgrade': str})\n",
    "agg_df      = pd.read_csv(AGGREGATE_CSV_PATH, dtype=str)\n",
    "\n",
    "# ---------------------- Clean aggregate costs ---------------------- #\n",
    "\n",
    "# Remove $ and commas, then convert to float\n",
    "for col in [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]:\n",
    "    agg_df[col] = (\n",
    "        agg_df[col]\n",
    "        .str.replace(r'[\\$,]', '', regex=True)\n",
    "        .astype(float)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "# ---------------------- Build aggregate lookup ---------------------- #\n",
    "\n",
    "agg_grouped = (\n",
    "    agg_df\n",
    "    .groupby('q_id', as_index=False)\n",
    "    .agg({\n",
    "        TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "        TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "    })\n",
    ")\n",
    "\n",
    "# create lookup\n",
    "agg_lookup = agg_grouped.set_index('q_id').to_dict(orient='index')\n",
    "agg_qids   = set(agg_grouped['q_id'])\n",
    "\n",
    "# ---------------------- Numeric convert itemized ---------------------- #\n",
    "\n",
    "for col in ['estimated_cost_x_1000','escalated_cost_x_1000']:\n",
    "    itemized_df[col] = (\n",
    "        itemized_df[col]\n",
    "        .astype(str)\n",
    "        .str.replace(r'[\\$,]', '', regex=True)\n",
    "        .astype(float)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "# ---------------------- Check missing upgrades for  Q_ids ---------------------- #\n",
    "\n",
    "# ---------------------- Check missing upgrades in totals_df (unconditionally) ---------------------- #\n",
    "\n",
    "print(\"=== Missing required upgrades in totals dataset ===\")\n",
    "missing = []\n",
    "for q in sorted(totals_df['q_id'].unique()):\n",
    "    ups = (\n",
    "        totals_df\n",
    "        .loc[totals_df['q_id'] == q, 'type_of_upgrade']\n",
    "        .dropna()\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    miss = [u for u in REQUIRED_UPGRADES if u not in ups]\n",
    "    if miss:\n",
    "        missing.append((q, miss))\n",
    "\n",
    "if missing:\n",
    "    for q, miss in missing:\n",
    "        print(f\"Q_id {q} missing: {miss}\")\n",
    "else:\n",
    "    print(\"None — every Q_id has all required upgrades in totals_df.\")\n",
    "\n",
    "\n",
    "# ---------------------- Check duplicate upgrades in totals dataset ---------------------- #\n",
    "\n",
    "print(\"\\n=== Duplicate upgrades in totals dataset ===\")\n",
    "dups = []\n",
    "for q, group in totals_df.groupby('q_id'):\n",
    "    dup_types = group['type_of_upgrade'][group['type_of_upgrade'].duplicated()].unique().tolist()\n",
    "    if dup_types:\n",
    "        dups.append((q, dup_types))\n",
    "\n",
    "if dups:\n",
    "    for q, dup in dups:\n",
    "        print(f\"Q_id {q} duplicates: {dup}\")\n",
    "else:\n",
    "    print(\"No duplicates found in totals dataset.\")\n",
    "\n",
    "# ---------------------- Compute per-q_id itemized total ---------------------- #\n",
    "\n",
    "itemized_totals = (\n",
    "    itemized_df[itemized_df['q_id'].isin(agg_qids)]\n",
    "    .groupby('q_id', as_index=False)\n",
    "    .agg({\n",
    "        'estimated_cost_x_1000':'sum',\n",
    "        'escalated_cost_x_1000':'sum'\n",
    "    })\n",
    ")\n",
    "\n",
    "itemized_totals['itemized_total'] = itemized_totals.apply(\n",
    "    lambda r: r['estimated_cost_x_1000'] if r['estimated_cost_x_1000']>0 else r['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Compare against aggregate totals ---------------------- #\n",
    "\n",
    "mismatches = []\n",
    "for _, row in itemized_totals.iterrows():\n",
    "    q = row['q_id']\n",
    "    it = row['itemized_total']\n",
    "    av = agg_lookup[q][TOTALS_ESTIMATED_COLUMN] if agg_lookup[q][TOTALS_ESTIMATED_COLUMN]>0 else agg_lookup[q][TOTALS_ESCALATED_COLUMN]\n",
    "    # skip both zero\n",
    "    if it==0 and av==0:\n",
    "        continue\n",
    "    if abs(it - av) > 1e-6:\n",
    "        mismatches.append({\n",
    "            'q_id': q,\n",
    "            'itemized_total': it,\n",
    "            'aggregate_total': av,\n",
    "            'difference': it - av\n",
    "        })\n",
    "\n",
    "mismatches_df = pd.DataFrame(mismatches)\n",
    "\n",
    "# ---------------------- Report & Save ---------------------- #\n",
    "\n",
    "if mismatches_df.empty:\n",
    "    print(\"\\n✅ All itemized sums match the aggregate totals for Q_ids in aggregate.\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Found {len(mismatches_df)} mismatches:\")\n",
    "    print(mismatches_df)\n",
    "\n",
    "mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "print(f\"\\nMismatches written to {MISMATCHES_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addendums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
