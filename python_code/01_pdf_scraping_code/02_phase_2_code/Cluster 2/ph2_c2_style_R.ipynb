{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2 Style R- table 11.1 and/or 11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note do not run again, had to punch in Q 558 manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process these project‐folders:\n",
      "['555', '557', '558', '559', '560', '568', '576', '577', '579', '581', '585', '586', '607']\n",
      "Skipped PDF: Appendix A - Q557 C1C2 Phase II report - final.pdf from Project 557 (No Table 11)\n",
      "Skipped PDF: Appendix A - Q559 C1C2 Phase II report - final.pdf from Project 559 (No Table 11)\n",
      "Skipped PDF: Appendix A - Q560 C1C2 Phase II report - final.pdf from Project 560 (No Table 11)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1285\u001b[0m\n\u001b[1;32m   1282\u001b[0m     process_pdfs_in_folder()\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1285\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[71], line 1282\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Main function to execute the PDF scraping process.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m     process_pdfs_in_folder()\n",
      "Cell \u001b[0;32mIn[71], line 1143\u001b[0m, in \u001b[0;36mprocess_pdfs_in_folder\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1140\u001b[0m addendum_pdfs\u001b[38;5;241m.\u001b[39mappend(pdf_name)\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1143\u001b[0m     has_table7 \u001b[38;5;241m=\u001b[39m check_has_table7(pdf_path)\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_table7:\n\u001b[1;32m   1146\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m original_has_table7\u001b[38;5;241m.\u001b[39mget(project_id, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1147\u001b[0m             \u001b[38;5;66;03m# Attempt to scrape alternative tables is no longer needed\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m             \u001b[38;5;66;03m# According to the latest request, alternative table scraping is removed\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m             \u001b[38;5;66;03m# Therefore, we skip addendum PDFs that do not have Table 11\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[71], line 923\u001b[0m, in \u001b[0;36mcheck_has_table7\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pdfplumber\u001b[38;5;241m.\u001b[39mopen(pdf_path) \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39mpages:\n\u001b[0;32m--> 923\u001b[0m         text \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mextract_text() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    924\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*11[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.-][1-2]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mIGNORECASE):\n\u001b[1;32m    925\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfplumber/page.py:538\u001b[0m, in \u001b[0;36mPage.extract_text\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_textmap(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtuplify_list_kwargs(kwargs))\u001b[38;5;241m.\u001b[39mas_string\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfplumber/page.py:515\u001b[0m, in \u001b[0;36mPage._get_textmap\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m     defaults\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout_height\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight})\n\u001b[1;32m    514\u001b[0m full_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefaults, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mchars_to_textmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchars, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfplumber/container.py:52\u001b[0m, in \u001b[0;36mContainer.chars\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchars\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_obj_list:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfplumber/page.py:347\u001b[0m, in \u001b[0;36mPage.objects\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objects\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objects: Dict[\u001b[38;5;28mstr\u001b[39m, T_obj_list] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_objects()\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objects\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfplumber/page.py:451\u001b[0m, in \u001b[0;36mPage.parse_objects\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_objects\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, T_obj_list]:\n\u001b[1;32m    450\u001b[0m     objects: Dict[\u001b[38;5;28mstr\u001b[39m, T_obj_list] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_layout_objects(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout\u001b[38;5;241m.\u001b[39m_objs):\n\u001b[1;32m    452\u001b[0m         kind \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manno\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfplumber/page.py:277\u001b[0m, in \u001b[0;36mPage.layout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m device \u001b[38;5;241m=\u001b[39m PDFPageAggregatorWithMarkedContent(\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf\u001b[38;5;241m.\u001b[39mrsrcmgr,\n\u001b[1;32m    273\u001b[0m     pageno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_number,\n\u001b[1;32m    274\u001b[0m     laparams\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf\u001b[38;5;241m.\u001b[39mlaparams,\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    276\u001b[0m interpreter \u001b[38;5;241m=\u001b[39m PDFPageInterpreter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf\u001b[38;5;241m.\u001b[39mrsrcmgr, device)\n\u001b[0;32m--> 277\u001b[0m interpreter\u001b[38;5;241m.\u001b[39mprocess_page(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_obj)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout: LTPage \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mget_result()\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfminer/pdfinterp.py:997\u001b[0m, in \u001b[0;36mPDFPageInterpreter.process_page\u001b[0;34m(self, page)\u001b[0m\n\u001b[1;32m    995\u001b[0m     ctm \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39mx0, \u001b[38;5;241m-\u001b[39my0)\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mbegin_page(page, ctm)\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_contents(page\u001b[38;5;241m.\u001b[39mresources, page\u001b[38;5;241m.\u001b[39mcontents, ctm\u001b[38;5;241m=\u001b[39mctm)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mend_page(page)\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfminer/pdfinterp.py:1016\u001b[0m, in \u001b[0;36mPDFPageInterpreter.render_contents\u001b[0;34m(self, resources, streams, ctm)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_resources(resources)\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_state(ctm)\n\u001b[0;32m-> 1016\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(list_value(streams))\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfminer/pdfinterp.py:1027\u001b[0m, in \u001b[0;36mPDFPageInterpreter.execute\u001b[0;34m(self, streams)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1027\u001b[0m         (_, obj) \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mnextobject()\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m PSEOF:\n\u001b[1;32m   1029\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfminer/psparser.py:609\u001b[0m, in \u001b[0;36mPSStackParser.nextobject\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields a list of objects.\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mArrays and dictionaries are represented as Python lists and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m:return: keywords, literals, strings, numbers, arrays and dictionaries.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults:\n\u001b[0;32m--> 609\u001b[0m     (pos, token) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnexttoken()\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, PSLiteral)):\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;66;03m# normal token\u001b[39;00m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush((pos, token))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfminer/psparser.py:527\u001b[0m, in \u001b[0;36mPSBaseParser.nexttoken\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokens:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfillbuf()\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharpos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharpos)\n\u001b[1;32m    528\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokens\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    529\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnexttoken: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, token)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pdfminer/psparser.py:312\u001b[0m, in \u001b[0;36mPSBaseParser._parse_main\u001b[0;34m(self, s, i)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_literal\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m c\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curtoken \u001b[38;5;241m=\u001b[39m c\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_number\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY =\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/03_raw/ph2_rawdata_cluster2_style_R_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/03_raw/ph2_rawdata_cluster2_style_R_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/03_raw/ph2_scraping_cluster2_style_R_log.txt\"\n",
    "PROJECT_RANGE = range(552, 609)  # Inclusive range for q_ids in Clusters 2 range(667, 860)\n",
    "\n",
    "\n",
    "# Read the CSV file containing processed projects (with q_id column)\n",
    "processed_csv_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters/costs_phase_2_all_clusters_total.csv\"  # UPDATE THIS PATH\n",
    "processed_df = pd.read_csv(processed_csv_path)\n",
    "# Convert q_id values to numeric then to int for filtering\n",
    "processed_q_ids = pd.to_numeric(processed_df['q_id'], errors='coerce').dropna().astype(int).unique()\n",
    "# Now build the list of folders to process:\n",
    "projects_to_process = []\n",
    "for folder in os.listdir(BASE_DIRECTORY):\n",
    "    full = os.path.join(BASE_DIRECTORY, folder)\n",
    "    if not os.path.isdir(full):\n",
    "        continue\n",
    "    m = re.match(r'^(\\d+)', folder)\n",
    "    if not m:\n",
    "        continue\n",
    "    num = int(m.group(1))\n",
    "    if num in PROJECT_RANGE and num not in processed_q_ids:\n",
    "        projects_to_process.append(folder)\n",
    "\n",
    "# Sort by the integer prefix, not lexicographically as strings\n",
    "projects_to_process = sorted(projects_to_process, key=lambda f: int(re.match(r'^(\\d+)', f).group(1)))\n",
    "\n",
    "\n",
    "print(\"Will process these project‐folders:\")\n",
    "print(projects_to_process)\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "style_n_pdfs = []  # List to track style N PDFs\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "total_pdfs_skipped_extraction = 0\n",
    "original_has_table7 = {}  # Dictionary to track if original PDFs have table7\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters, but keeps parentheses.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            # collapse internal whitespace\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            # strip out everything except letters, digits, spaces, and parentheses\n",
    "            header = re.sub(r'[^a-z0-9\\s\\(\\)]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    elif value is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(value).replace('\\n', ' ').strip()\n",
    "     \n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "         \n",
    "        \n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Conditionally Assigned Network Upgrades\",\n",
    "        \"Local Off-Peak Network Upgrade\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if  re.search(rf\"\\b{re.escape(phrase)}\\b(?=\\d|\\W|$)\", title, re.IGNORECASE):\n",
    "        \n",
    "         #re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*2[\\.-]1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 2)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id =  str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        # Updated Cluster Extraction\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        if '2' in clusters:\n",
    "            cluster_number = '2'\n",
    "        elif clusters:\n",
    "            cluster_number = max(clusters, key=lambda x: int(x))  # Choose the highest cluster number found\n",
    "        else:\n",
    "            cluster_number = '2'  # Default to 2 if not found\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 11 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 11 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 11 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 11-1\" to \"Table 11-2\" with hyphen or dot\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*11\\s*[\\.-]\\s*[1-2]\\s*:\", text, re.IGNORECASE): # the \\s* is to match any whitespace between the table number and the colon\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 11-1 to 11-2 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus two to include possible continuation\n",
    "\n",
    "            print(f\"Table 11 starts on page {scrape_start} and ends on page {scrape_end}\", file=log_file)\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                    \n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*11[\\.-]([1-2])[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                try:\n",
    "                                    table_title = match.group(3).strip()  # match.group(2) is just the sub-table number (\"1\" or \"2\")   # match.group(3) is everything after \"Table 11-X:\", e.g. \"Upgrades, Estimated Costs,\n",
    "                                except IndexError:\n",
    "                                    table_title = match.group(0).strip()\n",
    "                                    print(\"Fallback to whole match for table title\", file=log_file)\n",
    "\n",
    " \n",
    "                \n",
    "\n",
    "\n",
    "                    if table_title:\n",
    "                        print(f\"Table Title: {table_title}\")\n",
    "                        if re.search(r\"PTO Interconnection Facilities Cost Estimate Summary\", table_title, re.IGNORECASE):\n",
    "                            print(f\"Skipping Table 11-1 PTO on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    " \n",
    "\n",
    "                        \n",
    "\n",
    "                        # New Table 11 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New Table 11 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        headers = clean_string_cell(tab[0])\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                            # ← INSERT this block:\n",
    "                        if df_new.empty:\n",
    "                            # store an empty DF with the right columns,\n",
    "                            # so that continuation blocks can append to it\n",
    "                            extracted_tables.append(pd.DataFrame(columns=headers))\n",
    "                            print(f\"Header-only Table 11 (‘{specific_phrase}’) detected on page {page_number+1}; waiting for continuation…\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        # Handle ADNU-specific grouping\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    # Group all adnu rows into one 'upgrade' row\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    # If 'type of upgrade' exists, just rename adnu if needed\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row is none, replace only first row if needed\n",
    "                                if df_new.empty:\n",
    "                                    # should never happen once you’ve done step 1, but safe to check\n",
    "                                    continue\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' first row for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            # Non-ADNU new tables\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row is none, replace only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for the first row in new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        #if df_new.columns.duplicated().any():\n",
    "                        #    print(\"Duplicate columns detected in new table. Dropping duplicates.\", file=log_file)\n",
    "                        #    df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(\"Duplicate columns detected in new table. Renaming instead of dropping.\", file=log_file)\n",
    "\n",
    "\n",
    "                            \n",
    "\n",
    "                            # Build a new list of column names, appending _1, _2, … to repeats\n",
    "                            new_cols = []\n",
    "                            counts = {}  # keep track of how many times we've seen each base name\n",
    "                            for orig in df_new.columns:\n",
    "                                # 1) Decide on a non‐blank base name:\n",
    "                                #    If `orig` is blank/None/whitespace, use \"column\" instead.\n",
    "                                if pd.isna(orig) or str(orig).strip() == \"\":\n",
    "                                    base = \"column\"\n",
    "                                else:\n",
    "                                    base = str(orig).strip()\n",
    "\n",
    "                                # 2) Increment a counter for that base‐name:\n",
    "                                if base not in counts:\n",
    "                                    counts[base] = 0\n",
    "                                    new_cols.append(base)\n",
    "                                else:\n",
    "                                    counts[base] += 1\n",
    "                                    new_cols.append(f\"{base}_{counts[base]}\")\n",
    " \n",
    "                            df_new.columns = new_cols\n",
    "\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table\n",
    "                        if not extracted_tables:\n",
    "                            print(f\"No previous Table 11 detected to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        last_table = extracted_tables[-1]\n",
    "                        expected_columns = last_table.columns.tolist()\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "\n",
    "                        # Check if the first row is a header row\n",
    "                        #  we will treat all continuation table rows as data points\n",
    "                        # without any header detection\n",
    "                        # However,  checking if there is a header row first,  \n",
    "                        # Detect if first row is a header\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\", \"MW at POI\", \"upgrade\"]\n",
    "                        first_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            any(re.search(rf\"\\b{kw}\\b\", clean_string_cell(cell).lower()) for kw in header_keywords)\n",
    "                            for cell in first_row\n",
    "                        )\n",
    "\n",
    "                        if is_header_row:\n",
    "                            # Handle header row in continuation table\n",
    "                            headers = clean_string_cell(first_row)\n",
    "                            headers = clean_column_headers(first_row)\n",
    "                            data_rows = data_rows[1:]  # Exclude header row\n",
    "\n",
    "                            # Update expected_columns by adding new columns if any\n",
    "                            new_columns = [col for col in headers if col not in expected_columns]\n",
    "                            if new_columns:\n",
    "                                expected_columns.extend(new_columns)\n",
    "                                print(f\"Added new columns from continuation table: {new_columns}\", file=log_file)\n",
    "\n",
    "                            # Create a mapping of new columns to add with default NaN\n",
    "                            for new_col in new_columns:\n",
    "                                last_table[new_col] = pd.NA\n",
    "\n",
    "                            # Reindex last_table to include new columns\n",
    "                            last_table = last_table.reindex(columns=expected_columns)\n",
    "                            extracted_tables[-1] = last_table\n",
    "\n",
    "                            # Update 'type of upgrade' column in the first row if needed\n",
    "                            if \"type of upgrade\" in headers:\n",
    "                                type_upgrade_idx = headers.index(\"type of upgrade\")\n",
    "                                if pd.isna(data_rows[0][type_upgrade_idx]) or data_rows[0][type_upgrade_idx] == \"\":\n",
    "                                    data_rows[0][type_upgrade_idx] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'type of upgrade' first row for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            elif \"upgrade\" in headers:\n",
    "                                upgrade_idx = headers.index(\"upgrade\")\n",
    "                                if pd.isna(data_rows[0][upgrade_idx]) or data_rows[0][upgrade_idx] == \"\":\n",
    "                                    data_rows[0][upgrade_idx] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'upgrade' first row for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' or 'upgrade' does not exist, add it\n",
    "                                headers.append(\"type of upgrade\")\n",
    "                                expected_columns.append(\"type of upgrade\")\n",
    "                                for idx, row in enumerate(data_rows):\n",
    "                                    data_rows[idx].append(specific_phrase)\n",
    "                                print(f\"Added 'type of upgrade' column and filled with '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                            # Handle ADNU-specific logic if applicable\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                if \"adnu\" in headers:\n",
    "                                    if \"upgrade\" not in headers:\n",
    "                                        # Rename 'adnu' to 'upgrade'\n",
    "                                        adnu_idx = headers.index(\"adnu\")\n",
    "                                        headers[adnu_idx] = \"upgrade\"\n",
    "                                        for row in data_rows:\n",
    "                                            row[adnu_idx] = \" \".join([str(cell) for cell in row[adnu_idx] if pd.notna(cell)])\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in continuation ADNU table.\", file=log_file)\n",
    "                                # Ensure 'type of upgrade' column is filled\n",
    "                                if \"type of upgrade\" not in headers:\n",
    "                                    headers.append(\"type of upgrade\")\n",
    "                                    expected_columns.append(\"type of upgrade\")\n",
    "                                    for row in data_rows:\n",
    "                                        row.append(specific_phrase)\n",
    "                                    print(\"Added 'type of upgrade' column with specific phrase for continuation ADNU table.\", file=log_file)\n",
    "\n",
    "                        else:\n",
    "                            # No header row detected, treat all rows as data points\n",
    "                            print(f\"No header row detected in continuation table on page {page_number + 1}, table {table_index + 1}. Treating all rows as data.\", file=log_file)\n",
    "\n",
    "                        # Create DataFrame for continuation table\n",
    "                        if is_header_row:\n",
    "                            try:\n",
    "                                df_continuation = pd.DataFrame(data_rows, columns=headers)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "                        else:\n",
    "                            # Create DataFrame with expected_columns\n",
    "                            # Handle cases where continuation table has more columns\n",
    "                            standardized_data = []\n",
    "                            for row in data_rows:\n",
    "                                if len(row) < len(expected_columns):\n",
    "                                    # Insert 'type of upgrade' or 'upgrade' with specific_phrase\n",
    "                                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                        # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                        missing_cols = len(expected_columns) - len(row)\n",
    "                                        #row += [specific_phrase] * missing_cols\n",
    "                                        data_rows = [row[:2] + [specific_phrase] + row[2:] for row in data_rows]\n",
    "                                        print(f\"Inserted '{specific_phrase}' for missing columns in ADNU continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                    else:\n",
    "                                        # For non-ADNU tables, assume missing \"type of upgrade\" column\n",
    "                                        missing_cols = len(expected_columns) - len(row)\n",
    "                                        #row += [specific_phrase] * missing_cols\n",
    "                                        data_rows = [ [specific_phrase]  for row in data_rows]\n",
    "                                        print(f\"Inserted '{specific_phrase}' for missing columns in non-ADNU continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                elif len(row) > len(expected_columns):\n",
    "                                    # Add new columns with default names\n",
    "                                    extra_cols = len(row) - len(expected_columns)\n",
    "                                    for i in range(extra_cols):\n",
    "                                        new_col_name = f\"column{len(expected_columns) + 1 + i}\"\n",
    "                                        expected_columns.append(new_col_name)\n",
    "                                        last_table[new_col_name] = pd.NA\n",
    "                                        print(f\"Added new column '{new_col_name}' for extra data in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                                    row = row[:len(expected_columns)]\n",
    "\n",
    "                                row_dict = dict(zip(expected_columns, [clean_string_cell(cell) for cell in row]))\n",
    "\n",
    "                                # Handle 'type of upgrade' column\n",
    "                                if \"type of upgrade\" in row_dict and (pd.isna(row_dict[\"type of upgrade\"]) or row_dict[\"type of upgrade\"] == \"\"):\n",
    "                                    row_dict[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"Replaced None in 'type of upgrade' for a row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "                                standardized_data.append(row_dict)\n",
    "\n",
    "                            try:\n",
    "                                df_continuation = pd.DataFrame(standardized_data, columns=expected_columns)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "\n",
    "\n",
    "                             # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                if \"type of upgrade\" in df_continuation.columns:\n",
    "                                    first_row = df_continuation.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                        df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                else:\n",
    "                                    # If \"type of upgrade\" column does not exist, add it\n",
    "                                    df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                            else:\n",
    "                                # General Handling for other tables\n",
    "                                if \"type of upgrade\" in df_continuation.columns:\n",
    "                                    first_row = df_continuation.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                        df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                else:\n",
    "                                    # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                    df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. renaming duplicates.\", file=log_file)\n",
    "                                                        # Build a new list of column names, appending _1, _2, … to repeats\n",
    "                            new_cols = []\n",
    "                            counts = {}  # keep track of how many times we've seen each base name\n",
    "                            for orig in df_continuation.columns:\n",
    "                                # 1) Decide on a non‐blank base name:\n",
    "                                #    If `orig` is blank/None/whitespace, use \"column\" instead.\n",
    "                                if pd.isna(orig) or str(orig).strip() == \"\":\n",
    "                                    base = \"column\"\n",
    "                                else:\n",
    "                                    base = str(orig).strip()\n",
    "\n",
    "                                # 2) Increment a counter for that base‐name:\n",
    "                                if base not in counts:\n",
    "                                    counts[base] = 0\n",
    "                                    new_cols.append(base)\n",
    "                                else:\n",
    "                                    counts[base] += 1\n",
    "                                    new_cols.append(f\"{base}_{counts[base]}\")\n",
    " \n",
    "                            df_continuation.columns = new_cols\n",
    "                            #df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                        print(f\"Appended continuation table data to the last extracted table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 11 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After processing all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 11 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 11 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "\n",
    "'''\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 11 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 11 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 11 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 11 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "'''\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 11 data and merges with base data.\n",
    "    Returns:\n",
    "      df       – either base_data or base_data×Table4 rows merged\n",
    "      status   – one of \"no_marker\", \"failed\", or \"success\"\n",
    "    \"\"\"\n",
    "    # 1) Pull out base data\n",
    "    base_data   = extract_base_data(pdf_path, project_id, log_file)\n",
    "    # 2) Did we even see a Table 11 marker in the text?\n",
    "    has_marker  = check_has_table7(pdf_path)\n",
    "    if not has_marker:\n",
    "        print(f\"No Table 11 marker found in {os.path.basename(pdf_path)}; skipping extraction.\", \n",
    "              file=log_file)\n",
    "        return base_data, \"no_marker\"\n",
    "\n",
    "    # 2) Try to scrape Table 11\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "    if table7_data.empty:\n",
    "        print(f\"Table 11 marker found in {os.path.basename(pdf_path)}, \"\n",
    "              f\"but extraction returned empty DataFrame.\", file=log_file)\n",
    "        return base_data, \"failed\"\n",
    "\n",
    "    # 2) We got actual rows → merge and return\n",
    "    #    Drop any overlapping columns first\n",
    "    overlapping = base_data.columns.intersection(table7_data.columns)\n",
    "    if not overlapping.empty:\n",
    "        table7_data = table7_data.drop(columns=overlapping, errors=\"ignore\")\n",
    "\n",
    "    #    Repeat base_data for each row of table7_data\n",
    "    base_rep   = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "    merged_df  = pd.concat([base_rep, table7_data.reset_index(drop=True)], axis=1, sort=False)\n",
    "\n",
    "    print(f\"Merged base data with {len(table7_data)} row(s) of Table 11 for \"\n",
    "          f\"{os.path.basename(pdf_path)}.\", file=log_file)\n",
    "    return merged_df, \"success\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 11-1 to 2-2.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*11[\\.-][1-2]\\b\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def has_network_upgrade_type_column(pdf_path, log_file):\n",
    "    \"\"\"Checks if any table in the PDF has a column header 'Network Upgrade Type'.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables()\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    headers = clean_column_headers(tab[0])\n",
    "                    if \"network upgrade type\" in headers:\n",
    "                        print(f\"Found 'Network Upgrade Type' in PDF {pdf_path} on page {page_number}, table {table_index}.\", file=log_file)\n",
    "                        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking 'Network Upgrade Type' in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "    return False\n",
    "\n",
    " \n",
    "\n",
    "def is_addendum(pdf_path, log_file):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching for 'Addendum', 'Addendum #3', or 'Revision' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            print(f\"Extracted Text: {text}\", file=log_file)  # Debug\n",
    "\n",
    "            # Compile a pattern that matches:\n",
    "            #   • “Addendum” or “ADDENDUM”\n",
    "            #   • optionally followed by whitespace, a ‘#’, then digits (e.g. “Addendum #3”)\n",
    "            #   • OR the word “Revision”\n",
    "            pattern = re.compile(\n",
    "                r\"\\b(?:addendum(?:\\s*#\\s*\\d+)?|revision)\\b\",\n",
    "                re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            return bool(pattern.search(text))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    \"\"\"\n",
    "    Appends a suffix to duplicate headers to make them unique.\n",
    "\n",
    "    Args:\n",
    "        headers (list): List of column headers.\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique column headers.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped, total_pdfs_skipped_extraction\n",
    "\n",
    "    SKIP_PROJECTS = {1860, 2003, 2006}\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "\n",
    "\n",
    "        for project_id in projects_to_process:\n",
    "            \n",
    "            # Skip the projects in the SKIP_PROJECTS set\n",
    "            if project_id in SKIP_PROJECTS:\n",
    "                print(f\"Skipping Project {project_id} (marked to skip)\", file=log_file)\n",
    "                continue\n",
    "\n",
    "         \n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"03_phase_2_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Separate PDFs into originals and addendums\n",
    "            list_pdfs = [pdf for pdf in os.listdir(project_path) if pdf.endswith(\".pdf\")]\n",
    "            originals = []\n",
    "            addendums = []\n",
    "            for pdf_name in list_pdfs:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                if is_addendum(pdf_path, log_file):\n",
    "                    addendums.append(pdf_name)\n",
    "                else:\n",
    "                    originals.append(pdf_name)\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Process original PDFs first\n",
    "            for pdf_name in originals:\n",
    "                \n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    # Still check if original has table7\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                original_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "                    original_has_table7[project_id] = has_table7\n",
    "\n",
    "                    if not has_table7:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 11)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 11)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    # Extract Table 11 and merge\n",
    "                    '''\n",
    "                    df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False)\n",
    "                    if not df.empty:\n",
    "                        core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        project_scraped = True\n",
    "                        total_pdfs_scraped += 1\n",
    "                        print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                    else:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "                    '''\n",
    "                        # Extract Table 11 and merge\n",
    "                    df, status = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False)\n",
    "\n",
    "                    if status == \"success\":\n",
    "                        core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        project_scraped = True\n",
    "                        total_pdfs_scraped += 1\n",
    "                        print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "\n",
    "                    elif status == \"failed\":\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        total_pdfs_skipped_extraction += 1\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Table 11 found but extraction failed)\"\n",
    "                             )\n",
    "\n",
    "                    else:  # status == \"no_marker\"\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        total_pdfs_skipped += 1\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 11 present)\" )\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # **START OF CHANGES**\n",
    "            # Then process addendum PDFs\n",
    "            for pdf_name in addendums:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                if has_network_upgrade_type_column(pdf_path, log_file):\n",
    "                    style_n_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipping PDF: {pdf_name} from Project {project_id} (Style N)\", file=log_file)\n",
    "                    continue  # Skip processing this PDF\n",
    "\n",
    "                print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                addendum_pdfs.append(pdf_name)\n",
    "\n",
    "                try:\n",
    "                    has_table7 = check_has_table7(pdf_path)\n",
    "\n",
    "                    if not has_table7:\n",
    "                        if original_has_table7.get(project_id, False):\n",
    "                            # Attempt to scrape alternative tables is no longer needed\n",
    "                            # According to the latest request, alternative table scraping is removed\n",
    "                            # Therefore, we skip addendum PDFs that do not have Table 11\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 11)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 11)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 11 and original does not have Table 11)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No Table 11 and original does not have Table 11)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not is_add and not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    if is_add and base_data_extracted:\n",
    "                        # For addendums, use the extracted base data\n",
    "                        table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                        if table7_data.empty and original_has_table7.get(project_id, False):\n",
    "                            # Scrape alternative tables is removed, so skip if no data\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (No relevant tables found)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                        if not table7_data.empty:\n",
    "                            # Merge base data with Table 11 data\n",
    "                            merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                            merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                            core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                            scraped_pdfs.append(pdf_name)\n",
    "                            scraped_projects.add(project_id)\n",
    "                            project_scraped = True\n",
    "                            total_pdfs_scraped += 1\n",
    "                            print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                        else:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                            print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    # Optionally, print to ipynb\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "            # **END OF CHANGES**\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "    # Rest of the code remains unchanged...\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped due to failed extraction of Table: {total_pdfs_skipped_extraction}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nList of Style N PDFs (Skipped due to 'Network Upgrade Type'):\")\n",
    "    print(style_n_pdfs)\n",
    "\n",
    "    print(\"\\nTotal Number of Style N PDFs:\", len(style_n_pdfs))\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.applymap(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    " \n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized and Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: ['q_id', 'cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection', 'type of upgrade', 'upgrade', 'description', 'cost allocation factor', 'estimated time to construct (note 3)', 'estimated cost x 1000 constant dollar (od year) (note 4)', 'upgrade (may include the following)', 'estimated cost x 1000 constant dollar (2011) (note 4)', 'Unnamed: 15', 'column_1', 'column_12', 'column_2', 'estimated_1', 'column_10', 'column_3', 'column_8', 'column_5', 'column_6', 'column', 'column_11', 'column_7', 'column_13', 'column_14', 'column_9', 'estimated', 'column_4', 'estimated time to construct (note 1)', 'estimated cost (x 1000)', 'estimated cost x 1000 constant dollar (od year) (note 2)']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/03_raw/ph2_rawdata_cluster2_style_R_originals.csv\", dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "\n",
    "\n",
    "#df.columns = clean_column_headers(df.columns)\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "#df = df.map(clean_string_cell)\n",
    "#df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "print(\"After cleaning:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 552\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 561\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 565\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 569\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 574\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 583\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 588\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 589\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 590\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 593\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 602\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 606\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 608\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "     q_id  cluster req_deliverability  latitude  longitude  capacity  \\\n",
      "0    552        2               None       NaN        NaN       NaN   \n",
      "1    552        2               None       NaN        NaN       NaN   \n",
      "2    552        2               None       NaN        NaN       NaN   \n",
      "3    561        2               Full       NaN        NaN       NaN   \n",
      "4    561        2               Full       NaN        NaN       NaN   \n",
      "5    561        2               Full       NaN        NaN       NaN   \n",
      "6    565        2               Full       NaN        NaN       9.0   \n",
      "7    565        2               Full       NaN        NaN       9.0   \n",
      "8    565        2               Full       NaN        NaN       9.0   \n",
      "9    569        2               None       NaN        NaN       NaN   \n",
      "10   569        2               None       NaN        NaN       NaN   \n",
      "11   574        2               Full       NaN        NaN       9.0   \n",
      "12   574        2               Full       NaN        NaN       9.0   \n",
      "13   574        2               Full       NaN        NaN       9.0   \n",
      "14   583        2               None       NaN        NaN       NaN   \n",
      "15   583        2               None       NaN        NaN       NaN   \n",
      "16   583        2               None       NaN        NaN       NaN   \n",
      "17   588        2               None       NaN        NaN       NaN   \n",
      "18   588        2               None       NaN        NaN       NaN   \n",
      "19   588        2               None       NaN        NaN       NaN   \n",
      "20   589        2               None       NaN        NaN       NaN   \n",
      "21   589        2               None       NaN        NaN       NaN   \n",
      "22   589        2               None       NaN        NaN       NaN   \n",
      "23   590        2               Full       NaN        NaN       NaN   \n",
      "24   590        2               Full       NaN        NaN       NaN   \n",
      "25   590        2               Full       NaN        NaN       NaN   \n",
      "26   593        2               None       NaN        NaN       NaN   \n",
      "27   593        2               None       NaN        NaN       NaN   \n",
      "28   593        2               None       NaN        NaN       NaN   \n",
      "29   602        2               Full       NaN        NaN       NaN   \n",
      "30   602        2               Full       NaN        NaN       NaN   \n",
      "31   602        2               Full       NaN        NaN       NaN   \n",
      "32   606        2               None       NaN        NaN       NaN   \n",
      "33   606        2               None       NaN        NaN       NaN   \n",
      "34   608        2               Full       NaN        NaN       NaN   \n",
      "35   608        2               Full       NaN        NaN       NaN   \n",
      "36   608        2               Full       NaN        NaN       NaN   \n",
      "\n",
      "                        point_of_interconnection type_of_upgrade  \\\n",
      "0                         Proposed Jasper 220 kV      Total LDNU   \n",
      "1                         Proposed Jasper 220 kV    Total PTO_IF   \n",
      "2                         Proposed Jasper 220 kV       Total RNU   \n",
      "3       230 kV bus at Imperial Valley Substation      Total LDNU   \n",
      "4       230 kV bus at Imperial Valley Substation    Total PTO_IF   \n",
      "5       230 kV bus at Imperial Valley Substation       Total RNU   \n",
      "6    Miguel-Mission 230 kV Line (TL 23023) via a      Total LDNU   \n",
      "7    Miguel-Mission 230 kV Line (TL 23023) via a    Total PTO_IF   \n",
      "8    Miguel-Mission 230 kV Line (TL 23023) via a       Total RNU   \n",
      "9    Table Mountain  Tesla 500 kV Line (Loop-in)    Total PTO_IF   \n",
      "10   Table Mountain  Tesla 500 kV Line (Loop-in)       Total RNU   \n",
      "11                   Otay Mesa 230 kV switchyard      Total LDNU   \n",
      "12                   Otay Mesa 230 kV switchyard    Total PTO_IF   \n",
      "13                   Otay Mesa 230 kV switchyard       Total RNU   \n",
      "14   Proposed 138 kV Bus at Boulevard Substation      Total LDNU   \n",
      "15   Proposed 138 kV Bus at Boulevard Substation    Total PTO_IF   \n",
      "16   Proposed 138 kV Bus at Boulevard Substation       Total RNU   \n",
      "17  Serial project proposed Red Bluff 220 kV Bus      Total LDNU   \n",
      "18  Serial project proposed Red Bluff 220 kV Bus    Total PTO_IF   \n",
      "19  Serial project proposed Red Bluff 220 kV Bus       Total RNU   \n",
      "20                             Victor 115 kV Bus      Total LDNU   \n",
      "21                             Victor 115 kV Bus    Total PTO_IF   \n",
      "22                             Victor 115 kV Bus       Total RNU   \n",
      "23      230 kV Bus at Imperial Valley Substation      Total LDNU   \n",
      "24      230 kV Bus at Imperial Valley Substation    Total PTO_IF   \n",
      "25      230 kV Bus at Imperial Valley Substation       Total RNU   \n",
      "26        Jointly-owned Mohave 500 kV switchyard      Total LDNU   \n",
      "27        Jointly-owned Mohave 500 kV switchyard    Total PTO_IF   \n",
      "28        Jointly-owned Mohave 500 kV switchyard       Total RNU   \n",
      "29               Whirlwind Substation 220 kV Bus      Total LDNU   \n",
      "30               Whirlwind Substation 220 kV Bus    Total PTO_IF   \n",
      "31               Whirlwind Substation 220 kV Bus       Total RNU   \n",
      "32    Schulte Switching Station. This Project is    Total PTO_IF   \n",
      "33    Schulte Switching Station. This Project is       Total RNU   \n",
      "34      230 kV Bus at Imperial Valley Substation      Total LDNU   \n",
      "35      230 kV Bus at Imperial Valley Substation    Total PTO_IF   \n",
      "36      230 kV Bus at Imperial Valley Substation       Total RNU   \n",
      "\n",
      "   type_of_upgrade_2 upgrade description cost_allocation_factor  \\\n",
      "0                                                                 \n",
      "1                                                                 \n",
      "2                                                                 \n",
      "3                                                                 \n",
      "4                                                                 \n",
      "5                                                                 \n",
      "6                                                                 \n",
      "7                                                                 \n",
      "8                                                                 \n",
      "9                                                                 \n",
      "10                                                                \n",
      "11                                                                \n",
      "12                                                                \n",
      "13                                                                \n",
      "14                                                                \n",
      "15                                                                \n",
      "16                                                                \n",
      "17                                                                \n",
      "18                                                                \n",
      "19                                                                \n",
      "20                                                                \n",
      "21                                                                \n",
      "22                                                                \n",
      "23                                                                \n",
      "24                                                                \n",
      "25                                                                \n",
      "26                                                                \n",
      "27                                                                \n",
      "28                                                                \n",
      "29                                                                \n",
      "30                                                                \n",
      "31                                                                \n",
      "32                                                                \n",
      "33                                                                \n",
      "34                                                                \n",
      "35                                                                \n",
      "36                                                                \n",
      "\n",
      "    estimated_cost_x_1000 escalated_cost_x_1000 estimated_time_to_construct  \\\n",
      "0                   14899                                                     \n",
      "1                    7327                                                     \n",
      "2                    3815                                                     \n",
      "3                   73844                                                     \n",
      "4                     216                                                     \n",
      "5                    3601                                                     \n",
      "6                   28268                                                     \n",
      "7                     218                                                     \n",
      "8                   30870                                                     \n",
      "9                     541                                                     \n",
      "10                  77267                                                     \n",
      "11                  86505                                                     \n",
      "12                    218                                                     \n",
      "13                   8642                                                     \n",
      "14                      0                                                     \n",
      "15                   1407                                                     \n",
      "16                    314                                                     \n",
      "17                 257946                                                     \n",
      "18                  10013                                                     \n",
      "19                   3711                                                     \n",
      "20                  14853                                                     \n",
      "21                  20530                                                     \n",
      "22                   2068                                                     \n",
      "23                  55383                                                     \n",
      "24                    202                                                     \n",
      "25                   2749                                                     \n",
      "26                 118768                                                     \n",
      "27                  12966                                                     \n",
      "28                  22439                                                     \n",
      "29                    150                                                     \n",
      "30                  10132                                                     \n",
      "31                   3686                                                     \n",
      "32                      0                                                     \n",
      "33                      0                                                     \n",
      "34                  92307                                                     \n",
      "35                    215                                                     \n",
      "36                   4450                                                     \n",
      "\n",
      "   item  \n",
      "0    no  \n",
      "1    no  \n",
      "2    no  \n",
      "3    no  \n",
      "4    no  \n",
      "5    no  \n",
      "6    no  \n",
      "7    no  \n",
      "8    no  \n",
      "9    no  \n",
      "10   no  \n",
      "11   no  \n",
      "12   no  \n",
      "13   no  \n",
      "14   no  \n",
      "15   no  \n",
      "16   no  \n",
      "17   no  \n",
      "18   no  \n",
      "19   no  \n",
      "20   no  \n",
      "21   no  \n",
      "22   no  \n",
      "23   no  \n",
      "24   no  \n",
      "25   no  \n",
      "26   no  \n",
      "27   no  \n",
      "28   no  \n",
      "29   no  \n",
      "30   no  \n",
      "31   no  \n",
      "32   no  \n",
      "33   no  \n",
      "34   no  \n",
      "35   no  \n",
      "36   no  \n",
      "👉 Duplicate column names in df: []\n",
      "Itemized rows saved to 'costs_phase_2_cluster_2_style_R_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_2_style_R_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU']\n",
      "[552 561 565 569 574 583 588 589 590 593 602 606 608]\n",
      "[2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_37248/1439461231.py:465: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean[required_cols] = df_clean[required_cols].applymap(lambda x: str(x).strip())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_37248/1439461231.py:955: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/03_raw/ph2_rawdata_cluster2_style_R_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    " \n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_cost_allocation(df, source_col, target_col=\"cost_allocation_factor\"):\n",
    "    \"\"\"\n",
    "    Extracts percentage values from a specified source column and moves them into a target column.\n",
    "    \n",
    "    - A percentage value is defined as a string that, when stripped of whitespace,\n",
    "      fully matches a pattern of digits (with optional commas or periods) followed by a percent sign.\n",
    "    - If a cell in the source column matches this pattern, its value is placed into the target column,\n",
    "      and the source column cell is cleared (set to an empty string).\n",
    "    - If the cell does not match a percentage pattern, it is left untouched in the source column.\n",
    "    \n",
    "    Parameters:\n",
    "      df         : pandas DataFrame.\n",
    "      source_col : string, the name of the column to scan for percentage values.\n",
    "      target_col : string, the name of the column to store the extracted percentage values.\n",
    "                   Defaults to \"cost_allocation_factor\".\n",
    "    \n",
    "    Returns:\n",
    "      The DataFrame with the updated columns.\n",
    "    \"\"\"\n",
    "    # Define a regex pattern to match a percentage value (e.g., \"78.25%\").\n",
    "    # The pattern allows digits, commas, and periods, followed immediately by a \"%\" (ignoring leading/trailing spaces).\n",
    "    pattern = r\"^\\s*[\\d,\\.]+%\\s*$\"\n",
    "    \n",
    "    def extract_percentage(text):\n",
    "        # If text matches the percentage pattern, return the stripped text; otherwise, return None.\n",
    "        if isinstance(text, str) and re.fullmatch(pattern, text):\n",
    "            return text.strip()\n",
    "        return None\n",
    "\n",
    "    def clear_percentage(text):\n",
    "        # If text matches the percentage pattern, clear it (return an empty string).\n",
    "        # Otherwise, return the text stripped of surrounding whitespace.\n",
    "        if isinstance(text, str) and re.fullmatch(pattern, text):\n",
    "            return \"\"\n",
    "        if isinstance(text, str):\n",
    "            return text.strip()\n",
    "        return text\n",
    "\n",
    "    # Create (or overwrite) the target column with extracted percentage values from the source column.\n",
    "    df[target_col] = df[source_col].apply(extract_percentage)\n",
    "    # In the source column, remove any percentage values (leaving other text intact).\n",
    "    df[source_col] = df[source_col].apply(clear_percentage)\n",
    "    \n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "def filter_numeric_costs(df, col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame and column name, this function extracts the numeric cost from each cell,\n",
    "    converting values with an optional '$' sign (and possible commas) to floats.\n",
    "    If a valid numeric cost cannot be extracted, the cell is set to NaN.\n",
    "    \n",
    "    Parameters:\n",
    "      df  : pandas DataFrame.\n",
    "      col : string, the name of the column to process.\n",
    "      \n",
    "    Returns:\n",
    "      The original DataFrame with the specified column converted to numeric values (or NaN if conversion fails).\n",
    "    \"\"\"\n",
    "    def extract_numeric(value):\n",
    "        value_str = str(value)\n",
    "        # This regex matches an optional '$', optional spaces, and a number with commas and an optional decimal part.\n",
    "        match = re.search(r'\\$?\\s*([\\d,]+(?:\\.\\d+)?)', value_str)\n",
    "        if match:\n",
    "            num_str = match.group(1).replace(',', '')\n",
    "            try:\n",
    "                return float(num_str)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    # Apply the extraction function to the specified column.\n",
    "    df[col] = df[col].apply(extract_numeric)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def extract_months_values(df, col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame and column name, this function extracts text patterns matching\n",
    "    durations expressed in months (e.g., \"32 months\", \"43 Month\", \"23 Months\", \"22Months\", or \"21-29months\").\n",
    "    If a valid pattern is found, it returns the matched text; otherwise, it returns an empty string.\n",
    "    \n",
    "    Parameters:\n",
    "        df  : pandas DataFrame.\n",
    "        col : string, the name of the column to process.\n",
    "        \n",
    "    Returns:\n",
    "        The DataFrame with the specified column updated.\n",
    "    \"\"\"\n",
    "    def extract_months(text):\n",
    "        text = str(text)\n",
    "        # Pattern explanation:\n",
    "        #   \\d+          : one or more digits\n",
    "        #   (?:-\\d+)?    : optionally, a hyphen followed by one or more digits (to capture ranges like 21-29)\n",
    "        #   \\s*          : optional whitespace\n",
    "        #   [Mm]onths?   : \"month\" or \"months\" (case insensitive for the first letter)\n",
    "        pattern = r'(\\d+(?:-\\d+)?\\s*[Mm]onths?)'\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group(1) if match else \"\"\n",
    "    \n",
    "    df[col] = df[col].apply(extract_months)\n",
    "    return df\n",
    "\n",
    "def move_months_values(df, source_col, target_col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame, this function extracts text patterns matching durations expressed in months\n",
    "    (e.g., \"32 months\", \"43 Month\", \"23 Months\", \"22Months\", or \"21-29months\") from the source column,\n",
    "    moves the extracted text to the target column, and removes it from the source column.\n",
    "    \n",
    "    Parameters:\n",
    "        df         : pandas DataFrame.\n",
    "        source_col : string, the name of the column to extract the month text from.\n",
    "        target_col : string, the name of the column where the extracted month text will be moved.\n",
    "        \n",
    "    Returns:\n",
    "        The updated DataFrame with the month values moved.\n",
    "    \"\"\"\n",
    "    # Pattern explanation:\n",
    "    #   \\d+          : one or more digits\n",
    "    #   (?:-\\d+)?    : optionally, a hyphen and one or more digits (to capture ranges like 21-29)\n",
    "    #   \\s*          : optional whitespace\n",
    "    #   [Mm]onths?   : \"month\" or \"months\" (case insensitive for the first letter)\n",
    "    pattern = r'(\\d+(?:-\\d+)?\\s*[Mm]onths?)'\n",
    "    \n",
    "    def process_text(text):\n",
    "        text = str(text)\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            extracted = match.group(1)\n",
    "            # Remove the extracted text from the source text and clean up extra spaces\n",
    "            updated_text = re.sub(pattern, \"\", text).strip()\n",
    "            return extracted, updated_text\n",
    "        else:\n",
    "            return \"\", text\n",
    "\n",
    "    # Prepare lists to store the extracted month text and the updated source text\n",
    "    extracted_vals = []\n",
    "    updated_source_vals = []\n",
    "    \n",
    "    for val in df[source_col]:\n",
    "        ext, updated = process_text(val)\n",
    "        extracted_vals.append(ext)\n",
    "        updated_source_vals.append(updated)\n",
    "    \n",
    "    # Create/update the target column with the extracted month text\n",
    "    df[target_col] = extracted_vals\n",
    "    # Replace the source column values with the text after removal of the month text\n",
    "    df[source_col] = updated_source_vals\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Filter numeric costs in 'estimated_cost_x_1000' and 'escalated_cost_x_1000' columns\n",
    " \n",
    "\n",
    "#df = filter_numeric_costs(df, 'unnamed_10')\n",
    "\n",
    "df = filter_numeric_costs(df, 'estimated')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "df = extract_months_values(df, 'column_12')\n",
    "df = extract_months_values(df, 'estimated_1')\n",
    "df = extract_months_values(df, 'column_11')\n",
    "df = extract_months_values(df, 'column_13')\n",
    "#df = move_months_values(df, 'unnamed_13', 'estimated time to construct')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['cost_allocation_factor']= None\n",
    "\n",
    "\n",
    "#df = extract_cost_allocation(df, \"unnamed_8\", \"cost_allocation_factor\")\n",
    "\n",
    "#df = extract_cost_allocation(df, \"unnamed_9\", \"cost_allocation_factor\")\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'unnamed_9' in df.columns:\n",
    " #  df['unnamed_9'] = df['unnamed_9'].apply(extract_non_numeric_text)\n",
    "  # df['unnamed_9'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "\n",
    "\n",
    "#if 'unnamed_8' in df.columns:\n",
    " #  df['unnamed_8'] = df['unnamed_8'].apply(extract_non_numeric_text)\n",
    "  # df['unnamed_8'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "            \"column_3\",\n",
    "            'upgrade (may include the following)',\n",
    " \n",
    "            ],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"MW\",\n",
    "            \n",
    "        ],   \n",
    "\n",
    "        \"description\": [\"description\",\n",
    "                         \"column_5\" ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [ \n",
    "            \"column_12\", 'estimated_1', \"column_11\", \"estimated time to construct (note 1)\",\n",
    "            'estimated time to construct (note 3)', 'column_13',\n",
    " \n",
    "                                         ],\n",
    "\n",
    "        \"type_of_upgrade\": [ \"type of upgrade\", \"column_1\",],\n",
    "\n",
    "        \"type_of_upgrade_2\": [ \"column\", 'column_6'  ],\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \"estimated_cost_x_1000\": [ 'column_10', \n",
    "             \"estimated cost (x 1000)\" ,  'estimated cost x 1000 constant dollar (2011) (note 4)', \n",
    " \n",
    "           \n",
    "\n",
    "             \n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\", \"estimated cost x 1000 constant dollar (od year) (note 2)\", \"estimated\",\n",
    "            'estimated cost x 1000 constant dollar (od year) (note 4)',\n",
    "            \n",
    " \n",
    "            \n",
    "             \n",
    "\n",
    "        ],\n",
    "\n",
    "         \n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "       \n",
    "         \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \"cost_allocation_factor\": [\n",
    "            \"column_7\",\n",
    "            'cost allocation factor', \n",
    " \n",
    "            \n",
    "           \n",
    "\n",
    "        ],\n",
    "       \n",
    "    }\n",
    "\n",
    "   # 1) If there are any truly “unnamed” columns (blank names or starting with \"Unnamed\"),\n",
    "    #    tack them onto the \"description\" group so they also get merged under \"description\".\n",
    "    unnamed_columns = [\n",
    "        col for col in df.columns\n",
    "        if (pd.isna(col) or str(col).strip() == \"\" or str(col).lower().startswith(\"nnamed\"))\n",
    "    ]\n",
    "    if unnamed_columns:\n",
    "        # Only add those that aren’t already listed\n",
    "        for uc in unnamed_columns:\n",
    "            if uc not in merge_columns_dict[\"description\"]:\n",
    "                merge_columns_dict[\"description\"].append(uc)\n",
    "\n",
    "    # 2) For each (new_col → list_of_old_cols), build new_col by picking\n",
    "    #    the first non‐missing value in row‐order. Then drop only the old columns\n",
    "    #    (but keep new_col).\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        # (a) Restrict to columns that actually exist in df\n",
    "        existing = [c for c in old_cols if c in df.columns]\n",
    "        if not existing:\n",
    "            continue\n",
    "\n",
    "        # (b) Define a helper that returns the first non‐missing, non‐blank value\n",
    "        def first_non_missing(row):\n",
    "            for val in row:\n",
    "                # treat \"\" or whitespace‐only strings as missing, too\n",
    "                if pd.notna(val) and not (isinstance(val, str) and val.strip() == \"\"):\n",
    "                    return val\n",
    "            return pd.NA\n",
    "\n",
    "        # (c) Apply it row‐wise to df[existing]\n",
    "        df[new_col] = df[existing].apply(first_non_missing, axis=1)\n",
    "\n",
    "        # (d) Drop only those source columns that are NOT equal to new_col.\n",
    "        #     That way, if “upgrade” was already a column name, we don’t drop the newly created “upgrade” column,\n",
    "        #     but _do_ drop “column4” (and any others in existing except new_col itself).\n",
    "        to_drop = [c for c in existing if c != new_col]\n",
    "        if to_drop:\n",
    "            df.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "#if 'upgrade' in df.columns:\n",
    " #   df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    " \n",
    "\n",
    "df.drop(['column_2', 'column_8',\"column_6\", \"Unnamed: 15\",  \"column_14\", \"column_9\", \"column_4\"], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "#df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "\n",
    " \n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Processes the DataFrame as follows:\n",
    "    \n",
    "    1. Drops any rows where any of these columns are empty or blank:\n",
    "       - 'upgrade', 'description', 'cost_allocation_factor',\n",
    "         'estimated_time_to_construct', 'type_of_upgrade_2', 'estimated_cost_x_1000'\n",
    "    \n",
    "    2. For each remaining row, if the value in 'type_of_upgrade' starts with\n",
    "       'SCE', 'SDG&E', or 'PG&E' (or is empty after stripping),\n",
    "       then the value in 'type_of_upgrade_2' is replaced with the value from 'type_of_upgrade'.\n",
    "       \n",
    "    Parameters:\n",
    "        df: pandas DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        A cleaned DataFrame with the above processing applied.\n",
    "    \"\"\"\n",
    "    # Define the required columns\n",
    "    required_cols = [\n",
    "        \"upgrade\", \"description\", \"cost_allocation_factor\",\n",
    "        \"estimated_time_to_construct\", \"type_of_upgrade_2\", \"estimated_cost_x_1000\"\n",
    "    ]\n",
    "    \n",
    "       # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Replace NaN with empty strings for checking emptiness\n",
    "    df_clean[required_cols] = df_clean[required_cols].fillna(\"\")\n",
    "\n",
    "    # Convert all required columns to strings and strip whitespace\n",
    "    df_clean[required_cols] = df_clean[required_cols].applymap(lambda x: str(x).strip())\n",
    "    \n",
    "    \n",
    " # Drop rows where all required columns are empty\n",
    "    df_clean = df_clean[~(df_clean[required_cols].apply(lambda row: all(row == \"\"), axis=1))]\n",
    "    \n",
    " \n",
    "    \n",
    "    # Define a function to update type_of_upgrade_2 if needed.\n",
    "    def update_type(row):\n",
    "        # Get the value from type_of_upgrade (converted to string and stripped)\n",
    "        val = str(row.get(\"type_of_upgrade\", \"\")).strip()\n",
    "        # If the value is empty or starts with SCE, SDG&E, or PG&E, then update type_of_upgrade_2\n",
    "        if val == \"\" or re.match(r'^(SCE|SDG&E|PG&E)', val):\n",
    "            row[\"type_of_upgrade\"] = row[\"type_of_upgrade_2\"]\n",
    "        return row\n",
    "\n",
    "\n",
    "    # Apply the function row-wise\n",
    "    df_clean = df_clean.apply(update_type, axis=1)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "df = process_dataframe(df)\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"type_of_upgrade_2\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "\n",
    "df = reorder_columns(df)\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].replace(\"\", np.nan).ffill() \n",
    "\n",
    "\n",
    "df= df[df['type_of_upgrade']!= '12. Local Furnishing Bonds']\n",
    "df= df[df['type_of_upgrade']!= '(when applicable):']\n",
    "df= df[df['type_of_upgrade']!= '12. Items Not Covered In This Study']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/03_raw/cluster_2_style_R.csv', index=False)\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "\n",
    "\n",
    "df= df[df['type_of_upgrade']!= '12. Local Furnishing Bonds']\n",
    "df= df[df['type_of_upgrade']!= '(when applicable):']\n",
    "df= df[df['type_of_upgrade']!= '12. Items Not Covered In This Study']\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/03_raw/cluster_2_style_R.csv', index=False)\n",
    "\n",
    "mask_agg = (\n",
    "    df['type_of_upgrade'].fillna('').eq('Total') |\n",
    "     df['type_of_upgrade'].fillna('').eq('Total Cost') |\n",
    "    df['cost_allocation_factor'].fillna('').eq('Total')\n",
    ")\n",
    "\n",
    "# 2) Extract them\n",
    "aggregate_total = df.loc[mask_agg].copy()\n",
    "\n",
    "# 3) Tag them in the original df\n",
    "df['is_aggregate_total'] = mask_agg\n",
    "\n",
    "\n",
    "agg_data = df[df['is_aggregate_total']].copy()\n",
    "agg_data.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_aggregate.csv', index=False) \n",
    "\n",
    "# 3) Then drop them from your main itemized set\n",
    "df = df.loc[~mask_agg].reset_index(drop=True)\n",
    "\n",
    "df.drop(columns=['is_aggregate_total'], inplace=True, errors='ignore')\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)  \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    \"PTO’s Interconnection Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTO’s Interconnectio n Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTOs Interconnection Facilities\": \"PTO_IF\",\n",
    "    \"PTOs Interconnectio n Facilities\": \"PTO_IF\",\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Delivery Network\": \"ADNU\",\n",
    " \"Plan of Service Reliability Network Upgrades\": \"RNU\",\n",
    " \"Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Reliability Network Upgrades\": \"RNU\",\n",
    " \"SDG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Upgrades\": \"LDNU\",\n",
    " \"SCE Distribution Upgrades\": \"LDNU\",\n",
    " \"SCE Reliability Network Upgrades for Short Circuit duty\": \"RNU\",\n",
    " \"SCE Network Upgrades\": \"RNU\",\n",
    " \"Plan of Service Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Upgrades, Estimated Costs, and Estimated Time to Construct Summary for C565 - Continued\": \"LDNU\",\n",
    " \"Upgrades, Estimated Costs, and Estimated Time to Construct Summary for C565 -\": \"LDNU\",\n",
    " \"Reliability Network Upgrades to Physically Interconnect\": \"RNU\",\n",
    " 'Reliability Network Upgrade': \"RNU\",\n",
    " \"Reliability Network Upgrades\": \"RNU\",\n",
    "    \"Local Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Area Deliverability Upgrades\": \"ADNU\",\n",
    "    \"Escalated Cost and Time to Construct for Interconnection Facilities, Reliability Network Upgrades, and Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Distribution\": \"ADNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "#df = stable_sort_by_type_of_upgrade(df)  \n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = df[df[\"type_of_upgrade\"] != \"may\"]    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ( (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    # 1) Diagnose\n",
    "    dups = df.columns[df.columns.duplicated()]\n",
    "    print(\"👉 Duplicate column names in df:\", dups.tolist())\n",
    "\n",
    "    # 2) Drop perfect duplicates\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    \"\"\"\n",
    "    Removes the word 'month' or 'months' (case insensitive) from the value.\n",
    "    Leaves behind any numbers or number ranges (e.g. \"6\", \"6-12\").\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Remove 'month' or 'months' (case-insensitive), optionally with spaces around them.\n",
    "        cleaned_value = re.sub(r'(?i)\\s*months?\\s*', '', value)\n",
    "        \n",
    "        return cleaned_value.strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Then apply it to your column, for example with Pandas:\n",
    "df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "         \n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "\n",
    "def pick_max_from_range(val):\n",
    "    \"\"\"\n",
    "    Given a value like \"12-24\" or \" 6 - 18 \" (or even \"20\"), return the larger number.\n",
    "    If nothing can be parsed, returns np.nan.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "\n",
    "    s = str(val).strip()\n",
    "    # Split on hyphen (either ASCII \"-\" or any unicode dash)\n",
    "    parts = re.split(r'\\s*[-–—]\\s*', s)\n",
    "    nums = []\n",
    "    for part in parts:\n",
    "        try:\n",
    "            # Convert each piece to float (or int)\n",
    "            nums.append(float(part))\n",
    "        except ValueError:\n",
    "            # If it isn’t purely a number, skip it\n",
    "            continue\n",
    "\n",
    "    if not nums:\n",
    "        return np.nan\n",
    "    return max(nums)\n",
    "\n",
    "# Then apply it:\n",
    "df[\"estimated_time_to_construct\"] = df[\"estimated_time_to_construct\"]\\\n",
    "    .apply(pick_max_from_range)    \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "if 'upgrade' in df.columns:\n",
    "    df['upgrade'] = df['upgrade'].ffill()      \n",
    "\n",
    "\n",
    "df.drop('type_of_upgrade_2', axis=1, inplace=True, errors='ignore') \n",
    "\n",
    "#df= reorder_columns(df)\n",
    "\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_2_style_R_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_2_style_R_total.csv'.\")\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 558\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 569\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "    q_id  cluster  req_deliverability  latitude  longitude capacity  \\\n",
      "0   558        2                 NaN       NaN        NaN     None   \n",
      "1   558        2                 NaN       NaN        NaN     None   \n",
      "2   558        2                 NaN       NaN        NaN     None   \n",
      "3   569        2                 NaN       NaN        NaN     None   \n",
      "4   569        2                 NaN       NaN        NaN     None   \n",
      "\n",
      "                      point_of_interconnection type_of_upgrade upgrade  \\\n",
      "0                                         None      Total LDNU           \n",
      "1                                         None    Total PTO_IF           \n",
      "2                                         None       Total RNU           \n",
      "3  Table Mountain  Tesla 500 kV Line (Loop-in)    Total PTO_IF           \n",
      "4  Table Mountain  Tesla 500 kV Line (Loop-in)       Total RNU           \n",
      "\n",
      "  description cost_allocation_factor  estimated_cost_x_1000  \\\n",
      "0                                                     886.0   \n",
      "1                                                     682.0   \n",
      "2                                                    1532.0   \n",
      "3                                                     541.0   \n",
      "4                                                   62467.0   \n",
      "\n",
      "  estimated_time_to_construct item  \n",
      "0                               no  \n",
      "1                               no  \n",
      "2                               no  \n",
      "3                               no  \n",
      "4                               no  \n",
      "👉 Duplicate column names in df: []\n",
      "Itemized rows saved to 'costs_phase_2_cluster_2_style_R_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_2_style_R_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU']\n",
      "[558 569]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/03_raw/ph2_rawdata_cluster2_style_R_addendums.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    " \n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_cost_allocation(df, source_col, target_col=\"cost_allocation_factor\"):\n",
    "    \"\"\"\n",
    "    Extracts percentage values from a specified source column and moves them into a target column.\n",
    "    \n",
    "    - A percentage value is defined as a string that, when stripped of whitespace,\n",
    "      fully matches a pattern of digits (with optional commas or periods) followed by a percent sign.\n",
    "    - If a cell in the source column matches this pattern, its value is placed into the target column,\n",
    "      and the source column cell is cleared (set to an empty string).\n",
    "    - If the cell does not match a percentage pattern, it is left untouched in the source column.\n",
    "    \n",
    "    Parameters:\n",
    "      df         : pandas DataFrame.\n",
    "      source_col : string, the name of the column to scan for percentage values.\n",
    "      target_col : string, the name of the column to store the extracted percentage values.\n",
    "                   Defaults to \"cost_allocation_factor\".\n",
    "    \n",
    "    Returns:\n",
    "      The DataFrame with the updated columns.\n",
    "    \"\"\"\n",
    "    # Define a regex pattern to match a percentage value (e.g., \"78.25%\").\n",
    "    # The pattern allows digits, commas, and periods, followed immediately by a \"%\" (ignoring leading/trailing spaces).\n",
    "    pattern = r\"^\\s*[\\d,\\.]+%\\s*$\"\n",
    "    \n",
    "    def extract_percentage(text):\n",
    "        # If text matches the percentage pattern, return the stripped text; otherwise, return None.\n",
    "        if isinstance(text, str) and re.fullmatch(pattern, text):\n",
    "            return text.strip()\n",
    "        return None\n",
    "\n",
    "    def clear_percentage(text):\n",
    "        # If text matches the percentage pattern, clear it (return an empty string).\n",
    "        # Otherwise, return the text stripped of surrounding whitespace.\n",
    "        if isinstance(text, str) and re.fullmatch(pattern, text):\n",
    "            return \"\"\n",
    "        if isinstance(text, str):\n",
    "            return text.strip()\n",
    "        return text\n",
    "\n",
    "    # Create (or overwrite) the target column with extracted percentage values from the source column.\n",
    "    df[target_col] = df[source_col].apply(extract_percentage)\n",
    "    # In the source column, remove any percentage values (leaving other text intact).\n",
    "    df[source_col] = df[source_col].apply(clear_percentage)\n",
    "    \n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "def filter_numeric_costs(df, col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame and column name, this function extracts the numeric cost from each cell,\n",
    "    converting values with an optional '$' sign (and possible commas) to floats.\n",
    "    If a valid numeric cost cannot be extracted, the cell is set to NaN.\n",
    "    \n",
    "    Parameters:\n",
    "      df  : pandas DataFrame.\n",
    "      col : string, the name of the column to process.\n",
    "      \n",
    "    Returns:\n",
    "      The original DataFrame with the specified column converted to numeric values (or NaN if conversion fails).\n",
    "    \"\"\"\n",
    "    def extract_numeric(value):\n",
    "        value_str = str(value)\n",
    "        # This regex matches an optional '$', optional spaces, and a number with commas and an optional decimal part.\n",
    "        match = re.search(r'\\$?\\s*([\\d,]+(?:\\.\\d+)?)', value_str)\n",
    "        if match:\n",
    "            num_str = match.group(1).replace(',', '')\n",
    "            try:\n",
    "                return float(num_str)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    # Apply the extraction function to the specified column.\n",
    "    df[col] = df[col].apply(extract_numeric)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def extract_months_values(df, col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame and column name, this function extracts text patterns matching\n",
    "    durations expressed in months (e.g., \"32 months\", \"43 Month\", \"23 Months\", \"22Months\", or \"21-29months\").\n",
    "    If a valid pattern is found, it returns the matched text; otherwise, it returns an empty string.\n",
    "    \n",
    "    Parameters:\n",
    "        df  : pandas DataFrame.\n",
    "        col : string, the name of the column to process.\n",
    "        \n",
    "    Returns:\n",
    "        The DataFrame with the specified column updated.\n",
    "    \"\"\"\n",
    "    def extract_months(text):\n",
    "        text = str(text)\n",
    "        # Pattern explanation:\n",
    "        #   \\d+          : one or more digits\n",
    "        #   (?:-\\d+)?    : optionally, a hyphen followed by one or more digits (to capture ranges like 21-29)\n",
    "        #   \\s*          : optional whitespace\n",
    "        #   [Mm]onths?   : \"month\" or \"months\" (case insensitive for the first letter)\n",
    "        pattern = r'(\\d+(?:-\\d+)?\\s*[Mm]onths?)'\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group(1) if match else \"\"\n",
    "    \n",
    "    df[col] = df[col].apply(extract_months)\n",
    "    return df\n",
    "\n",
    "def move_months_values(df, source_col, target_col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame, this function extracts text patterns matching durations expressed in months\n",
    "    (e.g., \"32 months\", \"43 Month\", \"23 Months\", \"22Months\", or \"21-29months\") from the source column,\n",
    "    moves the extracted text to the target column, and removes it from the source column.\n",
    "    \n",
    "    Parameters:\n",
    "        df         : pandas DataFrame.\n",
    "        source_col : string, the name of the column to extract the month text from.\n",
    "        target_col : string, the name of the column where the extracted month text will be moved.\n",
    "        \n",
    "    Returns:\n",
    "        The updated DataFrame with the month values moved.\n",
    "    \"\"\"\n",
    "    # Pattern explanation:\n",
    "    #   \\d+          : one or more digits\n",
    "    #   (?:-\\d+)?    : optionally, a hyphen and one or more digits (to capture ranges like 21-29)\n",
    "    #   \\s*          : optional whitespace\n",
    "    #   [Mm]onths?   : \"month\" or \"months\" (case insensitive for the first letter)\n",
    "    pattern = r'(\\d+(?:-\\d+)?\\s*[Mm]onths?)'\n",
    "    \n",
    "    def process_text(text):\n",
    "        text = str(text)\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            extracted = match.group(1)\n",
    "            # Remove the extracted text from the source text and clean up extra spaces\n",
    "            updated_text = re.sub(pattern, \"\", text).strip()\n",
    "            return extracted, updated_text\n",
    "        else:\n",
    "            return \"\", text\n",
    "\n",
    "    # Prepare lists to store the extracted month text and the updated source text\n",
    "    extracted_vals = []\n",
    "    updated_source_vals = []\n",
    "    \n",
    "    for val in df[source_col]:\n",
    "        ext, updated = process_text(val)\n",
    "        extracted_vals.append(ext)\n",
    "        updated_source_vals.append(updated)\n",
    "    \n",
    "    # Create/update the target column with the extracted month text\n",
    "    df[target_col] = extracted_vals\n",
    "    # Replace the source column values with the text after removal of the month text\n",
    "    df[source_col] = updated_source_vals\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Filter numeric costs in 'estimated_cost_x_1000' and 'escalated_cost_x_1000' columns\n",
    " \n",
    "\n",
    "#df = filter_numeric_costs(df, 'unnamed_10')\n",
    " \n",
    "#df = move_months_values(df, 'unnamed_13', 'estimated time to construct')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['cost_allocation_factor']= None\n",
    "\n",
    "\n",
    "#df = extract_cost_allocation(df, \"unnamed_8\", \"cost_allocation_factor\")\n",
    "\n",
    "#df = extract_cost_allocation(df, \"unnamed_9\", \"cost_allocation_factor\")\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'unnamed_9' in df.columns:\n",
    " #  df['unnamed_9'] = df['unnamed_9'].apply(extract_non_numeric_text)\n",
    "  # df['unnamed_9'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "\n",
    "\n",
    "#if 'unnamed_8' in df.columns:\n",
    " #  df['unnamed_8'] = df['unnamed_8'].apply(extract_non_numeric_text)\n",
    "  # df['unnamed_8'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "            \"column_3\",\n",
    "            'upgrade (may include the following)',\n",
    " \n",
    "            ],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"MW\",\n",
    "            \n",
    "        ],   \n",
    "\n",
    "        \"description\": [\"description\",\n",
    "                         \"column_5\" ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [ \n",
    "            \"column_12\", 'estimated_1', \"column_11\", \"estimated time to construct (note 1)\",\n",
    "            'estimated time to construct (note 3)', 'column_13',\n",
    " \n",
    "                                         ],\n",
    "\n",
    "        \"type_of_upgrade\": [ \"type of upgrade\",  \"column_1\",],\n",
    "\n",
    "         \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \"estimated_cost_x_1000\": [ 'column_10', \n",
    "             \"estimated cost (x 1000)\" ,  'estimated cost x 1000 constant dollar (2011) (note 4)', \n",
    " \n",
    "           \n",
    "\n",
    "             \n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\", \"estimated cost x 1000 constant dollar (od year) (note 2)\", \"estimated\",\n",
    "            'estimated cost x 1000 constant dollar (od year) (note 4)',\n",
    "            \n",
    " \n",
    "            \n",
    "             \n",
    "\n",
    "        ],\n",
    "\n",
    "         \n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "       \n",
    "         \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \"cost_allocation_factor\": [\n",
    "            \"column_7\",\n",
    "            'cost allocation factor', \n",
    " \n",
    "            \n",
    "           \n",
    "\n",
    "        ],\n",
    "       \n",
    "    }\n",
    "\n",
    "   # 1) If there are any truly “unnamed” columns (blank names or starting with \"Unnamed\"),\n",
    "    #    tack them onto the \"description\" group so they also get merged under \"description\".\n",
    "    unnamed_columns = [\n",
    "        col for col in df.columns\n",
    "        if (pd.isna(col) or str(col).strip() == \"\" or str(col).lower().startswith(\"nnamed\"))\n",
    "    ]\n",
    "    if unnamed_columns:\n",
    "        # Only add those that aren’t already listed\n",
    "        for uc in unnamed_columns:\n",
    "            if uc not in merge_columns_dict[\"description\"]:\n",
    "                merge_columns_dict[\"description\"].append(uc)\n",
    "\n",
    "    # 2) For each (new_col → list_of_old_cols), build new_col by picking\n",
    "    #    the first non‐missing value in row‐order. Then drop only the old columns\n",
    "    #    (but keep new_col).\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        # (a) Restrict to columns that actually exist in df\n",
    "        existing = [c for c in old_cols if c in df.columns]\n",
    "        if not existing:\n",
    "            continue\n",
    "\n",
    "        # (b) Define a helper that returns the first non‐missing, non‐blank value\n",
    "        def first_non_missing(row):\n",
    "            for val in row:\n",
    "                # treat \"\" or whitespace‐only strings as missing, too\n",
    "                if pd.notna(val) and not (isinstance(val, str) and val.strip() == \"\"):\n",
    "                    return val\n",
    "            return pd.NA\n",
    "\n",
    "        # (c) Apply it row‐wise to df[existing]\n",
    "        df[new_col] = df[existing].apply(first_non_missing, axis=1)\n",
    "\n",
    "        # (d) Drop only those source columns that are NOT equal to new_col.\n",
    "        #     That way, if “upgrade” was already a column name, we don’t drop the newly created “upgrade” column,\n",
    "        #     but _do_ drop “column4” (and any others in existing except new_col itself).\n",
    "        to_drop = [c for c in existing if c != new_col]\n",
    "        if to_drop:\n",
    "            df.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "def pick_max_from_range(val):\n",
    "    \"\"\"\n",
    "    Given a value like \"12-24\" or \" 6 - 18 \" (or even \"20\"), return the larger number.\n",
    "    If nothing can be parsed, returns np.nan.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "\n",
    "    s = str(val).strip()\n",
    "    # Split on hyphen (either ASCII \"-\" or any unicode dash)\n",
    "    parts = re.split(r'\\s*[-–—]\\s*', s)\n",
    "    nums = []\n",
    "    for part in parts:\n",
    "        try:\n",
    "            # Convert each piece to float (or int)\n",
    "            nums.append(float(part))\n",
    "        except ValueError:\n",
    "            # If it isn’t purely a number, skip it\n",
    "            continue\n",
    "\n",
    "    if not nums:\n",
    "        return np.nan\n",
    "    return max(nums)\n",
    "\n",
    "# Then apply it:\n",
    "df[\"estimated_time_to_construct\"] = df[\"estimated_time_to_construct\"]\\\n",
    "    .apply(pick_max_from_range)    \n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "#if 'upgrade' in df.columns:\n",
    " #   df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    " \n",
    "\n",
    "df.drop(['column_2', 'column_8',\"column_6\", \"Unnamed: 15\",  \"column_14\", \"column_9\", \"column_4\"], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "#df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "df= df[df['type_of_upgrade']!= '12. Local Furnishing Bonds']\n",
    "df= df[df['type_of_upgrade']!= '(when applicable):']\n",
    "df= df[df['type_of_upgrade']!= '12. Items Not Covered In This Study']\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/03_raw/cluster_2_style_R_addendums.csv', index=False)\n",
    "\n",
    "mask_agg = (\n",
    "    df['type_of_upgrade'].fillna('').eq('Total') |\n",
    "     df['type_of_upgrade'].fillna('').eq('Total Cost') |\n",
    "    df['cost_allocation_factor'].fillna('').eq('Total')\n",
    ")\n",
    "\n",
    "# 2) Extract them\n",
    "aggregate_total = df.loc[mask_agg].copy()\n",
    "\n",
    "# 3) Tag them in the original df\n",
    "df['is_aggregate_total'] = mask_agg\n",
    "\n",
    "\n",
    "agg_data = df[df['is_aggregate_total']].copy()\n",
    "agg_data.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_aggregate_addendums.csv', index=False) \n",
    "\n",
    "# 3) Then drop them from your main itemized set\n",
    "df = df.loc[~mask_agg].reset_index(drop=True)\n",
    "\n",
    "df.drop(columns=['is_aggregate_total'], inplace=True, errors='ignore')\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"type_of_upgrade_2\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "\n",
    "df = reorder_columns(df)\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].replace(\"\", np.nan).ffill() \n",
    "\n",
    "\n",
    "df= df[df['type_of_upgrade']!= '12. Local Furnishing Bonds']\n",
    "df= df[df['type_of_upgrade']!= '(when applicable):']\n",
    "df= df[df['type_of_upgrade']!= '12. Items Not Covered In This Study']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/03_raw/cluster_2_style_R.csv', index=False)\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)  \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    \"PTO’s Interconnection Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTO’s Interconnectio n Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTOs Interconnection Facilities\": \"PTO_IF\",\n",
    "    \"PTOs Interconnectio n Facilities\": \"PTO_IF\",\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Delivery Network\": \"ADNU\",\n",
    " \"Plan of Service Reliability Network Upgrades\": \"RNU\",\n",
    " \"Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Reliability Network Upgrades\": \"RNU\",\n",
    " \"SDG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Upgrades\": \"LDNU\",\n",
    " \"SCE Distribution Upgrades\": \"LDNU\",\n",
    " \"SCE Reliability Network Upgrades for Short Circuit duty\": \"RNU\",\n",
    " \"SCE Network Upgrades\": \"RNU\",\n",
    " \"Plan of Service Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Upgrades, Estimated Costs, and Estimated Time to Construct Summary for C565 - Continued\": \"LDNU\",\n",
    " \"Upgrades, Estimated Costs, and Estimated Time to Construct Summary for C565 -\": \"LDNU\",\n",
    " \"Reliability Network Upgrades to Physically Interconnect\": \"RNU\",\n",
    " 'Reliability Network Upgrade': \"RNU\",\n",
    " \"Reliability Network Upgrades\": \"RNU\",\n",
    "    \"Local Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Area Deliverability Upgrades\": \"ADNU\",\n",
    "    \"Escalated Cost and Time to Construct for Interconnection Facilities, Reliability Network Upgrades, and Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Distribution\": \"ADNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "#df = stable_sort_by_type_of_upgrade(df)  \n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = df[df[\"type_of_upgrade\"] != \"may\"]    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ( (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    # 1) Diagnose\n",
    "    dups = df.columns[df.columns.duplicated()]\n",
    "    print(\"👉 Duplicate column names in df:\", dups.tolist())\n",
    "\n",
    "    # 2) Drop perfect duplicates\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    \"\"\"\n",
    "    Removes the word 'month' or 'months' (case insensitive) from the value.\n",
    "    Leaves behind any numbers or number ranges (e.g. \"6\", \"6-12\").\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Remove 'month' or 'months' (case-insensitive), optionally with spaces around them.\n",
    "        cleaned_value = re.sub(r'(?i)\\s*months?\\s*', '', value)\n",
    "        \n",
    "        return cleaned_value.strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Then apply it to your column, for example with Pandas:\n",
    "df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "         \n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "if 'upgrade' in df.columns:\n",
    "    df['upgrade'] = df['upgrade'].ffill()      \n",
    "\n",
    "\n",
    "df.drop('type_of_upgrade_2', axis=1, inplace=True, errors='ignore') \n",
    "\n",
    "#df= reorder_columns(df)\n",
    "\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_itemized_addendums.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_total_addendums.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_2_style_R_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_2_style_R_total.csv'.\")\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge- Complete replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/02_intermediate/costs_phase_2_cluster_SGIP-TC_style_R_itemized.csv')\n",
    "df2 = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/02_intermediate/costs_phase_2_cluster_SGIP-TC_style_R_total.csv')\n",
    "\n",
    "df1.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/01_clean/costs_phase_2_cluster_1_style_SGIP-TC_itemized_updated.csv', index=False)\n",
    "df2.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster SGIP-TC/01_clean/costs_phase_2_cluster_1_style_SGIP-TC_total_updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Scraped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orignals only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to compare the total cost across all types of upgrade as that is given in the pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing required upgrades in totals dataset ===\n",
      "Q_id 552 missing: ['ADNU']\n",
      "Q_id 561 missing: ['ADNU']\n",
      "Q_id 565 missing: ['ADNU']\n",
      "Q_id 569 missing: ['LDNU', 'ADNU']\n",
      "Q_id 574 missing: ['ADNU']\n",
      "Q_id 583 missing: ['ADNU']\n",
      "Q_id 588 missing: ['ADNU']\n",
      "Q_id 589 missing: ['ADNU']\n",
      "Q_id 590 missing: ['ADNU']\n",
      "Q_id 593 missing: ['ADNU']\n",
      "Q_id 602 missing: ['ADNU']\n",
      "Q_id 606 missing: ['LDNU', 'ADNU']\n",
      "Q_id 608 missing: ['ADNU']\n",
      "\n",
      "=== Duplicate upgrades in totals dataset ===\n",
      "No duplicates found in totals dataset.\n",
      "\n",
      "✅ All itemized sums match the aggregate totals for Q_ids in aggregate.\n",
      "\n",
      "Mismatches written to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/mismatches.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "ITEMIZED_CSV_PATH       = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_itemized.csv'\n",
    "TOTALS_CSV_PATH         = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_total.csv'\n",
    "AGGREGATE_CSV_PATH      = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_aggregate.csv'\n",
    "\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "REQUIRED_UPGRADES       = ['PTO_IF', 'RNU', 'LDNU', 'ADNU']\n",
    "\n",
    "MISMATCHES_CSV_PATH     = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/mismatches.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "itemized_df = pd.read_csv(ITEMIZED_CSV_PATH, dtype={'type_of_upgrade': str})\n",
    "totals_df   = pd.read_csv(TOTALS_CSV_PATH, dtype={'type_of_upgrade': str})\n",
    "agg_df      = pd.read_csv(AGGREGATE_CSV_PATH, dtype=str)\n",
    "\n",
    "# ---------------------- Clean aggregate costs ---------------------- #\n",
    "\n",
    "# Remove $ and commas, then convert to float\n",
    "for col in [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]:\n",
    "    agg_df[col] = (\n",
    "        agg_df[col]\n",
    "        .str.replace(r'[\\$,]', '', regex=True)\n",
    "        .astype(float)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "# ---------------------- Build aggregate lookup ---------------------- #\n",
    "\n",
    "agg_grouped = (\n",
    "    agg_df\n",
    "    .groupby('q_id', as_index=False)\n",
    "    .agg({\n",
    "        TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "        TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "    })\n",
    ")\n",
    "\n",
    "# create lookup\n",
    "agg_lookup = agg_grouped.set_index('q_id').to_dict(orient='index')\n",
    "agg_qids   = set(agg_grouped['q_id'])\n",
    "\n",
    "# ---------------------- Numeric convert itemized ---------------------- #\n",
    "\n",
    "for col in ['estimated_cost_x_1000','escalated_cost_x_1000']:\n",
    "    itemized_df[col] = (\n",
    "        itemized_df[col]\n",
    "        .astype(str)\n",
    "        .str.replace(r'[\\$,]', '', regex=True)\n",
    "        .astype(float)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "# ---------------------- Check missing upgrades for  Q_ids ---------------------- #\n",
    "\n",
    "# ---------------------- Check missing upgrades in totals_df (unconditionally) ---------------------- #\n",
    "\n",
    "print(\"=== Missing required upgrades in totals dataset ===\")\n",
    "missing = []\n",
    "for q in sorted(totals_df['q_id'].unique()):\n",
    "    ups = (\n",
    "        totals_df\n",
    "        .loc[totals_df['q_id'] == q, 'type_of_upgrade']\n",
    "        .dropna()\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    miss = [u for u in REQUIRED_UPGRADES if u not in ups]\n",
    "    if miss:\n",
    "        missing.append((q, miss))\n",
    "\n",
    "if missing:\n",
    "    for q, miss in missing:\n",
    "        print(f\"Q_id {q} missing: {miss}\")\n",
    "else:\n",
    "    print(\"None — every Q_id has all required upgrades in totals_df.\")\n",
    "\n",
    "\n",
    "# ---------------------- Check duplicate upgrades in totals dataset ---------------------- #\n",
    "\n",
    "print(\"\\n=== Duplicate upgrades in totals dataset ===\")\n",
    "dups = []\n",
    "for q, group in totals_df.groupby('q_id'):\n",
    "    dup_types = group['type_of_upgrade'][group['type_of_upgrade'].duplicated()].unique().tolist()\n",
    "    if dup_types:\n",
    "        dups.append((q, dup_types))\n",
    "\n",
    "if dups:\n",
    "    for q, dup in dups:\n",
    "        print(f\"Q_id {q} duplicates: {dup}\")\n",
    "else:\n",
    "    print(\"No duplicates found in totals dataset.\")\n",
    "\n",
    "# ---------------------- Compute per-q_id itemized total ---------------------- #\n",
    "\n",
    "itemized_totals = (\n",
    "    itemized_df[itemized_df['q_id'].isin(agg_qids)]\n",
    "    .groupby('q_id', as_index=False)\n",
    "    .agg({\n",
    "        'estimated_cost_x_1000':'sum',\n",
    "        'escalated_cost_x_1000':'sum'\n",
    "    })\n",
    ")\n",
    "\n",
    "itemized_totals['itemized_total'] = itemized_totals.apply(\n",
    "    lambda r: r['estimated_cost_x_1000'] if r['estimated_cost_x_1000']>0 else r['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Compare against aggregate totals ---------------------- #\n",
    "\n",
    "mismatches = []\n",
    "for _, row in itemized_totals.iterrows():\n",
    "    q = row['q_id']\n",
    "    it = row['itemized_total']\n",
    "    av = agg_lookup[q][TOTALS_ESTIMATED_COLUMN] if agg_lookup[q][TOTALS_ESTIMATED_COLUMN]>0 else agg_lookup[q][TOTALS_ESCALATED_COLUMN]\n",
    "    # skip both zero\n",
    "    if it==0 and av==0:\n",
    "        continue\n",
    "    if abs(it - av) > 1e-6:\n",
    "        mismatches.append({\n",
    "            'q_id': q,\n",
    "            'itemized_total': it,\n",
    "            'aggregate_total': av,\n",
    "            'difference': it - av\n",
    "        })\n",
    "\n",
    "mismatches_df = pd.DataFrame(mismatches)\n",
    "\n",
    "# ---------------------- Report & Save ---------------------- #\n",
    "\n",
    "if mismatches_df.empty:\n",
    "    print(\"\\n✅ All itemized sums match the aggregate totals for Q_ids in aggregate.\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Found {len(mismatches_df)} mismatches:\")\n",
    "    print(mismatches_df)\n",
    "\n",
    "mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "print(f\"\\nMismatches written to {MISMATCHES_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing required upgrades in totals dataset ===\n",
      "Q_id 569 missing: ['LDNU', 'ADNU']\n",
      "\n",
      "=== Duplicate upgrades in totals dataset ===\n",
      "No duplicates found in totals dataset.\n",
      "\n",
      "✅ All itemized sums match the aggregate totals for Q_ids in aggregate.\n",
      "\n",
      "Mismatches written to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/mismatches.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "ITEMIZED_CSV_PATH       = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_itemized_addendums.csv'\n",
    "TOTALS_CSV_PATH         = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_total_addendums.csv'\n",
    "AGGREGATE_CSV_PATH      = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/02_intermediate/costs_phase_2_cluster_2_style_R_aggregate_addendums.csv'\n",
    "\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "\n",
    "REQUIRED_UPGRADES       = ['PTO_IF', 'RNU', 'LDNU', 'ADNU']\n",
    "\n",
    "MISMATCHES_CSV_PATH     = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 2/mismatches.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "itemized_df = pd.read_csv(ITEMIZED_CSV_PATH, dtype={'type_of_upgrade': str})\n",
    "totals_df   = pd.read_csv(TOTALS_CSV_PATH, dtype={'type_of_upgrade': str})\n",
    "agg_df      = pd.read_csv(AGGREGATE_CSV_PATH, dtype=str)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- Clean aggregate costs ---------------------- #\n",
    "\n",
    "dfs = [agg_df, itemized_df, totals_df]\n",
    "for df in dfs:\n",
    "    if TOTALS_ESTIMATED_COLUMN not in df.columns:\n",
    "        df[TOTALS_ESTIMATED_COLUMN] = np.nan\n",
    "\n",
    "    if TOTALS_ESCALATED_COLUMN not in df.columns:\n",
    "        df[TOTALS_ESCALATED_COLUMN] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "# Remove $ and commas, then convert to float\n",
    "for col in [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]:\n",
    "    # 1) If the column doesn't exist, create it as empty strings:\n",
    "    if col not in agg_df.columns:\n",
    "        agg_df[col] = \"\"\n",
    "    # 2) Make sure anything that’s not already a string becomes a string,\n",
    "    #    and replace NaN with the empty string so that .str won’t blow up:\n",
    "    agg_df[col] = agg_df[col].fillna(\"\").astype(str)\n",
    "    # 3) Now remove any “$” or “,”, coerce to numeric, and fill remaining NaN with 0.0:\n",
    "    agg_df[col] = (\n",
    "        agg_df[col]\n",
    "        .str.replace(r\"[\\$,]\", \"\", regex=True)\n",
    "        .pipe(pd.to_numeric, errors=\"coerce\")  # convert the stripped string to float or NaN\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "# ---------------------- Build aggregate lookup ---------------------- #\n",
    "\n",
    "agg_grouped = (\n",
    "    agg_df\n",
    "    .groupby('q_id', as_index=False)\n",
    "    .agg({\n",
    "        TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "        TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "    })\n",
    ")\n",
    "\n",
    "# create lookup\n",
    "agg_lookup = agg_grouped.set_index('q_id').to_dict(orient='index')\n",
    "agg_qids   = set(agg_grouped['q_id'])\n",
    "\n",
    "# ---------------------- Numeric convert itemized ---------------------- #\n",
    "\n",
    "for col in ['estimated_cost_x_1000','escalated_cost_x_1000']:\n",
    "    itemized_df[col] = (\n",
    "        itemized_df[col]\n",
    "        .astype(str)\n",
    "        .str.replace(r'[\\$,]', '', regex=True)\n",
    "        .astype(float)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "# ---------------------- Check missing upgrades for  Q_ids ---------------------- #\n",
    "\n",
    "# ---------------------- Check missing upgrades in totals_df (unconditionally) ---------------------- #\n",
    "\n",
    "print(\"=== Missing required upgrades in totals dataset ===\")\n",
    "missing = []\n",
    "for q in sorted(totals_df['q_id'].unique()):\n",
    "    ups = (\n",
    "        totals_df\n",
    "        .loc[totals_df['q_id'] == q, 'type_of_upgrade']\n",
    "        .dropna()\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    miss = [u for u in REQUIRED_UPGRADES if u not in ups]\n",
    "    if miss:\n",
    "        missing.append((q, miss))\n",
    "\n",
    "if missing:\n",
    "    for q, miss in missing:\n",
    "        print(f\"Q_id {q} missing: {miss}\")\n",
    "else:\n",
    "    print(\"None — every Q_id has all required upgrades in totals_df.\")\n",
    "\n",
    "\n",
    "# ---------------------- Check duplicate upgrades in totals dataset ---------------------- #\n",
    "\n",
    "print(\"\\n=== Duplicate upgrades in totals dataset ===\")\n",
    "dups = []\n",
    "for q, group in totals_df.groupby('q_id'):\n",
    "    dup_types = group['type_of_upgrade'][group['type_of_upgrade'].duplicated()].unique().tolist()\n",
    "    if dup_types:\n",
    "        dups.append((q, dup_types))\n",
    "\n",
    "if dups:\n",
    "    for q, dup in dups:\n",
    "        print(f\"Q_id {q} duplicates: {dup}\")\n",
    "else:\n",
    "    print(\"No duplicates found in totals dataset.\")\n",
    "\n",
    "# ---------------------- Compute per-q_id itemized total ---------------------- #\n",
    "\n",
    "itemized_totals = (\n",
    "    itemized_df[itemized_df['q_id'].isin(agg_qids)]\n",
    "    .groupby('q_id', as_index=False)\n",
    "    .agg({\n",
    "        'estimated_cost_x_1000':'sum',\n",
    "        'escalated_cost_x_1000':'sum'\n",
    "    })\n",
    ")\n",
    "\n",
    "itemized_totals['itemized_total'] = itemized_totals.apply(\n",
    "    lambda r: r['estimated_cost_x_1000'] if r['estimated_cost_x_1000']>0 else r['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Compare against aggregate totals ---------------------- #\n",
    "\n",
    "mismatches = []\n",
    "for _, row in itemized_totals.iterrows():\n",
    "    q = row['q_id']\n",
    "    it = row['itemized_total']\n",
    "    av = agg_lookup[q][TOTALS_ESTIMATED_COLUMN] if agg_lookup[q][TOTALS_ESTIMATED_COLUMN]>0 else agg_lookup[q][TOTALS_ESCALATED_COLUMN]\n",
    "    # skip both zero\n",
    "    if it==0 and av==0:\n",
    "        continue\n",
    "    if abs(it - av) > 1e-6:\n",
    "        mismatches.append({\n",
    "            'q_id': q,\n",
    "            'itemized_total': it,\n",
    "            'aggregate_total': av,\n",
    "            'difference': it - av\n",
    "        })\n",
    "\n",
    "mismatches_df = pd.DataFrame(mismatches)\n",
    "\n",
    "# ---------------------- Report & Save ---------------------- #\n",
    "\n",
    "if mismatches_df.empty:\n",
    "    print(\"\\n✅ All itemized sums match the aggregate totals for Q_ids in aggregate.\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Found {len(mismatches_df)} mismatches:\")\n",
    "    print(mismatches_df)\n",
    "\n",
    "mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "print(f\"\\nMismatches written to {MISMATCHES_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
