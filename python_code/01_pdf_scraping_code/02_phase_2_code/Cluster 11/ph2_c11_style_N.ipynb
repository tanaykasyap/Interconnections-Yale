{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: QC11PhII_Q1531_Bateria Del Sur_Appendix A_11-20-2019.pdf from Project 1531\n",
      "Scraped PDF: QC11PhII_Q1532_Kettle Solar One_Appendix A_11-20-2019.pdf from Project 1532\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/03_raw/ph2_rawdata_cluster11_style_N_originals.csv\n",
      "No data to save for addendums.\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 2\n",
      "Total Projects Scraped: 2\n",
      "Total Projects Skipped: 0\n",
      "Total Projects Missing: 0\n",
      "Total PDFs Accessed: 2\n",
      "Total PDFs Scraped: 2\n",
      "Total PDFs Skipped: 0\n",
      "\n",
      "List of Scraped Projects:\n",
      "[1531, 1532]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[]\n",
      "\n",
      "List of Missing Projects:\n",
      "[]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['QC11PhII_Q1531_Bateria Del Sur_Appendix A_11-20-2019.pdf', 'QC11PhII_Q1532_Kettle Solar One_Appendix A_11-20-2019.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "[]\n",
      "\n",
      "List of Addendum PDFs:\n",
      "[]\n",
      "\n",
      "List of Original PDFs:\n",
      "[]\n",
      "\n",
      "List of Style N PDFs (Skipped due to 'Network Upgrade Type'):\n",
      "[]\n",
      "\n",
      "Total Number of Style N PDFs: 0\n",
      "\n",
      "Number of Original PDFs Scraped: 0\n",
      "Number of Addendum PDFs Scraped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_32162/1306010521.py:1183: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project list\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/03_raw/ph2_rawdata_cluster11_style_N_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/03_raw/ph2_rawdata_cluster11_style_N_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/03_raw/ph2_scraping_cluster11_style_N_log.txt\"\n",
    "\n",
    "# List of Style N PDFs\n",
    "\n",
    "\n",
    "# List of Style N PDFs\n",
    "STYLE_N_PDF_NAMES = [\n",
    "    'Q1531.pdf',\n",
    "    'Q1532.pdf',\n",
    " \n",
    "]\n",
    "\n",
    "\n",
    "# Extract q_ids from Style N PDF filenames\n",
    "def extract_q_id(pdf_name):\n",
    "    match = re.search(r'_Q(\\d+)_', pdf_name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "STYLE_N_Q_IDS = [extract_q_id(name) for name in STYLE_N_PDF_NAMES if extract_q_id(name) is not None]\n",
    " \n",
    "\n",
    "STYLE_N_Q_IDS = [\n",
    "    1531,\n",
    "    1532\n",
    "]\n",
    "\n",
    "# Set PROJECT_LIST to the list of Style N q_ids\n",
    "PROJECT_LIST = STYLE_N_Q_IDS\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "style_n_pdfs = []  # List to track style N PDFs\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "original_has_table8 = {}  # Dictionary to track if original PDFs have table8\n",
    "\n",
    "# Define the list of specific phrases 2\n",
    "SPECIFIC_PHRASES_2 = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"CANU-GR\"]\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"\n",
    "    Cleans column headers by normalizing and removing unwanted characters.\n",
    "\n",
    "    Args:\n",
    "        headers (list): List of raw column headers from the table.\n",
    "\n",
    "    Returns:\n",
    "        list: List of cleaned and normalized column headers.\n",
    "    \"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Normalize whitespace\n",
    "            \n",
    "            # Extract 'Escalated' if present inside parentheses\n",
    "            match = re.search(r'\\(.*?(escalated).*?\\)', header, re.IGNORECASE)\n",
    "            escalated_text = match.group(1).strip() if match else \"\"\n",
    "\n",
    "            # Remove everything inside parentheses but retain 'escalated' if found\n",
    "            header = re.sub(r'\\(.*?\\)', '', header).strip()\n",
    "            \n",
    "            # If 'Escalated' was found, append it to the cleaned header\n",
    "            if escalated_text:\n",
    "                header += f\" {escalated_text}\"\n",
    "            \n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)  # Remove special characters\n",
    "            header = header.strip()\n",
    "\n",
    "        cleaned_headers.append(header)\n",
    "\n",
    "    # Handle specific header issues (e.g., 'Max of Estimated Time to Construct')\n",
    "#    for i, col in enumerate(cleaned_headers):\n",
    "#        if col.startswith(\"max of\"):\n",
    "#            cleaned_headers[i] = \"max of estimated time to construct\"\n",
    "    return cleaned_headers\n",
    "\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Conditionally Assigned Network Upgrades\",\n",
    "        \"PTO's Interconnection Facilities\",\n",
    "        \"Network Upgrades\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\",\n",
    "        \n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus two to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        # Updated Cluster Extraction\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        if '11' in clusters:\n",
    "            cluster_number = '11'\n",
    "        elif clusters:\n",
    "            cluster_number = max(clusters, key=lambda x: int(x))  # Choose the highest cluster number found\n",
    "        else:\n",
    "            cluster_number = '11'  # Default to 11 if not found\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    " \n",
    "def extract_table8(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 8-1 to 8-3 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 8 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 8 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None  # Holds the last extracted table title\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 8-1\" to \"Table 8-3\" or \"8.1\" to \"8.3\"\n",
    "            table8_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*8[-.]([1-2])\\b\", text, re.IGNORECASE):\n",
    "                    table8_pages.append(i)\n",
    "\n",
    "            if not table8_pages:\n",
    "                print(\"No Table 8-1 to 8-3 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table8_pages[0]\n",
    "            last_page = table8_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                            \"snap_tolerance\":6,  # Optimal value to group close elements\n",
    "                    \"join_tolerance\": 6,  # Helps combine fragmented text within cells\n",
    "                    \"intersection_x_tolerance\": 1,  # Fine-tune vertical alignment\n",
    "                    \"intersection_y_tolerance\": 8,  # Adjust for row merging\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    # Extract table title from the region above the table\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "\n",
    "                    if re.search(r\"Network Upgrade Cost Responsibility\", title_text, re.IGNORECASE):\n",
    "                        print(\n",
    "                            f\"Skipping table {table_index} on page {page_number + 1} in pdf {pdf_path} due to 'Network Upgrade Cost Responsibility' in the bounding box above.\",\n",
    "                        \n",
    "                        )\n",
    "                        continue  # Skip this table and move to the next\n",
    "\n",
    "                    # If no skip condition, proceed with processing the table\n",
    "                    print(f\"Processing table {table_index} on page {page_number + 1}...\", file=log_file)\n",
    "\n",
    "                    if title_text.strip():  # If title is found in the bounding box above the table\n",
    "                        title_lines = title_text.split(\"\\n\")[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*8[-.]([1-2])[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(3).strip()\n",
    "                                break\n",
    "                    else:\n",
    "                        # If no title is found, use the last table title\n",
    "                        print(f\"No title found above table on page {page_number + 1}, table {table_index}. Using last specific phrase.\", file=log_file)\n",
    "                        table_title = specific_phrase\n",
    "\n",
    "                    if table_title:\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "\n",
    "                    # Clean headers and data\n",
    "                    headers = tab[0]\n",
    "                    data_rows = tab[1:]\n",
    "\n",
    "                     \n",
    "\n",
    "                    # Clean headers\n",
    "                                        # Clean headers and replace empty ones\n",
    "                    headers = clean_column_headers(headers)\n",
    "                    headers = [f\"unnamed_{i+1}\" if not header else header for i, header in enumerate(headers)]\n",
    "                    \n",
    "                    print(f\"Table {table_index} on page {page_number + 1} headers: {headers}\", file=log_file)\n",
    "\n",
    "                    # Check for required columns\n",
    "                    if not any(col in headers for col in [\"upgrade\", \"network upgrade type\"]):\n",
    "                        print(f\"Required columns not found on page {page_number + 1}, table {table_index}. Retrying extraction with basic settings...\", file=log_file)\n",
    "                        \n",
    "                        # Retry extraction with basic settings\n",
    "                        tables_basic = page.find_tables(table_settings={\n",
    "                            \"horizontal_strategy\": \"lines\",\n",
    "                            \"vertical_strategy\": \"lines\",\n",
    "                        })\n",
    "\n",
    "                        if not tables_basic:\n",
    "                            print(f\"No tables found on retry. Skipping page {page_number + 1}.\", file=log_file)\n",
    "                            continue  # Move to the next page\n",
    "\n",
    "                        # Re-extract with basic settings\n",
    "                        tab_basic = tables_basic[0].extract()\n",
    "                        \n",
    "                        headers = clean_column_headers(tab_basic[0])\n",
    "                        headers = [f\"unnamed_{i+1}\" if not header else header for i, header in enumerate(headers)]\n",
    "                        data_rows = tab_basic[1:]\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "                     \n",
    "\n",
    "                    \n",
    "\n",
    "                     \n",
    "\n",
    "                    # Create DataFrame\n",
    "                    try:\n",
    "                        df = pd.DataFrame(data_rows, columns=headers)\n",
    "\n",
    "                        if \"max of\" in df.columns:\n",
    "                            df.drop(columns=[\"max of\"], inplace=True)\n",
    "                            print(f\"Dropped 'Max of' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "\n",
    "                        if \"estimated\" in df.columns:\n",
    "                            df.drop(columns = [\"estimated\"], inplace=True)\n",
    "                            print(f\"Dropped 'estimated' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                           \n",
    "\n",
    "                        if \"nu estimated\" in df.columns:\n",
    "                            df.drop(columns = [\"nu estimated\"], inplace=True)\n",
    "                            print(f\"Dropped 'nu estimated' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"nu estimated time\" in df.columns:\n",
    "                            df.drop(columns = [\"nu estimated time\"], inplace=True)\n",
    "                            print(f\"Dropped 'nu estimated time' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)    \n",
    "\n",
    "                        if \"nu\" in df.columns:\n",
    "                            df.drop(columns = [\"nu\"], inplace=True)\n",
    "                            print(f\"Dropped 'nu' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)    \n",
    "\n",
    "                        if \"project\" in df.columns:\n",
    "                            df.drop(columns = [\"project\"], inplace=True)\n",
    "                            print(f\"Dropped 'project' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"assigned\" in df.columns:\n",
    "                            df.drop(columns = [\"assigned\"], inplace=True)\n",
    "                            print(f\"Dropped 'assigned' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "                             \n",
    "\n",
    "                        #required_columns = [\"type of upgrade\", \"upgrade\", \"network upgrade type\", \"cost rate\"]\n",
    "\n",
    "                        #if not any(\n",
    "                        #    any(req_col.lower() == col.lower() for col in df.columns) for req_col in required_columns\n",
    "                        #):\n",
    "                        #    print(\n",
    "                        #        f\"Skipping table {table_index} on page {page_number + 1} in PDF {pdf_path}, \"\n",
    "                        #        f\"as it does not have any of the required columns (case-insensitive): {', '.join(required_columns)}\"\n",
    "                        #    )\n",
    "                        #   continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # Replace \"None\" in \"type of upgrade\" for tables without \"network upgrade type\"\n",
    "                        if \"type of upgrade\" in df.columns and \"network upgrade type\" not in df.columns:\n",
    "                            first_row = df.iloc[0]\n",
    "                            if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                df[\"type of upgrade\"] = specific_phrase\n",
    "\n",
    "\n",
    "                                \n",
    "\n",
    "                        # Special logic for ADNU tables\n",
    "                        if re.search(r\"(Area\\s*Delivery\\s*Network\\s*Upgrades)?ADNU\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"adnu\" in df.columns:\n",
    "                                if \"type of upgrade\" not in df.columns:\n",
    "                                    # Group all adnu rows into one \"upgrade\" row\n",
    "                                    adnu_values = df[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    df = df_grouped\n",
    "                                else:\n",
    "                                    # Rename \"adnu\" if necessary\n",
    "                                    if \"upgrade\" in df.columns:\n",
    "                                        df.drop(columns=[\"adnu\"], inplace=True)\n",
    "                                    else:\n",
    "                                        df.rename(columns={\"adnu\": \"upgrade\"}, inplace=True)\n",
    "\n",
    "                            if \"type of upgrade\" not in df.columns:\n",
    "                                df[\"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                first_row = df.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        elif re.search(r\"Network\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" not in df.columns:\n",
    "                                df[\"type of upgrade\"] = \"RNU\"    \n",
    "\n",
    "\n",
    "                        elif re.search(r\"PTO’s Interconnection Facilities\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" not in df.columns:\n",
    "                                df[\"type of upgrade\"] = \"PTO's Interconnection Facilities\"\n",
    "\n",
    "                            elif pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df.at[0, \"type of upgrade\"] = \"PTO's Interconnection Facilities\"\n",
    "\n",
    "\n",
    "                        if \"type of upgrade\" not in df.columns:\n",
    "                                df[\"type of upgrade\"] = specific_phrase            \n",
    "\n",
    "\n",
    "                                    \n",
    "\n",
    "                                    \n",
    "\n",
    "                        base_columns = [\n",
    "                             \"type of upgrade\"\n",
    "                        ]\n",
    "                        remaining_columns = [col for col in df.columns if col not in base_columns]\n",
    "\n",
    "                        df = df.dropna(subset=remaining_columns, how='all')\n",
    "\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df.columns.duplicated().any():\n",
    "                            df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df)\n",
    "                        print(f\"Extracted table from page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                    except ValueError as ve:\n",
    "                        print(f\"Error creating DataFrame for table on page {page_number + 1}, table {table_index}: {ve}\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing page {page_number + 1}: {e}\", file=log_file)\n",
    "                        continue  # Skip the page if an error occurs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 8 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Concatenate all extracted tables\n",
    "    if extracted_tables:\n",
    "        try:\n",
    "            combined_df = pd.concat(extracted_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully combined {len(extracted_tables)} tables.\", file=log_file)\n",
    "            return combined_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating extracted tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 8 data extracted.\", file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def scrape_alternative_tables(pdf_path, log_file, specific_phrases):\n",
    "    \"\"\"\n",
    "    Scrapes all tables in the PDF that contain any of the specific phrases in their cells.\n",
    "    For Style N tables with 'Network Upgrade Type' column, handles 'type of upgrade' accordingly.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        specific_phrases (list): List of specific phrases to search for.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted data from alternative tables containing the specific phrases.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for alternative table scraping...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    # Check if any cell contains any of the specific phrases\n",
    "                    match_found = False\n",
    "                    for row in tab:\n",
    "                        for cell in row:\n",
    "                            if cell and any(re.search(rf\"\\b{re.escape(phrase)}\\b\", cell, re.IGNORECASE) for phrase in specific_phrases):\n",
    "                                match_found = True\n",
    "                                break\n",
    "                        if match_found:\n",
    "                            break\n",
    "                    if match_found:\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        headers = make_unique_headers(headers)  # Ensure headers are unique\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Extra Check: Only process tables with 'Upgrade' or 'Network Upgrade Type' columns\n",
    "                        if not any(col in headers for col in [\"upgrade\", \"network upgrade type\"]):\n",
    "                            print(f\"Skipping alternative Table {table_index} on page {page_number} as it lacks 'Upgrade' or 'Network Upgrade Type' columns.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        has_network_upgrade_type = \"network upgrade type\" in headers\n",
    "\n",
    "                        if has_network_upgrade_type:\n",
    "                            print(f\"'Network Upgrade Type' column found in alternative Table {table_index} on page {page_number}.\", file=log_file)\n",
    "                            # Create 'type of upgrade' column if it doesn't exist\n",
    "                            if \"type of upgrade\" not in headers:\n",
    "                                headers.append(\"type of upgrade\")\n",
    "                                tab[0].append(\"type of upgrade\")  # Add header to the table data\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "\n",
    "                            # Initialize 'type of upgrade' column with None\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = None\n",
    "\n",
    "                            current_upgrade = None\n",
    "                            for idx, row in df_new.iterrows():\n",
    "                                network_upgrade = str(row.get(\"network upgrade type\", \"\")).strip()\n",
    "                                if network_upgrade.lower() == \"total\":\n",
    "                                    current_upgrade = None\n",
    "                                elif network_upgrade in SPECIFIC_PHRASES_2:\n",
    "                                    current_upgrade = network_upgrade\n",
    "                                    df_new.at[idx, \"type of upgrade\"] = current_upgrade\n",
    "                                elif current_upgrade:\n",
    "                                    df_new.at[idx, \"type of upgrade\"] = current_upgrade\n",
    "                            print(f\"Populated 'type of upgrade' column based on 'Network Upgrade Type' in alternative Table {table_index}.\", file=log_file)\n",
    "                        else:\n",
    "                            # Handle tables without 'Network Upgrade Type'\n",
    "                            try:\n",
    "                                df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                            except ValueError as ve:\n",
    "                                print(f\"Error creating DataFrame for alternative Table {table_index} on page {page_number}: {ve}\", file=log_file)\n",
    "                                continue\n",
    "\n",
    "                            # Continue with existing specific phrase checks\n",
    "                            # Assuming you have a specific_phrase variable from elsewhere; adjust as needed\n",
    "                            if specific_phrase:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                    print(f\"Added 'type of upgrade' to all rows in alternative Table {table_index}.\", file=log_file)\n",
    "                                else:\n",
    "                                    first_row = df_new.iloc[0]\n",
    "                                    if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                        df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                        print(f\"Replaced None in 'type of upgrade' for alternative Table {table_index} on page {page_number}.\", file=log_file)\n",
    "\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in alternative Table {table_index} on page {page_number}. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping alternative tables in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted alternative tables...\", file=log_file)\n",
    "        try:\n",
    "            alternative_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} alternative tables.\", file=log_file)\n",
    "            return alternative_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating alternative tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No alternative tables found containing specific phrases.\", file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def extract_table8_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 8 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table8_data = extract_table8(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table8_data.empty:\n",
    "        if is_addendum and original_has_table8.get(project_id, False):\n",
    "            # Scrape alternative tables based on specific phrases\n",
    "            specific_phrases = [\n",
    "                \"PTO\",\n",
    "                \"Reliability Network Upgrade\",\n",
    "                \"Area Delivery Network Upgrade\",\n",
    "                \"Local Delivery Network\",\n",
    "                \"Other Potential Network Upgrade\",\n",
    "                \"Area Delivery Network Upgrades\",\n",
    "                \"Conditionally Assigned Network Upgrades\",\n",
    "                \"ADNU\",\n",
    "                \"LDNU\",\n",
    "                \"RNU\"\n",
    "            ]\n",
    "            alternative_data = scrape_alternative_tables(pdf_path, log_file, specific_phrases)\n",
    "            if not alternative_data.empty:\n",
    "                table8_data = alternative_data\n",
    "    if table8_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table8_data.columns).difference(['point_of_interconnection'])\n",
    "        table8_data = table8_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "\n",
    "        # Repeat base data for each row in table8_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table8_data), ignore_index=True)\n",
    "\n",
    "        try:\n",
    "            # Concatenate base data with Table 8 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table8_data], axis=1, sort=False)\n",
    "           # if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "           #     merged_df[\"is_duplicate\"] = merged_df.duplicated(subset=[\"q_id\", \"type of upgrade\", \"upgrade\"], keep=\"first\")\n",
    "            #    merged_df = merged_df[merged_df[\"is_duplicate\"] == False].drop(columns=[\"is_duplicate\"])\n",
    "            #    print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade'.\", file=log_file)\n",
    "\n",
    "\n",
    "            if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "                # Identify rows where 'type of upgrade' and 'upgrade' are not empty\n",
    "                non_empty_rows = merged_df[\n",
    "                    merged_df[\"type of upgrade\"].notna() & merged_df[\"upgrade\"].notna() &\n",
    "                    (merged_df[\"type of upgrade\"].str.strip() != \"\") & (merged_df[\"upgrade\"].str.strip() != \"\")\n",
    "                ]\n",
    "\n",
    "                # Group by q_id, type of upgrade, and upgrade, keeping the first occurrence\n",
    "                grouped_df = non_empty_rows.groupby([\"q_id\", \"type of upgrade\", \"upgrade\"], as_index=False).first()\n",
    "\n",
    "                # Get the original order of the rows in merged_df before filtering\n",
    "                merged_df[\"original_index\"] = merged_df.index\n",
    "\n",
    "                # Combine unique grouped rows with originally empty rows\n",
    "                final_df = pd.concat([\n",
    "                    grouped_df,\n",
    "                    merged_df[merged_df[\"type of upgrade\"].isna() | (merged_df[\"type of upgrade\"].str.strip() == \"\") |\n",
    "                            merged_df[\"upgrade\"].isna() | (merged_df[\"upgrade\"].str.strip() == \"\")]\n",
    "                ], ignore_index=True, sort=False)\n",
    "\n",
    "                # Restore the original order of the rows based on the saved index\n",
    "                final_df.sort_values(by=\"original_index\", inplace=True)\n",
    "                final_df.drop(columns=[\"original_index\"], inplace=True)\n",
    "                merged_df = final_df\n",
    "\n",
    "                print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade', excluding empty rows while preserving order.\", file=log_file)\n",
    "\n",
    "            \n",
    "\n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "\n",
    "            print(f\"Merged base data with Table 8 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 8 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "def check_has_table8(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 8-1 to 8-3.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*8[-.]([1-3])\\b\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def has_network_upgrade_type_column(pdf_path, log_file):\n",
    "    \"\"\"Checks if any table in the PDF has a column header 'Network Upgrade Type'.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.find_tables()\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        continue\n",
    "                    headers = clean_column_headers(tab[0])\n",
    "                    if \"network upgrade type\" in headers:\n",
    "                        print(f\"Found 'Network Upgrade Type' in PDF {pdf_path} on page {page_number}, table {table_index}.\", file=log_file)\n",
    "                        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking 'Network Upgrade Type' in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path, log_file):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' or 'Revision' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            print(f\"Extracted Text: {text}\", file= log_file)  # Debugging step\n",
    "            # Case-insensitive check for 'Addendum' or 'Revision'\n",
    "            text_lower = text.lower()\n",
    "            return \"addendum\" in text_lower in text_lower\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    \"\"\"\n",
    "    Appends a suffix to duplicate headers to make them unique.\n",
    "\n",
    "    Args:\n",
    "        headers (list): List of column headers.\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique column headers.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all Style N PDFs in the specified project list and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "\n",
    "    SKIP_PROJECTS = {0} #1811\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    \n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "\n",
    "        for project_id in PROJECT_LIST:\n",
    "            # Skip the projects in the SKIP_PROJECTS set\n",
    "            if project_id in SKIP_PROJECTS:\n",
    "                print(f\"Skipping Project {project_id} (marked to skip)\", file=log_file)\n",
    "                continue\n",
    "        \n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"03_phase_2_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            # Instead of filtering by name, take every PDF in this project folder\n",
    "            list_pdfs = [f for f in os.listdir(project_path) if f.lower().endswith(\".pdf\")]\n",
    "            print(f\"Project {project_id} — found PDFs: {list_pdfs}\", file=log_file)\n",
    "\n",
    "            # Treat all of these as Style N candidates\n",
    "            style_n_pdfs_in_project = list_pdfs.copy()\n",
    "\n",
    "\n",
    "            if not style_n_pdfs_in_project:\n",
    "                skipped_projects.add(project_id)\n",
    "                print(f\"No Style N PDFs found in Project {project_id}.\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            for pdf_name in style_n_pdfs_in_project:\n",
    "                pdf_path = os.path.join(project_path, pdf_name)\n",
    "                total_pdfs_accessed += 1\n",
    "\n",
    "                is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                # Check if PDF has 'Network Upgrade Type' column\n",
    "                has_network_upgrade = has_network_upgrade_type_column(pdf_path, log_file)\n",
    "\n",
    "                if has_network_upgrade:\n",
    "                    print(f\"Processing Style N PDF: {pdf_name} from Project {project_id} with 'Network Upgrade Type' column.\", file=log_file)\n",
    "                else:\n",
    "                    print(f\"Processing Style N PDF: {pdf_name} from Project {project_id} without 'Network Upgrade Type' column.\", file=log_file)\n",
    "\n",
    "                try:\n",
    "                    has_table8 = check_has_table8(pdf_path)\n",
    "                    original_has_table8[project_id] = has_table8\n",
    "\n",
    "                    if not has_table8:\n",
    "                        print(f\"No Table 8 in {pdf_name}: extracting base data only\", file=log_file)\n",
    "                        base_df = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_df['table_8_missing'] = True\n",
    "\n",
    "                        if is_add:\n",
    "                            core_addendums = pd.concat([core_addendums, base_df], ignore_index=True)\n",
    "                            addendum_pdfs.append(pdf_name)\n",
    "                        else:\n",
    "                            core_originals  = pd.concat([core_originals,  base_df], ignore_index=True)\n",
    "                            original_pdfs.append(pdf_name)\n",
    "\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        total_pdfs_scraped += 1\n",
    "                        continue\n",
    "\n",
    "                    if not base_data_extracted:\n",
    "                        # Extract base data from original PDF\n",
    "                        base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                        base_data_extracted = True\n",
    "                        print(f\"Extracted base data from Style N PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                    # Extract Table 8 and merge\n",
    "                    df = extract_table8_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                    if not df.empty:\n",
    "                        if is_add:\n",
    "                            core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                        else:\n",
    "                            core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                        scraped_pdfs.append(pdf_name)\n",
    "                        scraped_projects.add(project_id)\n",
    "                        project_scraped = True\n",
    "                        total_pdfs_scraped += 1\n",
    "                        print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                    else:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    skipped_pdfs.append(pdf_name)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                    print(traceback.format_exc(), file=log_file)\n",
    "                    print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                    total_pdfs_skipped += 1\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nList of Style N PDFs (Skipped due to 'Network Upgrade Type'):\")\n",
    "    print(style_n_pdfs)\n",
    "\n",
    "    print(\"\\nTotal Number of Style N PDFs:\", len(style_n_pdfs))\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.applymap(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized and Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 1531\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 1532\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "    q_id  cluster req_deliverability   latitude  longitude  capacity  \\\n",
      "0  1531       11               Full  32.624500 -115.45230       NaN   \n",
      "1  1531       11               Full  32.624500 -115.45230       NaN   \n",
      "2  1532       11               Full  32.625903 -116.17489       NaN   \n",
      "3  1532       11               Full  32.625903 -116.17489       NaN   \n",
      "\n",
      "                            point_of_interconnection type_of_upgrade upgrade  \\\n",
      "0           230 kV bus at Imperial Valley Substation    Total PTO_IF           \n",
      "1           230 kV bus at Imperial Valley Substation       Total RNU           \n",
      "2  New switchyard with the East County-Boulevard ...    Total PTO_IF           \n",
      "3  New switchyard with the East County-Boulevard ...       Total RNU           \n",
      "\n",
      "  description cost_allocation_factor  estimated_cost_x_1000  \\\n",
      "0                                                      0.00   \n",
      "1                                                  21358.00   \n",
      "2                                                   1110.48   \n",
      "3                                                  29518.00   \n",
      "\n",
      "   escalated_cost_x_1000 total_estimated_cost_x_1000  \\\n",
      "0                   0.00                               \n",
      "1               23426.00                               \n",
      "2                1191.24                               \n",
      "3               31758.00                               \n",
      "\n",
      "   total_estimated_cost_x_1000_escalated estimated_time_to_construct  \\\n",
      "0                                    0.0                               \n",
      "1                                32240.0                               \n",
      "2                                    0.0                               \n",
      "3                                55403.0                               \n",
      "\n",
      "  network_upgrade_type item  adnu_cost_rate_x_1000_escalated  \n",
      "0                        no                                0  \n",
      "1                        no                                0  \n",
      "2                        no                                0  \n",
      "3                        no                                0  \n",
      "Itemized rows saved to 'costs_phase_2_cluster_11_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_11_total.csv'.\n",
      "['PTO_IF' 'RNU']\n",
      "[1531 1532]\n",
      "[11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_32162/2502371624.py:186: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_32162/2502371624.py:355: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(q_col, group_keys=False).apply(_grp)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/03_raw/ph2_rawdata_cluster11_style_N_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'cost allocation factor' in df.columns:\n",
    "#    df['description'] = df['cost allocation factor'].apply(extract_non_numeric_text)\n",
    " #   df['cost_allocation_factor'] = df['cost allocation factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "#\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "            \"unnamed_1\",\n",
    "             ],\n",
    "\n",
    "        \n",
    "        \"estimated_cost_x_1000\": [\n",
    "\n",
    "            \"estimated cost x 1000\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"allocated_cost\",\n",
    "            \"assigned cost\",\n",
    "            \"allocated cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"project allocated cost\",\n",
    "            \"assigned cost\",\n",
    "            \"unnamed_14\",\n",
    "            \"unnamed_4\",\n",
    "            \n",
    "            \n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"assigned cost escalated\",\n",
    "            \"assigned escalated cost\",\n",
    "            \n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"project allocated cost escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"unnamed_17\",\n",
    "            \"unnamed_16\",\n",
    "            \"allocat ed cost\",\n",
    "            \n",
    "           \n",
    "\n",
    "        ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "            \"max of estimated time to construct\",\n",
    "            \n",
    "         \n",
    "            \"nu estimated time to construct\",\n",
    "            \n",
    "            \"unnamed_18\",\n",
    "            \"unnamed_19\",\n",
    "            \n",
    "            \"time to construct\",\n",
    "             \n",
    "\n",
    "\n",
    "        ],\n",
    "\n",
    "        \"network upgrade type\": [\n",
    "            \"network upgrade type\",\n",
    "            \"unnamed_2\",\n",
    "        ],\n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"nu total cost\",\n",
    "            \"total cost constant\",\n",
    "            \"sum of total cost constant\",\n",
    "            \"total cost\",\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\",\n",
    "            \"nu total cost escalated\",\n",
    "            \"total cost escalated\",\n",
    "\n",
    "        ],\n",
    "       \n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "\n",
    "        \"adnu_cost_rate_escalated_x_1000\": [\n",
    "        \"cost rate escalated\",\n",
    "        \"escalated cost rate\",\n",
    "        ],\n",
    "\n",
    "        \"description\": [\"description\", \"unnamed_3\"],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "            \"project allocation\",\n",
    "            \n",
    "\n",
    "        ],\n",
    "        \"estimated cost x 1000 escalated with itcca\": [\n",
    "            \"estimated cost x 1000 escalated with itcca\",\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "#if 'estimated_time_to_construct' in df.columns:\n",
    "#    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "#if 'upgrade' in df.columns:\n",
    " #   df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    "#if 'cost_allocation_factor' in df.columns:\n",
    "#    df['description'] = df['cost_allocation_factor'].apply(extract_non_numeric_text)\n",
    " #   df['cost_allocation_factor'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values    \n",
    "\n",
    "\n",
    "# Clear cost_allocation_factor for rows where upgrade is \"Total\" (case-insensitive)\n",
    "#df.loc[df['upgrade'].str.lower() == 'total', 'cost_allocation_factor'] = None  # or use \"\" if you prefer an empty string\n",
    "\n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    "\n",
    "df.drop([ 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "#df.drop([   \"unnamed_3\"\n",
    "#            ], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def move_description_to_upgrade(df):\n",
    "    \"\"\"Moves description to upgrade if type_of_upgrade is 'PTO' and upgrade is empty.\"\"\"\n",
    "    \n",
    "    # Ensure columns are strings and replace NaNs with empty strings for processing\n",
    "    df['upgrade'] = df['upgrade'].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
    "    df['description'] = df['description'].astype(str).replace(\"nan\", \"\").fillna(\"\")\n",
    "\n",
    "    # Debug: Print before update\n",
    "    #print(\"Before update (only PTO rows):\")\n",
    "    #print(df[df['type_of_upgrade'] == 'PTO'][['type_of_upgrade', 'upgrade', 'description']])\n",
    "\n",
    "    # Apply row-wise transformation\n",
    "    def move_if_empty(row):\n",
    "        if row['type_of_upgrade'] == 'PTO' and row['upgrade'].strip() == \"\" and row['description'].strip() != \"\":\n",
    "            row['upgrade'] = row['description']  # Move description to upgrade\n",
    "            row['description'] = None # Clear description\n",
    "        return row\n",
    "\n",
    "    df = df.apply(move_if_empty, axis=1)\n",
    "\n",
    "    # Debug: Print after update\n",
    "    #print(\"\\nAfter update (only PTO rows):\")\n",
    "    #print(df[df['type_of_upgrade'] == 'PTO'][['type_of_upgrade', 'upgrade', 'description']])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply function\n",
    "#df = move_description_to_upgrade(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def force_canu_final(df,\n",
    "                     q_col='q_id',\n",
    "                     net_col='network_upgrade_type',\n",
    "                     type_col='type_of_upgrade'):\n",
    "    df = df.copy()\n",
    "    if type_col not in df.columns:\n",
    "        df[type_col] = pd.NA\n",
    "\n",
    "    # compile once\n",
    "    PAT_START = r'^\\s*CANU-GR\\b'\n",
    "    PAT_TOTAL = r'^\\s*Total\\s+CANU-GR\\b'\n",
    "    # add here any other \"stop‑on‑seeing‑this\" patterns:\n",
    "    PAT_OTHER = r'^\\s*(?:GRNU|IRNU-A)\\b'\n",
    "\n",
    "    def _grp(grp):\n",
    "        txt = grp[net_col].fillna('').astype(str)\n",
    "\n",
    "        start_mask = txt.str.match(PAT_START, case=False)\n",
    "        if not start_mask.any():\n",
    "            return grp\n",
    "\n",
    "        i1 = np.argmax(start_mask.values)\n",
    "\n",
    "        # Find any “true end” markers\n",
    "        total_mask = txt.str.match(PAT_TOTAL, case=False)\n",
    "        total_idxs = np.where(total_mask.values)[0]\n",
    "\n",
    "        # Find any “other” markers\n",
    "        other_mask = txt.str.match(PAT_OTHER, case=False)\n",
    "        other_idxs = np.where(other_mask.values)[0]\n",
    "\n",
    "        # collect the first boundary ≥ i1 of either kind\n",
    "        candidates = []\n",
    "        # total is inclusive\n",
    "        for ti in total_idxs:\n",
    "            if ti >= i1:\n",
    "                candidates.append((\"total\", ti))\n",
    "        # other is exclusive\n",
    "        for oi in other_idxs:\n",
    "            if oi >= i1:\n",
    "                candidates.append((\"other\", oi))\n",
    "        if not candidates:\n",
    "            return grp\n",
    "\n",
    "        # pick the earliest boundary\n",
    "        kind, boundary_idx = min(candidates, key=lambda x: x[1])\n",
    "\n",
    "        if kind == \"total\":\n",
    "            i2 = boundary_idx\n",
    "        else:\n",
    "            # stop one above the “other” marker\n",
    "            i2 = boundary_idx - 1\n",
    "\n",
    "        if i2 >= i1:\n",
    "            grp.iloc[i1:i2+1, grp.columns.get_loc(type_col)] = 'CANU'\n",
    "\n",
    "        return grp\n",
    "\n",
    "    return df.groupby(q_col, group_keys=False).apply(_grp)\n",
    "\n",
    "\n",
    "df = force_canu_final(df,\n",
    "                      q_col='q_id',\n",
    "                      net_col='network_upgrade_type',\n",
    "                      type_col='type_of_upgrade')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "df['tot'] = df.apply(\n",
    "    lambda row: 'yes' if (\n",
    "        (pd.notna(row.get('upgrade')) and 'Total' in str(row['upgrade']))  \n",
    "    ) else 'no',\n",
    "    axis=1\n",
    ") \n",
    "\n",
    "# Now extract ONLY \"Total\" rows with a foolproof match\n",
    "total_rows_df = df[df['tot'] == 'yes']\n",
    "\n",
    "total_rows_df = total_rows_df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "total_rows_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/02_intermediate/costs_phase_2_cluster_11_style_N_total_network.csv', index=False) \n",
    "df = df[df['upgrade'].str.strip().str.lower() != 'total']\n",
    "df = df[df['network_upgrade_type'].str.strip().str.lower() != 'total']\n",
    " \n",
    "\n",
    " \n",
    "df.drop('tot', axis=1, inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)  \n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"PTO's Interconnection Facilities (Note 1)\": \"PTO_IF\",\n",
    " \"PTO's Interconnection Facilities (Note 4)\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'CANU-GR': 'CANU-GR',\n",
    " 'Total CANU-GR': 'CANU-GR',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    "}\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# STEP 8½: For certain q_ids with no cost data, insert placeholder rows\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# list of q_ids that need 3 “empty” upgrade rows\n",
    "placeholder_qids = []   # ← your set of missing‐data q_ids\n",
    "\n",
    "# columns that carry only the base data\n",
    "base_cols = [\n",
    "    \"q_id\",\n",
    "    \"cluster\",\n",
    "    \"req_deliverability\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"capacity\",\n",
    "    \"point_of_interconnection\",\n",
    "]\n",
    "\n",
    "# the three upgrade codes to inject\n",
    "placeholder_upgrades = [\"PTO_IF\", \"RNU\", \"LDNU\"]\n",
    "\n",
    "new_rows = []\n",
    "for qid in placeholder_qids:\n",
    "    # find base‐data rows for this qid\n",
    "    base_mask = df[\"q_id\"] == qid\n",
    "    if not base_mask.any():\n",
    "        print(f\"Warning: no row at all for q_id {qid}, skipping placeholders\")\n",
    "        continue\n",
    "\n",
    "    # extract the one base row\n",
    "    base = df.loc[base_mask, base_cols].iloc[0]\n",
    "\n",
    "    # build three empty‐detail rows\n",
    "    for code in placeholder_upgrades:\n",
    "        row = {c: \"\" for c in df.columns}\n",
    "        # copy only the base columns\n",
    "        for c in base_cols:\n",
    "            row[c] = base[c]\n",
    "        # assign the placeholder upgrade code\n",
    "        row[\"type_of_upgrade\"] = code\n",
    "        new_rows.append(row)\n",
    "\n",
    "# append placeholders if any\n",
    "if new_rows:\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    # re‐order columns so the new rows fit your desired layout\n",
    "    df = reorder_columns(df)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# now continue with STEP 9: Create Total rows…\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'escalated_cost_rate','total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000',  ]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\"]\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "import re\n",
    "\n",
    "# Define the list of upgrade phrases\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\",  \"PNU\" , \"CANU-GR\"]\n",
    "\n",
    "def classify_and_clean_upgrades(df):\n",
    "    \"\"\"\n",
    "    For rows with type_of_upgrade equal to \"RNU\", moves the value from \n",
    "    'network_upgrade_type' to a new 'upgrade' column, and then removes rows where:\n",
    "      - 'network_upgrade_type' exactly matches an upgrade phrase, or\n",
    "      - 'network_upgrade_type' matches the pattern 'Total <upgrade_phrase>'.\n",
    "    Other rows remain unchanged.\n",
    "    \"\"\"\n",
    "    # Ensure 'network_upgrade_type' is a string and fill missing values with an empty string\n",
    "    df['network_upgrade_type'] = df['network_upgrade_type'].astype(str).fillna(\"\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # Process only rows where type_of_upgrade is \"RNU\"\n",
    "    rnu_mask = df['type_of_upgrade'] == \"RNU\"\n",
    "    \n",
    "    # For these rows, copy the network_upgrade_type to a new column called 'upgrade'\n",
    "    df.loc[rnu_mask, 'upgrade'] = df.loc[rnu_mask, 'network_upgrade_type']\n",
    "\n",
    "    \n",
    "    \n",
    "    # Build a regex pattern to match strings like \"Total IRNU\", \"Total GRNU\", etc.\n",
    "    pattern = r\"^Total\\s+(\" + \"|\".join(re.escape(phrase) for phrase in upgrade_phrases) + r\")$\"\n",
    "    \n",
    "    # Create a condition that checks for rows (within type_of_upgrade==\"RNU\") that should be removed:\n",
    "    # - The network_upgrade_type is exactly one of the upgrade phrases.\n",
    "    # - OR the network_upgrade_type matches the pattern \"Total <upgrade_phrase>\".\n",
    "    remove_condition = rnu_mask & (\n",
    "        df['network_upgrade_type'].isin(upgrade_phrases) |\n",
    "        df['network_upgrade_type'].str.match(pattern)\n",
    "    )\n",
    "    \n",
    "    # Remove the rows meeting the remove_condition\n",
    "    df = df[~remove_condition]\n",
    "    #df.loc[rnu_mask, 'upgrade'] = df.loc[rnu_mask, 'network_upgrade_type']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the updated function\n",
    "df = classify_and_clean_upgrades(df)\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def force_canu_final(df,\n",
    "                     q_col='q_id',\n",
    "                     net_col='network_upgrade_type',\n",
    "                     type_col='type_of_upgrade'):\n",
    "    df = df.copy()\n",
    "    if type_col not in df.columns:\n",
    "        df[type_col] = pd.NA\n",
    "\n",
    "    # compile once\n",
    "    PAT_START = r'^\\s*CANU-GR\\b'\n",
    "    PAT_TOTAL = r'^\\s*Total\\s+CANU-GR\\b'\n",
    "    # add here any other \"stop‑on‑seeing‑this\" patterns:\n",
    "    PAT_OTHER = r'^\\s*(?:GRNU|IRNU-A)\\b'\n",
    "\n",
    "    def _grp(grp):\n",
    "        txt = grp[net_col].fillna('').astype(str)\n",
    "\n",
    "        start_mask = txt.str.match(PAT_START, case=False)\n",
    "        if not start_mask.any():\n",
    "            return grp\n",
    "\n",
    "        i1 = np.argmax(start_mask.values)\n",
    "\n",
    "        # Find any “true end” markers\n",
    "        total_mask = txt.str.match(PAT_TOTAL, case=False)\n",
    "        total_idxs = np.where(total_mask.values)[0]\n",
    "\n",
    "        # Find any “other” markers\n",
    "        other_mask = txt.str.match(PAT_OTHER, case=False)\n",
    "        other_idxs = np.where(other_mask.values)[0]\n",
    "\n",
    "        # collect the first boundary ≥ i1 of either kind\n",
    "        candidates = []\n",
    "        # total is inclusive\n",
    "        for ti in total_idxs:\n",
    "            if ti >= i1:\n",
    "                candidates.append((\"total\", ti))\n",
    "        # other is exclusive\n",
    "        for oi in other_idxs:\n",
    "            if oi >= i1:\n",
    "                candidates.append((\"other\", oi))\n",
    "        if not candidates:\n",
    "            return grp\n",
    "\n",
    "        # pick the earliest boundary\n",
    "        kind, boundary_idx = min(candidates, key=lambda x: x[1])\n",
    "\n",
    "        if kind == \"total\":\n",
    "            i2 = boundary_idx\n",
    "        else:\n",
    "            # stop one above the “other” marker\n",
    "            i2 = boundary_idx - 1\n",
    "\n",
    "        if i2 >= i1:\n",
    "            grp.iloc[i1:i2+1, grp.columns.get_loc(type_col)] = 'CANU'\n",
    "\n",
    "        return grp\n",
    "\n",
    "    return df.groupby(q_col, group_keys=False).apply(_grp)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "# only drop rows whose upgrade is exactly \"total\"\n",
    "df = df[df['upgrade']\n",
    "          .astype(str)            # ensure it's string dtype\n",
    "          .str.strip()\n",
    "          .str.lower()\n",
    "          .fillna('')             # turn NaN → ''\n",
    "          .ne('total')]           # keep all rows not equal to 'total'\n",
    "\n",
    "\n",
    " \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5 , \"CANU\": 6, \"CANU-GR\": 7, \"LOPNU\": 8}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def force_canu_final(df,\n",
    "                     q_col='q_id',\n",
    "                     net_col='network_upgrade_type',\n",
    "                     type_col='type_of_upgrade'):\n",
    "    df = df.copy()\n",
    "    if type_col not in df.columns:\n",
    "        df[type_col] = pd.NA\n",
    "\n",
    "    # compile once\n",
    "    PAT_START = r'^\\s*CANU-GR\\b'\n",
    "    PAT_TOTAL = r'^\\s*Total\\s+CANU-GR\\b'\n",
    "    # add here any other \"stop‑on‑seeing‑this\" patterns:\n",
    "    PAT_OTHER = r'^\\s*(?:GRNU|IRNU-A)\\b'\n",
    "\n",
    "    def _grp(grp):\n",
    "        txt = grp[net_col].fillna('').astype(str)\n",
    "\n",
    "        start_mask = txt.str.match(PAT_START, case=False)\n",
    "        if not start_mask.any():\n",
    "            return grp\n",
    "\n",
    "        i1 = np.argmax(start_mask.values)\n",
    "\n",
    "        # Find any “true end” markers\n",
    "        total_mask = txt.str.match(PAT_TOTAL, case=False)\n",
    "        total_idxs = np.where(total_mask.values)[0]\n",
    "\n",
    "        # Find any “other” markers\n",
    "        other_mask = txt.str.match(PAT_OTHER, case=False)\n",
    "        other_idxs = np.where(other_mask.values)[0]\n",
    "\n",
    "        # collect the first boundary ≥ i1 of either kind\n",
    "        candidates = []\n",
    "        # total is inclusive\n",
    "        for ti in total_idxs:\n",
    "            if ti >= i1:\n",
    "                candidates.append((\"total\", ti))\n",
    "        # other is exclusive\n",
    "        for oi in other_idxs:\n",
    "            if oi >= i1:\n",
    "                candidates.append((\"other\", oi))\n",
    "        if not candidates:\n",
    "            return grp\n",
    "\n",
    "        # pick the earliest boundary\n",
    "        kind, boundary_idx = min(candidates, key=lambda x: x[1])\n",
    "\n",
    "        if kind == \"total\":\n",
    "            i2 = boundary_idx\n",
    "        else:\n",
    "            # stop one above the “other” marker\n",
    "            i2 = boundary_idx - 1\n",
    "\n",
    "        if i2 >= i1:\n",
    "            grp.iloc[i1:i2+1, grp.columns.get_loc(type_col)] = 'CANU'\n",
    "\n",
    "        return grp\n",
    "\n",
    "    return df.groupby(q_col, group_keys=False).apply(_grp)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "# the codes you want to remove (with or without a leading “Total ”)\n",
    "codes = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\"]\n",
    "\n",
    "# build a single regex that matches either “XYZ” or “Total XYZ” at the start\n",
    "pat = r'^(?:Total\\s+)?(?:' + '|'.join(map(re.escape, codes)) + r')\\b'\n",
    "\n",
    "# filter them out\n",
    "mask = df['network_upgrade_type'].str.contains(pat, case=False, na=False)\n",
    "#df = df[~mask].reset_index(drop=True)\n",
    "df = stable_sort_by_type_of_upgrade(df)  \n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/03_raw/cluster_11_style_N.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'adnu_cost_rate_x_1000_escalated']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ((group['type_of_upgrade'] == f\"Total {upgrade}\") & (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['type_of_upgrade'] == f\"Total {upgrade}\") & (group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    " \n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "     \n",
    "     \n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/02_intermediate/costs_phase_2_cluster_11_style_N_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    \n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/02_intermediate/costs_phase_2_cluster_11_style_N_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_11_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_11_total.csv'.\")\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/02_intermediate/costs_phase_2_cluster_11_style_N_itemized.csv')\n",
    "df2 = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/02_intermediate/costs_phase_2_cluster_11_style_N_total.csv')\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/01_clean/costs_phase_2_cluster_11_style_N_itemized_updated.csv', index=False)\n",
    "df2.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 11/01_clean/costs_phase_2_cluster_11_style_N_total_updated.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
