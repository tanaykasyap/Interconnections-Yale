{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C9 Style Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects to process: [1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347]\n",
      "\n",
      "--- Processing project 1223 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1223/03_phase_2_study/Q1223 American Kings 9_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1223\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('36.1728', '119.9248')\n",
      "Base data extracted:\n",
      "{'q_id': ['1223'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['36.1728'], 'longitude': ['119.9248'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1223/03_phase_2_study/Q1223 American Kings 9_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1223 American Kings 9_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1224 ---\n",
      "No Appendix A and no Attachment 2 for project 1224. Skipping.\n",
      "\n",
      "--- Processing project 1225 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1225/03_phase_2_study/Q1225 Cinco_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1225\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('36.3821', '120.1481')\n",
      "Base data extracted:\n",
      "{'q_id': ['1225'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['36.3821'], 'longitude': ['120.1481'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1225/03_phase_2_study/Q1225 Cinco_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1225 Cinco_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1226 ---\n",
      "No Appendix A and no Attachment 2 for project 1226. Skipping.\n",
      "\n",
      "--- Processing project 1227 ---\n",
      "No Appendix A and no Attachment 2 for project 1227. Skipping.\n",
      "\n",
      "--- Processing project 1228 ---\n",
      "No Appendix A and no Attachment 2 for project 1228. Skipping.\n",
      "\n",
      "--- Processing project 1229 ---\n",
      "No Appendix A and no Attachment 2 for project 1229. Skipping.\n",
      "\n",
      "--- Processing project 1230 ---\n",
      "No Appendix A and no Attachment 2 for project 1230. Skipping.\n",
      "\n",
      "--- Processing project 1231 ---\n",
      "No Appendix A and no Attachment 2 for project 1231. Skipping.\n",
      "\n",
      "--- Processing project 1232 ---\n",
      "No Appendix A and no Attachment 2 for project 1232. Skipping.\n",
      "\n",
      "--- Processing project 1233 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1233/03_phase_2_study/Q1233 Hobbs BESS_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1233\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('36.91988', '119.9846')\n",
      "Base data extracted:\n",
      "{'q_id': ['1233'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['36.91988'], 'longitude': ['119.9846'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1233/03_phase_2_study/Q1233 Hobbs BESS_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1233 Hobbs BESS_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1234 ---\n",
      "No Appendix A and no Attachment 2 for project 1234. Skipping.\n",
      "\n",
      "--- Processing project 1235 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1235/03_phase_2_study/Q1235 Hudson Solar 1_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1235\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('36.609079', '120.589121')\n",
      "Base data extracted:\n",
      "{'q_id': ['1235'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['36.609079'], 'longitude': ['120.589121'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1235/03_phase_2_study/Q1235 Hudson Solar 1_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1235 Hudson Solar 1_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1236 ---\n",
      "No Appendix A and no Attachment 2 for project 1236. Skipping.\n",
      "\n",
      "--- Processing project 1237 ---\n",
      "No Appendix A and no Attachment 2 for project 1237. Skipping.\n",
      "\n",
      "--- Processing project 1238 ---\n",
      "No Appendix A and no Attachment 2 for project 1238. Skipping.\n",
      "\n",
      "--- Processing project 1239 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1239/03_phase_2_study/Q1239 Medeiros Solar_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1239\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('37.0983417', '121.0475056')\n",
      "Base data extracted:\n",
      "{'q_id': ['1239'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['37.0983417'], 'longitude': ['121.0475056'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1239/03_phase_2_study/Q1239 Medeiros Solar_C9PhII_Appendix A-Addendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1239\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1239'], 'cluster': ['9'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1239/03_phase_2_study/Q1239 Medeiros Solar_C9PhII_Appendix A-Addendum1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1239 Medeiros Solar_C9PhII_Appendix A-Addendum1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1239/03_phase_2_study/Q1239 Medeiros Solar_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1239 Medeiros Solar_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1240 ---\n",
      "No Appendix A and no Attachment 2 for project 1240. Skipping.\n",
      "\n",
      "--- Processing project 1241 ---\n",
      "No Appendix A and no Attachment 2 for project 1241. Skipping.\n",
      "\n",
      "--- Processing project 1242 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1242/03_phase_2_study/Q1242 Pluot_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1242\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('36.086933', '119.940825')\n",
      "Base data extracted:\n",
      "{'q_id': ['1242'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['36.086933'], 'longitude': ['119.940825'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1242/03_phase_2_study/Q1242 Pluot_C9PhII_Appendix A-Addendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1242\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1242'], 'cluster': ['9'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1242/03_phase_2_study/Q1242 Pluot_C9PhII_Appendix A-Addendum1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1242 Pluot_C9PhII_Appendix A-Addendum1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1242/03_phase_2_study/16AS0160-Q1242_Pluot_C9PhII_Appendix_AAddendum1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping 16AS0160-Q1242_Pluot_C9PhII_Appendix_AAddendum1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1242/03_phase_2_study/Q1242 Pluot_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1242 Pluot_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1243 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1243/03_phase_2_study/Q1243 Pomegranate_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1243\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('36.138025', '119.941015')\n",
      "Base data extracted:\n",
      "{'q_id': ['1243'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['36.138025'], 'longitude': ['119.941015'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1243/03_phase_2_study/Q1243Pomegranate_Appendix_A_Addendum5.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1243\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1243'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1243/03_phase_2_study/Q1243 Pomegranate_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1243 Pomegranate_Appendix A-C9PhII.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1243/03_phase_2_study/Q1243Pomegranate_Appendix_A_Addendum5.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1243Pomegranate_Appendix_A_Addendum5.pdf\n",
      "\n",
      "--- Processing project 1244 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1244/03_phase_2_study/Q1244 Proxima_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1244\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('37.37', '121.15')\n",
      "Base data extracted:\n",
      "{'q_id': ['1244'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['37.37'], 'longitude': ['121.15'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1244/03_phase_2_study/Q1244 Proxima_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1244 Proxima_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1245 ---\n",
      "No Appendix A and no Attachment 2 for project 1245. Skipping.\n",
      "\n",
      "--- Processing project 1246 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1246/03_phase_2_study/Q1246 Romero Creek Hybrid_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1246\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('37.1249944', '121.064378')\n",
      "Base data extracted:\n",
      "{'q_id': ['1246'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['37.1249944'], 'longitude': ['121.064378'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1246/03_phase_2_study/Q1246 Romero Creek Hybrid_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1246 Romero Creek Hybrid_Appendix A-C9PhII.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1247\n",
      "\n",
      "--- Processing project 1248 ---\n",
      "No Appendix A and no Attachment 2 for project 1248. Skipping.\n",
      "\n",
      "--- Processing project 1249 ---\n",
      "No Appendix A and no Attachment 2 for project 1249. Skipping.\n",
      "\n",
      "--- Processing project 1250 ---\n",
      "No Appendix A and no Attachment 2 for project 1250. Skipping.\n",
      "\n",
      "--- Processing project 1251 ---\n",
      "No Appendix A and no Attachment 2 for project 1251. Skipping.\n",
      "\n",
      "--- Processing project 1252 ---\n",
      "No Appendix A and no Attachment 2 for project 1252. Skipping.\n",
      "\n",
      "--- Processing project 1253 ---\n",
      "No Appendix A and no Attachment 2 for project 1253. Skipping.\n",
      "\n",
      "--- Processing project 1254 ---\n",
      "No Appendix A and no Attachment 2 for project 1254. Skipping.\n",
      "\n",
      "--- Processing project 1255 ---\n",
      "No Appendix A and no Attachment 2 for project 1255. Skipping.\n",
      "\n",
      "--- Processing project 1256 ---\n",
      "No Appendix A and no Attachment 2 for project 1256. Skipping.\n",
      "\n",
      "--- Processing project 1257 ---\n",
      "No Appendix A and no Attachment 2 for project 1257. Skipping.\n",
      "\n",
      "--- Processing project 1258 ---\n",
      "No Appendix A and no Attachment 2 for project 1258. Skipping.\n",
      "\n",
      "--- Processing project 1259 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1259/03_phase_2_study/C9PhII-Q1259 Northern Orchard 2 Solar_Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1259\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('35.121652', '119.268653')\n",
      "Base data extracted:\n",
      "{'q_id': ['1259'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['35.121652'], 'longitude': ['119.268653'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1259/03_phase_2_study/C9PhII-Q1259 Northern Orchard 2 Solar_Appendix A.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9PhII-Q1259 Northern Orchard 2 Solar_Appendix A.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1259/03_phase_2_study/C13PhII - Attachment 1- Allocation of NU Cost Estimates -Q1259.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C13PhII - Attachment 1- Allocation of NU Cost Estimates -Q1259.pdf\n",
      "\n",
      "--- Processing project 1260 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1260/03_phase_2_study/C9PhII-Q1260 Northern Orchard 3 Solar_Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1260\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('35.131376', '119.226156')\n",
      "Base data extracted:\n",
      "{'q_id': ['1260'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['35.131376'], 'longitude': ['119.226156'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1260/03_phase_2_study/C13PhII - Attachment 1- Allocation of NU Cost Estimates -Q1260.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C13PhII - Attachment 1- Allocation of NU Cost Estimates -Q1260.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1260/03_phase_2_study/C9PhII-Q1260 Northern Orchard 3 Solar_Appendix A.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9PhII-Q1260 Northern Orchard 3 Solar_Appendix A.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1261\n",
      "\n",
      "--- Processing project 1262 ---\n",
      "No Appendix A and no Attachment 2 for project 1262. Skipping.\n",
      "\n",
      "--- Processing project 1263 ---\n",
      "No Appendix A and no Attachment 2 for project 1263. Skipping.\n",
      "\n",
      "--- Processing project 1264 ---\n",
      "No Appendix A and no Attachment 2 for project 1264. Skipping.\n",
      "\n",
      "--- Processing project 1265 ---\n",
      "No Appendix A and no Attachment 2 for project 1265. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1266\n",
      "\n",
      "--- Processing project 1267 ---\n",
      "No Appendix A and no Attachment 2 for project 1267. Skipping.\n",
      "\n",
      "--- Processing project 1268 ---\n",
      "No Appendix A and no Attachment 2 for project 1268. Skipping.\n",
      "\n",
      "--- Processing project 1269 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1269/03_phase_2_study/Q1269 Capetown Wind Farm_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1269\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('40.4113861', '124.308086')\n",
      "Base data extracted:\n",
      "{'q_id': ['1269'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['40.4113861'], 'longitude': ['124.308086'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1269/03_phase_2_study/16AS0132-Q1269_Capetown_Wind_Farm_Appendix_AC9PhII__Addendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1269\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('40.4113861', '124.308086')\n",
      "Base data extracted:\n",
      "{'q_id': ['1269'], 'cluster': ['9'], 'req_deliverability': [None], 'latitude': ['40.4113861'], 'longitude': ['124.308086'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1269/03_phase_2_study/C9Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1269.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1269.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1269/03_phase_2_study/16AS0132-Q1269_Capetown_Wind_Farm_Appendix_AC9PhII__Addendum1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping 16AS0132-Q1269_Capetown_Wind_Farm_Appendix_AC9PhII__Addendum1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1269/03_phase_2_study/Q1269 Capetown Wind Farm_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1269 Capetown Wind Farm_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1270 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1270/03_phase_2_study/Q1270 Corby_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1270\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('38.412998', '121.910902')\n",
      "Base data extracted:\n",
      "{'q_id': ['1270'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['38.412998'], 'longitude': ['121.910902'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1270/03_phase_2_study/Q1270 Corby_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1270 Corby_Appendix A-C9PhII.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1270/03_phase_2_study/C9Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1270.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1270.pdf\n",
      "\n",
      "--- Processing project 1271 ---\n",
      "No Appendix A and no Attachment 2 for project 1271. Skipping.\n",
      "\n",
      "--- Processing project 1272 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1272/03_phase_2_study/Q1272 Griswald BESS_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1272\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('37.9201', '121.2392')\n",
      "Base data extracted:\n",
      "{'q_id': ['1272'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['37.9201'], 'longitude': ['121.2392'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1272/03_phase_2_study/16AS0091-Q1272_Griswald_BESS_Appendix_AC9PhII__Addendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1272\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('37.9201', '121.2392')\n",
      "Base data extracted:\n",
      "{'q_id': ['1272'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['37.9201'], 'longitude': ['121.2392'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1272/03_phase_2_study/16AS0091-Q1272_Griswald_BESS_Appendix_AC9PhII__Addendum1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping 16AS0091-Q1272_Griswald_BESS_Appendix_AC9PhII__Addendum1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1272/03_phase_2_study/Q1272 Griswald BESS_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1272 Griswald BESS_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1273 ---\n",
      "No Appendix A and no Attachment 2 for project 1273. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1274\n",
      "\n",
      "--- Processing project 1275 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1275/03_phase_2_study/Q1275 Kola_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1275\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('37.719165', '121.591278')\n",
      "Base data extracted:\n",
      "{'q_id': ['1275'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['37.719165'], 'longitude': ['121.591278'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1275/03_phase_2_study/Q1275Kola_Appendix_A_2022_Addendum6_09152023.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1275\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1275'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1275/03_phase_2_study/Q1275 Kola_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1275 Kola_Appendix A-C9PhII.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1275/03_phase_2_study/Q1275Kola_Appendix_A_2022_Addendum6_09152023.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1275Kola_Appendix_A_2022_Addendum6_09152023.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1276\n",
      "\n",
      "--- Processing project 1277 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1277/03_phase_2_study/Q1277 Mulqueeney Ranch Wind Project_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1277\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('37.708941', '121.583837')\n",
      "Base data extracted:\n",
      "{'q_id': ['1277'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['37.708941'], 'longitude': ['121.583837'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1277/03_phase_2_study/P2RPT-Q1277_Mulqueeney_Ranch_Wind_Project_C9PhII_Appendix_AAddendum1.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1277\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1277'], 'cluster': ['9'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1277/03_phase_2_study/P2RPT-Q1277_Mulqueeney_Ranch_Wind_Project_C9PhII_Appendix_AAddendum1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping P2RPT-Q1277_Mulqueeney_Ranch_Wind_Project_C9PhII_Appendix_AAddendum1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1277/03_phase_2_study/Q1277 Mulqueeney Ranch Wind Project_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1277 Mulqueeney Ranch Wind Project_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1278 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1278/03_phase_2_study/Q1278 Northern Pines Energy Center_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1278\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('40.2977722', '120.997125')\n",
      "Base data extracted:\n",
      "{'q_id': ['1278'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['40.2977722'], 'longitude': ['120.997125'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1278/03_phase_2_study/Q1278 Northern Pines Energy Center_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1278 Northern Pines Energy Center_Appendix A-C9PhII.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1279\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1280\n",
      "\n",
      "--- Processing project 1281 ---\n",
      "No Appendix A and no Attachment 2 for project 1281. Skipping.\n",
      "\n",
      "--- Processing project 1282 ---\n",
      "No Appendix A and no Attachment 2 for project 1282. Skipping.\n",
      "\n",
      "--- Processing project 1283 ---\n",
      "No Appendix A and no Attachment 2 for project 1283. Skipping.\n",
      "\n",
      "--- Processing project 1284 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1284/03_phase_2_study/Q1284 Walker Ridge_Appendix A-C9PhII.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1284\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found directional GPS coordinates: ('39.028623', '122.457131')\n",
      "Base data extracted:\n",
      "{'q_id': ['1284'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['39.028623'], 'longitude': ['122.457131'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1284/03_phase_2_study/C9Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1284.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1284.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1284/03_phase_2_study/Q1284 Walker Ridge_Appendix A-C9PhII.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1284 Walker Ridge_Appendix A-C9PhII.pdf\n",
      "\n",
      "--- Processing project 1285 ---\n",
      "No Appendix A and no Attachment 2 for project 1285. Skipping.\n",
      "\n",
      "--- Processing project 1286 ---\n",
      "No Appendix A and no Attachment 2 for project 1286. Skipping.\n",
      "\n",
      "--- Processing project 1287 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1287/03_phase_2_study/QC9PhII_Q1287_Mt Laguna Wind_Appendix A_11-22-2017.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1287\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.79563056', '-116.447972')\n",
      "Base data extracted:\n",
      "{'q_id': ['1287'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['32.79563056'], 'longitude': ['-116.447972'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1287/03_phase_2_study/QC9PhII_Q1287_Mt Laguna Wind_Appendix A_11-22-2017.pdf\n",
      "--> Not an Attachment 2 PDF: skipping QC9PhII_Q1287_Mt Laguna Wind_Appendix A_11-22-2017.pdf\n",
      "\n",
      "--- Processing project 1288 ---\n",
      "No Appendix A and no Attachment 2 for project 1288. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1289\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1290\n",
      "\n",
      "--- Processing project 1291 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1291/03_phase_2_study/QC9PhII_Q1291_Mesquite Solar 5_Appendix A_11-22-2017.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1291\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('39.11', '-100.11')\n",
      "Base data extracted:\n",
      "{'q_id': ['1291'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['39.11'], 'longitude': ['-100.11'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1291/03_phase_2_study/QC9PhII_Q1291_Mesquite Solar 5_Appendix A_11-22-2017.pdf\n",
      "--> Not an Attachment 2 PDF: skipping QC9PhII_Q1291_Mesquite Solar 5_Appendix A_11-22-2017.pdf\n",
      "\n",
      "--- Processing project 1292 ---\n",
      "No Appendix A and no Attachment 2 for project 1292. Skipping.\n",
      "\n",
      "--- Processing project 1293 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1293/03_phase_2_study/QC9PhII_Q1293_Signal Peak_Appendix A_11-22-2017.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1293\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('32.9286', '-113.5444')\n",
      "Base data extracted:\n",
      "{'q_id': ['1293'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['32.9286'], 'longitude': ['-113.5444'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Missing base columns ['capacity', 'point_of_interconnection']—attempting updates\n",
      "Still missing after updates: ['capacity', 'point_of_interconnection']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1293/03_phase_2_study/QC9PhII_Q1293_Signal Peak_Appendix A_11-22-2017.pdf\n",
      "--> Not an Attachment 2 PDF: skipping QC9PhII_Q1293_Signal Peak_Appendix A_11-22-2017.pdf\n",
      "\n",
      "--- Processing project 1294 ---\n",
      "No Appendix A and no Attachment 2 for project 1294. Skipping.\n",
      "\n",
      "--- Processing project 1295 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1295/03_phase_2_study/C9P2-EAST-Q1295-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1295\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.9295', '-116.5720')\n",
      "Base data extracted:\n",
      "{'q_id': ['1295'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['33.9295'], 'longitude': ['-116.5720'], 'capacity': [None], 'point_of_interconnection': ['Devers 220 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1295/03_phase_2_study/C9PII-EAST-Q1295-AppendixA_Addendum-01.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1295\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.9295', '-116.5720')\n",
      "Base data extracted:\n",
      "{'q_id': ['1295'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['33.9295'], 'longitude': ['-116.5720'], 'capacity': [None], 'point_of_interconnection': ['Devers 220 kV Substation']}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1295/03_phase_2_study/C9P2-EAST-Q1295-Atchmnt1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-EAST-Q1295-Atchmnt1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1295/03_phase_2_study/C9P2-EAST-Q1295- Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1295/03_phase_2_study/C9P2-EAST-Q1295- Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1295/03_phase_2_study/C9P2-EAST-Q1295-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-EAST-Q1295-AppendixA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1295/03_phase_2_study/C9PII-EAST-Q1295-AppendixA_Addendum-01.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9PII-EAST-Q1295-AppendixA_Addendum-01.pdf\n",
      "\n",
      "--- Processing project 1296 ---\n",
      "No Appendix A and no Attachment 2 for project 1296. Skipping.\n",
      "\n",
      "--- Processing project 1297 ---\n",
      "No Appendix A and no Attachment 2 for project 1297. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1298\n",
      "\n",
      "--- Processing project 1299 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1299/03_phase_2_study/C9P2-EAST-Q1299-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1299\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.9410972', '-117.057561')\n",
      "Base data extracted:\n",
      "{'q_id': ['1299'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['33.9410972'], 'longitude': ['-117.057561'], 'capacity': [None], 'point_of_interconnection': ['El Casco 220 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1299/03_phase_2_study/C9P2-EAST-Q1299-Atchmnt1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-EAST-Q1299-Atchmnt1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1299/03_phase_2_study/C9P2-EAST-Q1299-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-EAST-Q1299-AppendixA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1299/03_phase_2_study/C9P2-EAST-Q1299 Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1299/03_phase_2_study/C9P2-EAST-Q1299 Attachment 2.pdf\n",
      "\n",
      "--- Processing project 1300 ---\n",
      "No Appendix A and no Attachment 2 for project 1300. Skipping.\n",
      "\n",
      "--- Processing project 1301 ---\n",
      "No Appendix A and no Attachment 2 for project 1301. Skipping.\n",
      "\n",
      "--- Processing project 1302 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1302/03_phase_2_study/C9P2-EAST-Q1302-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1302\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('33.710169', '-115.267583')\n",
      "Base data extracted:\n",
      "{'q_id': ['1302'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['33.710169'], 'longitude': ['-115.267583'], 'capacity': [None], 'point_of_interconnection': ['Red Bluff 220 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1302/03_phase_2_study/C9P2-EAST-Q1302_Atchmnt1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-EAST-Q1302_Atchmnt1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1302/03_phase_2_study/C9P2-EAST-Q1302 Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1302/03_phase_2_study/C9P2-EAST-Q1302 Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1302/03_phase_2_study/C9P2-EAST-Q1302-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-EAST-Q1302-AppendixA.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1303\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1304\n",
      "\n",
      "--- Processing project 1305 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1305/03_phase_2_study/C9P2-NOL-Q1305-AppendixA_v4.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1305\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.540', '-116.949649')\n",
      "Base data extracted:\n",
      "{'q_id': ['1305'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.540'], 'longitude': ['-116.949649'], 'capacity': [None], 'point_of_interconnection': ['Calcite 220 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1305/03_phase_2_study/Q1305-TOT786 Attachment 3.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1305-TOT786 Attachment 3.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1305/03_phase_2_study/C9P2-NOL-Q1305-Atchmnt1_v2.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1305-Atchmnt1_v2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1305/03_phase_2_study/C9P2-NOL-Q1305-AppendixA_v4.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1305-AppendixA_v4.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1305/03_phase_2_study/Q1305 TOT786 NOL Attachment 2 Q9P2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1305/03_phase_2_study/Q1305 TOT786 NOL Attachment 2 Q9P2.pdf\n",
      "\n",
      "--- Processing project 1306 ---\n",
      "No Appendix A and no Attachment 2 for project 1306. Skipping.\n",
      "\n",
      "--- Processing project 1307 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1307/03_phase_2_study/C9P2-NOL-Q1307-AppendixA_re-issue.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1307\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.059', '-117.563')\n",
      "Base data extracted:\n",
      "{'q_id': ['1307'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['35.059'], 'longitude': ['-117.563'], 'capacity': [None], 'point_of_interconnection': ['Kramer 220 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1307/03_phase_2_study/C9P2-NOL-Q1307-Atchmnt1-v2.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1307-Atchmnt1-v2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1307/03_phase_2_study/Q1307 TOT820 NOL Attachment 2 Q9P2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1307/03_phase_2_study/Q1307 TOT820 NOL Attachment 2 Q9P2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1307/03_phase_2_study/C9P2-NOL-Q1307-AppendixA_re-issue.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1307-AppendixA_re-issue.pdf\n",
      "\n",
      "--- Processing project 1308 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1308/03_phase_2_study/C9P2-NOL-Q1308-AppendixA_v4.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1308\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.871208', '-116.792792')\n",
      "Base data extracted:\n",
      "{'q_id': ['1308'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.871208'], 'longitude': ['-116.792792'], 'capacity': [None], 'point_of_interconnection': ['Coolwater 115 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1308/03_phase_2_study/C9P2-NOL-Q1308-AppendixA_v4.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1308-AppendixA_v4.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1308/03_phase_2_study/Q1308 TOT808 NOL Attachment 2 Q9P2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1308/03_phase_2_study/Q1308 TOT808 NOL Attachment 2 Q9P2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1308/03_phase_2_study/Q1308-TOT808 Attachment 3.pdf\n",
      "--> Not an Attachment 2 PDF: skipping Q1308-TOT808 Attachment 3.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1308/03_phase_2_study/C9P2-NOL-Q1308-Atchmnt1_v2.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1308-Atchmnt1_v2.pdf\n",
      "\n",
      "--- Processing project 1309 ---\n",
      "No Appendix A and no Attachment 2 for project 1309. Skipping.\n",
      "\n",
      "--- Processing project 1310 ---\n",
      "No Appendix A and no Attachment 2 for project 1310. Skipping.\n",
      "\n",
      "--- Processing project 1311 ---\n",
      "No Appendix A and no Attachment 2 for project 1311. Skipping.\n",
      "\n",
      "--- Processing project 1312 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1312/03_phase_2_study/C9P2-NOL-Q1312-AppendixA_Resissue.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1312\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.876239', '-116.794628')\n",
      "Base data extracted:\n",
      "{'q_id': ['1312'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.876239'], 'longitude': ['-116.794628'], 'capacity': [None], 'point_of_interconnection': ['Coolwater 115 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1312\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.876239', '-116.794628')\n",
      "Base data extracted:\n",
      "{'q_id': ['1312'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.876239'], 'longitude': ['-116.794628'], 'capacity': [None], 'point_of_interconnection': ['Coolwater 115 kV Substation']}\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1312/03_phase_2_study/C9P2-NOL-Q1312-AppendixA_Resissue.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1312-AppendixA_Resissue.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1312/03_phase_2_study/C9P2-NOL-Q1312-Atchmnt1_v2.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1312-Atchmnt1_v2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1312/03_phase_2_study/C9P2-NOL-Q1312-AppendixA_v4.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1312-AppendixA_v4.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1312/03_phase_2_study/Q1312 TOT812 NOL Attachment 2 Q9P2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1312/03_phase_2_study/Q1312 TOT812 NOL Attachment 2 Q9P2.pdf\n",
      "\n",
      "--- Processing project 1313 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1313/03_phase_2_study/C9P2-NOL-Q1313-AppendixA_v5-FSA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1313\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.842866', '-116.750181')\n",
      "Base data extracted:\n",
      "{'q_id': ['1313'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.842866'], 'longitude': ['-116.750181'], 'capacity': [None], 'point_of_interconnection': ['Kramer 220 kV via radial line to Coolwater 220 kV']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1313\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.842866', '-116.750181')\n",
      "Base data extracted:\n",
      "{'q_id': ['1313'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.842866'], 'longitude': ['-116.750181'], 'capacity': [None], 'point_of_interconnection': ['Kramer 220 kV via radial line to Coolwater 220 kV']}\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1313\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.842866', '-116.750181')\n",
      "Base data extracted:\n",
      "{'q_id': ['1313'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.842866'], 'longitude': ['-116.750181'], 'capacity': [None], 'point_of_interconnection': ['Kramer 220 kV via radial line to Coolwater 220 kV']}\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1313/03_phase_2_study/C9P2-NOL-Q1313-AppendixA_v5-FSA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1313-AppendixA_v5-FSA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1313/03_phase_2_study/C9P2-NOL-Q1313-Atchmnt1_v3-FSA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1313-Atchmnt1_v3-FSA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1313/03_phase_2_study/C9P2-NOL-Q1313-AppendixA_v5-FSA-reissue.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1313-AppendixA_v5-FSA-reissue.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1313/03_phase_2_study/Q1313 TOT811 NOL Attachment 2 Q9P2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1313/03_phase_2_study/Q1313 TOT811 NOL Attachment 2 Q9P2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1313/03_phase_2_study/C9P2-NOL-Q1313-AppendixA_v6-FSA-2ndreissue.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1313-AppendixA_v6-FSA-2ndreissue.pdf\n",
      "\n",
      "--- Processing project 1314 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1314/03_phase_2_study/C9P2-NOL-Q1314-AppendixA-v6-FSA-2ndReissue.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1314\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.860983', '-116.851752')\n",
      "Base data extracted:\n",
      "{'q_id': ['1314'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.860983'], 'longitude': ['-116.851752'], 'capacity': [None], 'point_of_interconnection': ['Coolwater 220 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1314\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.860983', '-116.851752')\n",
      "Base data extracted:\n",
      "{'q_id': ['1314'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.860983'], 'longitude': ['-116.851752'], 'capacity': [None], 'point_of_interconnection': ['Coolwater 220 kV Substation']}\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1314/03_phase_2_study/C9P2-NOL-Q1314-AppendixA-v6-FSA-2ndReissue.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1314-AppendixA-v6-FSA-2ndReissue.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1314/03_phase_2_study/C9P2-NOL-Q1314-AppendixA-v5-FSA-reissue.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1314-AppendixA-v5-FSA-reissue.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1314/03_phase_2_study/Q1314 TOT810 NOL Attachment 2 Q9P2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1314/03_phase_2_study/Q1314 TOT810 NOL Attachment 2 Q9P2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1314/03_phase_2_study/C9P2-NOL-Q1314-Atchmnt1_v3-FSA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-NOL-Q1314-Atchmnt1_v3-FSA.pdf\n",
      "\n",
      "--- Processing project 1315 ---\n",
      "No Appendix A and no Attachment 2 for project 1315. Skipping.\n",
      "\n",
      "--- Processing project 1316 ---\n",
      "No Appendix A and no Attachment 2 for project 1316. Skipping.\n",
      "\n",
      "--- Processing project 1317 ---\n",
      "No Appendix A and no Attachment 2 for project 1317. Skipping.\n",
      "\n",
      "--- Processing project 1318 ---\n",
      "No Appendix A and no Attachment 2 for project 1318. Skipping.\n",
      "\n",
      "--- Processing project 1319 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1319/03_phase_2_study/C9P2-Northern-Q1319-Willy 9-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1319\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.84', '-118.325')\n",
      "Base data extracted:\n",
      "{'q_id': ['1319'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.84'], 'longitude': ['-118.325'], 'capacity': [None], 'point_of_interconnection': ['Whirlwind 220 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1319/03_phase_2_study/C9P2-Northern-Q1319-Willy 9-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1319-Willy 9-AppendixA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1319/03_phase_2_study/C9P2-Northern-Q1319-Willy 9-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1319/03_phase_2_study/C9P2-Northern-Q1319-Willy 9-AppendixA-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1319/03_phase_2_study/C9P2-Northern-Q1319-Willy 9-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1319-Willy 9-AppendixA-Attachment 1.pdf\n",
      "\n",
      "--- Processing project 1320 ---\n",
      "No Appendix A and no Attachment 2 for project 1320. Skipping.\n",
      "\n",
      "--- Processing project 1321 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1321/03_phase_2_study/C9P2-Northern-Q1321-All American Solar-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1321\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1321'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['Internal Project Losses (MW)']}\n",
      "Missing base columns ['latitude', 'longitude', 'capacity']—attempting updates\n",
      "Still missing after updates: ['latitude', 'longitude', 'capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1321/03_phase_2_study/C9P2-Northern-Q1321-All American Solar-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1321-All American Solar-AppendixA-Attachment 1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1321/03_phase_2_study/C9P2-Northern-Q1321-All American Solar-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1321/03_phase_2_study/C9P2-Northern-Q1321-All American Solar-AppendixA-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1321/03_phase_2_study/C9P2-Northern-Q1321-All American Solar-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1321-All American Solar-AppendixA.pdf\n",
      "\n",
      "--- Processing project 1322 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1322/03_phase_2_study/C9P2-Northern-Q1322-BES 1-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1322\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.84166', '-118.4551')\n",
      "Base data extracted:\n",
      "{'q_id': ['1322'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.84166'], 'longitude': ['-118.4551'], 'capacity': [None], 'point_of_interconnection': ['Whirlwind 220 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1322/03_phase_2_study/C9P2-Northern-Q1322-BES 1-AppendixA-Addendum.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1322\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.84166', '-118.4551')\n",
      "Base data extracted:\n",
      "{'q_id': ['1322'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.84166'], 'longitude': ['-118.4551'], 'capacity': [None], 'point_of_interconnection': ['Whirlwind 220 kV Substation']}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1322/03_phase_2_study/C9P2-Northern-Q1322-BES 1-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1322-BES 1-AppendixA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1322/03_phase_2_study/C9P2-Northern-Q1321-All American Solar-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1321-All American Solar-AppendixA-Attachment 1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1322/03_phase_2_study/C9P2-Northern-Q1322-BES 1-AppendixA-Addendum.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1322-BES 1-AppendixA-Addendum.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1322/03_phase_2_study/C9P2-Northern-Q1322-BES 1-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1322/03_phase_2_study/C9P2-Northern-Q1322-BES 1-AppendixA-Attachment 2.pdf\n",
      "\n",
      "--- Processing project 1323 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1323/03_phase_2_study/C9P2-Northern-Q1323-BES 2-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1323\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.84166', '-118.4551')\n",
      "Base data extracted:\n",
      "{'q_id': ['1323'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.84166'], 'longitude': ['-118.4551'], 'capacity': [None], 'point_of_interconnection': ['Whirlwind 220 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1323/03_phase_2_study/C9P2-Northern-Q1323-BES 2-AppendixA-Addendum.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1323\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.84166', '-118.4551')\n",
      "Base data extracted:\n",
      "{'q_id': ['1323'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.84166'], 'longitude': ['-118.4551'], 'capacity': [None], 'point_of_interconnection': ['Whirlwind 220 kV Substation']}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1323/03_phase_2_study/C9P2-Northern-Q1323-BES 2-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1323-BES 2-AppendixA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1323/03_phase_2_study/C9P2-Northern-Q1323-BES 2-AppendixA-Addendum.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1323-BES 2-AppendixA-Addendum.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1323/03_phase_2_study/C9P2-Northern-Q1323-BES 2-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1323/03_phase_2_study/C9P2-Northern-Q1323-BES 2-AppendixA-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1323/03_phase_2_study/C9P2-Northern-Q1323-BES 2-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1323-BES 2-AppendixA-Attachment 1.pdf\n",
      "\n",
      "--- Processing project 1324 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1324/03_phase_2_study/C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1324\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.976336', '-118.247070')\n",
      "Base data extracted:\n",
      "{'q_id': ['1324'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.976336'], 'longitude': ['-118.247070'], 'capacity': [None], 'point_of_interconnection': ['Windhub 220 kV Substation']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1324/03_phase_2_study/C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1324/03_phase_2_study/C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1324/03_phase_2_study/C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA-Attachment 1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1324/03_phase_2_study/C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA.pdf\n",
      "\n",
      "--- Processing project 1325 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1325/03_phase_2_study/C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1325\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1325'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['Pad-Mount']}\n",
      "Missing base columns ['latitude', 'longitude', 'capacity']—attempting updates\n",
      "Still missing after updates: ['latitude', 'longitude', 'capacity']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1325/03_phase_2_study/C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Addendum.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1325\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1325'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['Pad-Mount']}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1325/03_phase_2_study/C9P2-Northern-Q1325-Sagebursh2-AppendixA-Attachment 2-Addendum.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1325/03_phase_2_study/C9P2-Northern-Q1325-Sagebursh2-AppendixA-Attachment 2-Addendum.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1325/03_phase_2_study/C9P2-Northern-Q1325-Sagebursh2-AppendixA-Attachment 1-Addendum.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1325-Sagebursh2-AppendixA-Attachment 1-Addendum.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1325/03_phase_2_study/C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1325/03_phase_2_study/C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Attachment 1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1325/03_phase_2_study/C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1325/03_phase_2_study/C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1325/03_phase_2_study/C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Addendum.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Addendum.pdf\n",
      "\n",
      "--- Processing project 1326 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1326/03_phase_2_study/C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1326\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1326'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['Pad-Mount']}\n",
      "Missing base columns ['latitude', 'longitude', 'capacity']—attempting updates\n",
      "Still missing after updates: ['latitude', 'longitude', 'capacity']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1326/03_phase_2_study/C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Addendum.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1326\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1326'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['Pad-Mount']}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1326/03_phase_2_study/C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1326/03_phase_2_study/C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 2-Addendum.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1326/03_phase_2_study/C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 2-Addendum.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1326/03_phase_2_study/C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Addendum.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Addendum.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1326/03_phase_2_study/C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 1-Addendum.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 1-Addendum.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1326/03_phase_2_study/C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1326/03_phase_2_study/C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1326/03_phase_2_study/C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 1.pdf\n",
      "\n",
      "--- Processing project 1327 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1327/03_phase_2_study/C9P2-Northern-Q1327-Cyclone Solar-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1327\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.831219', '-118.365769')\n",
      "Base data extracted:\n",
      "{'q_id': ['1327'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.831219'], 'longitude': ['-118.365769'], 'capacity': [None], 'point_of_interconnection': ['Internal Project Losses (MW)']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1327/03_phase_2_study/C9P2-Northern-Q1327-Cyclone Solar-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1327/03_phase_2_study/C9P2-Northern-Q1327-Cyclone Solar-AppendixA-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1327/03_phase_2_study/C9P2-Northern-Q1327-Cyclone Solar-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1327-Cyclone Solar-AppendixA-Attachment 1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1327/03_phase_2_study/C9P2-Northern-Q1327-Cyclone Solar-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1327-Cyclone Solar-AppendixA.pdf\n",
      "\n",
      "--- Processing project 1328 ---\n",
      "No Appendix A and no Attachment 2 for project 1328. Skipping.\n",
      "\n",
      "--- Processing project 1329 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1329/03_phase_2_study/C9P2-Northern-Q1329-Tropico Solar-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1329\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.986843', '-118.290524')\n",
      "Base data extracted:\n",
      "{'q_id': ['1329'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.986843'], 'longitude': ['-118.290524'], 'capacity': [None], 'point_of_interconnection': ['Internal Project Losses (MW)']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1329/03_phase_2_study/C9P2-Northern-Q1329-Tropico Solar-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1329-Tropico Solar-AppendixA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1329/03_phase_2_study/C9P2-Northern-Q1329-Tropico Solar-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1329-Tropico Solar-AppendixA-Attachment 1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1329/03_phase_2_study/C9P2-Northern-Q1329-Tropico Solar-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1329/03_phase_2_study/C9P2-Northern-Q1329-Tropico Solar-AppendixA-Attachment 2.pdf\n",
      "\n",
      "--- Processing project 1330 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1330/03_phase_2_study/C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-reissue.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1330\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('34.692326', '-118.367667')\n",
      "Base data extracted:\n",
      "{'q_id': ['1330'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['34.692326'], 'longitude': ['-118.367667'], 'capacity': [None], 'point_of_interconnection': ['Antelope 66 kV switchrack']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1330/03_phase_2_study/C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1330/03_phase_2_study/C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1330/03_phase_2_study/C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-Attachment 1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1330/03_phase_2_study/C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-reissue.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-reissue.pdf\n",
      "\n",
      "--- Processing project 1331 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1331/03_phase_2_study/C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-reissue.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1331\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.220792', '-117.977442')\n",
      "Base data extracted:\n",
      "{'q_id': ['1331'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['35.220792'], 'longitude': ['-117.977442'], 'capacity': [None], 'point_of_interconnection': ['Internal Project Losses (MW)']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1331/03_phase_2_study/C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-reissue.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-reissue.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1331/03_phase_2_study/C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1331/03_phase_2_study/C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1331/03_phase_2_study/C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-Attachment 1.pdf\n",
      "\n",
      "--- Processing project 1332 ---\n",
      "No Appendix A and no Attachment 2 for project 1332. Skipping.\n",
      "\n",
      "--- Processing project 1333 ---\n",
      "No Appendix A and no Attachment 2 for project 1333. Skipping.\n",
      "\n",
      "--- Processing project 1334 ---\n",
      "No Appendix A and no Attachment 2 for project 1334. Skipping.\n",
      "\n",
      "--- Processing project 1335 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1335/03_phase_2_study/C9P2-Northern-Q1335-Pastoria Solar-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1335\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1335'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['Internal Project Losses (MW)']}\n",
      "Missing base columns ['latitude', 'longitude', 'capacity']—attempting updates\n",
      "Still missing after updates: ['latitude', 'longitude', 'capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1335/03_phase_2_study/C9P2-Northern-Q1335-Pastoria Solar-AppendixA-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1335-Pastoria Solar-AppendixA-Attachment 1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1335/03_phase_2_study/C9P2-Northern-Q1335-Pastoria Solar-AppendixA-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1335/03_phase_2_study/C9P2-Northern-Q1335-Pastoria Solar-AppendixA-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1335/03_phase_2_study/C9P2-Northern-Q1335-Pastoria Solar-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-Northern-Q1335-Pastoria Solar-AppendixA.pdf\n",
      "\n",
      "--- Processing project 1336 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1336/03_phase_2_study/C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1336\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1336'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['Mohave 500 kV Switchyard']}\n",
      "Missing base columns ['latitude', 'longitude', 'capacity']—attempting updates\n",
      "Still missing after updates: ['latitude', 'longitude', 'capacity']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1336/03_phase_2_study/C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA_Addendum-01.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1336\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1336'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['Mohave 500 kV Switchyard']}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1336/03_phase_2_study/C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA-Attachment 2 wCASE B.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1336/03_phase_2_study/C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA-Attachment 2 wCASE B.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1336/03_phase_2_study/C9P2-SCE-EOP-Q1336-TOT804-AppendixA-Atchmnt1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-SCE-EOP-Q1336-TOT804-AppendixA-Atchmnt1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1336/03_phase_2_study/C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1336/03_phase_2_study/C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA_Addendum-01.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA_Addendum-01.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1337\n",
      "\n",
      "--- Processing project 1338 ---\n",
      "No Appendix A and no Attachment 2 for project 1338. Skipping.\n",
      "\n",
      "--- Processing project 1339 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1339/03_phase_2_study/C9P2-SCE-EOP-Q1339-Techren Solar-AppendixA.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1339\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.8618', '-114.9662')\n",
      "Base data extracted:\n",
      "{'q_id': ['1339'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['35.8618'], 'longitude': ['-114.9662'], 'capacity': [None], 'point_of_interconnection': ['Eldorado 220 kV Switchyard within the jointly-']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1339/03_phase_2_study/C9P2-SCE-EOP-Q1339-Techren Solar-AppendixA.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-SCE-EOP-Q1339-Techren Solar-AppendixA.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1339/03_phase_2_study/C9P2-SCE-EOP-Q1339-TOT809-Techren Solar-AppendixA-Atchmnt1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-SCE-EOP-Q1339-TOT809-Techren Solar-AppendixA-Atchmnt1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1339/03_phase_2_study/C9P2-SCE-EOP-Q1339-TOT809-Techren Solar-AppendixA-Attachment2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1339/03_phase_2_study/C9P2-SCE-EOP-Q1339-TOT809-Techren Solar-AppendixA-Attachment2.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1340\n",
      "\n",
      "--- Processing project 1341 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1341/03_phase_2_study/C9P2-VEA-EOP-Q1341-Yellow Pine2-Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1341\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1341'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['Pad-Mount + Collector + Main transformer banks']}\n",
      "Missing base columns ['latitude', 'longitude', 'capacity']—attempting updates\n",
      "Still missing after updates: ['latitude', 'longitude', 'capacity']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1341/03_phase_2_study/C9P2-VEA-EOP-Q1341-Yellow Pine2-Appendix A_Addendum-01.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1341\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1341'], 'cluster': ['9'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1341/03_phase_2_study/C9P2-VEA-EOP-Q1341-Yellow Pine2-Appendix A_Addendum-01.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-VEA-EOP-Q1341-Yellow Pine2-Appendix A_Addendum-01.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1341/03_phase_2_study/C9P2-VEA-EOP-Q1341-Yellow Pine2-Appendix A.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-VEA-EOP-Q1341-Yellow Pine2-Appendix A.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1341/03_phase_2_study/C9P2-VEA-EOP-Q1341-Yellow Pine 2-Appendix A-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1341/03_phase_2_study/C9P2-VEA-EOP-Q1341-Yellow Pine 2-Appendix A-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1341/03_phase_2_study/C9P2-VEA-EOP-Q1341-Yellow Pine 2-Appendix A-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-VEA-EOP-Q1341-Yellow Pine 2-Appendix A-Attachment 1.pdf\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1342\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1343\n",
      "\n",
      "--- Processing project 1344 ---\n",
      "No Appendix A and no Attachment 2 for project 1344. Skipping.\n",
      "\n",
      "--- Processing project 1345 ---\n",
      "No Appendix A and no Attachment 2 for project 1345. Skipping.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1346\n",
      "\n",
      "--- Processing project 1347 ---\n",
      "Scraped base data from original Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1347/03_phase_2_study/C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1347\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Found project coordinates: ('35.517564', '-115.152834')\n",
      "Base data extracted:\n",
      "{'q_id': ['1347'], 'cluster': ['9'], 'req_deliverability': ['Full'], 'latitude': ['35.517564'], 'longitude': ['-115.152834'], 'capacity': [None], 'point_of_interconnection': ['Pad-Mount + Collector + Main transformer banks']}\n",
      "Missing base columns ['capacity']—attempting updates\n",
      "Still missing after updates: ['capacity']\n",
      "Scraped base data from addendum Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1347/03_phase_2_study/C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A_Addendum-01.pdf\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 1347\n",
      "Extracted Cluster Number: 9\n",
      "Extracted Deliverability Status: None\n",
      "Extracted Capacity: None\n",
      "GPS coordinates not found.\n",
      "Base data extracted:\n",
      "{'q_id': ['1347'], 'cluster': ['9'], 'req_deliverability': [None], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1347/03_phase_2_study/C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A-Attachment 1.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A-Attachment 1.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1347/03_phase_2_study/C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A-Attachment 2.pdf\n",
      "Scraping Attachment 2 PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1347/03_phase_2_study/C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A-Attachment 2.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1347/03_phase_2_study/C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A.pdf\n",
      "Accessing PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1347/03_phase_2_study/C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A_Addendum-01.pdf\n",
      "--> Not an Attachment 2 PDF: skipping C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A_Addendum-01.pdf\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_rawdata_cluster9_style_others_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_rawdata_cluster9_style_others_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 108\n",
      "Total Projects Scraped: 25\n",
      "Total Projects Skipped: 83\n",
      "Total Projects Missing: 17\n",
      "Total PDFs Accessed: 127\n",
      "Total PDFs Scraped: 27\n",
      "Total PDFs Skipped: 100\n",
      "\n",
      "List of Scraped Projects: [1295, 1299, 1302, 1305, 1307, 1308, 1312, 1313, 1314, 1319, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1329, 1330, 1331, 1335, 1336, 1339, 1341, 1347]\n",
      "\n",
      "List of Skipped Projects: [1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1262, 1263, 1264, 1265, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1275, 1277, 1278, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1291, 1292, 1293, 1294, 1296, 1297, 1300, 1301, 1306, 1309, 1310, 1311, 1315, 1316, 1317, 1318, 1320, 1328, 1332, 1333, 1334, 1338, 1344, 1345]\n",
      "\n",
      "List of Missing Projects: [1247, 1261, 1266, 1274, 1276, 1279, 1280, 1289, 1290, 1298, 1303, 1304, 1337, 1340, 1342, 1343, 1346]\n",
      "\n",
      "List of Scraped PDFs: ['C9P2-EAST-Q1295- Attachment 2.pdf', 'C9P2-EAST-Q1299 Attachment 2.pdf', 'C9P2-EAST-Q1302 Attachment 2.pdf', 'Q1305 TOT786 NOL Attachment 2 Q9P2.pdf', 'Q1307 TOT820 NOL Attachment 2 Q9P2.pdf', 'Q1308 TOT808 NOL Attachment 2 Q9P2.pdf', 'Q1312 TOT812 NOL Attachment 2 Q9P2.pdf', 'Q1313 TOT811 NOL Attachment 2 Q9P2.pdf', 'Q1314 TOT810 NOL Attachment 2 Q9P2.pdf', 'C9P2-Northern-Q1319-Willy 9-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1321-All American Solar-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1322-BES 1-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1323-BES 2-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1325-Sagebursh2-AppendixA-Attachment 2-Addendum.pdf', 'C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 2-Addendum.pdf', 'C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1327-Cyclone Solar-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1329-Tropico Solar-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1335-Pastoria Solar-AppendixA-Attachment 2.pdf', 'C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA-Attachment 2 wCASE B.pdf', 'C9P2-SCE-EOP-Q1339-TOT809-Techren Solar-AppendixA-Attachment2.pdf', 'C9P2-VEA-EOP-Q1341-Yellow Pine 2-Appendix A-Attachment 2.pdf', 'C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A-Attachment 2.pdf']\n",
      "\n",
      "List of Skipped PDFs: ['Q1223 American Kings 9_Appendix A-C9PhII.pdf', 'Q1225 Cinco_Appendix A-C9PhII.pdf', 'Q1233 Hobbs BESS_Appendix A-C9PhII.pdf', 'Q1235 Hudson Solar 1_Appendix A-C9PhII.pdf', 'Q1239 Medeiros Solar_C9PhII_Appendix A-Addendum1.pdf', 'Q1239 Medeiros Solar_Appendix A-C9PhII.pdf', 'Q1242 Pluot_C9PhII_Appendix A-Addendum1.pdf', '16AS0160-Q1242_Pluot_C9PhII_Appendix_AAddendum1.pdf', 'Q1242 Pluot_Appendix A-C9PhII.pdf', 'Q1243 Pomegranate_Appendix A-C9PhII.pdf', 'Q1243Pomegranate_Appendix_A_Addendum5.pdf', 'Q1244 Proxima_Appendix A-C9PhII.pdf', 'Q1246 Romero Creek Hybrid_Appendix A-C9PhII.pdf', 'C9PhII-Q1259 Northern Orchard 2 Solar_Appendix A.pdf', 'C13PhII - Attachment 1- Allocation of NU Cost Estimates -Q1259.pdf', 'C13PhII - Attachment 1- Allocation of NU Cost Estimates -Q1260.pdf', 'C9PhII-Q1260 Northern Orchard 3 Solar_Appendix A.pdf', 'C9Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1269.pdf', '16AS0132-Q1269_Capetown_Wind_Farm_Appendix_AC9PhII__Addendum1.pdf', 'Q1269 Capetown Wind Farm_Appendix A-C9PhII.pdf', 'Q1270 Corby_Appendix A-C9PhII.pdf', 'C9Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1270.pdf', '16AS0091-Q1272_Griswald_BESS_Appendix_AC9PhII__Addendum1.pdf', 'Q1272 Griswald BESS_Appendix A-C9PhII.pdf', 'Q1275 Kola_Appendix A-C9PhII.pdf', 'Q1275Kola_Appendix_A_2022_Addendum6_09152023.pdf', 'P2RPT-Q1277_Mulqueeney_Ranch_Wind_Project_C9PhII_Appendix_AAddendum1.pdf', 'Q1277 Mulqueeney Ranch Wind Project_Appendix A-C9PhII.pdf', 'Q1278 Northern Pines Energy Center_Appendix A-C9PhII.pdf', 'C9Ph2 - Attachment 1 - Allocation of NU Cost Estimates - Q1284.pdf', 'Q1284 Walker Ridge_Appendix A-C9PhII.pdf', 'QC9PhII_Q1287_Mt Laguna Wind_Appendix A_11-22-2017.pdf', 'QC9PhII_Q1291_Mesquite Solar 5_Appendix A_11-22-2017.pdf', 'QC9PhII_Q1293_Signal Peak_Appendix A_11-22-2017.pdf', 'C9P2-EAST-Q1295-Atchmnt1.pdf', 'C9P2-EAST-Q1295-AppendixA.pdf', 'C9PII-EAST-Q1295-AppendixA_Addendum-01.pdf', 'C9P2-EAST-Q1299-Atchmnt1.pdf', 'C9P2-EAST-Q1299-AppendixA.pdf', 'C9P2-EAST-Q1302_Atchmnt1.pdf', 'C9P2-EAST-Q1302-AppendixA.pdf', 'Q1305-TOT786 Attachment 3.pdf', 'C9P2-NOL-Q1305-Atchmnt1_v2.pdf', 'C9P2-NOL-Q1305-AppendixA_v4.pdf', 'C9P2-NOL-Q1307-Atchmnt1-v2.pdf', 'C9P2-NOL-Q1307-AppendixA_re-issue.pdf', 'C9P2-NOL-Q1308-AppendixA_v4.pdf', 'Q1308-TOT808 Attachment 3.pdf', 'C9P2-NOL-Q1308-Atchmnt1_v2.pdf', 'C9P2-NOL-Q1312-AppendixA_Resissue.pdf', 'C9P2-NOL-Q1312-Atchmnt1_v2.pdf', 'C9P2-NOL-Q1312-AppendixA_v4.pdf', 'C9P2-NOL-Q1313-AppendixA_v5-FSA.pdf', 'C9P2-NOL-Q1313-Atchmnt1_v3-FSA.pdf', 'C9P2-NOL-Q1313-AppendixA_v5-FSA-reissue.pdf', 'C9P2-NOL-Q1313-AppendixA_v6-FSA-2ndreissue.pdf', 'C9P2-NOL-Q1314-AppendixA-v6-FSA-2ndReissue.pdf', 'C9P2-NOL-Q1314-AppendixA-v5-FSA-reissue.pdf', 'C9P2-NOL-Q1314-Atchmnt1_v3-FSA.pdf', 'C9P2-Northern-Q1319-Willy 9-AppendixA.pdf', 'C9P2-Northern-Q1319-Willy 9-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1321-All American Solar-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1321-All American Solar-AppendixA.pdf', 'C9P2-Northern-Q1322-BES 1-AppendixA.pdf', 'C9P2-Northern-Q1321-All American Solar-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1322-BES 1-AppendixA-Addendum.pdf', 'C9P2-Northern-Q1323-BES 2-AppendixA.pdf', 'C9P2-Northern-Q1323-BES 2-AppendixA-Addendum.pdf', 'C9P2-Northern-Q1323-BES 2-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA.pdf', 'C9P2-Northern-Q1325-Sagebursh2-AppendixA-Attachment 1-Addendum.pdf', 'C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA.pdf', 'C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Addendum.pdf', 'C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA.pdf', 'C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Addendum.pdf', 'C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 1-Addendum.pdf', 'C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1327-Cyclone Solar-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1327-Cyclone Solar-AppendixA.pdf', 'C9P2-Northern-Q1329-Tropico Solar-AppendixA.pdf', 'C9P2-Northern-Q1329-Tropico Solar-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-reissue.pdf', 'C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-reissue.pdf', 'C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1335-Pastoria Solar-AppendixA-Attachment 1.pdf', 'C9P2-Northern-Q1335-Pastoria Solar-AppendixA.pdf', 'C9P2-SCE-EOP-Q1336-TOT804-AppendixA-Atchmnt1.pdf', 'C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA.pdf', 'C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA_Addendum-01.pdf', 'C9P2-SCE-EOP-Q1339-Techren Solar-AppendixA.pdf', 'C9P2-SCE-EOP-Q1339-TOT809-Techren Solar-AppendixA-Atchmnt1.pdf', 'C9P2-VEA-EOP-Q1341-Yellow Pine2-Appendix A_Addendum-01.pdf', 'C9P2-VEA-EOP-Q1341-Yellow Pine2-Appendix A.pdf', 'C9P2-VEA-EOP-Q1341-Yellow Pine 2-Appendix A-Attachment 1.pdf', 'C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A-Attachment 1.pdf', 'C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A.pdf', 'C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A_Addendum-01.pdf']\n",
      "\n",
      "List of Addendum PDFs: ['C9P2-Northern-Q1325-Sagebursh2-AppendixA-Attachment 2-Addendum.pdf', 'C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 2-Addendum.pdf']\n",
      "\n",
      "List of Original PDFs: ['C9P2-EAST-Q1295- Attachment 2.pdf', 'C9P2-EAST-Q1299 Attachment 2.pdf', 'C9P2-EAST-Q1302 Attachment 2.pdf', 'Q1305 TOT786 NOL Attachment 2 Q9P2.pdf', 'Q1307 TOT820 NOL Attachment 2 Q9P2.pdf', 'Q1308 TOT808 NOL Attachment 2 Q9P2.pdf', 'Q1312 TOT812 NOL Attachment 2 Q9P2.pdf', 'Q1313 TOT811 NOL Attachment 2 Q9P2.pdf', 'Q1314 TOT810 NOL Attachment 2 Q9P2.pdf', 'C9P2-Northern-Q1319-Willy 9-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1321-All American Solar-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1322-BES 1-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1323-BES 2-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1324-Sagebrush Solar 3-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1325-Sagebursh Solar 2-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1326-Sagebrush Solar 1-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1327-Cyclone Solar-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1329-Tropico Solar-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1330-Clover Energy Storage Plant-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1331-Canella Solar Farm-AppendixA-Attachment 2.pdf', 'C9P2-Northern-Q1335-Pastoria Solar-AppendixA-Attachment 2.pdf', 'C9P2-SCE-EOP-Q1336-TOT804-Southland-AppendixA-Attachment 2 wCASE B.pdf', 'C9P2-SCE-EOP-Q1339-TOT809-Techren Solar-AppendixA-Attachment2.pdf', 'C9P2-VEA-EOP-Q1341-Yellow Pine 2-Appendix A-Attachment 2.pdf', 'C9P2-VEA-EOP-Q1347-Crescent Peak Wind-Appendix A-Attachment 2.pdf']\n",
      "\n",
      "List of Style N PDFs (Skipped due to 'Network Upgrade Type'): []\n",
      "\n",
      "Total Number of Style N PDFs: 0\n",
      "\n",
      "Number of Original PDFs Scraped: 25\n",
      "Number of Addendum PDFs Scraped: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_35443/1742041088.py:730: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_35443/1742041088.py:730: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import traceback\n",
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------- Configuration -------------------\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_rawdata_cluster9_style_others_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_rawdata_cluster9_style_others_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_scraping_cluster9_style_others_log.txt\"\n",
    "PROJECT_RANGE = range(1223, 1348)   # Original range #(1831, 2193)\n",
    "\n",
    "# Read the CSV file containing processed projects (with q_id column)\n",
    "processed_csv_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/all_clusters/costs_phase_2_all_clusters_total.csv\"  # UPDATE THIS PATH\n",
    "processed_df = pd.read_csv(processed_csv_path)\n",
    "# Convert q_id values to numeric then to int for filtering\n",
    "processed_q_ids = pd.to_numeric(processed_df['q_id'], errors='coerce').dropna().astype(int).unique()\n",
    "projects_to_process = sorted([q_id for q_id in PROJECT_RANGE if q_id not in processed_q_ids])\n",
    "\n",
    "# ------------------- Global Tracking Variables -------------------\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "style_n_pdfs = []  # Not used in this version but kept for consistency\n",
    "\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "# ------------------- Helper Function for Logging -------------------\n",
    "def log_msg(msg, log_file):\n",
    "    \"\"\"Prints a message to both the log file and console.\"\"\"\n",
    "    print(msg, file=log_file)\n",
    "    print(msg)\n",
    "\n",
    "# ------------------- Other Helper Functions -------------------\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    elif value is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(value).replace('\\n', ' ').strip()\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Conditionally Assigned Network Upgrades\",\n",
    "        \"Local Off-Peak Network Upgrade\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b(?=\\d|\\W|$)\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\"\n",
    "    ]\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "    new_order = existing_desired + remaining\n",
    "    return df[new_order]\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        log_msg(f\"Found GPS coordinates: {gps_coords.groups()}\", log_file)\n",
    "        return gps_coords.groups()\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        log_msg(f\"Found project coordinates: {project_coords.groups()}\", log_file)\n",
    "        return project_coords.groups()\n",
    "    gps_coords_directional = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"\n",
    "        log_msg(f\"Found directional GPS coordinates: {(latitude, longitude)}\", log_file)\n",
    "        return (latitude, longitude)\n",
    "    log_msg(\"GPS coordinates not found.\", log_file)\n",
    "    return (None, None)\n",
    "\n",
    "# ------------------- Appendix PDF Check -------------------\n",
    "def is_appendix_pdf(pdf_path):\n",
    "    \"\"\"Returns True if the first page of the PDF contains 'Appendix A'.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "            first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "            return \"Appendix A\" in first_page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# ------------------- Base Data & Table 1 Extraction (Appendix A Only) -------------------\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    This function is intended to run only on the Appendix A PDF.\n",
    "    Now it searches for pages containing \"Table A.2\", \"Table B.2\" or \"Table C.2\"\n",
    "    and within the tables it looks for either \"Point of Interconnection\" or \"POI\".\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "    # Modified to match either \"Point of Interconnection\" or \"POI\"\n",
    "    poi_pattern = re.compile(r\"(Point\\s+of\\s+Interconnection|POI)\", re.IGNORECASE)\n",
    "    table_settings_list = [\n",
    "        {\"horizontal_strategy\": \"text\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 1},\n",
    "        {\"horizontal_strategy\": \"lines\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 2}\n",
    "    ]\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            table1_pages = []\n",
    "            # Modified regex: look for \"Table A.2\", \"Table B.2\" or \"Table C.2\"\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*[ABC]\\.[12]\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "            extraction_successful = False\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"Attempt {attempt} with settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1}\", file=log_file)\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty.\", file=log_file)\n",
    "                            continue\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"Found POI: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            print(f\"POI label found but adjacent value empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            poi_value_parts = []\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                            if poi_value_parts:\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"Concatenated POI: '{point_of_interconnection}'\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break\n",
    "                                    else:\n",
    "                                        print(f\"POI label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                            if extraction_successful:\n",
    "                                break\n",
    "                        if extraction_successful:\n",
    "                            break\n",
    "                    if extraction_successful:\n",
    "                        break\n",
    "                if extraction_successful:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "    if not extraction_successful:\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            print(\"POI label found but no value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            print(\"POI not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "    return point_of_interconnection\n",
    "\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"\n",
    "    Extracts base data from the Appendix A PDF.\n",
    "    (This function is meant to run only on a PDF verified as an Appendix A PDF.)\n",
    "    \"\"\"\n",
    "    if not is_appendix_pdf(pdf_path):\n",
    "        log_msg(f\"Skipping base extraction because {pdf_path} is not an Appendix A PDF.\", log_file)\n",
    "        return pd.DataFrame()\n",
    "    log_msg(\"Extracting base data from Appendix A PDF...\", log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "        text = clean_string_cell(text)\n",
    "        #queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        #queue_id = queue_id.group(1) if queue_id else str(project_id)\n",
    "        queue_id = str(project_id)\n",
    "        log_msg(f\"Extracted Queue ID: {queue_id}\", log_file)\n",
    "        #clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        #if '9' in clusters:\n",
    "        #    cluster_number = '9'\n",
    "        #elif clusters:\n",
    "        #    cluster_number = max(clusters, key=lambda x: int(x))\n",
    "        #else:\n",
    "        #   cluster_number = '9'\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = '9'    \n",
    "        log_msg(f\"Extracted Cluster Number: {cluster_number}\", log_file)\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        log_msg(f\"Extracted Deliverability Status: {deliverability_status}\", log_file)\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        log_msg(f\"Extracted Capacity: {capacity}\", log_file)\n",
    "        poi_value = extract_table1(pdf_path, log_file)\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [poi_value]\n",
    "        }\n",
    "        log_msg(\"Base data extracted:\", log_file)\n",
    "        log_msg(str(base_data), log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "    except Exception as e:\n",
    "        log_msg(f\"Error extracting base data from {pdf_path}: {e}\", log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------------- Attachment 2 Processing & Merging -------------------\n",
    "def is_attachment2_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Returns True if the first page of the PDF contains \"Attachment 2\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "            first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "            return \"Attachment 2\" in first_page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def is_addendum_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Returns True if the first page of the PDF contains \"Addendum\" or \"Revision\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "            first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "            return (\"Addendum\" in first_page_text) or (\"Revision\" in first_page_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    \"\"\"\n",
    "    Appends a suffix to duplicate headers to make them unique.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def extract_first_table(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Iterates over all pages and returns the first non-empty table found as a DataFrame,\n",
    "    treating the first row as the column headers and ensuring unique header names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    if table and any(any(cell is not None and cell.strip() for cell in row) for row in table):\n",
    "                        df = pd.DataFrame(table)\n",
    "                        if not df.empty:\n",
    "                            # Use the first row as headers\n",
    "                            headers = df.iloc[0].tolist()\n",
    "                            headers = make_unique_headers(headers)\n",
    "                            df.columns = headers\n",
    "                            df = df[1:].reset_index(drop=True)\n",
    "                            # Remove any duplicate column names (safeguard)\n",
    "                            df = df.loc[:, ~df.columns.duplicated()]\n",
    "                            return df\n",
    "    except Exception as e:\n",
    "        log_msg(f\"Error extracting table from {pdf_path}: {e}\",log_file)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "'''\n",
    "def extract_first_table(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    1) Grab the very first non-empty table in the PDF (row 0 => header).\n",
    "    2) On that same page, scan only the tables *after* it for the exact phrase\n",
    "       \"Other Potential Network Upgrad\" (covers both Upgrade/Upgrades).\n",
    "    3) If found, merge row-wise (filling blanks in the first table, appending the rest).\n",
    "    4) Otherwise return only the first table.\n",
    "    \"\"\"\n",
    "    import pdfplumber, pandas as pd\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # loop pages\n",
    "            for page in pdf.pages:\n",
    "                tables = page.extract_tables()\n",
    "                first_table_df = None\n",
    "                first_idx = None\n",
    "\n",
    "                # 1) find first non-empty table\n",
    "                for idx, table in enumerate(tables):\n",
    "                    if any(cell and cell.strip() for row in table for cell in row):\n",
    "                        df = pd.DataFrame(table)\n",
    "                        # header = first row\n",
    "                        headers = make_unique_headers(df.iloc[0].tolist())\n",
    "                        df.columns = headers\n",
    "                        df = df[1:].reset_index(drop=True)\n",
    "                        df = df.loc[:, ~df.columns.duplicated()]\n",
    "                        first_table_df = df\n",
    "                        first_idx = idx\n",
    "                        break\n",
    "\n",
    "                if first_table_df is None:\n",
    "                    # no table on this page, try next page\n",
    "                    continue\n",
    "\n",
    "                # 2) scan only the *later* tables for the exact phrase\n",
    "                for table in tables[first_idx+1:]:\n",
    "                    if not any(cell and cell.strip() for row in table for cell in row):\n",
    "                        continue\n",
    "\n",
    "                    # precise match (covers Upgrade and Upgrades)\n",
    "                    found = any(\n",
    "                        cell and \"other potential network upgrad\" in cell.lower()\n",
    "                        for row in table for cell in row\n",
    "                    )\n",
    "                    if not found:\n",
    "                        continue\n",
    "\n",
    "                    # 3) convert and merge\n",
    "                    df2 = pd.DataFrame(table)\n",
    "                    headers2 = make_unique_headers(df2.iloc[0].tolist())\n",
    "                    df2.columns = headers2\n",
    "                    df2 = df2[1:].reset_index(drop=True)\n",
    "                    df2 = df2.loc[:, ~df2.columns.duplicated()]\n",
    "\n",
    "                    # fill-first-then-append\n",
    "                    common = set(first_table_df.columns) & set(df2.columns)\n",
    "                    for col in common:\n",
    "                        first_table_df[col] = first_table_df[col].combine_first(df2[col])\n",
    "                    df2.drop(columns=common, inplace=True)\n",
    "\n",
    "                    return pd.concat([first_table_df, df2], ignore_index=True)\n",
    "\n",
    "                # 4) no matching second table → return only the first\n",
    "                return first_table_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting table from {pdf_path}: {e}\", file=log_file)\n",
    "\n",
    "    # fallback: empty\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def extract_first_table(pdf_path,log_file):\n",
    "    \"\"\"\n",
    "    Iterates over all pages and returns the first non-empty table found as a DataFrame.\n",
    "    It first extracts the table normally (using the first row as header).\n",
    "    Then it scans the raw table rows for a row that, after replacing newline characters with a space,\n",
    "    has a contiguous block of cells (starting at index 1) exactly equal to:\n",
    "        [\"Costs per Category w/o ITCC (A)\",\n",
    "         \"One Time Costs (B)\",\n",
    "         \"Total Costs w/o ITCC (C=A+B)\",\n",
    "         \"Total Escalated Costs w/o ITCC\",\n",
    "         \"Estimated Time for Licensing, Permitting, & Construction (Months)\",\n",
    "         \"Maximum Escalation Duration (Months)\"]\n",
    "    If found, it discards all rows above that row, uses that row as the header (taking columns 1 to 6),\n",
    "    and returns the resulting DataFrame. Otherwise, it returns the table as originally extracted.\n",
    "    \"\"\"\n",
    "    import pdfplumber, pandas as pd\n",
    "\n",
    "    # Define the expected header row exactly as you want it.\n",
    "    expected_header = [\n",
    "        \"Costs per Category w/o ITCC (A)\",\n",
    "        \"One Time Costs (B)\",\n",
    "        \"Total Costs w/o ITCC (C=A+B)\",\n",
    "        \"Total Escalated Costs w/o ITCC\",\n",
    "        \"Estimated Time for Licensing, Permitting, & Construction (Months)\",\n",
    "        \"Maximum Escalation Duration (Months)\"\n",
    "    ]\n",
    "    \n",
    "    # First, extract the raw table from the PDF.\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            raw_table = None\n",
    "            for page in pdf.pages:\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    if table and any(any(cell is not None and cell.strip() for cell in row) for row in table):\n",
    "                        raw_table = table\n",
    "                        break\n",
    "                if raw_table is not None:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting table from {pdf_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if raw_table is None:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Build a DataFrame from the raw table (without assigning header)\n",
    "    df_raw = pd.DataFrame(raw_table)\n",
    "    if df_raw.empty or len(df_raw) < 2:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Also build a fallback DataFrame using the first row as header\n",
    "    df_fallback = df_raw.copy()\n",
    "    fallback_header = df_fallback.iloc[0].tolist()\n",
    "    df_fallback.columns = fallback_header\n",
    "    df_fallback = df_fallback.iloc[1:].reset_index(drop=True)\n",
    "    \n",
    "    # Now, scan each row of the raw table to see if it contains the expected header.\n",
    "    # We replace newline characters with a space for the comparison.\n",
    "    special_idx = None\n",
    "    for i in range(len(df_raw)):\n",
    "        row_vals = df_raw.iloc[i].tolist()\n",
    "        cleaned = [cell.replace(\"\\n\", \" \") if cell is not None else \"\" for cell in row_vals]\n",
    "        # Check if the row has at least 1 + len(expected_header) cells\n",
    "        if len(cleaned) >= 1 + len(expected_header):\n",
    "            # Compare cells 1 to 1+len(expected_header)\n",
    "            if cleaned[1:1+len(expected_header)] == expected_header:\n",
    "                special_idx = i\n",
    "                break\n",
    "\n",
    "    if special_idx is not None:\n",
    "        # Found the special header row.\n",
    "        # Remove all rows above that row.\n",
    "        df_new = df_raw.iloc[special_idx:].reset_index(drop=True)\n",
    "        # Use columns 1 to 1+len(expected_header) from that row as header.\n",
    "        new_header = df_new.iloc[0].tolist()[1:1+len(expected_header)]\n",
    "        log_msg(f\"Special header found at row {special_idx}. New header: {new_header}\", log_file)\n",
    "        # Now, select only those columns.\n",
    "        df_new = df_new.iloc[:, 1:1+len(expected_header)]\n",
    "        # Drop the header row from data.\n",
    "        df_new = df_new.iloc[1:].reset_index(drop=True)\n",
    "        df_new.columns = new_header\n",
    "        return df_new\n",
    "    else:\n",
    "        log_msg(\"No special header row found. Using fallback header.\", log_file)\n",
    "        return df_fallback\n",
    "'''\n",
    "\n",
    "\n",
    "def update_base_data(existing_df, new_df):\n",
    "    \"\"\"\n",
    "    For each column in existing_df (assumed to be a single-row DataFrame),\n",
    "    if the value is missing (empty string, \"None\", or NA), update it with the corresponding\n",
    "    value from new_df (if provided and not missing).\n",
    "    Returns the updated DataFrame.\n",
    "    \"\"\"\n",
    "    for col in existing_df.columns:\n",
    "        existing_val = existing_df.at[0, col]\n",
    "        # Use pd.isna() to check for NA values.\n",
    "        if pd.isna(existing_val) or existing_val == \"\" or existing_val == \"None\":\n",
    "            if col in new_df.columns:\n",
    "                new_val = new_df.at[0, col]\n",
    "                if not (pd.isna(new_val) or new_val == \"\" or new_val == \"None\"):\n",
    "                    existing_df.at[0, col] = new_val\n",
    "    return existing_df\n",
    "\n",
    "\n",
    "\n",
    "def process_attachment2_for_project(project_id, log_file):\n",
    "    \"\"\"\n",
    "    For the given project:\n",
    "      1. Identify all original Appendix A PDFs (non‑revision) in the project's \"03_phase_2_study\" folder.\n",
    "      2. If none are found, build a minimal base_data_df with only q_id (others None).\n",
    "      3. Otherwise extract base data from the first Appendix A and fill missing from the rest.\n",
    "      4. Process addendum Appendix A similarly.\n",
    "      5. Scrape Attachment 2 PDFs and merge with whichever base_data_df you have.\n",
    "    \"\"\"\n",
    "    global total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    project_folder = os.path.join(BASE_DIRECTORY, str(project_id), \"03_phase_2_study\")\n",
    "    if not os.path.exists(project_folder):\n",
    "        log_msg(f\"Project folder not found: {project_folder}\", log_file)\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # 1) find Appendix A PDFs\n",
    "    original_appendix_pdfs = []\n",
    "    addendum_appendix_pdf = None\n",
    "    for f in os.listdir(project_folder):\n",
    "        if not f.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_path = os.path.join(project_folder, f)\n",
    "        if is_appendix_pdf(pdf_path):\n",
    "            if is_addendum_pdf(pdf_path) and addendum_appendix_pdf is None:\n",
    "                addendum_appendix_pdf = f\n",
    "            elif not is_addendum_pdf(pdf_path):\n",
    "                original_appendix_pdfs.append(f)\n",
    "\n",
    "    # 2) find Attachment 2 PDFs\n",
    "    attachment2_pdfs = [\n",
    "        f for f in os.listdir(project_folder)\n",
    "        if f.lower().endswith(\".pdf\") and is_attachment2_pdf(os.path.join(project_folder, f))\n",
    "    ]\n",
    "\n",
    "\n",
    "    # 1.2) if neither Appendix A nor Attachment 2 → skip entirely\n",
    "    if not original_appendix_pdfs and not attachment2_pdfs:\n",
    "        log_msg(f\"No Appendix A and no Attachment 2 for project {project_id}. Skipping.\", log_file)\n",
    "        skipped_projects.add(project_id)\n",
    "        return pd.DataFrame(), pd.DataFrame()      \n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "    # 2) build base_data_df\n",
    "    if not original_appendix_pdfs:\n",
    "        if addendum_appendix_pdf:\n",
    "            # ── new: use addendum as the base\n",
    "            log_msg(f\"No original Appendix A PDF found for project {project_id}. Using addendum for base data.\", log_file)\n",
    "            path = os.path.join(project_folder, addendum_appendix_pdf)\n",
    "            base_data_df = extract_base_data(path, project_id, log_file)\n",
    "            if base_data_df.empty:\n",
    "                log_msg(f\"--> Addendum scrape yielded nothing; falling back to minimal base.\", log_file)\n",
    "                base_data_df = pd.DataFrame({\n",
    "                    \"q_id\":                   [str(project_id)],\n",
    "                    \"cluster\":                [None],\n",
    "                    \"req_deliverability\":     [None],\n",
    "                    \"latitude\":               [None],\n",
    "                    \"longitude\":              [None],\n",
    "                    \"capacity\":               [None],\n",
    "                    \"point_of_interconnection\":[None]\n",
    "                })\n",
    "    else:\n",
    "        # extract from first Appendix A\n",
    "        base_pdf = original_appendix_pdfs[0]\n",
    "        path = os.path.join(project_folder, base_pdf)\n",
    "        log_msg(f\"Scraped base data from original Appendix A PDF: {path}\", log_file)\n",
    "        base_data_df = extract_base_data(path, project_id, log_file)\n",
    "\n",
    "        # fill any missing fields from other originals\n",
    "        missing = [c for c in base_data_df.columns\n",
    "                   if pd.isna(base_data_df.at[0,c]) or base_data_df.at[0,c] in [\"\", \"None\"]]\n",
    "        if missing:\n",
    "            log_msg(f\"Missing base columns {missing}—attempting updates\", log_file)\n",
    "            for other in original_appendix_pdfs[1:]:\n",
    "                new_df = extract_base_data(os.path.join(project_folder, other), project_id, log_file)\n",
    "                base_data_df = update_base_data(base_data_df, new_df)\n",
    "                missing = [c for c in base_data_df.columns\n",
    "                           if pd.isna(base_data_df.at[0,c]) or base_data_df.at[0,c] in [\"\", \"None\"]]\n",
    "                if not missing:\n",
    "                    log_msg(\"Successfully filled all missing base data.\", log_file)\n",
    "                    break\n",
    "            if missing:\n",
    "                log_msg(f\"Still missing after updates: {missing}\", log_file)\n",
    "\n",
    "    # 3) prepare addendum base_data_df\n",
    "    if addendum_appendix_pdf and original_appendix_pdfs:\n",
    "        add_path = os.path.join(project_folder, addendum_appendix_pdf)\n",
    "        log_msg(f\"Scraped base data from addendum Appendix A PDF: {add_path}\", log_file)\n",
    "        addendum_base_data_df = extract_base_data(add_path, project_id, log_file)\n",
    "        if addendum_base_data_df.empty:\n",
    "            addendum_base_data_df = base_data_df.copy()\n",
    "    else:\n",
    "        addendum_base_data_df = base_data_df.copy()\n",
    "\n",
    "    # 4) now scrape Attachment 2 tables\n",
    "    attachment_data_list = []\n",
    "    attachment_addendum_list = []\n",
    "    for f in os.listdir(project_folder):\n",
    "        if not f.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_path = os.path.join(project_folder, f)\n",
    "        log_msg(f\"Accessing PDF: {pdf_path}\", log_file)\n",
    "        total_pdfs_accessed += 1\n",
    "\n",
    "        if is_attachment2_pdf(pdf_path):\n",
    "            log_msg(f\"Scraping Attachment 2 PDF: {pdf_path}\", log_file)\n",
    "            table_df = extract_first_table(pdf_path, log_file)\n",
    "            if table_df.empty:\n",
    "                log_msg(f\"--> No table found in {f}, skipping.\", log_file)\n",
    "                skipped_pdfs.append(f)\n",
    "                total_pdfs_skipped += 1\n",
    "                continue\n",
    "\n",
    "            # avoid collisions\n",
    "            for col in set(base_data_df.columns) & set(table_df.columns):\n",
    "                table_df.rename(columns={col: f\"{col}_table\"}, inplace=True)\n",
    "\n",
    "            # choose base vs addendum\n",
    "            if is_addendum_pdf(pdf_path):\n",
    "                base_df = addendum_base_data_df\n",
    "                addendum_pdfs.append(f)\n",
    "            else:\n",
    "                base_df = base_data_df\n",
    "                original_pdfs.append(f)\n",
    "\n",
    "            # merge side‑by‑side\n",
    "            repeated = pd.concat([base_df] * len(table_df), ignore_index=True)\n",
    "            merged = pd.concat([repeated, table_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "            if is_addendum_pdf(pdf_path):\n",
    "                attachment_addendum_list.append(merged)\n",
    "            else:\n",
    "                attachment_data_list.append(merged)\n",
    "\n",
    "            scraped_pdfs.append(f)\n",
    "            total_pdfs_scraped += 1\n",
    "        else:\n",
    "            log_msg(f\"--> Not an Attachment 2 PDF: skipping {f}\", log_file)\n",
    "            skipped_pdfs.append(f)\n",
    "            total_pdfs_skipped += 1\n",
    "\n",
    "    # 5) record project status\n",
    "    if not attachment_data_list and not attachment_addendum_list:\n",
    "        skipped_projects.add(project_id)\n",
    "    else:\n",
    "        scraped_projects.add(project_id)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    project_attachment_df = pd.concat(attachment_data_list, ignore_index=True) if attachment_data_list else pd.DataFrame()\n",
    "    project_attachment_addendum_df = pd.concat(attachment_addendum_list, ignore_index=True) if attachment_addendum_list else pd.DataFrame()\n",
    "    return project_attachment_df, project_attachment_addendum_df\n",
    "\n",
    "\n",
    "# ------------------- CSV Saving & Summary Functions -------------------\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "    df = df.applymap(clean_string_cell)\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "# ------------------- Main Processing Function -------------------\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"\n",
    "    Processes all projects in ascending order (filtered via projects_to_process).\n",
    "    For each project, it:\n",
    "      - Checks for the project folder.\n",
    "      - Processes Attachment 2 PDFs by merging base data (from Appendix A PDFs) with\n",
    "        scraped table data (only the first nonempty table).\n",
    "      - Aggregates results across projects.\n",
    "    After processing, it saves the combined results to CSV files and prints a summary.\n",
    "    \"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    all_attachment_data = []\n",
    "    all_attachment_addendum_data = []\n",
    "\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        log_msg(f\"Projects to process: {projects_to_process}\", log_file)\n",
    "        for project_id in projects_to_process:\n",
    "            project_folder = os.path.join(BASE_DIRECTORY, str(project_id))\n",
    "            if not os.path.exists(project_folder):\n",
    "                missing_projects.add(project_id)\n",
    "                log_msg(f\"Project folder not found: {project_folder}\", log_file)\n",
    "                continue\n",
    "            log_msg(f\"\\n--- Processing project {project_id} ---\", log_file)\n",
    "            proj_attach_df, proj_attach_add_df = process_attachment2_for_project(project_id, log_file)\n",
    "            if not proj_attach_df.empty:\n",
    "                all_attachment_data.append(proj_attach_df)\n",
    "            if not proj_attach_add_df.empty:\n",
    "                all_attachment_addendum_data.append(proj_attach_add_df)\n",
    "        \n",
    "        if all_attachment_data:\n",
    "            core_originals = pd.concat(all_attachment_data, ignore_index=True)\n",
    "            save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "        else:\n",
    "            log_msg(\"\\nNo Attachment 2 data processed for regular PDFs.\", log_file)\n",
    "        \n",
    "        if all_attachment_addendum_data:\n",
    "            core_addendums = pd.concat(all_attachment_addendum_data, ignore_index=True)\n",
    "            save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "        else:\n",
    "            log_msg(\"\\nNo Attachment 2 data processed for addendum PDFs.\", log_file)\n",
    "\n",
    "        total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "        log_msg(\"\\n=== Scraping Summary ===\", log_file)\n",
    "        log_msg(f\"Total Projects Processed: {total_projects_processed}\", log_file)\n",
    "        log_msg(f\"Total Projects Scraped: {len(scraped_projects)}\", log_file)\n",
    "        log_msg(f\"Total Projects Skipped: {len(skipped_projects)}\", log_file)\n",
    "        log_msg(f\"Total Projects Missing: {len(missing_projects)}\", log_file)\n",
    "        log_msg(f\"Total PDFs Accessed: {total_pdfs_accessed}\", log_file)\n",
    "        log_msg(f\"Total PDFs Scraped: {total_pdfs_scraped}\", log_file)\n",
    "        log_msg(f\"Total PDFs Skipped: {total_pdfs_skipped}\", log_file)\n",
    "        log_msg(\"\\nList of Scraped Projects: \" + str(sorted(scraped_projects)), log_file)\n",
    "        log_msg(\"\\nList of Skipped Projects: \" + str(sorted(skipped_projects)), log_file)\n",
    "        log_msg(\"\\nList of Missing Projects: \" + str(sorted(missing_projects)), log_file)\n",
    "        log_msg(\"\\nList of Scraped PDFs: \" + str(scraped_pdfs), log_file)\n",
    "        log_msg(\"\\nList of Skipped PDFs: \" + str(skipped_pdfs), log_file)\n",
    "        log_msg(\"\\nList of Addendum PDFs: \" + str(addendum_pdfs), log_file)\n",
    "        log_msg(\"\\nList of Original PDFs: \" + str(original_pdfs), log_file)\n",
    "        log_msg(\"\\nList of Style N PDFs (Skipped due to 'Network Upgrade Type'): \" + str(style_n_pdfs), log_file)\n",
    "        log_msg(\"\\nTotal Number of Style N PDFs: \" + str(len(style_n_pdfs)), log_file)\n",
    "        log_msg(\"\\nNumber of Original PDFs Scraped: \" + str(len([pdf for pdf in scraped_pdfs if pdf in original_pdfs])), log_file)\n",
    "        log_msg(\"Number of Addendum PDFs Scraped: \" + str(len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs])), log_file)\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "def main():\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized and Addendums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: ['q_id', 'cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1295tot789_updated_as_of11132017', 'unnamed_8', 'none_2', 'none_3', 'none_4', 'none_5', 'none_6', 'none_7', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1299tot813_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1302tot830_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1305tot786_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1307tot820_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1308tot808_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1312tot812_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1313tot811_updated_as_of11222017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1314tot810_updated_as_of11222017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1319tot793_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1321tot814_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1322tot795_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1323tot821_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1324tot819_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1325tot818_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1326tot817_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1327tot806_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1329tot823_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1330_tot827_updated_as_of11142017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1331tot828_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1335tot833_updated_as_of11132017', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1339tot809_updated_as_of11132017', 'cost_category_notes_1a_to_1f', 'total_estimated_costs_x_1000_constant_dollar_2017', 'total_estimated_costs_x_1000_escalated_constant_dollars_od_year', 'estimated_time_to_construct_months_note_1g', 'none_8', 'od_dollar_escalation_duration_months_note_1g', 'none_9', 'none_10', 'other_potential_network_upgrades_note_1h', 'od_dollar_escalation_duration_months_note1g']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_rawdata_cluster9_style_others_originals.csv\", dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "\n",
    "\n",
    "df.columns = clean_column_headers(df.columns)\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "print(\"After cleaning:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_22638/1663911045.py:250: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_22638/1663911045.py:250: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_2_cluster_14_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_14_total.csv'.\n",
      "['PTO_IF' 'RNU' 'Distribution Upgrades']\n",
      "[1295 1299 1302 1305 1307 1308 1312 1313 1314 1319 1321 1322 1323 1324\n",
      " 1325 1326 1327 1329 1330 1331 1335 1336 1339 1341 1347]\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_rawdata_cluster9_style_others_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "#df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "df.columns = clean_column_headers(df.columns)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#STEP 2: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def move_dollar_values(df, source_column, target_column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in `source_column` starts with a '$',\n",
    "    move that value to `target_column` and clear the value in the source column.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      source_column (str): The column to check for values starting with '$'.\n",
    "      target_column (str): The column to move the values into.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure target_column exists; if not, create it filled with empty strings.\n",
    "    if target_column not in df.columns:\n",
    "        df[target_column] = \"\"\n",
    "    \n",
    "    # Create a boolean mask for rows where the source column starts with '$'\n",
    "    mask = df[source_column].astype(str).str.startswith('$', na=False)\n",
    "    \n",
    "    # Move the values: assign the source values to the target column where the mask is True.\n",
    "    df.loc[mask, target_column] = df.loc[mask, source_column]\n",
    "    \n",
    "    # Clear the source column values for those rows (set to empty string)\n",
    "    df.loc[mask, source_column] = \"\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Move values from 'unnamed_8' to a new column 'moved_value'\n",
    "#df = move_dollar_values(df, 'none_5', 'total_estimated_costs_x_1000_escalated_constant_dollars_od_year')\n",
    "\n",
    "\n",
    "#df = move_dollar_values(df, 'none_3','total_estimated_costs_x_1000_constant_dollar_2020')\n",
    "\n",
    "def remove_dollar_values_and_fill_nan(df, column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in the specified column starts with '$',\n",
    "    set that value to NaN. Also, replace any empty strings in that column with NaN.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      column (str): The column to check and clean.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the column is treated as string\n",
    "    df[column] = df[column].astype(str)\n",
    "    \n",
    "    # Set values starting with '$' to NaN\n",
    "    mask = df[column].str.startswith('$', na=False)\n",
    "    df.loc[mask, column] = np.nan\n",
    "    \n",
    "    # Replace any remaining empty strings with NaN\n",
    "    df[column] = df[column].replace(\"\", np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = remove_dollar_values_and_fill_nan(df, 'unnamed_8')\n",
    "\n",
    "\n",
    "df = df[~df.apply(lambda row: any(str(cell).startswith(\"Costs per Category w/o ITCC (A)\") for cell in row), axis=1)]\n",
    "df = df[~df.apply(lambda row: any(str(cell).startswith(\"ITCC @ 35% Constant Dollar in $1000s (2017)\") for cell in row), axis=1)]\n",
    "df = df[~df.apply(lambda row: any(str(cell).startswith(\"Max Duration for ITCC Calculation\") for cell in row), axis=1)]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def merge_columns(df):\n",
    "\n",
    "    merge_columns_dict = {\n",
    "\n",
    " \n",
    "\n",
    "        \"one_time_costs_b\": [\n",
    "            \"none_2\",\n",
    "        ],\n",
    "\n",
    " \n",
    "    \n",
    "        \"type_of_upgrade\": [\n",
    "            \"cost_ca_tegory\",\n",
    "            \"cost_category\",\n",
    "            \"cost_category_notes_1a_to_1f\",\n",
    "           \n",
    "            'other_potential_cost', \n",
    "            'other_potential_network_cost',\n",
    "          'other_potential_network_upgrades', \n",
    "          'other_potential_network_upgrades_note_1h',\n",
    "          'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1295tot789_updated_as_of11132017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1299tot813_updated_as_of11132017',\n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1302tot830_updated_as_of11132017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1305tot786_updated_as_of11132017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1307tot820_updated_as_of11132017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1308tot808_updated_as_of11132017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1312tot812_updated_as_of11132017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1313tot811_updated_as_of11222017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1314tot810_updated_as_of11222017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1319tot793_updated_as_of11132017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1321tot814_updated_as_of11132017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1322tot795_updated_as_of11132017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1323tot821_updated_as_of11132017',\n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1324tot819_updated_as_of11132017',\n",
    "                'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1325tot818_updated_as_of11132017',\n",
    "                'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1326tot817_updated_as_of11132017',\n",
    "                    'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1327tot806_updated_as_of11132017', \n",
    "                    'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1329tot823_updated_as_of11132017', \n",
    "                    'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1330_tot827_updated_as_of11142017', \n",
    "                    'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1331tot828_updated_as_of11132017', \n",
    "                    'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1335tot833_updated_as_of11132017', \n",
    "                    'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades', \n",
    "                    'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1339tot809_updated_as_of11132017', \n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1341tot796_updated_as_of11132017', \n",
    "            'cost_category_notes_1a_to_1f',\n",
    "            'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1347tot837_updated_as_of11132017'\n",
    "\n",
    "                 \n",
    "            \n",
    "           \n",
    "           \n",
    "            \n",
    "        ],\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"total_escalated_costs_wo_itcc\",\n",
    "            \n",
    "            \"total_estimated_costs_x_1000_escalated_constant_dollars_od_year\",\n",
    "            \"none_4\",\n",
    "            'total_escalated_costs_in_1000s',\n",
    "            'escalated_cost_in_1000s_note_8',\n",
    "            'total_estimated_costs_x_1000_escalated_constant_dollars_od_year',\n",
    "            \n",
    "\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"none_3\",\n",
    "            'total_costs_wo_itcc_cab',\n",
    "       'total_allocated_costs_constant_2020_dollar_in_1000s',\n",
    "       'total_estimated_costs_x_1000_constant_dollar_2020',\n",
    "       'total_allocated_costs_constant_2020_dollar_in_1000s',\n",
    "       'total_estimated_costs_x_1000_constant_dollar_2019',\n",
    "       'total_estimated_costs_x_1000_constant_dollar_2019',\n",
    "       'total_estimated_costs_x_1000_constant_dollar_2017',\n",
    "       \n",
    "\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            'estimated_time_for_licensing_permitting_construction_months',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months2',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months_note_1g',\n",
    "            'estimated_time_to_construct_months_note_1g',\n",
    "            'estimated_time_to_construct_months',\n",
    "            \"none_5\",\n",
    "            'estimated_time_to_construct_months',\n",
    "            'upgrade_duration_months',\n",
    "            'estimated_time_to_construct_months_note_12', \n",
    "             'estimated_time_to_construct_months_note_1g', \n",
    "             'estimated_time_to_construct_months_note_1g',\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    " \n",
    "        \"max_time_to_construct\": [\n",
    "            'maximum_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months_note1g',\n",
    "            'od_dollar_escalation_duration_months_note1g',\n",
    "            \"none_6\",\n",
    "            'maximum_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months_note1g', \n",
    "            'od_dollar_escalation_duration_months_note1g',\n",
    "            'potential_duration_months',\n",
    "            'od_dollar_escalation_duration_months_note_1g',\n",
    "            \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "        # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"type_of_upgrade\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "df.drop([ 'none_7','none_8', 'none_9',  'none_10',     'unnamed_8',\n",
    "         'one_time_costs_b'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    " \n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "cost_cols = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "# drop rows where *both* cost columns are null (or NaN)\n",
    "#df = df[df[cost_cols].notna().any(axis=1)]\n",
    "\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_rawdata_cluster9_style_other.csv', index=False)\n",
    "\n",
    "df = df[~df.apply(lambda row: row.astype(str).isin([\"Total Escalated Costs w/o ITCC\", \"Constant 2018 Dollar in $1000s (Estimate)\", \"Eastern\", \"Cost Category Note (a - f)\"]).any(), axis=1)]\n",
    "df = df[~df.apply(lambda row: any(str(cell).startswith(\"Project #:\") for cell in row), axis=1)]\n",
    "df = df[df['type_of_upgrade'].notna() & (df['type_of_upgrade'].astype(str).str.strip() != \"\")]\n",
    "\n",
    " \n",
    "def process_upgrade_columns(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame df with a column \"type_of_upgrade\" that contains both group headers and upgrade data,\n",
    "    this function:\n",
    "      1. Inserts a new column \"upgrade\" as a duplicate of \"type_of_upgrade\" (placed immediately after it).\n",
    "      2. Renames rows in \"type_of_upgrade\" that contain specific phrases as follows:\n",
    "           - If it contains \"Interconnection Facilities\", rename to \"PTO_IF\" (or \"PTO_IF Total\" if \"Total\" is present)\n",
    "           - If it contains \"Reliability Network Upgrade\", rename to \"RNU\" (or \"RNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Local Delivery Network Upgrades\", rename to \"LDNU\" (or \"LDNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Area Deliverability Network Upgrades\", rename to \"ADNU\" (or \"ADNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Distribution Upgrades\", leave it as is.\n",
    "      3. Creates a temporary column that only holds the header values (from rows that were detected as header rows) and forward-fills it downward.\n",
    "         The forward fill stops (i.e. does not fill into a row) if that row’s original \"type_of_upgrade\" contains any of the \"total\" indicators.\n",
    "      4. Replaces \"type_of_upgrade\" with the forward-filled header values.\n",
    "      5. Drops the rows that originally were header rows.\n",
    "      6. This deletes any rows which are either Total or Subtotal or Total cost assigned, the reason is some proejcts have multiple pdfs thus we rather calculate the total in the end.\n",
    "      \n",
    "    Returns the updated DataFrame.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 1. Create a new column \"upgrade\" immediately after \"type_of_upgrade\"\n",
    "    loc = df.columns.get_loc(\"type_of_upgrade\")\n",
    "    df.insert(loc+1, \"upgrade\", df[\"type_of_upgrade\"])\n",
    "    \n",
    "    # 2. Define a helper to rename header rows.\n",
    "    def rename_header(val):\n",
    "        # If the cell contains any of these phrases, rename accordingly.\n",
    "        # We'll check using the substring test (case-sensitive) per your request.\n",
    "        \n",
    "        if \"Potential Interconnection Facilities\" in val:\n",
    "            return \"Potential PTO_IF\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        if \"Interconnection Facilities\" in val:\n",
    "            return \"PTO_IF\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Reliability Network Upgrade\" in val:\n",
    "            return \"RNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Local Delivery Network Upgrades\" in val:\n",
    "            return \"LDNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Area Deliverability Network Upgrades\" in val:\n",
    "            return \"ADNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Distribution Upgrades\" in val:\n",
    "            return val  # leave unchanged\n",
    "        elif \"Conditional Assigned Network Upgrades\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"CANU\" \n",
    "        elif \"Non-Allocated IRNU\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"Non-Allocated IRNU\"\n",
    "        elif \"Non-allocated IRNU\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"Non-Allocated IRNU\"\n",
    "        \n",
    "        elif \"IRNU-NA\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"IRNU-NA\"\n",
    "        \n",
    "\n",
    "\n",
    "        elif \"Area Delivery Network Upgrades\" in val:\n",
    "            return \"ADNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    # 3. Identify header rows. We consider a row to be a header row if its \"type_of_upgrade\" cell \n",
    "    # contains any of the target phrases.\n",
    "    target_phrases = [\n",
    "        \"Interconnection Facilities\",\n",
    "        \"Potential Interconnection Facilities\" ,\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Local Delivery Network Upgrades\",\n",
    "        \"Area Deliverability Network Upgrades\",\n",
    "        \"Distribution Upgrades\",\n",
    "        \"Conditional Assigned Network Upgrades\",\n",
    "        \"Non-Allocated IRNU\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Non-allocated IRNU\",\n",
    "        \"IRNU-NA\",\n",
    "\n",
    "    ]\n",
    "    # Create a boolean mask for header rows.\n",
    "    header_mask = df[\"type_of_upgrade\"].apply(lambda x: any(phrase in x for phrase in target_phrases))\n",
    "    \n",
    "    # Apply renaming to the header rows.\n",
    "    df.loc[header_mask, \"type_of_upgrade\"] = df.loc[header_mask, \"type_of_upgrade\"].apply(rename_header)\n",
    "    \n",
    "    # 4. Create a temporary column 'header_temp' that holds only the header rows, then forward fill it.\n",
    "    df[\"header_temp\"] = df[\"type_of_upgrade\"].where(header_mask)\n",
    "    df[\"header_temp\"] = df[\"header_temp\"].ffill()\n",
    "    \n",
    "    # We want to stop the forward fill if we encounter a row that indicates totals.\n",
    "    # Define a simple function that returns True if a cell contains \"Total\" or \"Subtotal\" or \"Total cost assigned\".\n",
    "    def is_total_indicator(val):\n",
    "        return (\"Total\" in val) or (\"Subtotal\" in val) or (\"Total cost assigned\" in val)\n",
    "    \n",
    "    # For rows that themselves are total indicators in the \"upgrade\" column, do not forward-fill (set header_temp to NaN)\n",
    "    df.loc[df[\"upgrade\"].apply(lambda x: is_total_indicator(x)), \"header_temp\"] = None\n",
    "    \n",
    "    # Now, replace the \"type_of_upgrade\" column with the forward-filled header\n",
    "    df[\"type_of_upgrade\"] = df[\"header_temp\"]\n",
    "    df.drop(\"header_temp\", axis=1, inplace=True)\n",
    "    \n",
    "    # 5. Finally, drop the rows that were header rows (i.e. where header_mask is True)\n",
    "    df = df[~header_mask].reset_index(drop=True)\n",
    "    \n",
    "    # Also, drop any rows that have an empty \"type_of_upgrade\"\n",
    "    df = df[df[\"type_of_upgrade\"].notna() & (df[\"type_of_upgrade\"].str.strip() != \"\")]\n",
    "    \n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    "df = process_upgrade_columns(df)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"Potential Interconnection Facilities\" : \"Potential PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    " \"Non-Allocated IRNU\": \"RNU\",\n",
    " \"Non-allocated IRNU\": \"RNU\",\n",
    "    \"IRNU-NA\": \"RNU\",\n",
    "    \"Total IRNU-NA\": \"Total RNU\",\n",
    " \"Total Non-allocated IRNU\": \"Total RNU\",\n",
    " \"Total Non-Allocated IRNU\": \"Total RNU\",\n",
    " }\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "   \n",
    "\n",
    "\n",
    " \n",
    "   \n",
    " \n",
    " \n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "# Step 7: Remove $ signs and convert to numeric\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "                # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = group[\n",
    "            (group['type_of_upgrade'] == upgrade) & (group['item'] == 'no')\n",
    "        ].shape[0] > 0\n",
    "        \n",
    "        if total_exists:\n",
    "             \n",
    "            continue\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "        if not total_exists:\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate specified columns from the existing row\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns (single row, so it remains the same)\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate the specified columns from the first row in the group\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 8: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    "'PTO_IF Total': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'RNU Total': 'RNU',\n",
    " 'LDNU Total': 'LDNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    " \"Potential Interconnection Facilities\" : \"Potential PTO_IF\",\n",
    " \"Total Potential Interconnection Facilities\" : \"Potential PTO_IF\",\n",
    "    'Total Potential PTO_IF': 'Potential PTO_IF',\n",
    " 'Total Distribution Upgrades': 'Distribution Upgrades',\n",
    " 'Distribution Upgrades Total': 'Distribution Upgrades',\n",
    " \"Non-allocated IRNU\": \"RNU\",\n",
    "    \"IRNU-NA\": \"RNU\",\n",
    "    \"Total IRNU-NA\": \"Total RNU\",\n",
    " \"Total Non-allocated IRNU\": \"Total RNU\",\n",
    "}\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    \n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "     \n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "     \n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    " \n",
    "     \n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/02_intermediate/costs_phase_2_cluster_9_style_others_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    # Define the cost columns.\n",
    "    cost_cols = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "\n",
    "    # Build an aggregation dictionary:\n",
    "    # For columns not in grouping or cost_cols, we assume they are identical and take the first value.\n",
    "    agg_dict = {col: 'first' for col in totals_df.columns \n",
    "                if col not in ['q_id', 'type_of_upgrade'] + cost_cols}\n",
    "\n",
    "    # For the cost columns, we want to sum them.\n",
    "    agg_dict.update({col: 'sum' for col in cost_cols})\n",
    "\n",
    "    \n",
    "\n",
    "    # Group by both q_id and type_of_upgrade using the aggregation dictionary.\n",
    "    totals_df = totals_df.groupby(['q_id', 'type_of_upgrade'], as_index=False).agg(agg_dict)\n",
    "    totals_df = reorder_columns(totals_df)\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/02_intermediate/costs_phase_2_cluster_9_style_others_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_14_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_14_total.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addendums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: ['q_id', 'cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1325tot818_updated_as_of192018', 'unnamed_8', 'none_2', 'none_3', 'none_4', 'none_5', 'none_6', 'none_7', 'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1326tot817_updated_as_of192018']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_rawdata_cluster9_style_others_addendums.csv\", dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "\n",
    "\n",
    "df.columns = clean_column_headers(df.columns)\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "print(\"After cleaning:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_2_cluster_14_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_2_cluster_14_total.csv'.\n",
      "['PTO_IF']\n",
      "[1325 1326]\n",
      "[9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_35443/2177411790.py:675: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_rawdata_cluster9_style_others_addendums.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "#df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "df.columns = clean_column_headers(df.columns)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#STEP 2: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def move_dollar_values(df, source_column, target_column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in `source_column` starts with a '$',\n",
    "    move that value to `target_column` and clear the value in the source column.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      source_column (str): The column to check for values starting with '$'.\n",
    "      target_column (str): The column to move the values into.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure target_column exists; if not, create it filled with empty strings.\n",
    "    if target_column not in df.columns:\n",
    "        df[target_column] = \"\"\n",
    "    \n",
    "    # Create a boolean mask for rows where the source column starts with '$'\n",
    "    mask = df[source_column].astype(str).str.startswith('$', na=False)\n",
    "    \n",
    "    # Move the values: assign the source values to the target column where the mask is True.\n",
    "    df.loc[mask, target_column] = df.loc[mask, source_column]\n",
    "    \n",
    "    # Clear the source column values for those rows (set to empty string)\n",
    "    df.loc[mask, source_column] = \"\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Move values from 'unnamed_8' to a new column 'moved_value'\n",
    "#df = move_dollar_values(df, 'none_5', 'total_estimated_costs_x_1000_escalated_constant_dollars_od_year')\n",
    "\n",
    "\n",
    "#df = move_dollar_values(df, 'none_3','total_estimated_costs_x_1000_constant_dollar_2020')\n",
    "\n",
    "def remove_dollar_values_and_fill_nan(df, column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in the specified column starts with '$',\n",
    "    set that value to NaN. Also, replace any empty strings in that column with NaN.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      column (str): The column to check and clean.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the column is treated as string\n",
    "    df[column] = df[column].astype(str)\n",
    "    \n",
    "    # Set values starting with '$' to NaN\n",
    "    mask = df[column].str.startswith('$', na=False)\n",
    "    df.loc[mask, column] = np.nan\n",
    "    \n",
    "    # Replace any remaining empty strings with NaN\n",
    "    df[column] = df[column].replace(\"\", np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = remove_dollar_values_and_fill_nan(df, 'unnamed_8')\n",
    "\n",
    "\n",
    "df = df[~df.apply(lambda row: any(str(cell).startswith(\"Costs per Category w/o ITCC (A)\") for cell in row), axis=1)]\n",
    "df = df[~df.apply(lambda row: row.astype(str).isin([\"Total Escalated Costs w/o ITCC\", \"Constant 2018 Dollar in $1000s (Estimate)\", \"Eastern\", \"Cost Category Note (a - f)\"]).any(), axis=1)]\n",
    "df = df[~df.apply(lambda row: any(str(cell).startswith(\"Project #:\") for cell in row), axis=1)]\n",
    "\n",
    "#df = df[df['type_of_upgrade'].notna() & (df['type_of_upgrade'].astype(str).str.strip() != \"\")]\n",
    " \n",
    "\n",
    "\n",
    "def merge_columns(df):\n",
    "\n",
    "    merge_columns_dict = {\n",
    "\n",
    " \n",
    "\n",
    "        \"one_time_costs_b\": [\n",
    "            \"none_2\",\n",
    "        ],\n",
    "\n",
    " \n",
    "    \n",
    "        \"type_of_upgrade\": [\n",
    "            \"cost_ca_tegory\",\n",
    "            \"cost_category\",\n",
    "            \"cost_category_notes_1a_to_1f\",\n",
    "           \n",
    "            'other_potential_cost', \n",
    "            'other_potential_network_cost',\n",
    "          'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1325tot818_updated_as_of192018',\n",
    "           'qc9_phase_ii_study_report_attachment_2_escalated_cost_and_time_to_construct_for_interconnection_facilities_reliability_network_upgrades_delivery_network_upgrades_and_distribution_upgrades_project_q1326tot817_updated_as_of192018'\n",
    "           \n",
    "           \n",
    "           \n",
    "           \n",
    "            \n",
    "        ],\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"total_escalated_costs_wo_itcc\",\n",
    "            \n",
    "            \"total_estimated_costs_x_1000_escalated_constant_dollars_od_year\",\n",
    "            \"none_4\",\n",
    "            'total_escalated_costs_in_1000s',\n",
    "            'escalated_cost_in_1000s_note_8',\n",
    "            'total_estimated_costs_x_1000_escalated_constant_dollars_od_year',\n",
    "            \n",
    "\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"none_3\",\n",
    "            'total_costs_wo_itcc_cab',\n",
    "       'total_allocated_costs_constant_2020_dollar_in_1000s',\n",
    "       'total_estimated_costs_x_1000_constant_dollar_2020',\n",
    "       'total_allocated_costs_constant_2020_dollar_in_1000s',\n",
    "       'total_estimated_costs_x_1000_constant_dollar_2019',\n",
    "       'total_estimated_costs_x_1000_constant_dollar_2019',\n",
    "\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            'estimated_time_for_licensing_permitting_construction_months',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months2',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months',\n",
    "            'estimated_time_to_construct_months_note_1g',\n",
    "            'estimated_time_to_construct_months_note_1g',\n",
    "            'estimated_time_to_construct_months',\n",
    "            \"none_5\",\n",
    "            'estimated_time_to_construct_months',\n",
    "            'upgrade_duration_months',\n",
    "            'estimated_time_to_construct_months_note_12', \n",
    "             'estimated_time_to_construct_months_note_1g', \n",
    "             'estimated_time_to_construct_months_note_1g',\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    " \n",
    "        \"max_time_to_construct\": [\n",
    "            'maximum_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months_note1g',\n",
    "            'od_dollar_escalation_duration_months_note1g',\n",
    "            \"none_6\",\n",
    "            'maximum_escalation_duration_months',\n",
    "            'od_dollar_escalation_duration_months_note1g', \n",
    "            'od_dollar_escalation_duration_months_note1g',\n",
    "            'potential_duration_months',\n",
    "            \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "        # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"type_of_upgrade\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "df.drop([ 'none_7',   'unnamed_8',\n",
    "         'one_time_costs_b'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    " \n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/03_raw/ph2_rawdata_cluster9_style_other_addendums.csv', index=False)\n",
    "\n",
    "df = df[~df.apply(lambda row: row.astype(str).isin([\"Total Escalated Costs w/o ITCC\", \"Constant 2019 Dollar in $1000s (Estimate)\", \"Eastern\", \"Cost Category Note (a - f)\"]).any(), axis=1)]\n",
    "df = df[~df.apply(lambda row: any(str(cell).startswith(\"Project #:\") for cell in row), axis=1)]\n",
    "df = df[df['type_of_upgrade'].notna() & (df['type_of_upgrade'].astype(str).str.strip() != \"\")]\n",
    "\n",
    " \n",
    "def process_upgrade_columns(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame df with a column \"type_of_upgrade\" that contains both group headers and upgrade data,\n",
    "    this function:\n",
    "      1. Inserts a new column \"upgrade\" as a duplicate of \"type_of_upgrade\" (placed immediately after it).\n",
    "      2. Renames rows in \"type_of_upgrade\" that contain specific phrases as follows:\n",
    "           - If it contains \"Interconnection Facilities\", rename to \"PTO_IF\" (or \"PTO_IF Total\" if \"Total\" is present)\n",
    "           - If it contains \"Reliability Network Upgrade\", rename to \"RNU\" (or \"RNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Local Delivery Network Upgrades\", rename to \"LDNU\" (or \"LDNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Area Deliverability Network Upgrades\", rename to \"ADNU\" (or \"ADNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Distribution Upgrades\", leave it as is.\n",
    "      3. Creates a temporary column that only holds the header values (from rows that were detected as header rows) and forward-fills it downward.\n",
    "         The forward fill stops (i.e. does not fill into a row) if that row’s original \"type_of_upgrade\" contains any of the \"total\" indicators.\n",
    "      4. Replaces \"type_of_upgrade\" with the forward-filled header values.\n",
    "      5. Drops the rows that originally were header rows.\n",
    "      6. This deletes any rows which are either Total or Subtotal or Total cost assigned, the reason is some proejcts have multiple pdfs thus we rather calculate the total in the end.\n",
    "      \n",
    "    Returns the updated DataFrame.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 1. Create a new column \"upgrade\" immediately after \"type_of_upgrade\"\n",
    "    loc = df.columns.get_loc(\"type_of_upgrade\")\n",
    "    df.insert(loc+1, \"upgrade\", df[\"type_of_upgrade\"])\n",
    "    \n",
    "    # 2. Define a helper to rename header rows.\n",
    "    def rename_header(val):\n",
    "        # If the cell contains any of these phrases, rename accordingly.\n",
    "        # We'll check using the substring test (case-sensitive) per your request.\n",
    "        \n",
    "        if \"Potential Interconnection Facilities\" in val:\n",
    "            return \"Potential PTO_IF\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        if \"Interconnection Facilities\" in val:\n",
    "            return \"PTO_IF\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Reliability Network Upgrade\" in val:\n",
    "            return \"RNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Local Delivery Network Upgrades\" in val:\n",
    "            return \"LDNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Area Deliverability Network Upgrades\" in val:\n",
    "            return \"ADNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Distribution Upgrades\" in val:\n",
    "            return val  # leave unchanged\n",
    "        elif \"Conditional Assigned Network Upgrades\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"CANU\" \n",
    "        elif \"Non-Allocated IRNU\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"Non-Allocated IRNU\"\n",
    "        elif \"Non-allocated IRNU\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"Non-Allocated IRNU\"\n",
    "        \n",
    "        elif \"IRNU-NA\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"IRNU-NA\"\n",
    "        \n",
    "\n",
    "\n",
    "        elif \"Area Delivery Network Upgrades\" in val:\n",
    "            return \"ADNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    # 3. Identify header rows. We consider a row to be a header row if its \"type_of_upgrade\" cell \n",
    "    # contains any of the target phrases.\n",
    "    target_phrases = [\n",
    "        \"Interconnection Facilities\",\n",
    "        \"Potential Interconnection Facilities\" ,\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Local Delivery Network Upgrades\",\n",
    "        \"Area Deliverability Network Upgrades\",\n",
    "        \"Distribution Upgrades\",\n",
    "        \"Conditional Assigned Network Upgrades\",\n",
    "        \"Non-Allocated IRNU\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"Non-allocated IRNU\",\n",
    "        \"IRNU-NA\",\n",
    "\n",
    "    ]\n",
    "    # Create a boolean mask for header rows.\n",
    "    header_mask = df[\"type_of_upgrade\"].apply(lambda x: any(phrase in x for phrase in target_phrases))\n",
    "    \n",
    "    # Apply renaming to the header rows.\n",
    "    df.loc[header_mask, \"type_of_upgrade\"] = df.loc[header_mask, \"type_of_upgrade\"].apply(rename_header)\n",
    "    \n",
    "    # 4. Create a temporary column 'header_temp' that holds only the header rows, then forward fill it.\n",
    "    df[\"header_temp\"] = df[\"type_of_upgrade\"].where(header_mask)\n",
    "    df[\"header_temp\"] = df[\"header_temp\"].ffill()\n",
    "    \n",
    "    # We want to stop the forward fill if we encounter a row that indicates totals.\n",
    "    # Define a simple function that returns True if a cell contains \"Total\" or \"Subtotal\" or \"Total cost assigned\".\n",
    "    def is_total_indicator(val):\n",
    "        return (\"Total\" in val) or (\"Subtotal\" in val) or (\"Total cost assigned\" in val)\n",
    "    \n",
    "    # For rows that themselves are total indicators in the \"upgrade\" column, do not forward-fill (set header_temp to NaN)\n",
    "    df.loc[df[\"upgrade\"].apply(lambda x: is_total_indicator(x)), \"header_temp\"] = None\n",
    "    \n",
    "    # Now, replace the \"type_of_upgrade\" column with the forward-filled header\n",
    "    df[\"type_of_upgrade\"] = df[\"header_temp\"]\n",
    "    df.drop(\"header_temp\", axis=1, inplace=True)\n",
    "    \n",
    "    # 5. Finally, drop the rows that were header rows (i.e. where header_mask is True)\n",
    "    df = df[~header_mask].reset_index(drop=True)\n",
    "    \n",
    "    # Also, drop any rows that have an empty \"type_of_upgrade\"\n",
    "    df = df[df[\"type_of_upgrade\"].notna() & (df[\"type_of_upgrade\"].str.strip() != \"\")]\n",
    "    \n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    "df = process_upgrade_columns(df)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"Potential Interconnection Facilities\" : \"Potential PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    " \"Non-Allocated IRNU\": \"RNU\",\n",
    " \"Non-allocated IRNU\": \"RNU\",\n",
    "    \"IRNU-NA\": \"RNU\",\n",
    "    \"Total IRNU-NA\": \"Total RNU\",\n",
    " \"Total Non-allocated IRNU\": \"Total RNU\",\n",
    " \"Total Non-Allocated IRNU\": \"Total RNU\",\n",
    " }\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "   \n",
    "\n",
    "\n",
    " \n",
    "   \n",
    " \n",
    " \n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "# Step 7: Remove $ signs and convert to numeric\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "                # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = group[\n",
    "            (group['type_of_upgrade'] == upgrade) & (group['item'] == 'no')\n",
    "        ].shape[0] > 0\n",
    "        \n",
    "        if total_exists:\n",
    "             \n",
    "            continue\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "        if not total_exists:\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate specified columns from the existing row\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns (single row, so it remains the same)\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate the specified columns from the first row in the group\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 8: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    "'PTO_IF Total': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'RNU Total': 'RNU',\n",
    " 'LDNU Total': 'LDNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    " \"Potential Interconnection Facilities\" : \"Potential PTO_IF\",\n",
    " \"Total Potential Interconnection Facilities\" : \"Potential PTO_IF\",\n",
    "    'Total Potential PTO_IF': 'Potential PTO_IF',\n",
    " 'Total Distribution Upgrades': 'Distribution Upgrades',\n",
    " 'Distribution Upgrades Total': 'Distribution Upgrades',\n",
    " \"Non-allocated IRNU\": \"RNU\",\n",
    "    \"IRNU-NA\": \"RNU\",\n",
    "    \"Total IRNU-NA\": \"Total RNU\",\n",
    " \"Total Non-allocated IRNU\": \"Total RNU\",\n",
    "}\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    \n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "     \n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "     \n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    " \n",
    "     \n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/02_intermediate/costs_phase_2_cluster_9_style_others_itemized_addendums.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    # Define the cost columns.\n",
    "    cost_cols = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "\n",
    "    # Build an aggregation dictionary:\n",
    "    # For columns not in grouping or cost_cols, we assume they are identical and take the first value.\n",
    "    agg_dict = {col: 'first' for col in totals_df.columns \n",
    "                if col not in ['q_id', 'type_of_upgrade'] + cost_cols}\n",
    "\n",
    "    # For the cost columns, we want to sum them.\n",
    "    agg_dict.update({col: 'sum' for col in cost_cols})\n",
    "\n",
    "    \n",
    "\n",
    "    # Group by both q_id and type_of_upgrade using the aggregation dictionary.\n",
    "    totals_df = totals_df.groupby(['q_id', 'type_of_upgrade'], as_index=False).agg(agg_dict)\n",
    "    totals_df = reorder_columns(totals_df)\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_2_cost_data/Cluster 9/02_intermediate/costs_phase_2_cluster_9_style_others_total_addendums.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_2_cluster_14_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_2_cluster_14_total.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge- Complete replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_22638/1996451717.py:206: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df[c].replace('', np.nan)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_22638/1996451717.py:210: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_22638/1996451717.py:210: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_22638/1996451717.py:212: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].replace(np.nan, '', inplace=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_22638/1996451717.py:206: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df[c].replace('', np.nan)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_22638/1996451717.py:210: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_22638/1996451717.py:210: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_22638/1996451717.py:212: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[c].replace(np.nan, '', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Load a CSV file and ensure specific columns are treated as character.\n",
    "    \"\"\"\n",
    "    available_columns = pd.read_csv(file_path, nrows=0).columns\n",
    "    char_cols = [c for c in char_columns if c in available_columns]\n",
    "    return pd.read_csv(\n",
    "        file_path,\n",
    "        dtype={c: str for c in char_cols},\n",
    "        na_values=[], \n",
    "        keep_default_na=False\n",
    "    )\n",
    "\n",
    "def save_data(df, file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Save a dataframe to CSV, forcing certain columns to string.\n",
    "    \"\"\"\n",
    "    for col in char_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def merge_with_addendums(itemized, itemized_addendums, total, total_addendums):\n",
    "    # mark originals & keep row order\n",
    "    itemized['original'] = \"yes\"\n",
    "    total['original']    = \"yes\"\n",
    "    itemized['row_order'] = itemized.index\n",
    "    total['row_order']    = total.index\n",
    "\n",
    "    # ensure numeric q_id\n",
    "    for df in (itemized, itemized_addendums, total, total_addendums):\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors=\"coerce\")\n",
    "\n",
    "    conditional_columns = [\n",
    "        \"req_deliverability\",\"latitude\",\"longitude\",\n",
    "        \"capacity\",\"point_of_interconnection\"\n",
    "    ]\n",
    "\n",
    "    # --- ITEMIZED: replace only matching (q_id, type_of_upgrade) blocks ---\n",
    "    updated_itemized_rows = []\n",
    "    # iterate over each unique (q_id, type_of_upgrade) in the addendums\n",
    "    for q, t in itemized_addendums[['q_id','type_of_upgrade']].drop_duplicates().itertuples(index=False):\n",
    "        adds = itemized_addendums[\n",
    "            (itemized_addendums['q_id'] == q) &\n",
    "            (itemized_addendums['type_of_upgrade'] == t)\n",
    "        ].reset_index(drop=True)\n",
    "        orig = itemized[\n",
    "            (itemized['q_id'] == q) &\n",
    "            (itemized['type_of_upgrade'] == t)\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # combine conditional columns\n",
    "        for col in conditional_columns:\n",
    "            if col in adds.columns and col in orig.columns:\n",
    "                adds[col] = (\n",
    "                    adds[col].replace(\"\", pd.NA)\n",
    "                              .combine_first(orig[col])\n",
    "                              .fillna(\"\")\n",
    "                )\n",
    "\n",
    "        # carry over or pad row_order\n",
    "        if 'row_order' in orig:\n",
    "            ro = orig['row_order'].tolist()\n",
    "            if len(ro) < len(adds):\n",
    "                ro += [pd.NA] * (len(adds) - len(ro))\n",
    "        else:\n",
    "            ro = [pd.NA] * len(adds)\n",
    "\n",
    "        adds = adds.assign(original=\"no\", row_order=ro[:len(adds)])\n",
    "\n",
    "        # drop only those matching (q_id, type_of_upgrade) from the master\n",
    "        itemized = itemized[\n",
    "            ~((itemized['q_id'] == q) & (itemized['type_of_upgrade'] == t))\n",
    "        ]\n",
    "\n",
    "        updated_itemized_rows.append(adds)\n",
    "\n",
    "    # stitch back untouched originals + updated blocks\n",
    "    updated_itemized = pd.concat(\n",
    "        [itemized] + updated_itemized_rows,\n",
    "        ignore_index=True\n",
    "    ) if updated_itemized_rows else itemized.copy()\n",
    "\n",
    "    updated_itemized['row_order'] = updated_itemized['row_order'].fillna(-1).astype(int)\n",
    "    updated_itemized = (\n",
    "        updated_itemized\n",
    "        .sort_values('row_order')\n",
    "        .drop(columns='row_order')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # --- TOTAL: per type_of_upgrade (unchanged logic) ---\n",
    "    updated_total_rows = []\n",
    "    for q in total_addendums['q_id'].unique():\n",
    "        for t in total_addendums['type_of_upgrade'].unique():\n",
    "            adds = total_addendums[\n",
    "                (total_addendums['q_id']==q)&\n",
    "                (total_addendums['type_of_upgrade']==t)\n",
    "            ].reset_index(drop=True)\n",
    "            if adds.empty:\n",
    "                continue\n",
    "\n",
    "            mask = (total['q_id']==q)&(total['type_of_upgrade']==t)\n",
    "            orig = total[mask].reset_index(drop=True)\n",
    "            if orig.empty:\n",
    "                orig = pd.DataFrame({'row_order':[pd.NA]*len(adds)}, index=adds.index)\n",
    "\n",
    "            # align lengths\n",
    "            if len(adds) > len(orig):\n",
    "                extra = pd.DataFrame({c: pd.NA for c in orig.columns},\n",
    "                                     index=range(len(adds)-len(orig)))\n",
    "                orig = pd.concat([orig, extra], ignore_index=True)\n",
    "            elif len(adds) < len(orig):\n",
    "                orig = orig.iloc[:len(adds)].reset_index(drop=True)\n",
    "\n",
    "            for col in conditional_columns:\n",
    "                if col in adds.columns and col in orig.columns:\n",
    "                    adds[col] = (\n",
    "                        adds[col].replace(\"\", pd.NA)\n",
    "                                  .combine_first(orig[col])\n",
    "                                  .fillna(\"\")\n",
    "                    )\n",
    "\n",
    "            total.loc[mask, 'original'] = \"no\"\n",
    "            updated_total_rows.append(\n",
    "                adds.assign(original=\"no\", row_order=orig['row_order'].tolist()[:len(adds)])\n",
    "            )\n",
    "            total = total[~mask]\n",
    "\n",
    "    updated_total = pd.concat([total] + updated_total_rows, ignore_index=True) \\\n",
    "                    if updated_total_rows else total.copy()\n",
    "    updated_total['row_order'] = updated_total['row_order'].fillna(-1).astype(int)\n",
    "    updated_total = (\n",
    "        updated_total\n",
    "        .sort_values('row_order')\n",
    "        .drop(columns='row_order')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # move 'original' to end\n",
    "    def move_last(df):\n",
    "        cols = [c for c in df.columns if c!='original'] + ['original']\n",
    "        return df[cols]\n",
    "\n",
    "    return move_last(updated_itemized), move_last(updated_total)\n",
    "\n",
    "\n",
    "# ── main script ──\n",
    "\n",
    "char_columns = [\n",
    "    \"req_deliverability\",\"point_of_interconnection\",\"type_of_upgrade\",\n",
    "    \"upgrade\",\"description\",\"estimated_time_to_construct\",\"original\",\"item\"\n",
    "]\n",
    "\n",
    "itemized = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 9/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_9_style_others_itemized.csv\",\n",
    "    char_columns\n",
    ")\n",
    "itemized_addendums = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 9/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_9_style_others_itemized_addendums.csv\",\n",
    "    char_columns\n",
    ")\n",
    "total = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 9/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_9_style_others_total.csv\",\n",
    "    char_columns\n",
    ")\n",
    "total_addendums = load_data(\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 9/02_intermediate/\"\n",
    "    \"costs_phase_2_cluster_9_style_others_total_addendums.csv\",\n",
    "    char_columns\n",
    ")\n",
    "\n",
    "updated_itemized, updated_total = merge_with_addendums(\n",
    "    itemized, itemized_addendums, total, total_addendums\n",
    ")\n",
    "\n",
    "# drop unwanted columns\n",
    "to_drop = [\n",
    "    \"upgrade_classification\",\"estimated\",\"caiso_queue\",\n",
    "    \"project_type\",\"dependent_system_upgrade\"\n",
    "]\n",
    "updated_itemized = updated_itemized.drop(columns=[c for c in to_drop if c in updated_itemized], errors='ignore')\n",
    "updated_total   = updated_total.drop(columns=[c for c in to_drop if c in updated_total],   errors='ignore')\n",
    "\n",
    "# fill & sort\n",
    "fill_cols = [\n",
    "    \"point_of_interconnection\",\"latitude\",\"longitude\",\n",
    "    \"req_deliverability\",\"capacity\"\n",
    "]\n",
    "for df in (updated_itemized, updated_total):\n",
    "    for c in fill_cols:\n",
    "        df[c] = df[c].replace('', np.nan)\n",
    "    df.sort_values('q_id', kind='stable', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    for c in fill_cols:\n",
    "        df[c] = df.groupby('q_id')[c].apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
    "    for c in fill_cols:\n",
    "        df[c].replace(np.nan, '', inplace=True)\n",
    "\n",
    "# save\n",
    "save_data(\n",
    "    updated_itemized,\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 9/01_clean/\"\n",
    "    \"costs_phase_2_cluster_9_style_others_itemized_updated.csv\",\n",
    "    char_columns\n",
    ")\n",
    "save_data(\n",
    "    updated_total,\n",
    "    \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/\"\n",
    "    \"04_intermediate_scraped_data/phase_2_cost_data/\"\n",
    "    \"Cluster 9/01_clean/\"\n",
    "    \"costs_phase_2_cluster_9_style_others_total_updated.csv\",\n",
    "    char_columns\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
