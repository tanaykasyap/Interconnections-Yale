{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting inflect\n",
      "  Downloading inflect-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting more_itertools>=8.5.0 (from inflect)\n",
      "  Downloading more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting typeguard>=4.0.1 (from inflect)\n",
      "  Downloading typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/vk365/Library/Python/3.9/lib/python/site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /Users/vk365/Library/Python/3.9/lib/python/site-packages (from typeguard>=4.0.1->inflect) (8.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vk365/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=3.6->typeguard>=4.0.1->inflect) (3.20.1)\n",
      "Downloading inflect-7.5.0-py3-none-any.whl (35 kB)\n",
      "Downloading more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "Downloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: more_itertools, typeguard, inflect\n",
      "Successfully installed inflect-7.5.0 more_itertools-10.6.0 typeguard-4.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install inflect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['upgrade', 'sale', 'discount', 'product id', 'type of upgrade', 'estimated cost x 1000 constant dollar']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import inflect\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing, removing unwanted characters, and singularizing words.\"\"\"\n",
    "    p = inflect.engine()\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            # Convert to lowercase\n",
    "            header = header.lower()\n",
    "            # Normalize whitespace\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            # Remove contents in parentheses\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            # Remove unwanted characters\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            # Trim leading/trailing whitespace\n",
    "            header = header.strip()\n",
    "            \n",
    "            # Singularize each word using inflect\n",
    "            words = header.split()\n",
    "            singular_words = [p.singular_noun(word) if p.singular_noun(word) else word for word in words]\n",
    "            header = \" \".join(singular_words)\n",
    "            \n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "# Example usage:\n",
    "headers = [\"Upgrades (New)\", \"Sales\", \"Discounts\", \"Product IDs\", \"Type of Upgrade\", \"Estimated Cost x 1,000 Constant Dollars (2012)\"]\n",
    "print(clean_column_headers(headers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped PDF: 11AS763732-C5PIEastQ871AppendixA.pdf from Project 871 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS763774-C5PIMetroQ893AppA.pdf from Project 873 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 12AS779216-Appendix_AQ877_California_Flats_Solar_C5_Ph_I_Study_Report.pdf from Project 877\n",
      "Scraped PDF: 12AS779571-Appendix_AQ885_SKIC_Solar_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf from Project 885\n",
      "Scraped PDF: 12AS780495-QC5PIEOPQ887AppendixA.pdf from Project 887\n",
      "Skipped PDF: 12AS780604-QC5PINOLQ888Appendix_A.pdf from Project 888 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS778778-QC5PINorthernQ890Backus_Solar_EnergyAppendix_A.pdf from Project 890 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 12AS782071-QC5PIEOPQ891AppendixA.pdf from Project 891\n",
      "Scraped PDF: 12AS780639-QC5PIEOPQ892AppendixA.pdf from Project 892\n",
      "Skipped PDF: 12AS780344-C5PIMetroQ893AppA.pdf from Project 893 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 12AS779610-Appendix_AQ894_Prospect_Energy.pdf from Project 894\n",
      "Skipped PDF: 12AS781853-Appendix_A__Q895_1312013.pdf from Project 895 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS781741-Appendix_A__Q896_1312013.pdf from Project 896 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS780679-QC5PINOLQ897Appendix_A.pdf from Project 897 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 12AS780230-Appendix_AQ898_KPP45_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf from Project 898\n",
      "Scraped PDF: 12AS779299-Appendix_AQ899_Dunningan_Hills.pdf from Project 899\n",
      "Scraped PDF: 12AS779494-Appendix_AQ900_Rio_Bravo_Solar_I_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf from Project 900\n",
      "Scraped PDF: 12AS780155-Appendix_AQ901_Wildwood_Solar_II_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf from Project 901\n",
      "Skipped PDF: 12AS781389-C5PIEastQ902AppendixA.pdf from Project 902 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS780940-QC5PINorthernQ903Desert_Butte_Solar_FarmAppendix_A.pdf from Project 903 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS780754-C5PIEastQ904AppendixA.pdf from Project 904 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 12AS781466-QC5PIEOPQ908AppendixA.pdf from Project 908\n",
      "Skipped PDF: 12AS780826-QC5PINOLQ909Appendix_A.pdf from Project 909 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 12AS780268-Appendix_AQ910_Honey_Lake.pdf from Project 910\n",
      "Skipped PDF: 12AS780864-QC5PINorthernQ911Pastoria_Energy_FacilityAppendix_A.pdf from Project 911 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS781352-C5PIEastQ913AppendixA.pdf from Project 913 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 12AS781125-QC5PIEOPQ914AppendixA.pdf from Project 914\n",
      "Scraped PDF: 12AS779532-Appendix_AQ916_Stratford_Solar_C5_Ph_I_Study_Report.pdf from Project 916\n",
      "Scraped PDF: 12AS780118-Appendix_AQ917_Lemoore_Solar_C5_Ph_I_Study_Report.pdf from Project 917\n",
      "Skipped PDF: 12AS781816-Appendix_A__Q919_1312013.pdf from Project 919 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 12AS779648-Appendix_AQ921_SPS_AS_40MW_PV_Generation_Facility_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf from Project 921\n",
      "Skipped PDF: 12AS780381-QC5PINorthernQ922Highwind_Power_ProjectAppendix_A.pdf from Project 922 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS781314-C5PIEastQ923AppendixA.pdf from Project 923 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS781277-QC5PINorthernQ925Tehachapi_SpindleAppendix_A.pdf from Project 925 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS781050-QC5PINorthernQ926Golden_Hills_PowerAppendix_A.pdf from Project 926 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A.pdf from Project 927 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A20130201.pdf from Project 927 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 12AS782675-QC5PIEOPQ929AppendixA.pdf from Project 929\n",
      "Scraped PDF: 12AS783552-QC5PIEOPQ930AppendixA.pdf from Project 930\n",
      "Scraped PDF: 12AS780420-QC5PIEOPQ931AppendixA.pdf from Project 931\n",
      "Scraped PDF: 12AS785803-QC5PIEOPQ932AppendixA.pdf from Project 932\n",
      "Skipped PDF: 12AS783586-C5PIMetroQ941AppA.pdf from Project 941 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 12AS797833-QC5PINOLQ942Appendix_A.pdf from Project 942 (No Table 3 or Attachment data)\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 5/03_raw/rawdata_cluster5_style_D_originals.csv\n",
      "No data to save for addendums.\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 50\n",
      "Total Projects Scraped: 20\n",
      "Total Projects Skipped: 30\n",
      "Total Projects Missing: 24\n",
      "Total PDFs Accessed: 43\n",
      "Total PDFs Scraped: 20\n",
      "Total PDFs Skipped: 23\n",
      "\n",
      "List of Scraped Projects:\n",
      "[877, 885, 887, 891, 892, 894, 898, 899, 900, 901, 908, 910, 914, 916, 917, 921, 929, 930, 931, 932]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[869, 870, 871, 873, 878, 879, 888, 890, 893, 895, 896, 897, 902, 903, 904, 909, 911, 913, 919, 922, 923, 925, 926, 927, 928, 933, 934, 937, 941, 942]\n",
      "\n",
      "List of Missing Projects:\n",
      "[872, 874, 875, 876, 880, 881, 882, 883, 884, 886, 889, 905, 906, 907, 912, 915, 918, 920, 924, 935, 936, 938, 939, 940]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['12AS779216-Appendix_AQ877_California_Flats_Solar_C5_Ph_I_Study_Report.pdf', '12AS779571-Appendix_AQ885_SKIC_Solar_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780495-QC5PIEOPQ887AppendixA.pdf', '12AS782071-QC5PIEOPQ891AppendixA.pdf', '12AS780639-QC5PIEOPQ892AppendixA.pdf', '12AS779610-Appendix_AQ894_Prospect_Energy.pdf', '12AS780230-Appendix_AQ898_KPP45_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS779299-Appendix_AQ899_Dunningan_Hills.pdf', '12AS779494-Appendix_AQ900_Rio_Bravo_Solar_I_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780155-Appendix_AQ901_Wildwood_Solar_II_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS781466-QC5PIEOPQ908AppendixA.pdf', '12AS780268-Appendix_AQ910_Honey_Lake.pdf', '12AS781125-QC5PIEOPQ914AppendixA.pdf', '12AS779532-Appendix_AQ916_Stratford_Solar_C5_Ph_I_Study_Report.pdf', '12AS780118-Appendix_AQ917_Lemoore_Solar_C5_Ph_I_Study_Report.pdf', '12AS779648-Appendix_AQ921_SPS_AS_40MW_PV_Generation_Facility_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS782675-QC5PIEOPQ929AppendixA.pdf', '12AS783552-QC5PIEOPQ930AppendixA.pdf', '12AS780420-QC5PIEOPQ931AppendixA.pdf', '12AS785803-QC5PIEOPQ932AppendixA.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['11AS763732-C5PIEastQ871AppendixA.pdf', '11AS763774-C5PIMetroQ893AppA.pdf', '12AS780604-QC5PINOLQ888Appendix_A.pdf', '12AS778778-QC5PINorthernQ890Backus_Solar_EnergyAppendix_A.pdf', '12AS780344-C5PIMetroQ893AppA.pdf', '12AS781853-Appendix_A__Q895_1312013.pdf', '12AS781741-Appendix_A__Q896_1312013.pdf', '12AS780679-QC5PINOLQ897Appendix_A.pdf', '12AS781389-C5PIEastQ902AppendixA.pdf', '12AS780940-QC5PINorthernQ903Desert_Butte_Solar_FarmAppendix_A.pdf', '12AS780754-C5PIEastQ904AppendixA.pdf', '12AS780826-QC5PINOLQ909Appendix_A.pdf', '12AS780864-QC5PINorthernQ911Pastoria_Energy_FacilityAppendix_A.pdf', '12AS781352-C5PIEastQ913AppendixA.pdf', '12AS781816-Appendix_A__Q919_1312013.pdf', '12AS780381-QC5PINorthernQ922Highwind_Power_ProjectAppendix_A.pdf', '12AS781314-C5PIEastQ923AppendixA.pdf', '12AS781277-QC5PINorthernQ925Tehachapi_SpindleAppendix_A.pdf', '12AS781050-QC5PINorthernQ926Golden_Hills_PowerAppendix_A.pdf', '12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A.pdf', '12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A20130201.pdf', '12AS783586-C5PIMetroQ941AppA.pdf', '12AS797833-QC5PINOLQ942Appendix_A.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "[]\n",
      "\n",
      "List of Original PDFs:\n",
      "['11AS763732-C5PIEastQ871AppendixA.pdf', '11AS763774-C5PIMetroQ893AppA.pdf', '12AS779216-Appendix_AQ877_California_Flats_Solar_C5_Ph_I_Study_Report.pdf', '12AS779571-Appendix_AQ885_SKIC_Solar_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780495-QC5PIEOPQ887AppendixA.pdf', '12AS780604-QC5PINOLQ888Appendix_A.pdf', '12AS778778-QC5PINorthernQ890Backus_Solar_EnergyAppendix_A.pdf', '12AS782071-QC5PIEOPQ891AppendixA.pdf', '12AS780639-QC5PIEOPQ892AppendixA.pdf', '12AS780344-C5PIMetroQ893AppA.pdf', '12AS779610-Appendix_AQ894_Prospect_Energy.pdf', '12AS781853-Appendix_A__Q895_1312013.pdf', '12AS781741-Appendix_A__Q896_1312013.pdf', '12AS780679-QC5PINOLQ897Appendix_A.pdf', '12AS780230-Appendix_AQ898_KPP45_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS779299-Appendix_AQ899_Dunningan_Hills.pdf', '12AS779494-Appendix_AQ900_Rio_Bravo_Solar_I_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780155-Appendix_AQ901_Wildwood_Solar_II_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS781389-C5PIEastQ902AppendixA.pdf', '12AS780940-QC5PINorthernQ903Desert_Butte_Solar_FarmAppendix_A.pdf', '12AS780754-C5PIEastQ904AppendixA.pdf', '12AS781466-QC5PIEOPQ908AppendixA.pdf', '12AS780826-QC5PINOLQ909Appendix_A.pdf', '12AS780268-Appendix_AQ910_Honey_Lake.pdf', '12AS780864-QC5PINorthernQ911Pastoria_Energy_FacilityAppendix_A.pdf', '12AS781352-C5PIEastQ913AppendixA.pdf', '12AS781125-QC5PIEOPQ914AppendixA.pdf', '12AS779532-Appendix_AQ916_Stratford_Solar_C5_Ph_I_Study_Report.pdf', '12AS780118-Appendix_AQ917_Lemoore_Solar_C5_Ph_I_Study_Report.pdf', '12AS781816-Appendix_A__Q919_1312013.pdf', '12AS779648-Appendix_AQ921_SPS_AS_40MW_PV_Generation_Facility_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780381-QC5PINorthernQ922Highwind_Power_ProjectAppendix_A.pdf', '12AS781314-C5PIEastQ923AppendixA.pdf', '12AS781277-QC5PINorthernQ925Tehachapi_SpindleAppendix_A.pdf', '12AS781050-QC5PINorthernQ926Golden_Hills_PowerAppendix_A.pdf', '12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A.pdf', '12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A20130201.pdf', '12AS782675-QC5PIEOPQ929AppendixA.pdf', '12AS783552-QC5PIEOPQ930AppendixA.pdf', '12AS780420-QC5PIEOPQ931AppendixA.pdf', '12AS785803-QC5PIEOPQ932AppendixA.pdf', '12AS783586-C5PIMetroQ941AppA.pdf', '12AS797833-QC5PINOLQ942Appendix_A.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 20\n",
      "Number of Addendum PDFs Scraped: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "import inflect\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/rawdata_cluster5_style_D_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/rawdata_cluster5_style_D_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/scraping_cluster5_style_D_log.txt\"\n",
    "PROJECT_RANGE = range(869, 943)  # Example range for q_ids in Clusters 5\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing, removing unwanted characters, and singularizing words.\"\"\"\n",
    "    p = inflect.engine()\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "            # Correct any mis\u2010spellings of \u201ctype of upgrade\u201d\n",
    "            header = re.sub(r'\\btype of upgr\\s*ade\\b', 'type of upgrade', header)\n",
    "            words = header.split()\n",
    "            singular_words = [p.singular_noun(word) if p.singular_noun(word) else word for word in words]\n",
    "            header = \" \".join(singular_words)\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback if none found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "    new_order = existing_desired + remaining\n",
    "    df = df[new_order]\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"\n",
    "    Ensures each row in data_rows has exactly len(headers) columns.\n",
    "    If a row is too short, it is padded with empty strings.\n",
    "    If too long, it is truncated.\n",
    "    \"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"] * (col_count - len(row)))\n",
    "\n",
    "def extract_table2(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "    table_settings_list = [\n",
    "        {\"horizontal_strategy\": \"text\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 1},\n",
    "        {\"horizontal_strategy\": \"lines\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 2},\n",
    "    ]\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*(?:2|B\\.1)\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 2 found in the PDF.\", file=log_file)\n",
    "                return None\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Table 2 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "            extraction_successful = False\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 2...\", file=log_file)\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    poi_col_index = cell_index\n",
    "                                    adjacent_col_index = poi_col_index + 1\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            poi_value_parts = []\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "                                            if poi_value_parts:\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                            if extraction_successful:\n",
    "                                break\n",
    "                        if extraction_successful:\n",
    "                            break\n",
    "                    if extraction_successful:\n",
    "                        break\n",
    "                if extraction_successful:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 2 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "    if not extraction_successful:\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            print(\"Point of Interconnection not found in Table 2.\", file=log_file)\n",
    "            return None\n",
    "    return point_of_interconnection\n",
    "\n",
    "def fix_column_names(columns):\n",
    "    \"\"\"\n",
    "    Renames duplicate and empty column names.\n",
    "    Duplicate names are suffixed with _1, _2, etc.\n",
    "    Empty or whitespace-only names are replaced with unnamed_1, unnamed_2, etc.\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    counts = {}\n",
    "    unnamed_count = 1\n",
    "    for col in columns:\n",
    "        # Treat empty or whitespace-only names as unnamed.\n",
    "        if not col or col.strip() == \"\":\n",
    "            new_col = f\"unnamed_{unnamed_count}\"\n",
    "            unnamed_count += 1\n",
    "        else:\n",
    "            new_col = col.strip()\n",
    "        if new_col in counts:\n",
    "            new_col_with_suffix = f\"{new_col}_{counts[new_col]}\"\n",
    "            counts[new_col] += 1\n",
    "            new_cols.append(new_col_with_suffix)\n",
    "        else:\n",
    "            counts[new_col] = 1\n",
    "            new_cols.append(new_col)\n",
    "    return new_cols\n",
    "\n",
    "def post_process_columns(df, log_file):\n",
    "    \"\"\"\n",
    "    Post-processes DataFrame column names:\n",
    "      1. For any column named 'unnamed_#' (or empty), look at its first non-empty cell.\n",
    "         If that cell is not a dollar amount (i.e. does not match /^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$/)\n",
    "         and it contains 2 or 3 words, then rename the column to that value (after cleaning).\n",
    "         If a column already exists with that name, merge the data from the renamed column into the\n",
    "         existing column and drop the renamed column.\n",
    "      2. If a column is named \"Needed For\", then rename it to \"description\" (merging with an existing\n",
    "         description column if necessary).\n",
    "    \"\"\"\n",
    "    # Process unnamed columns.\n",
    "    for col in list(df.columns):\n",
    "        if col.lower().startswith(\"unnamed_\") or col.strip() == \"\":\n",
    "            # Find the first non-empty cell in this column.\n",
    "            first_non_empty = None\n",
    "            for val in df[col]:\n",
    "                cell_val = \"\"\n",
    "                if isinstance(val, str):\n",
    "                    cell_val = val.strip()\n",
    "                elif val is not None:\n",
    "                    cell_val = str(val).strip()\n",
    "                if cell_val:\n",
    "                    first_non_empty = cell_val\n",
    "                    break\n",
    "            if first_non_empty:\n",
    "                # Check if the value is a dollar amount.\n",
    "                if not re.match(r\"^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$\", first_non_empty):\n",
    "                    words = first_non_empty.split()\n",
    "                    if 2 <= len(words) <= 3:\n",
    "                        # Clean the candidate name.\n",
    "                        new_name = clean_column_headers([first_non_empty])[0]\n",
    "                        log_file.write(f\"Renaming column '{col}' to '{new_name}' based on first non-empty value '{first_non_empty}'.\\n\")\n",
    "                        if new_name in df.columns and new_name != col:\n",
    "                            # Merge the two columns: fill empty cells in existing new_name from the renamed col.\n",
    "                            for idx in df.index:\n",
    "                                existing_val = df.at[idx, new_name]\n",
    "                                candidate_val = df.at[idx, col]\n",
    "                                if (pd.isna(existing_val) or existing_val == \"\") and (not pd.isna(candidate_val) and candidate_val != \"\"):\n",
    "                                    df.at[idx, new_name] = candidate_val\n",
    "                            df.drop(columns=[col], inplace=True)\n",
    "                        else:\n",
    "                            df.rename(columns={col: new_name}, inplace=True)\n",
    "    # Process \"Needed For\" column: rename or merge it into \"description\".\n",
    "    if \"Needed For\" in df.columns:\n",
    "        if \"description\" in df.columns:\n",
    "            log_file.write(\"Merging 'Needed For' column into existing 'description' column.\\n\")\n",
    "            for idx in df.index:\n",
    "                desc_val = df.at[idx, \"description\"]\n",
    "                needed_for_val = df.at[idx, \"Needed For\"]\n",
    "                if (pd.isna(desc_val) or desc_val == \"\") and (not pd.isna(needed_for_val) and needed_for_val != \"\"):\n",
    "                    df.at[idx, \"description\"] = needed_for_val\n",
    "            df.drop(columns=[\"Needed For\"], inplace=True)\n",
    "        else:\n",
    "            log_file.write(\"Renaming 'Needed For' column to 'description'.\\n\")\n",
    "            df.rename(columns={\"Needed For\": \"description\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def extract_table3(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 3 data and also tables under Attachment 1/Attachment 2 from the provided PDF.\n",
    "    Implements:\n",
    "      1. Refinement of table title when Attachment 1/2 is found by searching the bounding box above the table.\n",
    "      2. Renaming of duplicate/empty columns (using fix_column_names) and then post-processing\n",
    "         unnamed columns as described.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 3 / Attachment extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain either Table 3 patterns or Attachment 1/Attachment 2.\n",
    "            table3_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*3[-.]([1-3])\\b\", text, re.IGNORECASE):\n",
    "                    #re.search(r\"Attachment\\s*[12]\", text, re.IGNORECASE)):\n",
    "                    table3_pages.append(i)\n",
    "            if not table3_pages:\n",
    "                print(\"No Table 3  found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "            first_page = table3_pages[0]\n",
    "            last_page = table3_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Candidate pages start on {scrape_start + 1} and end on {scrape_end}\", file=log_file)\n",
    "            # Process each page that might contain table data.\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                # This variable keeps track of the bottom y-coordinate of the previous table on the page.\n",
    "                previous_table_bottom = None\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "                    table_bbox = table.bbox  # (x0, top, x1, bottom)\n",
    "                    # Define the title region for the table: above the table bounding box.\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*3[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(2).strip()\n",
    "                                break\n",
    "                        \n",
    "                  \n",
    "                    # Extract the specific phrase using the refined table title.\n",
    "                    if table_title:\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New table detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        # Rename header 'type' to 'type of upgrade' if needed.\n",
    "                        if \"type\" in headers and \"type of upgrade\" not in headers:\n",
    "                            headers = [(\"type of upgrade\" if h == \"type\" else h) for h in headers]\n",
    "                        if \"need for\" in headers:\n",
    "                            headers = [(\"description\" if h == \"need for\" else h) for h in headers]  \n",
    "                    \n",
    "                        # Apply the duplicate/empty column fixing.\n",
    "                        headers = fix_column_names(headers)\n",
    "                        data_rows = tab[1:]\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        if \"allocated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"allocated\"], inplace=True)\n",
    "                            print(f\"Dropped 'Max of' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate x \" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate x \"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file) \n",
    "\n",
    "                        #if \"3339615 9\" in df_new.columns:\n",
    "                         #   df_new.drop(columns=[\"3339615 9\"], inplace=True)\n",
    "                         #   print(f\"Dropped '3339615 9' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)     \n",
    "                            \n",
    "                        if \"6 steady state reliability and posttransient voltage stability\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"6 steady state reliability and posttransient voltage stability\"], inplace=True)\n",
    "                            print(f\"Dropped '6 steady state reliability and posttransient voltage stability' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)  \n",
    "\n",
    "\n",
    "\n",
    "                        if \"escalated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"escalated\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)    \n",
    "\n",
    "\n",
    "                        # Also, if the DataFrame has a column named \"type\" (and not already \"type of upgrade\"), rename it.\n",
    "                        if 'type' in df_new.columns and 'type of upgrade' not in df_new.columns:\n",
    "                            df_new.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        # Special handling for ADNU tables if needed.\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        # Fix duplicate and unnamed columns in the new table.\n",
    "                        df_new.columns = fix_column_names(df_new.columns.tolist())\n",
    "                        # Now apply the post-processing of column names:\n",
    "                        df_new = post_process_columns(df_new, log_file)\n",
    "                        if \"cost rate x \" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate x \"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file) \n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation table branch.\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous table title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"Continuation table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "                        # Use the number of columns from the last extracted table as expected.\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            data_rows = data_rows[1:]\n",
    "                        # Ensure every row has the same length as expected_headers.\n",
    "                        adjust_rows_length(data_rows, expected_headers)\n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "                        # Rename column 'type' if needed.\n",
    "                        if 'type' in df_continuation.columns and 'type of upgrade' not in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        if \"need for\" in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={\"need for\": \"description\"}, inplace=True)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        # Fix duplicate and unnamed columns in the continuation table.\n",
    "                        df_continuation.columns = fix_column_names(df_continuation.columns.tolist())\n",
    "                        # Post-process the columns in the continuation table.\n",
    "                        df_continuation = post_process_columns(df_continuation, log_file)\n",
    "                        # Concatenate the continuation table with the previous extracted table.\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                    # Update the previous_table_bottom for the page using the current table's bbox.\n",
    "                    previous_table_bottom = table_bbox[3]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 3 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "        print(\"\\nConcatenating all extracted Table 3/Attachment data...\", file=log_file)\n",
    "        try:\n",
    "            table3_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table3_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 3/Attachment data extracted.\", file=log_file)\n",
    "        table3_data = pd.DataFrame()\n",
    "    return table3_data\n",
    "\n",
    "\n",
    "def extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 3 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table3_data = extract_table3(pdf_path, log_file, is_addendum)\n",
    "    if table3_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        overlapping_columns = base_data.columns.intersection(table3_data.columns).difference(['point_of_interconnection'])\n",
    "        table3_data = table3_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        base_data_repeated = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "        try:\n",
    "\n",
    "                        # Concatenate base data with Table 8 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "           # if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "           #     merged_df[\"is_duplicate\"] = merged_df.duplicated(subset=[\"q_id\", \"type of upgrade\", \"upgrade\"], keep=\"first\")\n",
    "            #    merged_df = merged_df[merged_df[\"is_duplicate\"] == False].drop(columns=[\"is_duplicate\"])\n",
    "            #    print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade'.\", file=log_file)\n",
    "\n",
    "\n",
    "            if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "                # Identify rows where 'type of upgrade' and 'upgrade' are not empty\n",
    "                non_empty_rows = merged_df[\n",
    "                    merged_df[\"type of upgrade\"].notna() & merged_df[\"upgrade\"].notna() &\n",
    "                    (merged_df[\"type of upgrade\"].str.strip() != \"\") & (merged_df[\"upgrade\"].str.strip() != \"\")\n",
    "                ]\n",
    "\n",
    "                # Group by q_id, type of upgrade, and upgrade, keeping the first occurrence\n",
    "                grouped_df = non_empty_rows.groupby([\"q_id\", \"type of upgrade\", \"upgrade\"], as_index=False).first()\n",
    "\n",
    "                # Get the original order of the rows in merged_df before filtering\n",
    "                merged_df[\"original_index\"] = merged_df.index\n",
    "\n",
    "                # Combine unique grouped rows with originally empty rows\n",
    "                final_df = pd.concat([\n",
    "                    grouped_df,\n",
    "                    merged_df[merged_df[\"type of upgrade\"].isna() | (merged_df[\"type of upgrade\"].str.strip() == \"\") |\n",
    "                            merged_df[\"upgrade\"].isna() | (merged_df[\"upgrade\"].str.strip() == \"\")]\n",
    "                ], ignore_index=True, sort=False)\n",
    "\n",
    "                # Restore the original order of the rows based on the saved index\n",
    "                final_df.sort_values(by=\"original_index\", inplace=True)\n",
    "                final_df.drop(columns=[\"original_index\"], inplace=True)\n",
    "                merged_df = final_df\n",
    "\n",
    "                print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade', excluding empty rows while preserving order.\", file=log_file)\n",
    "\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            print(f\"Merged base data with Table 3 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 3 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data\n",
    "\n",
    "def check_has_table3(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 3 or Attachment 1/Attachment 2.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*3[-.]?\\d*\", text, re.IGNORECASE): \n",
    "                    #re.search(r\"Attachment\\s*[12]\", text, re.IGNORECASE))\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            return \"Addendum\" in text\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "        text = clean_string_cell(text)\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "        point_of_interconnection = extract_table2(pdf_path, log_file)\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "    df = df.map(clean_string_cell)\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "            project_scraped = False\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    try:\n",
    "                        has_table3 = check_has_table3(pdf_path)\n",
    "                        if not has_table3:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\", file=log_file)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "                        if is_add and base_data_extracted:\n",
    "                            table3_data = extract_table3(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table3_data.empty:\n",
    "                                merged_df = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table3_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            df = extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create clean and itemized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 877\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 885\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Processing q_id: 894\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 898\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 899\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 900\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Processing q_id: 901\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Processing q_id: 910\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 916\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 917\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 921\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "     q_id  cluster req_deliverability   latitude  longitude  capacity  \\\n",
      "0    877        5               None        NaN        NaN     280.0   \n",
      "1    877        5               None        NaN        NaN     280.0   \n",
      "2    877        5               None        NaN        NaN     280.0   \n",
      "3    885        5               None  35.120000 -119.24000       NaN   \n",
      "4    885        5               None  35.120000 -119.24000       NaN   \n",
      "5    894        5               None        NaN        NaN     310.0   \n",
      "6    894        5               None        NaN        NaN     310.0   \n",
      "7    894        5               None        NaN        NaN     310.0   \n",
      "8    894        5               None        NaN        NaN     310.0   \n",
      "9    898        5               None  35.567827  119.20285      45.0   \n",
      "10   898        5               None  35.567827  119.20285      45.0   \n",
      "11   898        5               None  35.567827  119.20285      45.0   \n",
      "12   899        5               None        NaN        NaN       NaN   \n",
      "13   899        5               None        NaN        NaN       NaN   \n",
      "14   899        5               None        NaN        NaN       NaN   \n",
      "15   900        5               None        NaN        NaN      40.0   \n",
      "16   900        5               None        NaN        NaN      40.0   \n",
      "17   900        5               None        NaN        NaN      40.0   \n",
      "18   901        5               None        NaN        NaN      15.0   \n",
      "19   901        5               None        NaN        NaN      15.0   \n",
      "20   901        5               None        NaN        NaN      15.0   \n",
      "21   910        5               None        NaN        NaN       NaN   \n",
      "22   910        5               None        NaN        NaN       NaN   \n",
      "23   916        5               None        NaN        NaN      20.0   \n",
      "24   916        5               None        NaN        NaN      20.0   \n",
      "25   916        5               None        NaN        NaN      20.0   \n",
      "26   916        5               None        NaN        NaN      20.0   \n",
      "27   917        5               None        NaN        NaN      20.0   \n",
      "28   917        5               None        NaN        NaN      20.0   \n",
      "29   917        5               None        NaN        NaN      20.0   \n",
      "30   917        5               None        NaN        NaN      20.0   \n",
      "31   921        5               None        NaN        NaN      40.0   \n",
      "32   921        5               None        NaN        NaN      40.0   \n",
      "33   921        5               None        NaN        NaN      40.0   \n",
      "34   921        5               None        NaN        NaN      40.0   \n",
      "\n",
      "                             point_of_interconnection type_of_upgrade upgrade  \\\n",
      "0                       Morro Bay  Gates 230 kV lines      Total ADNU           \n",
      "1                       Morro Bay  Gates 230 kV lines    Total PTO_IF           \n",
      "2                       Morro Bay  Gates 230 kV lines       Total RNU           \n",
      "3                               Copus-Old River 70 kV      Total ADNU           \n",
      "4                               Copus-Old River 70 kV    Total PTO_IF           \n",
      "5            PG&Es Table Mountain-Rio Oso 230 kV Line      Total ADNU           \n",
      "6            PG&Es Table Mountain-Rio Oso 230 kV Line      Total LDNU           \n",
      "7            PG&Es Table Mountain-Rio Oso 230 kV Line    Total PTO_IF           \n",
      "8            PG&Es Table Mountain-Rio Oso 230 kV Line       Total RNU           \n",
      "9                            Famoso 115 kV Substation      Total ADNU           \n",
      "10                           Famoso 115 kV Substation    Total PTO_IF           \n",
      "11                           Famoso 115 kV Substation       Total RNU           \n",
      "12  PG&Es Vaca Dixon-Delevan # 1 and Vaca Dixon- C...      Total ADNU           \n",
      "13  PG&Es Vaca Dixon-Delevan # 1 and Vaca Dixon- C...    Total PTO_IF           \n",
      "14  PG&Es Vaca Dixon-Delevan # 1 and Vaca Dixon- C...       Total RNU           \n",
      "15              Midway- Temblor 115 KV circuit #1 tap      Total ADNU           \n",
      "16              Midway- Temblor 115 KV circuit #1 tap      Total LDNU           \n",
      "17              Midway- Temblor 115 KV circuit #1 tap    Total PTO_IF           \n",
      "18                 Semitropic - Midway 115 kV line #2      Total ADNU           \n",
      "19                 Semitropic - Midway 115 kV line #2      Total LDNU           \n",
      "20                 Semitropic - Midway 115 kV line #2    Total PTO_IF           \n",
      "21                     PG&Es Westwood 60 kV Sustation      Total ADNU           \n",
      "22                     PG&Es Westwood 60 kV Sustation       Total RNU           \n",
      "23  Henrietta-GWF 115 kV Line at Leprino Jct Switc...      Total ADNU           \n",
      "24  Henrietta-GWF 115 kV Line at Leprino Jct Switc...      Total LDNU           \n",
      "25  Henrietta-GWF 115 kV Line at Leprino Jct Switc...    Total PTO_IF           \n",
      "26  Henrietta-GWF 115 kV Line at Leprino Jct Switc...       Total RNU           \n",
      "27  Henrietta-GWF 115 kV Line at Leprino Jct Switc...      Total ADNU           \n",
      "28  Henrietta-GWF 115 kV Line at Leprino Jct Switc...      Total LDNU           \n",
      "29  Henrietta-GWF 115 kV Line at Leprino Jct Switc...    Total PTO_IF           \n",
      "30  Henrietta-GWF 115 kV Line at Leprino Jct Switc...       Total RNU           \n",
      "31                        Alpaugh - Smyrna 115kV Line      Total ADNU           \n",
      "32                        Alpaugh - Smyrna 115kV Line      Total LDNU           \n",
      "33                        Alpaugh - Smyrna 115kV Line    Total PTO_IF           \n",
      "34                        Alpaugh - Smyrna 115kV Line       Total RNU           \n",
      "\n",
      "   description cost_allocation_factor  estimated_cost_x_1000  \\\n",
      "0                                                        0.0   \n",
      "1                                                     2299.0   \n",
      "2                                                    26024.0   \n",
      "3                                                        0.0   \n",
      "4                                                      409.0   \n",
      "5                                                        0.0   \n",
      "6                                                     5849.0   \n",
      "7                                                     1911.0   \n",
      "8                                                    30908.0   \n",
      "9                                                        0.0   \n",
      "10                                                    2249.0   \n",
      "11                                                   43316.0   \n",
      "12                                                       0.0   \n",
      "13                                                    1498.0   \n",
      "14                                                   35004.0   \n",
      "15                                                       0.0   \n",
      "16                                                   11632.0   \n",
      "17                                                     409.0   \n",
      "18                                                       0.0   \n",
      "19                                                    4695.0   \n",
      "20                                                     719.0   \n",
      "21                                                       0.0   \n",
      "22                                                     700.0   \n",
      "23                                                       0.0   \n",
      "24                                                   25590.0   \n",
      "25                                                    1831.0   \n",
      "26                                                    1289.0   \n",
      "27                                                       0.0   \n",
      "28                                                   25590.0   \n",
      "29                                                    1831.0   \n",
      "30                                                    1289.0   \n",
      "31                                                       0.0   \n",
      "32                                                   50653.0   \n",
      "33                                                    3519.0   \n",
      "34                                                   17180.0   \n",
      "\n",
      "    escalated_cost_x_1000 estimated_time_to_construct item  \n",
      "0               229779.20                               no  \n",
      "1                 2655.00                               no  \n",
      "2                30193.00                               no  \n",
      "3                 9847.68                               no  \n",
      "4                  439.27                               no  \n",
      "5                    0.00                               no  \n",
      "6                 6609.00                               no  \n",
      "7                 2207.00                               no  \n",
      "8                35369.00                               no  \n",
      "9                36928.80                               no  \n",
      "10                2599.00                               no  \n",
      "11               50327.00                               no  \n",
      "12               77179.05                               no  \n",
      "13                1730.00                               no  \n",
      "14               40224.00                               no  \n",
      "15               32825.60                               no  \n",
      "16               13435.00                               no  \n",
      "17                 439.00                               no  \n",
      "18               12309.60                               no  \n",
      "19                5595.00                               no  \n",
      "20                 792.00                               no  \n",
      "21               56407.40                               no  \n",
      "22                 771.00                               no  \n",
      "23               16412.80                               no  \n",
      "24               33056.00                               no  \n",
      "25                2017.00                               no  \n",
      "26                1451.00                               no  \n",
      "27               16412.80                               no  \n",
      "28               33056.00                               no  \n",
      "29                2017.00                               no  \n",
      "30                1451.00                               no  \n",
      "31               32825.60                               no  \n",
      "32               60941.00                               no  \n",
      "33                4065.00                               no  \n",
      "34               19844.00                               no  \n",
      "Itemized rows saved to 'costs_phase_1_cluster_5_style_D_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_1_cluster_5_style_D_total.csv'.\n",
      "['PTO_IF' 'RNU' 'ADNU' 'LDNU']\n",
      "[877 885 894 898 899 900 901 910 916 917 921]\n",
      "[5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_65073/3529828934.py:567: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/rawdata_cluster5_style_D_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'cost allocation factor' in df.columns:\n",
    "#    df['description'] = df['cost allocation factor'].apply(extract_non_numeric_text)\n",
    "#    df['cost_allocation_factor'] = df['cost allocation factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "            \"unnamed_1\",\n",
    "            ],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"MW\",\n",
    "            \"unnamed_9\",\n",
    "            \"mw\",\n",
    "        ],   \n",
    "\n",
    "\n",
    "\n",
    "        \"estimated_cost_x_1000\": [\n",
    "\n",
    "            \"estimated cost x 1000\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"allocated_cost\",\n",
    "            \"assigned cost\",\n",
    "            \"allocated cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "            \"estimated cost x 1000 constant dollar\",\n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"escalated cost x 1000 constant dollar\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"assigned cost escalated\",\n",
    "            \"unnamed_17\",\n",
    "            \"unnamed_18\",\n",
    "            \"3339615 9\"\n",
    "             \n",
    "\n",
    "        ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "            \n",
    "\n",
    "\n",
    "        ],\n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "       \n",
    "         \n",
    "\n",
    "        \"description\": [\"description\", \"unnamed_7\"],\n",
    "\n",
    "        \n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "            \"project allocation\",\n",
    "\n",
    "        ],\n",
    "       \n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "#if 'upgrade' in df.columns:\n",
    " #   df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    "\n",
    "#df.drop(['unnamed_3', 'unnamed_15', 'unnamed_18', 'unnamed_16', 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "df.drop(['unnamed_2','unnamed_3', 'unnamed_4','unnamed_5','unnamed_6','unnamed_10', 'unnamed_11','unnamed_12','unnamed_13','unnamed_14', 'unnamed_15','unnamed_16', \n",
    "           'unnamed_19', 'unnamed_20', 'cost rate x 1000mw constant dollar', 'needed for',\n",
    "         'cost rate x'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "df['tot'] = df.apply(\n",
    "    lambda row: 'yes' if (\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor'])) \n",
    "        ) else 'no',\n",
    "    axis=1\n",
    ") \n",
    "\n",
    "# Now extract ONLY \"Total\" rows with a foolproof match\n",
    "total_rows_df = df[df['tot'] == 'yes']\n",
    "\n",
    "total_rows_df = total_rows_df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "total_rows_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_D_total_network.csv', index=False) \n",
    "df = df[df['cost_allocation_factor'].str.strip().str.lower() != 'total']\n",
    " \n",
    "df.drop('tot', axis=1, inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)  \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    \"PTO\u2019s Interconnection Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTO\u2019s Interconnectio n Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTOs Interconnection Facilities\": \"PTO_IF\",\n",
    "    \"PTOs Interconnectio n Facilities\": \"PTO_IF\",\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Reliability Network Upgrades\": \"RNU\",\n",
    "    \"Local Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Area Deliverability Upgrades\": \"ADNU\",\n",
    "    \"Escalated Cost and Time to Construct for Interconnection Facilities, Reliability Network Upgrades, and Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Distribution\": \"ADNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    "}\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()    \n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/cluster_5_style_D.csv', index=False)\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "#df = stable_sort_by_type_of_upgrade(df)  \n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000',]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000',]\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ( (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "#df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_D_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_D_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_1_cluster_5_style_D_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_1_cluster_5_style_D_total.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n",
    "\n",
    "\n",
    "#df.to_csv('Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 14/03_raw/rawdata_cluster14_style_Q.csv')\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded itemized data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_D_itemized.csv\n",
      "Loaded totals data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_D_total.csv\n",
      "Loaded total network data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_D_total_network.csv\n",
      "\n",
      "Q_ids with missing upgrades:\n",
      "  Q_id 877 is missing upgrades: LDNU, CANU, OPNU\n",
      "  Q_id 885 is missing upgrades: RNU, LDNU, CANU, OPNU\n",
      "  Q_id 894 is missing upgrades: CANU, OPNU\n",
      "  Q_id 898 is missing upgrades: LDNU, CANU, OPNU\n",
      "  Q_id 899 is missing upgrades: LDNU, CANU, OPNU\n",
      "  Q_id 900 is missing upgrades: RNU, CANU, OPNU\n",
      "  Q_id 901 is missing upgrades: RNU, CANU, OPNU\n",
      "  Q_id 910 is missing upgrades: PTO_IF, LDNU, CANU, OPNU\n",
      "  Q_id 916 is missing upgrades: CANU, OPNU\n",
      "  Q_id 917 is missing upgrades: CANU, OPNU\n",
      "  Q_id 921 is missing upgrades: CANU, OPNU\n",
      "\n",
      "No type of upgrade is repeated for any Q_id.\n",
      "\n",
      "Mismatches saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 5/mismatches.csv'.\n",
      "\n",
      "\ud83d\udd0d **ADNU Comparison Results:**\n",
      "\u274c Mismatch in ADNU for Q_ID 877:\n",
      "   - Total Network (ADNU): 0\n",
      "   - Totals Dataset (ADNU): 229779.2\n",
      "   - Difference: 229779.2\n",
      "\n",
      "\u274c Mismatch in ADNU for Q_ID 885:\n",
      "   - Total Network (ADNU): 0\n",
      "   - Totals Dataset (ADNU): 9847.68\n",
      "   - Difference: 9847.68\n",
      "\n",
      "\u2705 Match for Q_ID 894: 0 (Total Network) == 0.0 (Totals)\n",
      "\u274c Mismatch in ADNU for Q_ID 898:\n",
      "   - Total Network (ADNU): 0\n",
      "   - Totals Dataset (ADNU): 36928.8\n",
      "   - Difference: 36928.8\n",
      "\n",
      "\u274c Mismatch in ADNU for Q_ID 899:\n",
      "   - Total Network (ADNU): 0\n",
      "   - Totals Dataset (ADNU): 77179.05\n",
      "   - Difference: 77179.05\n",
      "\n",
      "\u274c Mismatch in ADNU for Q_ID 900:\n",
      "   - Total Network (ADNU): 0\n",
      "   - Totals Dataset (ADNU): 32825.6\n",
      "   - Difference: 32825.6\n",
      "\n",
      "\u274c Mismatch in ADNU for Q_ID 901:\n",
      "   - Total Network (ADNU): 0\n",
      "   - Totals Dataset (ADNU): 12309.6\n",
      "   - Difference: 12309.6\n",
      "\n",
      "\u274c Mismatch in ADNU for Q_ID 910:\n",
      "   - Total Network (ADNU): 0\n",
      "   - Totals Dataset (ADNU): 56407.399999999994\n",
      "   - Difference: 56407.399999999994\n",
      "\n",
      "\u274c Mismatch in ADNU for Q_ID 916:\n",
      "   - Total Network (ADNU): 0\n",
      "   - Totals Dataset (ADNU): 16412.8\n",
      "   - Difference: 16412.8\n",
      "\n",
      "\u274c Mismatch in ADNU for Q_ID 917:\n",
      "   - Total Network (ADNU): 0\n",
      "   - Totals Dataset (ADNU): 16412.8\n",
      "   - Difference: 16412.8\n",
      "\n",
      "\u274c Mismatch in ADNU for Q_ID 921:\n",
      "   - Total Network (ADNU): 0\n",
      "   - Totals Dataset (ADNU): 32825.6\n",
      "   - Difference: 32825.6\n",
      "\n",
      "\n",
      "\ud83d\udd0d **RNU Comparison Results:**\n",
      "\u274c Mismatch in RNU-related for Q_ID 877:\n",
      "   - Total Network (RNU): 0\n",
      "   - Sum of RNU, LDNU, PNU, OPNU from Totals: 26024.0\n",
      "   - Difference: 26024.0\n",
      "\n",
      "\u274c Mismatch in RNU-related for Q_ID 894:\n",
      "   - Total Network (RNU): 0\n",
      "   - Sum of RNU, LDNU, PNU, OPNU from Totals: 36757.0\n",
      "   - Difference: 36757.0\n",
      "\n",
      "\u274c Mismatch in RNU-related for Q_ID 898:\n",
      "   - Total Network (RNU): 0\n",
      "   - Sum of RNU, LDNU, PNU, OPNU from Totals: 43316.0\n",
      "   - Difference: 43316.0\n",
      "\n",
      "\u274c Mismatch in RNU-related for Q_ID 899:\n",
      "   - Total Network (RNU): 0\n",
      "   - Sum of RNU, LDNU, PNU, OPNU from Totals: 35004.0\n",
      "   - Difference: 35004.0\n",
      "\n",
      "\u274c Mismatch in RNU-related for Q_ID 900:\n",
      "   - Total Network (RNU): 0\n",
      "   - Sum of RNU, LDNU, PNU, OPNU from Totals: 11632.0\n",
      "   - Difference: 11632.0\n",
      "\n",
      "\u274c Mismatch in RNU-related for Q_ID 901:\n",
      "   - Total Network (RNU): 0\n",
      "   - Sum of RNU, LDNU, PNU, OPNU from Totals: 4695.0\n",
      "   - Difference: 4695.0\n",
      "\n",
      "\u274c Mismatch in RNU-related for Q_ID 910:\n",
      "   - Total Network (RNU): 0\n",
      "   - Sum of RNU, LDNU, PNU, OPNU from Totals: 700.0\n",
      "   - Difference: 700.0\n",
      "\n",
      "\u274c Mismatch in RNU-related for Q_ID 916:\n",
      "   - Total Network (RNU): 0\n",
      "   - Sum of RNU, LDNU, PNU, OPNU from Totals: 26879.0\n",
      "   - Difference: 26879.0\n",
      "\n",
      "\u274c Mismatch in RNU-related for Q_ID 917:\n",
      "   - Total Network (RNU): 0\n",
      "   - Sum of RNU, LDNU, PNU, OPNU from Totals: 26879.0\n",
      "   - Difference: 26879.0\n",
      "\n",
      "\u274c Mismatch in RNU-related for Q_ID 921:\n",
      "   - Total Network (RNU): 0\n",
      "   - Sum of RNU, LDNU, PNU, OPNU from Totals: 67833.0\n",
      "   - Difference: 67833.0\n",
      "\n",
      "\n",
      "\u2705 ADNU and RNU mismatch checks completed.\n",
      "\n",
      "Direct Matches (Exact Point of Interconnection Names):\n",
      "                            point_of_interconnection        q_id\n",
      "3  HENRIETTA-GWF 115 KV LINE AT LEPRINO JCT SWITC...  [916, 917]\n",
      "\n",
      "Fuzzy Matches (>=80% Similarity in Point of Interconnection):\n",
      "No fuzzy matches found.\n",
      "Matched Q_ids saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 5/matched_qids.csv'.\n",
      "\n",
      "Total checks performed: 35\n",
      "Total mismatches found: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "# Paths to the CSV files\n",
    "ITEMIZED_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_D_itemized.csv'\n",
    "TOTALS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_D_total.csv'\n",
    "TOTAL_NETWORK_CSV_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_D_total_network.csv\"\n",
    "\n",
    "\n",
    "# Columns in totals_df that hold the reported total costs\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "# Upgrade types to check\n",
    "REQUIRED_UPGRADES = ['PTO_IF', 'RNU', 'LDNU', 'ADNU', 'CANU', \"OPNU\"]\n",
    "\n",
    "# Output paths\n",
    "MISMATCHES_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/mismatches.csv'\n",
    "MATCHED_QIDS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/matched_qids.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "def load_csv(path, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {dataset_name} from {path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Load datasets\n",
    "itemized_df = load_csv(ITEMIZED_CSV_PATH, \"itemized data\")\n",
    "totals_df = load_csv(TOTALS_CSV_PATH, \"totals data\")\n",
    "total_network_df = load_csv(TOTAL_NETWORK_CSV_PATH, \"total network data\")\n",
    "\n",
    "# ---------------------- Data Cleaning ---------------------- #\n",
    "\n",
    "def clean_text(df, column):\n",
    "    \"\"\"\n",
    "    Cleans text data by stripping leading/trailing spaces and converting to uppercase.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        df[column] = df[column].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        print(f\"Warning: '{column}' column is missing in the dataset. Filling with 'UNKNOWN'.\")\n",
    "        df[column] = 'UNKNOWN'\n",
    "    return df\n",
    "\n",
    "# Clean 'type_of_upgrade' and 'point_of_interconnection' in both datasets\n",
    "itemized_df = clean_text(itemized_df, 'type_of_upgrade')\n",
    "itemized_df = clean_text(itemized_df, 'point_of_interconnection')\n",
    "\n",
    "totals_df = clean_text(totals_df, 'type_of_upgrade')\n",
    "totals_df = clean_text(totals_df, 'point_of_interconnection')\n",
    "\n",
    "# ---------------------- Data Preparation ---------------------- #\n",
    "\n",
    "# Ensure necessary columns exist in itemized_df\n",
    "required_itemized_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', 'estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in required_itemized_columns:\n",
    "    if col not in itemized_df.columns:\n",
    "        print(f\"Warning: '{col}' column is missing in the itemized dataset.\")\n",
    "        if col in ['q_id', 'type_of_upgrade', 'point_of_interconnection']:\n",
    "            itemized_df[col] = 'UNKNOWN'\n",
    "        else:\n",
    "            itemized_df[col] = 0\n",
    "\n",
    "# Ensure necessary columns exist in totals_df\n",
    "required_totals_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in required_totals_columns:\n",
    "    if col not in totals_df.columns:\n",
    "        print(f\"Error: '{col}' column is missing in the totals dataset. Cannot proceed.\")\n",
    "        exit(1)\n",
    "\n",
    "# Convert cost columns to numeric, coercing errors to NaN and filling with 0\n",
    "cost_columns_itemized = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in cost_columns_itemized:\n",
    "    itemized_df[col] = pd.to_numeric(itemized_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "cost_columns_totals = [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in cost_columns_totals:\n",
    "    totals_df[col] = pd.to_numeric(totals_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "cost_columns_network = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in cost_columns_network:\n",
    "    total_network_df[col] = pd.to_numeric(total_network_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- Calculate Manual Totals ---------------------- #\n",
    "\n",
    "# Group itemized data by q_id and type_of_upgrade and calculate sums\n",
    "itemized_grouped = itemized_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    'estimated_cost_x_1000': 'sum',\n",
    "    'escalated_cost_x_1000': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "itemized_grouped['manual_total'] = itemized_grouped.apply(\n",
    "    lambda row: row['estimated_cost_x_1000'] if row['estimated_cost_x_1000'] > 0 else row['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Prepare Totals Data ---------------------- #\n",
    "\n",
    "# Group totals data by q_id and type_of_upgrade and calculate sums\n",
    "totals_grouped = totals_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "    TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "totals_grouped['reported_total'] = totals_grouped.apply(\n",
    "    lambda row: row[TOTALS_ESTIMATED_COLUMN] if row[TOTALS_ESTIMATED_COLUMN] > 0 else row[TOTALS_ESCALATED_COLUMN],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Merge Data ---------------------- #\n",
    "\n",
    "# Merge the itemized and totals data on q_id and type_of_upgrade\n",
    "comparison_df = pd.merge(\n",
    "    itemized_grouped,\n",
    "    totals_grouped[['q_id', 'type_of_upgrade', 'reported_total']],\n",
    "    on=['q_id', 'type_of_upgrade'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---------------------- Check for Missing Upgrades ---------------------- #\n",
    "\n",
    "# Identify q_ids that are missing any of the required upgrades\n",
    "missing_upgrades_report = []\n",
    "for q_id in comparison_df['q_id'].unique():\n",
    "    upgrades_present = comparison_df[comparison_df['q_id'] == q_id]['type_of_upgrade'].unique()\n",
    "    missing_upgrades = [upgrade for upgrade in REQUIRED_UPGRADES if upgrade not in upgrades_present]\n",
    "    if missing_upgrades:\n",
    "        missing_upgrades_report.append((q_id, missing_upgrades))\n",
    "\n",
    "# Report missing upgrades\n",
    "if missing_upgrades_report:\n",
    "    print(\"\\nQ_ids with missing upgrades:\")\n",
    "    for q_id, missing in missing_upgrades_report:\n",
    "        print(f\"  Q_id {q_id} is missing upgrades: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nAll q_ids have all required upgrades.\")\n",
    "\n",
    "\n",
    "# ------------------------ Check for no duplicates in type of upgrade in total data ------------------------ #\n",
    "\n",
    " \n",
    "\n",
    "# Identify duplicates by grouping by q_id and type_of_upgrade\n",
    "duplicates = totals_df[totals_df.duplicated(subset=['q_id', 'type_of_upgrade'], keep=False)]\n",
    "\n",
    "if not duplicates.empty:\n",
    "    print(\"\\nDuplicate upgrade types detected:\")\n",
    "    for q_id, group in duplicates.groupby('q_id'):\n",
    "        upgrade_types = group['type_of_upgrade'].unique()\n",
    "        print(f\"  Q_id {q_id} has repeated upgrade types: {', '.join(upgrade_types)}\")\n",
    "else:\n",
    "    print(\"\\nNo type of upgrade is repeated for any Q_id.\")        \n",
    "\n",
    "# ---------------------- Compare Totals and Identify Mismatches ---------------------- #\n",
    "\n",
    "# Initialize list to store mismatches\n",
    "mismatches = []\n",
    "\n",
    "# Iterate through each row to compare manual_total with reported_total\n",
    "for index, row in comparison_df.iterrows():\n",
    "    q_id = row['q_id']\n",
    "    upgrade = row['type_of_upgrade']\n",
    "    manual_total = row['manual_total']\n",
    "    reported_total = row['reported_total']\n",
    "    \n",
    "    # Determine if both manual_total and reported_total are zero\n",
    "    if manual_total == 0.0 and reported_total == 0.0:\n",
    "        continue  # No mismatch\n",
    "    # Determine if manual_total is zero and reported_total is missing or zero\n",
    "    elif manual_total == 0.0 and (pd.isna(row['reported_total']) or reported_total == 0.0):\n",
    "        continue  # No mismatch\n",
    "    # If reported_total is missing (NaN) and manual_total is not zero\n",
    "    elif pd.isna(row['reported_total']) and manual_total != 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: Missing\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': 'Missing'\n",
    "        })\n",
    "    # If manual_total is not zero and reported_total is zero\n",
    "    elif manual_total != 0.0 and reported_total == 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: 0.0\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # If both totals are non-zero but differ beyond tolerance\n",
    "    elif abs(manual_total - reported_total) > 1e+2:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: {reported_total}\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # Else, totals match; do nothing\n",
    "\n",
    "# Create a DataFrame for mismatches\n",
    "mismatches_df = pd.DataFrame(mismatches, columns=['q_id', 'type_of_upgrade', 'manual_total', 'reported_total'])\n",
    "\n",
    "# Save mismatches to a CSV file\n",
    "try:\n",
    "    mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "    print(f\"\\nMismatches saved to '{MISMATCHES_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving mismatches CSV: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- Compare Total Network vs Totals Data ---------------------- #\n",
    "\n",
    "# ---------------------- Compare Total Network vs Totals Data ---------------------- #\n",
    "\n",
    "# Function to extract summed cost per `q_id` while prioritizing `estimated_cost_x_1000`\n",
    "def get_total_cost(df, upgrade_types):\n",
    "    cost_sums = df[df['type_of_upgrade'].isin(upgrade_types)].groupby('q_id').agg({\n",
    "        'estimated_cost_x_1000': 'sum',\n",
    "        'escalated_cost_x_1000': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Apply logic: Use `estimated_cost_x_1000`, fallback to `escalated_cost_x_1000` if 0\n",
    "    cost_sums['total_cost'] = cost_sums.apply(\n",
    "        lambda row: row['estimated_cost_x_1000'] if row['estimated_cost_x_1000'] > 0 else row['escalated_cost_x_1000'], \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return cost_sums.set_index('q_id')['total_cost']\n",
    "\n",
    "# Extract ADNU from total_network_df\n",
    "total_adnu_network = get_total_cost(total_network_df, ['ADNU'])\n",
    "\n",
    "# Extract ADNU from totals_df\n",
    "total_adnu_totals = get_total_cost(totals_df, ['ADNU'])\n",
    "\n",
    "# Extract RNU, LDNU, PNU, OPNU from totals_df and sum them\n",
    "total_rnu_related = get_total_cost(totals_df, ['RNU', 'LDNU', 'PNU', 'OPNU'])\n",
    "\n",
    "# ---------------------- Loop Over Q_IDs and Extract Values ---------------------- #\n",
    "print(\"\\n\ud83d\udd0d **ADNU Comparison Results:**\")\n",
    "for q_id in total_adnu_totals.index:\n",
    "    # Extract ADNU values from both datasets\n",
    "    adnu_total_sum = total_adnu_totals.get(q_id, 0)\n",
    "\n",
    "    # Explicitly fetch ADNU from total_network_df per q_id\n",
    "    adnu_total_network = (\n",
    "        total_network_df[(total_network_df['q_id'] == q_id) & \n",
    "                         (total_network_df['type_of_upgrade'].str.strip().str.upper() == 'ADNU')]\n",
    "    )['estimated_cost_x_1000']\n",
    "\n",
    "    # Ensure we extract a single value\n",
    "    if not adnu_total_network.empty:\n",
    "        adnu_total_network = adnu_total_network.iloc[0]  # Take first value if multiple exist\n",
    "    else:\n",
    "        adnu_total_network = 0  # Default to 0 if not found\n",
    "\n",
    "    # Compare values\n",
    "    if abs(adnu_total_network - adnu_total_sum) > 10:\n",
    "        print(f\"\u274c Mismatch in ADNU for Q_ID {q_id}:\")\n",
    "        print(f\"   - Total Network (ADNU): {adnu_total_network}\")\n",
    "        print(f\"   - Totals Dataset (ADNU): {adnu_total_sum}\")\n",
    "        print(f\"   - Difference: {abs(adnu_total_network - adnu_total_sum)}\\n\")\n",
    "    else:\n",
    "        print(f\"\u2705 Match for Q_ID {q_id}: {adnu_total_network} (Total Network) == {adnu_total_sum} (Totals)\")\n",
    "\n",
    "# ---------------------- Compare Summed RNU-related Against Total Network RNU ---------------------- #\n",
    "print(\"\\n\ud83d\udd0d **RNU Comparison Results:**\")\n",
    "for q_id in total_rnu_related.index:\n",
    "    rnu_total_sum = total_rnu_related.get(q_id, 0)\n",
    "\n",
    "    # Explicitly fetch RNU from total_network_df per q_id\n",
    "    rnu_total_network = (\n",
    "        total_network_df[(total_network_df['q_id'] == q_id) & \n",
    "                         (total_network_df['type_of_upgrade'].str.strip().str.upper() == 'RNU')]\n",
    "    )['estimated_cost_x_1000']\n",
    "\n",
    "    # Ensure we extract a single value\n",
    "    if not rnu_total_network.empty:\n",
    "        rnu_total_network = rnu_total_network.iloc[0]  # Take first value if multiple exist\n",
    "    else:\n",
    "        rnu_total_network = 0  # Default to 0 if not found\n",
    "\n",
    "    # Compare values\n",
    "    if abs(rnu_total_network - rnu_total_sum) > 10:\n",
    "        print(f\"\u274c Mismatch in RNU-related for Q_ID {q_id}:\")\n",
    "        print(f\"   - Total Network (RNU): {rnu_total_network}\")\n",
    "        print(f\"   - Sum of RNU, LDNU, PNU, OPNU from Totals: {rnu_total_sum}\")\n",
    "        print(f\"   - Difference: {abs(rnu_total_network - rnu_total_sum)}\\n\")\n",
    "    else:\n",
    "        print(f\"\u2705 Match for Q_ID {q_id}: {rnu_total_network} (Total Network) == {rnu_total_sum} (Totals)\")\n",
    "\n",
    "print(\"\\n\u2705 ADNU and RNU mismatch checks completed.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- Point of Interconnection Matching ---------------------- #\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from itemized dataset\n",
    "itemized_poi = itemized_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from totals dataset\n",
    "totals_poi = totals_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Merge both to have a complete list of q_id and point_of_interconnection\n",
    "all_poi = pd.concat([itemized_poi, totals_poi]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ---------------------- Direct Match Identification ---------------------- #\n",
    "\n",
    "# Group by point_of_interconnection to find q_ids sharing the same point_of_interconnection\n",
    "direct_matches = all_poi.groupby('point_of_interconnection')['q_id'].apply(list).reset_index()\n",
    "\n",
    "# Filter groups with more than one q_id (i.e., shared points_of_interconnection)\n",
    "direct_matches = direct_matches[direct_matches['q_id'].apply(len) > 1]\n",
    "\n",
    "print(\"\\nDirect Matches (Exact Point of Interconnection Names):\")\n",
    "if not direct_matches.empty:\n",
    "    print(direct_matches)\n",
    "else:\n",
    "    print(\"No direct matches found.\")\n",
    "\n",
    "# ---------------------- Fuzzy Match Identification ---------------------- #\n",
    "\n",
    "# Prepare list of points_of_interconnection for fuzzy matching\n",
    "poi_list = all_poi['point_of_interconnection'].unique().tolist()\n",
    "\n",
    "# Initialize list to store fuzzy matches\n",
    "fuzzy_matches = []\n",
    "\n",
    "# Iterate through each point_of_interconnection to find similar ones\n",
    "for i, poi in enumerate(poi_list):\n",
    "    # Compare with the rest of the points to avoid redundant comparisons\n",
    "    similar_pois = process.extract(poi, poi_list[i+1:], scorer=fuzz.token_set_ratio)\n",
    "    \n",
    "    # Filter matches with similarity >= 80%\n",
    "    for match_poi, score in similar_pois:\n",
    "        if score >= 80:\n",
    "            # Retrieve q_ids for both points_of_interconnection\n",
    "            qids_poi1 = all_poi[all_poi['point_of_interconnection'] == poi]['q_id'].tolist()\n",
    "            qids_poi2 = all_poi[all_poi['point_of_interconnection'] == match_poi]['q_id'].tolist()\n",
    "            \n",
    "            # Append the matched pairs with their points_of_interconnection and similarity score\n",
    "            fuzzy_matches.append({\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_ids_1': qids_poi1,\n",
    "                'point_of_interconnection_2': match_poi,\n",
    "                'q_ids_2': qids_poi2,\n",
    "                'similarity_score': score\n",
    "            })\n",
    "\n",
    "# Convert fuzzy matches to DataFrame\n",
    "fuzzy_matches_df = pd.DataFrame(fuzzy_matches)\n",
    "\n",
    "print(\"\\nFuzzy Matches (>=80% Similarity in Point of Interconnection):\")\n",
    "if not fuzzy_matches_df.empty:\n",
    "    print(fuzzy_matches_df)\n",
    "else:\n",
    "    print(\"No fuzzy matches found.\")\n",
    "\n",
    "# ---------------------- Save Matched Q_ids to CSV ---------------------- #\n",
    "\n",
    "# For clarity, create a combined DataFrame for direct and fuzzy matches\n",
    "\n",
    "# Direct matches: list each pair of q_ids sharing the same point_of_interconnection\n",
    "direct_matches_expanded = []\n",
    "for _, row in direct_matches.iterrows():\n",
    "    qids = row['q_id']\n",
    "    poi = row['point_of_interconnection']\n",
    "    # Generate all possible unique pairs\n",
    "    for i in range(len(qids)):\n",
    "        for j in range(i+1, len(qids)):\n",
    "            direct_matches_expanded.append({\n",
    "                'match_type': 'Direct',\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_id_1': qids[i],\n",
    "                'point_of_interconnection_2': poi,\n",
    "                'q_id_2': qids[j],\n",
    "                'similarity_score': 100\n",
    "            })\n",
    "\n",
    "# Fuzzy matches: already have pairs\n",
    "fuzzy_matches_expanded = []\n",
    "for _, row in fuzzy_matches_df.iterrows():\n",
    "    fuzzy_matches_expanded.append({\n",
    "        'match_type': 'Fuzzy',\n",
    "        'point_of_interconnection_1': row['point_of_interconnection_1'],\n",
    "        'q_id_1': row['q_ids_1'],\n",
    "        'point_of_interconnection_2': row['point_of_interconnection_2'],\n",
    "        'q_id_2': row['q_ids_2'],\n",
    "        'similarity_score': row['similarity_score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "matched_qids_df = pd.DataFrame(direct_matches_expanded + fuzzy_matches_expanded)\n",
    "\n",
    "# Save matched q_ids to CSV\n",
    "try:\n",
    "    matched_qids_df.to_csv(MATCHED_QIDS_CSV_PATH, index=False)\n",
    "    print(f\"Matched Q_ids saved to '{MATCHED_QIDS_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving matched Q_ids CSV: {e}\")\n",
    "\n",
    "# ---------------------- Summary ---------------------- #\n",
    "\n",
    "# Print a summary\n",
    "total_checked = len(comparison_df)\n",
    "total_mismatches = len(mismatches_df)\n",
    "print(f\"\\nTotal checks performed: {total_checked}\")\n",
    "print(f\"Total mismatches found: {total_mismatches}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}