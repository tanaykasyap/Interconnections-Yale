{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: 11AS763732-C5PIEastQ871AppendixA.pdf from Project 871\n",
      "Scraped PDF: 11AS763774-C5PIMetroQ893AppA.pdf from Project 873\n",
      "Scraped PDF: 12AS779216-Appendix_AQ877_California_Flats_Solar_C5_Ph_I_Study_Report.pdf from Project 877\n",
      "Scraped PDF: 12AS779571-Appendix_AQ885_SKIC_Solar_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf from Project 885\n",
      "Scraped PDF: 12AS780495-QC5PIEOPQ887AppendixA.pdf from Project 887\n",
      "Scraped PDF: 12AS780604-QC5PINOLQ888Appendix_A.pdf from Project 888\n",
      "Scraped PDF: 12AS778778-QC5PINorthernQ890Backus_Solar_EnergyAppendix_A.pdf from Project 890\n",
      "Scraped PDF: 12AS782071-QC5PIEOPQ891AppendixA.pdf from Project 891\n",
      "Scraped PDF: 12AS780639-QC5PIEOPQ892AppendixA.pdf from Project 892\n",
      "Scraped PDF: 12AS780344-C5PIMetroQ893AppA.pdf from Project 893\n",
      "Scraped PDF: 12AS779610-Appendix_AQ894_Prospect_Energy.pdf from Project 894\n",
      "Scraped PDF: 12AS781853-Appendix_A__Q895_1312013.pdf from Project 895\n",
      "Scraped PDF: 12AS781741-Appendix_A__Q896_1312013.pdf from Project 896\n",
      "Scraped PDF: 12AS780679-QC5PINOLQ897Appendix_A.pdf from Project 897\n",
      "Scraped PDF: 12AS780230-Appendix_AQ898_KPP45_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf from Project 898\n",
      "Scraped PDF: 12AS779299-Appendix_AQ899_Dunningan_Hills.pdf from Project 899\n",
      "Scraped PDF: 12AS779494-Appendix_AQ900_Rio_Bravo_Solar_I_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf from Project 900\n",
      "Scraped PDF: 12AS780155-Appendix_AQ901_Wildwood_Solar_II_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf from Project 901\n",
      "Scraped PDF: 12AS781389-C5PIEastQ902AppendixA.pdf from Project 902\n",
      "Scraped PDF: 12AS780940-QC5PINorthernQ903Desert_Butte_Solar_FarmAppendix_A.pdf from Project 903\n",
      "Scraped PDF: 12AS780754-C5PIEastQ904AppendixA.pdf from Project 904\n",
      "Scraped PDF: 12AS781466-QC5PIEOPQ908AppendixA.pdf from Project 908\n",
      "Scraped PDF: 12AS780826-QC5PINOLQ909Appendix_A.pdf from Project 909\n",
      "Scraped PDF: 12AS780268-Appendix_AQ910_Honey_Lake.pdf from Project 910\n",
      "Scraped PDF: 12AS780864-QC5PINorthernQ911Pastoria_Energy_FacilityAppendix_A.pdf from Project 911\n",
      "Scraped PDF: 12AS781352-C5PIEastQ913AppendixA.pdf from Project 913\n",
      "Scraped PDF: 12AS781125-QC5PIEOPQ914AppendixA.pdf from Project 914\n",
      "Scraped PDF: 12AS779532-Appendix_AQ916_Stratford_Solar_C5_Ph_I_Study_Report.pdf from Project 916\n",
      "Scraped PDF: 12AS780118-Appendix_AQ917_Lemoore_Solar_C5_Ph_I_Study_Report.pdf from Project 917\n",
      "Scraped PDF: 12AS781816-Appendix_A__Q919_1312013.pdf from Project 919\n",
      "Scraped PDF: 12AS779648-Appendix_AQ921_SPS_AS_40MW_PV_Generation_Facility_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf from Project 921\n",
      "Scraped PDF: 12AS780381-QC5PINorthernQ922Highwind_Power_ProjectAppendix_A.pdf from Project 922\n",
      "Scraped PDF: 12AS781314-C5PIEastQ923AppendixA.pdf from Project 923\n",
      "Scraped PDF: 12AS781277-QC5PINorthernQ925Tehachapi_SpindleAppendix_A.pdf from Project 925\n",
      "Scraped PDF: 12AS781050-QC5PINorthernQ926Golden_Hills_PowerAppendix_A.pdf from Project 926\n",
      "Scraped PDF: 12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A.pdf from Project 927\n",
      "Scraped PDF: 12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A20130201.pdf from Project 927\n",
      "Scraped PDF: 12AS782675-QC5PIEOPQ929AppendixA.pdf from Project 929\n",
      "Scraped PDF: 12AS783552-QC5PIEOPQ930AppendixA.pdf from Project 930\n",
      "Scraped PDF: 12AS780420-QC5PIEOPQ931AppendixA.pdf from Project 931\n",
      "Scraped PDF: 12AS785803-QC5PIEOPQ932AppendixA.pdf from Project 932\n",
      "Scraped PDF: 12AS783586-C5PIMetroQ941AppA.pdf from Project 941\n",
      "Scraped PDF: 12AS797833-QC5PINOLQ942Appendix_A.pdf from Project 942\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 5/03_raw/rawdata_cluster5_style_E_originals.csv\n",
      "No data to save for addendums.\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 50\n",
      "Total Projects Scraped: 42\n",
      "Total Projects Skipped: 8\n",
      "Total Projects Missing: 24\n",
      "Total PDFs Accessed: 43\n",
      "Total PDFs Scraped: 43\n",
      "Total PDFs Skipped: 0\n",
      "\n",
      "List of Scraped Projects:\n",
      "[871, 873, 877, 885, 887, 888, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 908, 909, 910, 911, 913, 914, 916, 917, 919, 921, 922, 923, 925, 926, 927, 929, 930, 931, 932, 941, 942]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[869, 870, 878, 879, 928, 933, 934, 937]\n",
      "\n",
      "List of Missing Projects:\n",
      "[872, 874, 875, 876, 880, 881, 882, 883, 884, 886, 889, 905, 906, 907, 912, 915, 918, 920, 924, 935, 936, 938, 939, 940]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['11AS763732-C5PIEastQ871AppendixA.pdf', '11AS763774-C5PIMetroQ893AppA.pdf', '12AS779216-Appendix_AQ877_California_Flats_Solar_C5_Ph_I_Study_Report.pdf', '12AS779571-Appendix_AQ885_SKIC_Solar_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780495-QC5PIEOPQ887AppendixA.pdf', '12AS780604-QC5PINOLQ888Appendix_A.pdf', '12AS778778-QC5PINorthernQ890Backus_Solar_EnergyAppendix_A.pdf', '12AS782071-QC5PIEOPQ891AppendixA.pdf', '12AS780639-QC5PIEOPQ892AppendixA.pdf', '12AS780344-C5PIMetroQ893AppA.pdf', '12AS779610-Appendix_AQ894_Prospect_Energy.pdf', '12AS781853-Appendix_A__Q895_1312013.pdf', '12AS781741-Appendix_A__Q896_1312013.pdf', '12AS780679-QC5PINOLQ897Appendix_A.pdf', '12AS780230-Appendix_AQ898_KPP45_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS779299-Appendix_AQ899_Dunningan_Hills.pdf', '12AS779494-Appendix_AQ900_Rio_Bravo_Solar_I_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780155-Appendix_AQ901_Wildwood_Solar_II_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS781389-C5PIEastQ902AppendixA.pdf', '12AS780940-QC5PINorthernQ903Desert_Butte_Solar_FarmAppendix_A.pdf', '12AS780754-C5PIEastQ904AppendixA.pdf', '12AS781466-QC5PIEOPQ908AppendixA.pdf', '12AS780826-QC5PINOLQ909Appendix_A.pdf', '12AS780268-Appendix_AQ910_Honey_Lake.pdf', '12AS780864-QC5PINorthernQ911Pastoria_Energy_FacilityAppendix_A.pdf', '12AS781352-C5PIEastQ913AppendixA.pdf', '12AS781125-QC5PIEOPQ914AppendixA.pdf', '12AS779532-Appendix_AQ916_Stratford_Solar_C5_Ph_I_Study_Report.pdf', '12AS780118-Appendix_AQ917_Lemoore_Solar_C5_Ph_I_Study_Report.pdf', '12AS781816-Appendix_A__Q919_1312013.pdf', '12AS779648-Appendix_AQ921_SPS_AS_40MW_PV_Generation_Facility_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780381-QC5PINorthernQ922Highwind_Power_ProjectAppendix_A.pdf', '12AS781314-C5PIEastQ923AppendixA.pdf', '12AS781277-QC5PINorthernQ925Tehachapi_SpindleAppendix_A.pdf', '12AS781050-QC5PINorthernQ926Golden_Hills_PowerAppendix_A.pdf', '12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A.pdf', '12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A20130201.pdf', '12AS782675-QC5PIEOPQ929AppendixA.pdf', '12AS783552-QC5PIEOPQ930AppendixA.pdf', '12AS780420-QC5PIEOPQ931AppendixA.pdf', '12AS785803-QC5PIEOPQ932AppendixA.pdf', '12AS783586-C5PIMetroQ941AppA.pdf', '12AS797833-QC5PINOLQ942Appendix_A.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "[]\n",
      "\n",
      "List of Addendum PDFs:\n",
      "[]\n",
      "\n",
      "List of Original PDFs:\n",
      "['11AS763732-C5PIEastQ871AppendixA.pdf', '11AS763774-C5PIMetroQ893AppA.pdf', '12AS779216-Appendix_AQ877_California_Flats_Solar_C5_Ph_I_Study_Report.pdf', '12AS779571-Appendix_AQ885_SKIC_Solar_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780495-QC5PIEOPQ887AppendixA.pdf', '12AS780604-QC5PINOLQ888Appendix_A.pdf', '12AS778778-QC5PINorthernQ890Backus_Solar_EnergyAppendix_A.pdf', '12AS782071-QC5PIEOPQ891AppendixA.pdf', '12AS780639-QC5PIEOPQ892AppendixA.pdf', '12AS780344-C5PIMetroQ893AppA.pdf', '12AS779610-Appendix_AQ894_Prospect_Energy.pdf', '12AS781853-Appendix_A__Q895_1312013.pdf', '12AS781741-Appendix_A__Q896_1312013.pdf', '12AS780679-QC5PINOLQ897Appendix_A.pdf', '12AS780230-Appendix_AQ898_KPP45_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS779299-Appendix_AQ899_Dunningan_Hills.pdf', '12AS779494-Appendix_AQ900_Rio_Bravo_Solar_I_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780155-Appendix_AQ901_Wildwood_Solar_II_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS781389-C5PIEastQ902AppendixA.pdf', '12AS780940-QC5PINorthernQ903Desert_Butte_Solar_FarmAppendix_A.pdf', '12AS780754-C5PIEastQ904AppendixA.pdf', '12AS781466-QC5PIEOPQ908AppendixA.pdf', '12AS780826-QC5PINOLQ909Appendix_A.pdf', '12AS780268-Appendix_AQ910_Honey_Lake.pdf', '12AS780864-QC5PINorthernQ911Pastoria_Energy_FacilityAppendix_A.pdf', '12AS781352-C5PIEastQ913AppendixA.pdf', '12AS781125-QC5PIEOPQ914AppendixA.pdf', '12AS779532-Appendix_AQ916_Stratford_Solar_C5_Ph_I_Study_Report.pdf', '12AS780118-Appendix_AQ917_Lemoore_Solar_C5_Ph_I_Study_Report.pdf', '12AS781816-Appendix_A__Q919_1312013.pdf', '12AS779648-Appendix_AQ921_SPS_AS_40MW_PV_Generation_Facility_C5_Ph_I_Final_Study_Report_CMB_31JAN2013.pdf', '12AS780381-QC5PINorthernQ922Highwind_Power_ProjectAppendix_A.pdf', '12AS781314-C5PIEastQ923AppendixA.pdf', '12AS781277-QC5PINorthernQ925Tehachapi_SpindleAppendix_A.pdf', '12AS781050-QC5PINorthernQ926Golden_Hills_PowerAppendix_A.pdf', '12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A.pdf', '12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A20130201.pdf', '12AS782675-QC5PIEOPQ929AppendixA.pdf', '12AS783552-QC5PIEOPQ930AppendixA.pdf', '12AS780420-QC5PIEOPQ931AppendixA.pdf', '12AS785803-QC5PIEOPQ932AppendixA.pdf', '12AS783586-C5PIMetroQ941AppA.pdf', '12AS797833-QC5PINOLQ942Appendix_A.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 43\n",
      "Number of Addendum PDFs Scraped: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "import inflect\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/rawdata_cluster5_style_E_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/rawdata_cluster5_style_E_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/scraping_cluster5_style_E_log.txt\"\n",
    "PROJECT_RANGE = range(869, 943)  # Example range for q_ids in Clusters 5\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing, removing unwanted characters, and singularizing words.\"\"\"\n",
    "    p = inflect.engine()\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "            # Correct any mis‐spellings of “type of upgrade”\n",
    "            header = re.sub(r'\\btype of upgr\\s*ade\\b', 'type of upgrade', header)\n",
    "            words = header.split()\n",
    "            singular_words = [p.singular_noun(word) if p.singular_noun(word) else word for word in words]\n",
    "            header = \" \".join(singular_words)\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback if none found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "    new_order = existing_desired + remaining\n",
    "    df = df[new_order]\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"\n",
    "    Ensures each row in data_rows has exactly len(headers) columns.\n",
    "    If a row is too short, it is padded with empty strings.\n",
    "    If too long, it is truncated.\n",
    "    \"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"] * (col_count - len(row)))\n",
    "\n",
    "def extract_table2(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table B.1 in the provided PDF.\n",
    "    (Note: This function now searches for \"Table B.1\" rather than Table 2.)\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table B.1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "    table_settings_list = [\n",
    "        {\"horizontal_strategy\": \"text\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 1},\n",
    "        {\"horizontal_strategy\": \"lines\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 2},\n",
    "    ]\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                # Look only for \"Table B.1\"\n",
    "                if re.search(r\"Table\\s*B\\.1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "            if not table1_pages:\n",
    "                print(\"No Table B.1 found in the PDF.\", file=log_file)\n",
    "                return None\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Table B.1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "            extraction_successful = False\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table B.1...\", file=log_file)\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    poi_col_index = cell_index\n",
    "                                    adjacent_col_index = poi_col_index + 1\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            poi_value_parts = []\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "                                            if poi_value_parts:\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                            if extraction_successful:\n",
    "                                break\n",
    "                        if extraction_successful:\n",
    "                            break\n",
    "                    if extraction_successful:\n",
    "                        break\n",
    "                if extraction_successful:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table B.1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "    if not extraction_successful:\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            print(\"Point of Interconnection not found in Table B.1.\", file=log_file)\n",
    "            return None\n",
    "    return point_of_interconnection\n",
    "\n",
    "def fix_column_names(columns):\n",
    "    \"\"\"\n",
    "    Renames duplicate and empty column names.\n",
    "    Duplicate names are suffixed with _1, _2, etc.\n",
    "    Empty or whitespace-only names are replaced with unnamed_1, unnamed_2, etc.\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    counts = {}\n",
    "    unnamed_count = 1\n",
    "    for col in columns:\n",
    "        # Treat empty or whitespace-only names as unnamed.\n",
    "        if not col or col.strip() == \"\":\n",
    "            new_col = f\"unnamed_{unnamed_count}\"\n",
    "            unnamed_count += 1\n",
    "        else:\n",
    "            new_col = col.strip()\n",
    "        if new_col in counts:\n",
    "            new_col_with_suffix = f\"{new_col}_{counts[new_col]}\"\n",
    "            counts[new_col] += 1\n",
    "            new_cols.append(new_col_with_suffix)\n",
    "        else:\n",
    "            counts[new_col] = 1\n",
    "            new_cols.append(new_col)\n",
    "    return new_cols\n",
    "\n",
    "def post_process_columns(df, log_file):\n",
    "    \"\"\"\n",
    "    Post-processes DataFrame column names:\n",
    "      1. For any column named 'unnamed_#' (or empty), look at its first non-empty cell.\n",
    "         If that cell is not a dollar amount and it contains 2 or 3 words, then rename the column to that value.\n",
    "      2. If a column is named \"Needed For\", then rename it to \"description\" (merging with an existing description column if necessary).\n",
    "    \"\"\"\n",
    "    # Process unnamed columns.\n",
    "    for col in list(df.columns):\n",
    "        if col.lower().startswith(\"unnamed_\") or col.strip() == \"\":\n",
    "            # Find the first non-empty cell in this column.\n",
    "            first_non_empty = None\n",
    "            for val in df[col]:\n",
    "                cell_val = \"\"\n",
    "                if isinstance(val, str):\n",
    "                    cell_val = val.strip()\n",
    "                elif val is not None:\n",
    "                    cell_val = str(val).strip()\n",
    "                if cell_val:\n",
    "                    first_non_empty = cell_val\n",
    "                    break\n",
    "            if first_non_empty:\n",
    "                # Check if the value is a dollar amount.\n",
    "                if not re.match(r\"^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$\", first_non_empty):\n",
    "                    words = first_non_empty.split()\n",
    "                    if 2 <= len(words) <= 3:\n",
    "                        new_name = clean_column_headers([first_non_empty])[0]\n",
    "                        log_file.write(f\"Renaming column '{col}' to '{new_name}' based on first non-empty value '{first_non_empty}'.\\n\")\n",
    "                        if new_name in df.columns and new_name != col:\n",
    "                            for idx in df.index:\n",
    "                                existing_val = df.at[idx, new_name]\n",
    "                                candidate_val = df.at[idx, col]\n",
    "                                if (pd.isna(existing_val) or existing_val == \"\") and (not pd.isna(candidate_val) and candidate_val != \"\"):\n",
    "                                    df.at[idx, new_name] = candidate_val\n",
    "                            df.drop(columns=[col], inplace=True)\n",
    "                        else:\n",
    "                            df.rename(columns={col: new_name}, inplace=True)\n",
    "    # Process \"Needed For\" column.\n",
    "    if \"Needed For\" in df.columns:\n",
    "        if \"description\" in df.columns:\n",
    "            log_file.write(\"Merging 'Needed For' column into existing 'description' column.\\n\")\n",
    "            for idx in df.index:\n",
    "                desc_val = df.at[idx, \"description\"]\n",
    "                needed_for_val = df.at[idx, \"Needed For\"]\n",
    "                if (pd.isna(desc_val) or desc_val == \"\") and (not pd.isna(needed_for_val) and needed_for_val != \"\"):\n",
    "                    df.at[idx, \"description\"] = needed_for_val\n",
    "            df.drop(columns=[\"Needed For\"], inplace=True)\n",
    "        else:\n",
    "            log_file.write(\"Renaming 'Needed For' column to 'description'.\\n\")\n",
    "            df.rename(columns={\"Needed For\": \"description\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def extract_table3(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts the Attachment table data from the provided PDF.\n",
    "    Now the function looks for pages containing the attachment heading (\"Attachment 1\" or \"Attachment 2\")\n",
    "    and for each table it determines a subheading from the region between the attachment (or the previous table)\n",
    "    and the current table bounding box. That subheading is used to extract the specific phrase.\n",
    "    All other table cleaning logic remains the same.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Attachment table extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain either Attachment 1 or Attachment 2.\n",
    "            attachment_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Attachment\\s*[12]\\b\", text, re.IGNORECASE):\n",
    "                    attachment_pages.append(i)\n",
    "            if not attachment_pages:\n",
    "                print(\"No Attachment data found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "            first_page = attachment_pages[0]\n",
    "            last_page = attachment_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Candidate pages start on {scrape_start + 1} and end on {scrape_end}\", file=log_file)\n",
    "            # Process each page that might contain attachment table data.\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                # This variable keeps track of the bottom y-coordinate of the previous table on the page.\n",
    "                previous_table_bottom = None\n",
    "                # Find all tables on the page.\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "                    table_bbox = table.bbox  # (x0, top, x1, bottom)\n",
    "                    # Determine the top of the title region.\n",
    "                    # First, find the bounding box of the attachment heading on the page.\n",
    "                    words = page.extract_words()\n",
    "                    attachment_words = [w for w in words if re.search(r\"Attachment\\s*[12]\\b\", w['text'], re.IGNORECASE)]\n",
    "                    if attachment_words:\n",
    "                        attachment_bottom = max(float(w['bottom']) for w in attachment_words)\n",
    "                    else:\n",
    "                        attachment_bottom = 0\n",
    "                    # For the first table, use the attachment heading as the top boundary.\n",
    "                    # For subsequent tables, use the bottom of the previous table.\n",
    "                    start_y = attachment_bottom if previous_table_bottom is None else previous_table_bottom\n",
    "                    title_bbox = (0, start_y, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "                    if title_text.strip():\n",
    "                        # Use the last non-empty line in the region as the table subheading.\n",
    "                        title_lines = [line.strip() for line in title_text.splitlines() if line.strip()]\n",
    "                        if title_lines:\n",
    "                            table_title = title_lines[-1]\n",
    "                    # If a subheading is found, treat this as a new table.\n",
    "                    if table_title:\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New table detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        # Rename header 'type' to 'type of upgrade' if needed.\n",
    "                        if \"type\" in headers and \"type of upgrade\" not in headers:\n",
    "                            headers = [(\"type of upgrade\" if h == \"type\" else h) for h in headers]\n",
    "                        if \"need for\" in headers:\n",
    "                            headers = [(\"description\" if h == \"need for\" else h) for h in headers]  \n",
    "                    \n",
    "                        headers = fix_column_names(headers)\n",
    "                        data_rows = tab[1:]\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        if \"allocated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"allocated\"], inplace=True)\n",
    "                            print(f\"Dropped 'allocated' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate x\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate x\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"escalated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"escalated\"], inplace=True)\n",
    "                            print(f\"Dropped 'escalated' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)    \n",
    "\n",
    "                        if 'type' in df_new.columns and 'type of upgrade' not in df_new.columns:\n",
    "                            df_new.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        # Special handling for ADNU tables if needed.\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        df_new.columns = fix_column_names(df_new.columns.tolist())\n",
    "                        df_new = post_process_columns(df_new, log_file)\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # If no new subheading is found, treat this table as a continuation.\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous table title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"Continuation table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            data_rows = data_rows[1:]\n",
    "                        adjust_rows_length(data_rows, expected_headers)\n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "                        if 'type' in df_continuation.columns and 'type of upgrade' not in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        if \"need for\" in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={\"need for\": \"description\"}, inplace=True)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        df_continuation.columns = fix_column_names(df_continuation.columns.tolist())\n",
    "                        df_continuation = post_process_columns(df_continuation, log_file)\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                    # Update previous_table_bottom for use in the next table.\n",
    "                    previous_table_bottom = table_bbox[3]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Attachment tables in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "        print(\"\\nConcatenating all extracted Attachment data...\", file=log_file)\n",
    "        try:\n",
    "            table3_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table3_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Attachment data extracted.\", file=log_file)\n",
    "        table3_data = pd.DataFrame()\n",
    "    return table3_data\n",
    "\n",
    "def extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Attachment table data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table3_data = extract_table3(pdf_path, log_file, is_addendum)\n",
    "    if table3_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        overlapping_columns = base_data.columns.intersection(table3_data.columns).difference(['point_of_interconnection'])\n",
    "        table3_data = table3_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        base_data_repeated = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "        try:\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "            if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "                non_empty_rows = merged_df[\n",
    "                    merged_df[\"type of upgrade\"].notna() & merged_df[\"upgrade\"].notna() &\n",
    "                    (merged_df[\"type of upgrade\"].str.strip() != \"\") & (merged_df[\"upgrade\"].str.strip() != \"\")\n",
    "                ]\n",
    "                grouped_df = non_empty_rows.groupby([\"q_id\", \"type of upgrade\", \"upgrade\"], as_index=False).first()\n",
    "                merged_df[\"original_index\"] = merged_df.index\n",
    "                final_df = pd.concat([\n",
    "                    grouped_df,\n",
    "                    merged_df[merged_df[\"type of upgrade\"].isna() | (merged_df[\"type of upgrade\"].str.strip() == \"\") |\n",
    "                              merged_df[\"upgrade\"].isna() | (merged_df[\"upgrade\"].str.strip() == \"\")]\n",
    "                ], ignore_index=True, sort=False)\n",
    "                final_df.sort_values(by=\"original_index\", inplace=True)\n",
    "                final_df.drop(columns=[\"original_index\"], inplace=True)\n",
    "                merged_df = final_df\n",
    "                print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade', excluding empty rows while preserving order.\", file=log_file)\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            print(f\"Merged base data with Attachment data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Attachment data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data\n",
    "\n",
    "def check_has_table3(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Attachment 1 or Attachment 2.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Attachment\\s*[12]\\b\", text, re.IGNORECASE): \n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            return \"Addendum\" in text\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "        text = clean_string_cell(text)\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "        point_of_interconnection = extract_table2(pdf_path, log_file)\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "    df = df.map(clean_string_cell)\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "            project_scraped = False\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    try:\n",
    "                        has_table3 = check_has_table3(pdf_path)\n",
    "                        if not has_table3:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Attachment data)\", file=log_file)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Attachment data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "                        if is_add and base_data_extracted:\n",
    "                            table3_data = extract_table3(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table3_data.empty:\n",
    "                                merged_df = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table3_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            df = extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects to process: [869, 870, 871, 872, 873, 874, 875, 876, 878, 879, 880, 881, 882, 883, 884, 886, 887, 888, 889, 890, 891, 892, 893, 895, 896, 897, 902, 903, 904, 905, 906, 907, 908, 909, 911, 912, 913, 914, 915, 918, 919, 920, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942]\n",
      "\n",
      "--- Processing project 869 ---\n",
      "No Appendix A PDF (original or addendum) found for project 869.\n",
      "\n",
      "--- Processing project 870 ---\n",
      "No Appendix A PDF (original or addendum) found for project 870.\n",
      "\n",
      "--- Processing project 871 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/871/02_phase_1_study/11AS763732-C5PIEastQ871AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/871/02_phase_1_study/11AS763732-C5PIEastQ871AppendixA.pdf for project 871...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 871\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: 80\n",
      "Base data extracted:\n",
      "{'q_id': ['871'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [80], 'point_of_interconnection': ['Substation']}\n",
      "Found 'Attachment 1' header on page 20\n",
      "Found 'Attachment 2' header on page 23\n",
      "Extracted 2 table(s) from pages 20 to 22.\n",
      "Extracted cost tables for project 871.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/872\n",
      "\n",
      "--- Processing project 873 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/873/02_phase_1_study/11AS763774-C5PIMetroQ893AppA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/873/02_phase_1_study/11AS763774-C5PIMetroQ893AppA.pdf for project 873...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 873\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 9\n",
      "Base data extracted:\n",
      "{'q_id': ['873'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [9], 'point_of_interconnection': ['Participating TO’s Huntington Beach 220 kV Substation']}\n",
      "Found 'Attachment 1' header on page 17\n",
      "Found 'Attachment 2' header on page 18\n",
      "No tables found in the specified page range.\n",
      "No cost tables extracted for project 873.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/874\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/875\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/876\n",
      "\n",
      "--- Processing project 878 ---\n",
      "No Appendix A PDF (original or addendum) found for project 878.\n",
      "\n",
      "--- Processing project 879 ---\n",
      "No Appendix A PDF (original or addendum) found for project 879.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/880\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/881\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/882\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/883\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/884\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/886\n",
      "\n",
      "--- Processing project 887 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/887/02_phase_1_study/12AS780495-QC5PIEOPQ887AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/887/02_phase_1_study/12AS780495-QC5PIEOPQ887AppendixA.pdf for project 887...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 887\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 350\n",
      "Base data extracted:\n",
      "{'q_id': ['887'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [350], 'point_of_interconnection': ['SCE Owned Eldorado 220 kV Bus']}\n",
      "Found 'Attachment 1' header on page 21\n",
      "Found 'Attachment 2' header on page 23\n",
      "Extracted 3 table(s) from pages 21 to 22.\n",
      "Extracted cost tables for project 887.\n",
      "\n",
      "--- Processing project 888 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/888/02_phase_1_study/12AS780604-QC5PINOLQ888Appendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/888/02_phase_1_study/12AS780604-QC5PINOLQ888Appendix_A.pdf for project 888...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 888\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 100\n",
      "Base data extracted:\n",
      "{'q_id': ['888'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [100], 'point_of_interconnection': ['Participating TO’s proposed Jasper 220 kV Bus']}\n",
      "Found 'Attachment 1' header on page 21\n",
      "Found 'Attachment 2' header on page 23\n",
      "Extracted 4 table(s) from pages 21 to 22.\n",
      "Extracted cost tables for project 888.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/889\n",
      "\n",
      "--- Processing project 890 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/890/02_phase_1_study/12AS778778-QC5PINorthernQ890Backus_Solar_EnergyAppendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/890/02_phase_1_study/12AS778778-QC5PINorthernQ890Backus_Solar_EnergyAppendix_A.pdf for project 890...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 890\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: None\n",
      "Base data extracted:\n",
      "{'q_id': ['890'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [None], 'point_of_interconnection': ['Participating TO’s Antelope-Cal Cement- Rosamond 66 kV line']}\n",
      "Found 'Attachment 1' header on page 19\n",
      "Found 'Attachment 2' header on page 20\n",
      "Extracted 2 table(s) from pages 19 to 19.\n",
      "Extracted cost tables for project 890.\n",
      "\n",
      "--- Processing project 891 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/891/02_phase_1_study/12AS782071-QC5PIEOPQ891AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/891/02_phase_1_study/12AS782071-QC5PIEOPQ891AppendixA.pdf for project 891...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 891\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 180\n",
      "Base data extracted:\n",
      "{'q_id': ['891'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [180], 'point_of_interconnection': ['SCE Owned Eldorado 220 kV Bus']}\n",
      "Found 'Attachment 1' header on page 20\n",
      "Found 'Attachment 2' header on page 22\n",
      "Extracted 3 table(s) from pages 20 to 21.\n",
      "Extracted cost tables for project 891.\n",
      "\n",
      "--- Processing project 892 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/892/02_phase_1_study/12AS780639-QC5PIEOPQ892AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/892/02_phase_1_study/12AS780639-QC5PIEOPQ892AppendixA.pdf for project 892...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 892\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 9\n",
      "Base data extracted:\n",
      "{'q_id': ['892'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [9], 'point_of_interconnection': ['Pisgah 220 kV Bus']}\n",
      "Found 'Attachment 1' header on page 16\n",
      "Found 'Attachment 2' header on page 19\n",
      "Extracted 4 table(s) from pages 16 to 18.\n",
      "Extracted cost tables for project 892.\n",
      "\n",
      "--- Processing project 893 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/893/02_phase_1_study/12AS780344-C5PIMetroQ893AppA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/893/02_phase_1_study/12AS780344-C5PIMetroQ893AppA.pdf for project 893...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 893\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 9\n",
      "Base data extracted:\n",
      "{'q_id': ['893'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [9], 'point_of_interconnection': ['Participating TO’s Huntington Beach 220 kV Substation']}\n",
      "Found 'Attachment 1' header on page 17\n",
      "Found 'Attachment 2' header on page 18\n",
      "No tables found in the specified page range.\n",
      "No cost tables extracted for project 893.\n",
      "\n",
      "--- Processing project 895 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/895/02_phase_1_study/12AS781853-Appendix_A__Q895_1312013.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/895/02_phase_1_study/12AS781853-Appendix_A__Q895_1312013.pdf for project 895...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 895\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Base data extracted:\n",
      "{'q_id': ['895'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Attachment 1 header not found in PDF.\n",
      "No cost tables extracted for project 895.\n",
      "\n",
      "--- Processing project 896 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/896/02_phase_1_study/12AS781741-Appendix_A__Q896_1312013.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/896/02_phase_1_study/12AS781741-Appendix_A__Q896_1312013.pdf for project 896...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 896\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Base data extracted:\n",
      "{'q_id': ['896'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Attachment 1 header not found in PDF.\n",
      "No cost tables extracted for project 896.\n",
      "\n",
      "--- Processing project 897 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/897/02_phase_1_study/12AS780679-QC5PINOLQ897Appendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/897/02_phase_1_study/12AS780679-QC5PINOLQ897Appendix_A.pdf for project 897...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 897\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 200\n",
      "Base data extracted:\n",
      "{'q_id': ['897'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [200], 'point_of_interconnection': ['Participating TO’s proposed Jasper 220 kV Bus']}\n",
      "Found 'Attachment 1' header on page 20\n",
      "Found 'Attachment 2' header on page 22\n",
      "Extracted 4 table(s) from pages 20 to 21.\n",
      "Extracted cost tables for project 897.\n",
      "\n",
      "--- Processing project 902 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/902/02_phase_1_study/12AS781389-C5PIEastQ902AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/902/02_phase_1_study/12AS781389-C5PIEastQ902AppendixA.pdf for project 902...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 902\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: 150\n",
      "Base data extracted:\n",
      "{'q_id': ['902'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [150], 'point_of_interconnection': ['Substation']}\n",
      "Found 'Attachment 1' header on page 22\n",
      "Found 'Attachment 2' header on page 25\n",
      "Extracted 2 table(s) from pages 22 to 24.\n",
      "Extracted cost tables for project 902.\n",
      "\n",
      "--- Processing project 903 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/903/02_phase_1_study/12AS780940-QC5PINorthernQ903Desert_Butte_Solar_FarmAppendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/903/02_phase_1_study/12AS780940-QC5PINorthernQ903Desert_Butte_Solar_FarmAppendix_A.pdf for project 903...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 903\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 516\n",
      "Base data extracted:\n",
      "{'q_id': ['903'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [516], 'point_of_interconnection': ['Participating TO’s proposed Llano 500 kV Bus which will loop the Lugo-Vincent 500 kV T/L(s)']}\n",
      "Found 'Attachment 1' header on page 20\n",
      "Found 'Attachment 2' header on page 21\n",
      "Extracted 2 table(s) from pages 20 to 20.\n",
      "Extracted cost tables for project 903.\n",
      "\n",
      "--- Processing project 904 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/904/02_phase_1_study/12AS780754-C5PIEastQ904AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/904/02_phase_1_study/12AS780754-C5PIEastQ904AppendixA.pdf for project 904...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 904\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: 150\n",
      "Base data extracted:\n",
      "{'q_id': ['904'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [150], 'point_of_interconnection': ['Substation']}\n",
      "Found 'Attachment 1' header on page 22\n",
      "Found 'Attachment 2' header on page 25\n",
      "Extracted 2 table(s) from pages 22 to 24.\n",
      "Extracted cost tables for project 904.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/905\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/906\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/907\n",
      "\n",
      "--- Processing project 908 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/908/02_phase_1_study/12AS781466-QC5PIEOPQ908AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/908/02_phase_1_study/12AS781466-QC5PIEOPQ908AppendixA.pdf for project 908...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 908\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: 240\n",
      "Base data extracted:\n",
      "{'q_id': ['908'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [240], 'point_of_interconnection': ['VEA Crazy Eyes 220 kV Substation']}\n",
      "Found 'Attachment 1' header on page 17\n",
      "Found 'Attachment 2' header on page 19\n",
      "Extracted 3 table(s) from pages 17 to 18.\n",
      "Extracted cost tables for project 908.\n",
      "\n",
      "--- Processing project 909 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/909/02_phase_1_study/12AS780826-QC5PINOLQ909Appendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/909/02_phase_1_study/12AS780826-QC5PINOLQ909Appendix_A.pdf for project 909...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 909\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 9\n",
      "Base data extracted:\n",
      "{'q_id': ['909'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [9], 'point_of_interconnection': ['Participating TO’s future Water Valley 220 kV Bus']}\n",
      "Found 'Attachment 1' header on page 18\n",
      "Found 'Attachment 2' header on page 19\n",
      "Extracted 2 table(s) from pages 18 to 18.\n",
      "Extracted cost tables for project 909.\n",
      "\n",
      "--- Processing project 911 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/911/02_phase_1_study/12AS780864-QC5PINorthernQ911Pastoria_Energy_FacilityAppendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/911/02_phase_1_study/12AS780864-QC5PINorthernQ911Pastoria_Energy_FacilityAppendix_A.pdf for project 911...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 911\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 800\n",
      "Base data extracted:\n",
      "{'q_id': ['911'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [800], 'point_of_interconnection': ['Participating TO’s Pastoria 220 kV']}\n",
      "Found 'Attachment 1' header on page 15\n",
      "Found 'Attachment 2' header on page 16\n",
      "Extracted 2 table(s) from pages 15 to 15.\n",
      "Extracted cost tables for project 911.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/912\n",
      "\n",
      "--- Processing project 913 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/913/02_phase_1_study/12AS781352-C5PIEastQ913AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/913/02_phase_1_study/12AS781352-C5PIEastQ913AppendixA.pdf for project 913...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 913\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: 150\n",
      "Base data extracted:\n",
      "{'q_id': ['913'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [150], 'point_of_interconnection': ['Substation']}\n",
      "Found 'Attachment 1' header on page 21\n",
      "Found 'Attachment 2' header on page 24\n",
      "Extracted 2 table(s) from pages 21 to 23.\n",
      "Extracted cost tables for project 913.\n",
      "\n",
      "--- Processing project 914 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/914/02_phase_1_study/12AS781125-QC5PIEOPQ914AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/914/02_phase_1_study/12AS781125-QC5PIEOPQ914AppendixA.pdf for project 914...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 914\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Base data extracted:\n",
      "{'q_id': ['914'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [None], 'point_of_interconnection': ['VEA Vista 138 kV Substation']}\n",
      "Found 'Attachment 1' header on page 17\n",
      "Found 'Attachment 2' header on page 19\n",
      "Extracted 3 table(s) from pages 17 to 18.\n",
      "Extracted cost tables for project 914.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/915\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/918\n",
      "\n",
      "--- Processing project 919 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/919/02_phase_1_study/12AS781816-Appendix_A__Q919_1312013.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/919/02_phase_1_study/12AS781816-Appendix_A__Q919_1312013.pdf for project 919...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 919\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Base data extracted:\n",
      "{'q_id': ['919'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Attachment 1 header not found in PDF.\n",
      "No cost tables extracted for project 919.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/920\n",
      "\n",
      "--- Processing project 922 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/922/02_phase_1_study/12AS780381-QC5PINorthernQ922Highwind_Power_ProjectAppendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/922/02_phase_1_study/12AS780381-QC5PINorthernQ922Highwind_Power_ProjectAppendix_A.pdf for project 922...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 922\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 291\n",
      "Base data extracted:\n",
      "{'q_id': ['922'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [291], 'point_of_interconnection': ['Participating TO’s Highwind 220 kV']}\n",
      "Found 'Attachment 1' header on page 18\n",
      "Found 'Attachment 2' header on page 19\n",
      "Extracted 2 table(s) from pages 18 to 18.\n",
      "Extracted cost tables for project 922.\n",
      "\n",
      "--- Processing project 923 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/923/02_phase_1_study/12AS781314-C5PIEastQ923AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/923/02_phase_1_study/12AS781314-C5PIEastQ923AppendixA.pdf for project 923...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 923\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: 402\n",
      "Base data extracted:\n",
      "{'q_id': ['923'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [402], 'point_of_interconnection': ['Participating TO’s Vista 220 kV Substation']}\n",
      "Found 'Attachment 1' header on page 21\n",
      "Found 'Attachment 2' header on page 24\n",
      "Extracted 2 table(s) from pages 21 to 23.\n",
      "Extracted cost tables for project 923.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/924\n",
      "\n",
      "--- Processing project 925 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/925/02_phase_1_study/12AS781277-QC5PINorthernQ925Tehachapi_SpindleAppendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/925/02_phase_1_study/12AS781277-QC5PINorthernQ925Tehachapi_SpindleAppendix_A.pdf for project 925...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 925\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 20\n",
      "Base data extracted:\n",
      "{'q_id': ['925'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [20], 'point_of_interconnection': ['Participating TO’s Windhub 220 kV']}\n",
      "Found 'Attachment 1' header on page 17\n",
      "Found 'Attachment 2' header on page 18\n",
      "Extracted 2 table(s) from pages 17 to 17.\n",
      "Extracted cost tables for project 925.\n",
      "\n",
      "--- Processing project 926 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/926/02_phase_1_study/12AS781050-QC5PINorthernQ926Golden_Hills_PowerAppendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/926/02_phase_1_study/12AS781050-QC5PINorthernQ926Golden_Hills_PowerAppendix_A.pdf for project 926...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 926\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 550\n",
      "Base data extracted:\n",
      "{'q_id': ['926'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [550], 'point_of_interconnection': ['Participating TO’s Windhub 220 kV']}\n",
      "Found 'Attachment 1' header on page 17\n",
      "Found 'Attachment 2' header on page 18\n",
      "Extracted 2 table(s) from pages 17 to 17.\n",
      "Extracted cost tables for project 926.\n",
      "\n",
      "--- Processing project 927 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/927/02_phase_1_study/12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/927/02_phase_1_study/12AS780791-QC5PINorthernQ927Tehachapi_Valley_Solar_IAppendix_A.pdf for project 927...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 927\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 50\n",
      "Base data extracted:\n",
      "{'q_id': ['927'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [50], 'point_of_interconnection': ['Participating TO’s Highwind 66 kV']}\n",
      "Found 'Attachment 1' header on page 18\n",
      "Found 'Attachment 2' header on page 19\n",
      "Extracted 2 table(s) from pages 18 to 18.\n",
      "Extracted cost tables for project 927.\n",
      "\n",
      "--- Processing project 928 ---\n",
      "No Appendix A PDF (original or addendum) found for project 928.\n",
      "\n",
      "--- Processing project 929 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/929/02_phase_1_study/12AS782675-QC5PIEOPQ929AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/929/02_phase_1_study/12AS782675-QC5PIEOPQ929AppendixA.pdf for project 929...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 929\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 9\n",
      "Base data extracted:\n",
      "{'q_id': ['929'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [9], 'point_of_interconnection': ['Lugo – Mohave 500 kV transmission line']}\n",
      "Found 'Attachment 1' header on page 16\n",
      "Found 'Attachment 2' header on page 16\n",
      "No tables found in the specified page range.\n",
      "No cost tables extracted for project 929.\n",
      "\n",
      "--- Processing project 930 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/930/02_phase_1_study/12AS783552-QC5PIEOPQ930AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/930/02_phase_1_study/12AS783552-QC5PIEOPQ930AppendixA.pdf for project 930...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 930\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 400\n",
      "Base data extracted:\n",
      "{'q_id': ['930'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [400], 'point_of_interconnection': ['Lugo – Mohave 500 kV transmission line']}\n",
      "Found 'Attachment 1' header on page 16\n",
      "Found 'Attachment 2' header on page 17\n",
      "No tables found in the specified page range.\n",
      "No cost tables extracted for project 930.\n",
      "\n",
      "--- Processing project 931 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/931/02_phase_1_study/12AS780420-QC5PIEOPQ931AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/931/02_phase_1_study/12AS780420-QC5PIEOPQ931AppendixA.pdf for project 931...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 931\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 400\n",
      "Base data extracted:\n",
      "{'q_id': ['931'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [400], 'point_of_interconnection': ['Eldorado – Mohave 500 kV transmission line']}\n",
      "Found 'Attachment 1' header on page 16\n",
      "Found 'Attachment 2' header on page 18\n",
      "No tables found in the specified page range.\n",
      "No cost tables extracted for project 931.\n",
      "\n",
      "--- Processing project 932 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/932/02_phase_1_study/12AS785803-QC5PIEOPQ932AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/932/02_phase_1_study/12AS785803-QC5PIEOPQ932AppendixA.pdf for project 932...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 932\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 400\n",
      "Base data extracted:\n",
      "{'q_id': ['932'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [400], 'point_of_interconnection': ['Eldorado – Mohave 500 kV transmission line']}\n",
      "Found 'Attachment 1' header on page 16\n",
      "Found 'Attachment 2' header on page 16\n",
      "No tables found in the specified page range.\n",
      "No cost tables extracted for project 932.\n",
      "\n",
      "--- Processing project 933 ---\n",
      "No Appendix A PDF (original or addendum) found for project 933.\n",
      "\n",
      "--- Processing project 934 ---\n",
      "No Appendix A PDF (original or addendum) found for project 934.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/935\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/936\n",
      "\n",
      "--- Processing project 937 ---\n",
      "No Appendix A PDF (original or addendum) found for project 937.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/938\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/939\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/940\n",
      "\n",
      "--- Processing project 941 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/941/02_phase_1_study/12AS783586-C5PIMetroQ941AppA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/941/02_phase_1_study/12AS783586-C5PIMetroQ941AppA.pdf for project 941...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 941\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 9\n",
      "Base data extracted:\n",
      "{'q_id': ['941'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [9], 'point_of_interconnection': ['Participating TO’s Redondo 220 kV Substation']}\n",
      "Found 'Attachment 1' header on page 17\n",
      "Found 'Attachment 2' header on page 18\n",
      "No tables found in the specified page range.\n",
      "No cost tables extracted for project 941.\n",
      "\n",
      "--- Processing project 942 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/942/02_phase_1_study/12AS797833-QC5PINOLQ942Appendix_A.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/942/02_phase_1_study/12AS797833-QC5PINOLQ942Appendix_A.pdf for project 942...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 942\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 250\n",
      "Base data extracted:\n",
      "{'q_id': ['942'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [250], 'point_of_interconnection': ['Participating TO’s Kramer 220 kV Bus']}\n",
      "Found 'Attachment 1' header on page 19\n",
      "Found 'Attachment 2' header on page 21\n",
      "Extracted 2 table(s) from pages 19 to 20.\n",
      "Extracted cost tables for project 942.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/costdata_cluster_5_style_E.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 55\n",
      "Total Projects Scraped (with cost data): 21\n",
      "Total Projects Skipped: 10\n",
      "Total Projects with Multi-Page Table Skipped: 0\n",
      "Total Projects Missing: 24\n",
      "Total PDFs Accessed: 31\n",
      "\n",
      "List of Scraped Projects: [871, 887, 888, 890, 891, 892, 897, 902, 903, 904, 908, 909, 911, 913, 914, 922, 923, 925, 926, 927, 942]\n",
      "\n",
      "List of Skipped Projects: [873, 893, 895, 896, 919, 929, 930, 931, 932, 941]\n",
      "\n",
      "List of Projects with Multi-Page Tables Skipped: []\n",
      "\n",
      "List of Missing Projects: [872, 874, 875, 876, 880, 881, 882, 883, 884, 886, 889, 905, 906, 907, 912, 915, 918, 920, 924, 935, 936, 938, 939, 940]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_38130/3439133811.py:348: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import traceback\n",
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------- Configuration -------------------\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_COST = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/costdata_cluster_5_style_E.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/scraping_cluster_5_style_E_log.txt\"\n",
    "PROJECT_RANGE = range(869, 943)  # Original range\n",
    "\n",
    "# Set the page number (zero-indexed) after which to start scanning for attachments.\n",
    "# For example, if you want to start after page 12 (i.e. from page 13 onward), set it to 12.\n",
    "ATTACHMENT_SEARCH_START_PAGE = 12\n",
    "\n",
    "# Read the CSV file containing processed projects (with q_id column)\n",
    "processed_csv_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/all_clusters/costs_phase_1_all_clusters_total.csv\"\n",
    "processed_df = pd.read_csv(processed_csv_path)\n",
    "processed_q_ids = pd.to_numeric(processed_df['q_id'], errors='coerce').dropna().astype(int).unique()\n",
    "projects_to_process = sorted([q_id for q_id in PROJECT_RANGE if q_id not in processed_q_ids])\n",
    "\n",
    "# ------------------- Global Tracking Variables -------------------\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "multi_page_skipped_projects = set()\n",
    "missing_projects = set()\n",
    "total_pdfs_accessed = 0\n",
    "\n",
    "# ------------------- Helper Function for Logging -------------------\n",
    "def log_msg(msg, log_file):\n",
    "    print(msg, file=log_file)\n",
    "    print(msg)\n",
    "\n",
    "# ------------------- Other Helper Functions -------------------\n",
    "def clean_column_headers(headers):\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    elif value is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(value).replace('\\n', ' ').strip()\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "# ------------------- Appendix PDF Check & Base Data Extraction -------------------\n",
    "def is_appendix_pdf(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "            first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "            return \"Appendix A\" in first_page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def is_addendum_pdf(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "            first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "            return (\"Addendum\" in first_page_text) or (\"Revision\" in first_page_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    point_of_interconnection = None\n",
    "    poi_pattern = re.compile(r\"(Point\\s+of\\s+Interconnection|POI)\", re.IGNORECASE)\n",
    "    table_settings_list = [\n",
    "        {\"horizontal_strategy\": \"text\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 1},\n",
    "        {\"horizontal_strategy\": \"lines\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 2}\n",
    "    ]\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*[ABC]\\.1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "            extraction_successful = False\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"Attempt {attempt} with settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1}\", file=log_file)\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty.\", file=log_file)\n",
    "                            continue\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    poi_col_index = cell_index\n",
    "                                    adjacent_col_index = poi_col_index + 1\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"Found POI: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            print(f\"POI label found but adjacent value empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                    else:\n",
    "                                        print(f\"POI label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                            if extraction_successful:\n",
    "                                break\n",
    "                        if extraction_successful:\n",
    "                            break\n",
    "                    if extraction_successful:\n",
    "                        break\n",
    "                if extraction_successful:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "    if not extraction_successful:\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            print(\"POI label found but no value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            print(\"POI not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    if not is_appendix_pdf(pdf_path):\n",
    "        log_msg(f\"Skipping base extraction because {pdf_path} is not an Appendix A PDF.\", log_file)\n",
    "        return pd.DataFrame()\n",
    "    log_msg(\"Extracting base data from Appendix A PDF...\", log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "        text = clean_string_cell(text)\n",
    "        queue_id = str(project_id)\n",
    "        log_msg(f\"Extracted Queue ID: {queue_id}\", log_file)\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = '5'\n",
    "        log_msg(f\"Extracted Cluster Number: {cluster_number}\", log_file)\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        log_msg(f\"Extracted Deliverability Status: {deliverability_status}\", log_file)\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        log_msg(f\"Extracted Capacity: {capacity}\", log_file)\n",
    "        poi_value = extract_table1(pdf_path, log_file)\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [poi_value]\n",
    "        }\n",
    "        log_msg(\"Base data extracted:\", log_file)\n",
    "        log_msg(str(base_data), log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "    except Exception as e:\n",
    "        log_msg(f\"Error extracting base data from {pdf_path}: {e}\", log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------------- New: Cost Data Extraction from Appendix A -------------------\n",
    "def extract_cost_data_from_appendixA(pdf_path, project_id, log_file):\n",
    "    \"\"\"\n",
    "    Extracts cost data tables from the Appendix A PDF.\n",
    "    It looks for the page where \"Attachment 1\" appears (inclusive) and then extracts\n",
    "    all tables until the page where \"Attachment 2\" appears (exclusive).\n",
    "    Only pages after ATTACHMENT_SEARCH_START_PAGE are examined.\n",
    "    If any table appears to be spread over more than one page (determined by a header\n",
    "    that appears on different pages), the project is skipped and noted.\n",
    "    The extracted tables are merged with the base data from the same PDF.\n",
    "    \"\"\"\n",
    "    log_msg(f\"Extracting cost data tables from {pdf_path} for project {project_id}...\", log_file)\n",
    "    base_data_df = extract_base_data(pdf_path, project_id, log_file)\n",
    "    if base_data_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            start_page = None\n",
    "            end_page = None\n",
    "            # Only check pages after ATTACHMENT_SEARCH_START_PAGE\n",
    "            for i in range(ATTACHMENT_SEARCH_START_PAGE, len(pdf.pages)):\n",
    "                text = pdf.pages[i].extract_text() or \"\"\n",
    "                if start_page is None and \"Attachment 1\" in text:\n",
    "                    start_page = i\n",
    "                    log_msg(f\"Found 'Attachment 1' header on page {i+1}\", log_file)\n",
    "                if \"Attachment 2\" in text:\n",
    "                    end_page = i\n",
    "                    log_msg(f\"Found 'Attachment 2' header on page {i+1}\", log_file)\n",
    "                    break\n",
    "            if start_page is None:\n",
    "                log_msg(\"Attachment 1 header not found in PDF.\", log_file)\n",
    "                return pd.DataFrame()\n",
    "            if end_page is None:\n",
    "                end_page = len(pdf.pages)\n",
    "                log_msg(\"Attachment 2 header not found; processing until end of document.\", log_file)\n",
    "            \n",
    "            cost_tables = []\n",
    "            # We'll use a dictionary to track headers and the page where they were seen.\n",
    "            headers_seen = {}\n",
    "            multi_page_flag = False\n",
    "\n",
    "            # Process pages from start_page up to (but not including) end_page\n",
    "            for i in range(start_page, end_page):\n",
    "                page = pdf.pages[i]\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    if table and any(any(cell and cell.strip() for cell in row) for row in table):\n",
    "                        # Assume the first row is the header.\n",
    "                        header = tuple(make_unique_headers(table[0]))\n",
    "                        # If we've seen this header on a different page, mark as multi-page.\n",
    "                        if header in headers_seen and headers_seen[header] != i:\n",
    "                            multi_page_flag = True\n",
    "                        else:\n",
    "                            headers_seen[header] = i\n",
    "                        df = pd.DataFrame(table)\n",
    "                        if not df.empty:\n",
    "                            headers = df.iloc[0].tolist()\n",
    "                            headers = make_unique_headers(headers)\n",
    "                            df.columns = headers\n",
    "                            df = df[1:].reset_index(drop=True)\n",
    "                            df = df.loc[:, ~df.columns.duplicated()]\n",
    "                            cost_tables.append(df)\n",
    "            \n",
    "            if multi_page_flag:\n",
    "                log_msg(\"Detected a table that spans multiple pages. Skipping cost data for this project.\", log_file)\n",
    "                multi_page_skipped_projects.add(project_id)\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            if cost_tables:\n",
    "                cost_data_df = pd.concat(cost_tables, ignore_index=True)\n",
    "                log_msg(f\"Extracted {len(cost_tables)} table(s) from pages {start_page+1} to {end_page}.\", log_file)\n",
    "            else:\n",
    "                log_msg(\"No tables found in the specified page range.\", log_file)\n",
    "                cost_data_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        log_msg(f\"Error extracting cost tables from {pdf_path}: {e}\", log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Merge the cost data with the base data (repeat base data for each row in the cost table)\n",
    "    if not cost_data_df.empty and not base_data_df.empty:\n",
    "        repeated_base = pd.concat([base_data_df] * len(cost_data_df), ignore_index=True)\n",
    "        merged_df = pd.concat([repeated_base, cost_data_df], axis=1)\n",
    "    else:\n",
    "        merged_df = cost_data_df\n",
    "    return merged_df\n",
    "\n",
    "# ------------------- New: Process Cost Data for a Project -------------------\n",
    "def process_cost_data_for_project(project_id, log_file):\n",
    "    project_folder = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "    if not os.path.exists(project_folder):\n",
    "        log_msg(f\"Project folder not found: {project_folder}\", log_file)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Gather Appendix A PDFs (original vs. addendum)\n",
    "    original_appendix_pdfs = []\n",
    "    addendum_appendix_pdf = None\n",
    "    for f in os.listdir(project_folder):\n",
    "        if not f.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_path = os.path.join(project_folder, f)\n",
    "        if is_appendix_pdf(pdf_path):\n",
    "            if is_addendum_pdf(pdf_path):\n",
    "                if not addendum_appendix_pdf:\n",
    "                    addendum_appendix_pdf = f\n",
    "            else:\n",
    "                original_appendix_pdfs.append(f)\n",
    "    \n",
    "    if not original_appendix_pdfs:\n",
    "        if addendum_appendix_pdf:\n",
    "            log_msg(f\"No original Appendix A PDF found for project {project_id}. Using addendum PDF for cost data extraction.\", log_file)\n",
    "            base_pdf = addendum_appendix_pdf\n",
    "        else:\n",
    "            log_msg(f\"No Appendix A PDF (original or addendum) found for project {project_id}.\", log_file)\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        base_pdf = original_appendix_pdfs[0]\n",
    "        log_msg(f\"Using Appendix A PDF: {os.path.join(project_folder, base_pdf)}\", log_file)\n",
    "    \n",
    "    pdf_path = os.path.join(project_folder, base_pdf)\n",
    "    global total_pdfs_accessed\n",
    "    total_pdfs_accessed += 1\n",
    "    cost_data_df = extract_cost_data_from_appendixA(pdf_path, project_id, log_file)\n",
    "    if cost_data_df.empty:\n",
    "        log_msg(f\"No cost tables extracted for project {project_id}.\", log_file)\n",
    "        skipped_projects.add(project_id)\n",
    "    else:\n",
    "        log_msg(f\"Extracted cost tables for project {project_id}.\", log_file)\n",
    "        scraped_projects.add(project_id)\n",
    "    return cost_data_df\n",
    "\n",
    "# ------------------- CSV Saving & Summary Functions -------------------\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "    df = df.applymap(clean_string_cell)\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "# ------------------- Main Processing Function -------------------\n",
    "def process_pdfs_in_folder():\n",
    "    global total_pdfs_accessed\n",
    "    all_cost_data = []\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        log_msg(f\"Projects to process: {projects_to_process}\", log_file)\n",
    "        for project_id in projects_to_process:\n",
    "            project_folder = os.path.join(BASE_DIRECTORY, str(project_id))\n",
    "            if not os.path.exists(project_folder):\n",
    "                missing_projects.add(project_id)\n",
    "                log_msg(f\"Project folder not found: {project_folder}\", log_file)\n",
    "                continue\n",
    "            log_msg(f\"\\n--- Processing project {project_id} ---\", log_file)\n",
    "            cost_data_df = process_cost_data_for_project(project_id, log_file)\n",
    "            if not cost_data_df.empty:\n",
    "                all_cost_data.append(cost_data_df)\n",
    "        \n",
    "        if all_cost_data:\n",
    "            combined_cost_data = pd.concat(all_cost_data, ignore_index=True)\n",
    "            save_to_csv(combined_cost_data, OUTPUT_CSV_PATH_COST, \"cost data\")\n",
    "        else:\n",
    "            log_msg(\"\\nNo cost data extracted from any project.\", log_file)\n",
    "        \n",
    "        total_projects_processed = len(scraped_projects) + len(skipped_projects) + len(missing_projects)\n",
    "        log_msg(\"\\n=== Scraping Summary ===\", log_file)\n",
    "        log_msg(f\"Total Projects Processed: {total_projects_processed}\", log_file)\n",
    "        log_msg(f\"Total Projects Scraped (with cost data): {len(scraped_projects)}\", log_file)\n",
    "        log_msg(f\"Total Projects Skipped: {len(skipped_projects)}\", log_file)\n",
    "        log_msg(f\"Total Projects with Multi-Page Table Skipped: {len(multi_page_skipped_projects)}\", log_file)\n",
    "        log_msg(f\"Total Projects Missing: {len(missing_projects)}\", log_file)\n",
    "        log_msg(f\"Total PDFs Accessed: {total_pdfs_accessed}\", log_file)\n",
    "        log_msg(\"\\nList of Scraped Projects: \" + str(sorted(scraped_projects)), log_file)\n",
    "        log_msg(\"\\nList of Skipped Projects: \" + str(sorted(skipped_projects)), log_file)\n",
    "        log_msg(\"\\nList of Projects with Multi-Page Tables Skipped: \" + str(sorted(multi_page_skipped_projects)), log_file)\n",
    "        log_msg(\"\\nList of Missing Projects: \" + str(sorted(missing_projects)), log_file)\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "def main():\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Itemized and Total Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COlumn names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: ['q_id', 'cluster', 'req_deliverability', 'capacity', 'point_of_interconnection', 'type_of_upgr_ade', 'upgrade', 'description', 'estimat_ed_cost_x_1000_note', 'estimated_cost_x_1000_constant_dollar_od_year_note', 'unnamed_10', 'cost_share', 'allocated_cost_od_year_1000', 'estimated_cost_x_1000_constant_dollars_2012_note_4', 'estimated_cost_x_1000_constant_dollar_od_year_note_4', 'phase_i_incremental_mw', 'adnu_cost_rate_od_year_1000mw', 'estimated_time_to_construct_in_months_note_1_note_3', 'allocated_cost_od_year_1000', 'type', 'need_for', 'total_upgrade_cost_estimated_1000_2012_constant_dollars', 'cost_share', 'allocated_cost_responsibility_1000', 'total_upgrade_cost_estimated_1000', 'adnu_kw_rate', 'allocated_adnu_cost_1000', 'unnamed_27', 'none_2', '_2', 'none_3', 'none_4', '_3', 'none_5', 'none_6', '_4', 'none_7', 'none_8', '_5', 'none_9', 'none_10', '_6', 'allocated_cost', '_7', 'cost_rate', '_8', 'upgrade_the_pisgah_substation_to_500_kv_with_one_500230_kv_transformer_loop_the_eldorado_lugo_500_kv_line_into_the_pisgah_500_kv_substation', 'adnu', '100', '206', '20572', '200', '41144', 'cost', 'total_upgrade_cost_estimated_1000', 'cost_share', 'allocated']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/rawdata_cluster_5_style_E_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "\n",
    "\n",
    "df.columns = clean_column_headers(df.columns)\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "print(\"After cleaning:\", df.columns.tolist())\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_1_cluster_14_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_1_cluster_14_total.csv'.\n",
      "['RNU' 'LDNU' 'PTO_IF' 'ADNU' 'Distribution Upgrades']\n",
      "[871 887 888 890 891 892 897 902 903 904 908 909 911 913 914 922 923 925\n",
      " 926 927 942]\n",
      "[5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_38130/108559706.py:268: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_38130/108559706.py:268: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_38130/108559706.py:268: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_38130/108559706.py:268: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_38130/108559706.py:735: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/rawdata_cluster_5_style_E_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "#df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []  # Initialize an empty list to hold the cleaned header names.\n",
    "    for header in headers:  # Iterate over each header in the input.\n",
    "        if header is None:\n",
    "            header = \"\"  # If the header is None, set it to an empty string.\n",
    "        elif isinstance(header, str):  # Otherwise, if the header is a string:\n",
    "            #header = header.lower()  # Convert the header to lowercase.\n",
    "            header = re.sub(r'\\s+', ' ', header)  # Replace one or more whitespace characters with a single space.\n",
    "            #header = re.sub(r'\\(.*?\\)', '', header)  # Remove any text within parentheses (non-greedy).\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s()/+=_]', '', header)  # Remove any character that is not a letter, number, or whitespace.\n",
    "            header = header.strip()  # Remove any leading or trailing whitespace.\n",
    "        cleaned_headers.append(header)  # Append the cleaned header to the list.\n",
    "    return cleaned_headers  # Return the list of cleaned headers.\n",
    "\n",
    "df.columns = clean_column_headers(df.columns)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#STEP 2: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def move_dollar_values(df, source_column, target_column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in `source_column` starts with a '$',\n",
    "    move that value to `target_column` and clear the value in the source column.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      source_column (str): The column to check for values starting with '$'.\n",
    "      target_column (str): The column to move the values into.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure target_column exists; if not, create it filled with empty strings.\n",
    "    if target_column not in df.columns:\n",
    "        df[target_column] = \"\"\n",
    "    \n",
    "    # Create a boolean mask for rows where the source column starts with '$'\n",
    "    mask = df[source_column].astype(str).str.startswith('$', na=False)\n",
    "    \n",
    "    # Move the values: assign the source values to the target column where the mask is True.\n",
    "    df.loc[mask, target_column] = df.loc[mask, source_column]\n",
    "    \n",
    "    # Clear the source column values for those rows (set to empty string)\n",
    "    df.loc[mask, source_column] = \"\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Move values from 'unnamed_8' to a new column 'moved_value'\n",
    "#df = move_dollar_values(df, 'none_5', 'total_estimated_costs_x_1000_escalated_constant_dollars_od_year')\n",
    "\n",
    "\n",
    "#df = move_dollar_values(df, 'none_3','total_estimated_costs_x_1000_constant_dollar_2020')\n",
    "\n",
    "def remove_dollar_values_and_fill_nan(df, column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in the specified column starts with '$',\n",
    "    set that value to NaN. Also, replace any empty strings in that column with NaN.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      column (str): The column to check and clean.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the column is treated as string\n",
    "    df[column] = df[column].astype(str)\n",
    "    \n",
    "    # Set values starting with '$' to NaN\n",
    "    mask = df[column].str.startswith('$', na=False)\n",
    "    df.loc[mask, column] = np.nan\n",
    "    \n",
    "    # Replace any remaining empty strings with NaN\n",
    "    df[column] = df[column].replace(\"\", np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = remove_dollar_values_and_fill_nan(df, '_5')\n",
    "\n",
    "\n",
    "def remove_non_percent_values_and_fill_nan(df, column):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, if the value in the specified column does not contain '%',\n",
    "    set that value to NaN. Also, replace any empty strings in that column with NaN.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame.\n",
    "      column (str): The column to check and clean.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the column is treated as string\n",
    "    df[column] = df[column].astype(str)\n",
    "    \n",
    "    # Set values not containing '%' to NaN\n",
    "    mask = ~df[column].str.contains('%', na=False)\n",
    "    df.loc[mask, column] = np.nan\n",
    "    \n",
    "    # Replace any remaining empty strings with NaN\n",
    "    df[column] = df[column].replace(\"\", np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = remove_non_percent_values_and_fill_nan(df, '_5')\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def merge_columns(df):\n",
    "\n",
    "    merge_columns_dict = {\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    " \n",
    "    \n",
    "        \"type_of_upgrade\": [\n",
    "\n",
    "            'type_of_upgr_ade',\n",
    "            'type',\n",
    "            '_2',\n",
    "    \n",
    "       \n",
    "            \n",
    "           \n",
    "         \n",
    "            \n",
    " \n",
    "             \n",
    "        ],\n",
    "\n",
    "         \"description\": [\"description\",\n",
    "                         'need_for',\n",
    "                         \n",
    "                         \n",
    "                         \n",
    "                         \n",
    "                         ],\n",
    "\n",
    "         \"upgrade\": [\"upgrade\",\n",
    "                     'unnamed_27',\n",
    "                     '_3',\n",
    "                     ],\n",
    "\n",
    "         \"cost_allocation_factor\": [\n",
    "             \"cost_share\",\n",
    "             '_5',\n",
    "             'cost_share', \n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                    ],\n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "       'allocated_cost_od_year_1000',\n",
    "       'allocated_cost_od_year_1000',\n",
    "       'allocated_adnu_cost_1000',\n",
    "       \n",
    "              \n",
    "            \n",
    "\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            'allocated_cost_responsibility_1000',\n",
    "             '_6',\n",
    "             '_7',\n",
    " \n",
    "                    \n",
    " \n",
    "        ],\n",
    "\n",
    "\n",
    "\n",
    "        \"total_estimated_cost_x_1000\" : [\n",
    "\n",
    "            'estimat_ed_cost_x_1000_note',\n",
    "            'estimated_cost_x_1000_constant_dollars_2012_note_4',\n",
    "            'total_upgrade_cost_estimated_1000_2012_constant_dollars',\n",
    "            'total_upgrade_cost_estimated_1000',\n",
    "             '_4',\n",
    "             'total_upgrade_cost_estimated_1000', \n",
    "            \n",
    "        ],\n",
    "\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            'estimated_cost_x_1000_constant_dollar_od_year_note',\n",
    "            'estimated_cost_x_1000_constant_dollar_od_year_note_4',\n",
    "        ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [\n",
    "            'estimated_time_to_construct_months_note_345_9_10',\n",
    "            'estimated_time_to_construct_in_months_note_1_note_3', \n",
    " \n",
    "            \n",
    "        ],\n",
    "       \n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\",\n",
    "            'phase_i_incremental_mw',\n",
    "        ],\n",
    " \n",
    "        \"max_time_to_construct\": [\n",
    "            'od_dollar_escalation_duration_months_note_345_9_10',\n",
    "            'cod_dollar_escalation_duration_months_note_345_9_10'\n",
    " \n",
    "            \n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "        # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"type_of_upgrade\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "df.drop([  'unnamed_10', 'adnu_cost_rate_od_year_1000mw',   'adnu_kw_rate', 'none_2', 'none_3', 'none_4', 'none_5','none_6', 'none_7', 'none_8','none_9', 'none_10', 'allocated_cost',\n",
    "         'cost_rate', '_8', 'upgrade_the_pisgah_substation_to_500_kv_with_one_500230_kv_transformer_loop_the_eldorado_lugo_500_kv_line_into_the_pisgah_500_kv_substation', 'adnu', '100', '206', '20572', '200', \n",
    "         '41144', 'cost', 'allocated'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_time_to_construct\",\n",
    "        \"max_time_to_construct\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost\",\n",
    "        \"total_escalted_cost\",\n",
    " \n",
    "       \n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    "\n",
    "df = df[~df.apply(lambda row: row.astype(str).isin([\"Total Estimated Costs w ITCC x 1,000 Escalated Constant Dollar (OD Year)\", \"Allocated Cost (x1000) 2014 Dollars\", \"Eastern\",\"Note (h)\", \"Element\"]).any(), axis=1)]\n",
    "df = df[~df.apply(lambda row: any(str(cell).startswith(\"Project #:\") for cell in row), axis=1)]\n",
    "\n",
    "\n",
    "df = df[df['type_of_upgrade'].notna() & (df['type_of_upgrade'].astype(str).str.strip() != \"\")]\n",
    "\n",
    " \n",
    "def process_upgrade_columns(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame df with a column \"type_of_upgrade\" that contains both group headers and upgrade data,\n",
    "    this function:\n",
    "      1. Inserts a new column \"upgrade\" as a duplicate of \"type_of_upgrade\" (placed immediately after it).\n",
    "      2. Renames rows in \"type_of_upgrade\" that contain specific phrases as follows:\n",
    "           - If it contains \"Interconnection Facilities\", rename to \"PTO_IF\" (or \"PTO_IF Total\" if \"Total\" is present)\n",
    "           - If it contains \"Reliability Network Upgrade\", rename to \"RNU\" (or \"RNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Local Delivery Network Upgrades\", rename to \"LDNU\" (or \"LDNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Area Deliverability Network Upgrades\", rename to \"ADNU\" (or \"ADNU Total\" if \"Total\" is present)\n",
    "           - If it contains \"Distribution Upgrades\", leave it as is.\n",
    "      3. Creates a temporary column that only holds the header values (from rows that were detected as header rows) and forward-fills it downward.\n",
    "         The forward fill stops (i.e. does not fill into a row) if that row’s original \"type_of_upgrade\" contains any of the \"total\" indicators.\n",
    "      4. Replaces \"type_of_upgrade\" with the forward-filled header values.\n",
    "      5. Drops the rows that originally were header rows.\n",
    "      6. This deletes any rows which are either Total or Subtotal or Total cost assigned, the reason is some proejcts have multiple pdfs thus we rather calculate the total in the end.\n",
    "      \n",
    "    Returns the updated DataFrame.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 1. Create a new column \"upgrade\" immediately after \"type_of_upgrade\"\n",
    "    loc = df.columns.get_loc(\"type_of_upgrade\")\n",
    "    df.insert(loc+1, \"upgrade\", df[\"type_of_upgrade\"])\n",
    "    \n",
    "    # 2. Define a helper to rename header rows.\n",
    "    def rename_header(val):\n",
    "        # If the cell contains any of these phrases, rename accordingly.\n",
    "        # We'll check using the substring test (case-sensitive) per your request.\n",
    "        if \"Interconnection Facilities\" in val:\n",
    "            return \"PTO_IF\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Reliability Network Upgrade\" in val:\n",
    "            return \"RNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Local Delivery Network Upgrades\" in val:\n",
    "            return \"LDNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Area Deliverability Network Upgrades\" in val:\n",
    "            return \"ADNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"Distribution Upgrades\" in val:\n",
    "            return val  # leave unchanged\n",
    "        elif \"Conditional Assigned Network Upgrades\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"CANU\" \n",
    "        elif \"Non-Allocated IRNU\" in val:\n",
    "            return  (\"Total \" if \"Total\" in val else \"\") + \"Non-Allocated IRNU\"\n",
    "        elif \"Area Delivery Network Upgrades\" in val:\n",
    "            return \"ADNU\" + (\" Total\" if \"Total\" in val else \"\")\n",
    "        elif \"One Time Cost\" in val:\n",
    "            return val  # leave unchanged\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    # 3. Identify header rows. We consider a row to be a header row if its \"type_of_upgrade\" cell \n",
    "    # contains any of the target phrases.\n",
    "    target_phrases = [\n",
    "        \"Interconnection Facilities\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Local Delivery Network Upgrades\",\n",
    "        \"Area Deliverability Network Upgrades\",\n",
    "        \"Distribution Upgrades\",\n",
    "        \"Conditional Assigned Network Upgrades\",\n",
    "        \"Non-Allocated IRNU\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"One Time Cost\",\n",
    "\n",
    "    ]\n",
    "    # Create a boolean mask for header rows.\n",
    "    header_mask = df[\"type_of_upgrade\"].apply(lambda x: any(phrase in x for phrase in target_phrases))\n",
    "    \n",
    "    # Apply renaming to the header rows.\n",
    "    df.loc[header_mask, \"type_of_upgrade\"] = df.loc[header_mask, \"type_of_upgrade\"].apply(rename_header)\n",
    "    \n",
    "    # 4. Create a temporary column 'header_temp' that holds only the header rows, then forward fill it.\n",
    "    df[\"header_temp\"] = df[\"type_of_upgrade\"].where(header_mask)\n",
    "    df[\"header_temp\"] = df[\"header_temp\"].ffill()\n",
    "    \n",
    "    # We want to stop the forward fill if we encounter a row that indicates totals.\n",
    "    # Define a simple function that returns True if a cell contains \"Total\" or \"Subtotal\" or \"Total cost assigned\".\n",
    "    def is_total_indicator(val):\n",
    "        return (\"Total\" in val) or (\"Subtotal\" in val) or (\"Total cost assigned\" in val)\n",
    "    \n",
    "    # For rows that themselves are total indicators in the \"upgrade\" column, do not forward-fill (set header_temp to NaN)\n",
    "    df.loc[df[\"upgrade\"].apply(lambda x: is_total_indicator(x)), \"header_temp\"] = None\n",
    "    \n",
    "    # Now, replace the \"type_of_upgrade\" column with the forward-filled header\n",
    "    df[\"type_of_upgrade\"] = df[\"header_temp\"]\n",
    "    df.drop(\"header_temp\", axis=1, inplace=True)\n",
    "    \n",
    "    # 5. Finally, drop the rows that were header rows (i.e. where header_mask is True)\n",
    "    df = df[~header_mask].reset_index(drop=True)\n",
    "    \n",
    "    # Also, drop any rows that have an empty \"type_of_upgrade\"\n",
    "    df = df[df[\"type_of_upgrade\"].notna() & (df[\"type_of_upgrade\"].str.strip() != \"\")]\n",
    "    \n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    "#df = process_upgrade_columns(df)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"IF\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    " \"Reliability Network Upgrades\": \"RNU\",\n",
    " \"Non-Allocated IRNU\": \"RNU\",\n",
    " \"Total Non-Allocated IRNU\": \"Total RNU\",\n",
    " 'Local Delivery Network Upgrades' : 'LDNU',\n",
    " 'Distribution': 'Distribution Upgrades', \n",
    " }\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "   \n",
    "\n",
    "\n",
    " \n",
    "   \n",
    " \n",
    " \n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "# Step 7: Remove $ signs and convert to numeric\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000', 'total_estimated_cost_x_1000_escalated']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "df = df.drop_duplicates(subset=['q_id','type_of_upgrade','upgrade'])   \n",
    "df = df[~df['upgrade'].astype(str).isin([\n",
    "    \"CANUIRNU\", \n",
    "    \"CANUGRNU\", \n",
    "    \"SCD\",\n",
    "    \"CANU-LDNU\",\n",
    "    \"IRNUs\",\n",
    "    \"GRNUs\",\n",
    "    \"Maximum Cost Responsibility (Network Upgrades)\",\n",
    "    \"Network Upgrade\",\n",
    "    \"One Time Costs (Note 1)\",\n",
    "    \"(Note 1)\",\n",
    "    \"(Note 2)\"\n",
    "\n",
    "\n",
    "])]\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/rawdata_cluster_5_style_E.csv', index=False)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000', 'total_estimated_cost_x_1000_escalated']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "                # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = group[\n",
    "            (group['type_of_upgrade'] == upgrade) & (group['item'] == 'no')\n",
    "        ].shape[0] > 0\n",
    "        \n",
    "        if total_exists:\n",
    "             \n",
    "            continue\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        \n",
    "        if not total_exists:\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate specified columns from the existing row\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns (single row, so it remains the same)\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "\n",
    "                # Populate the necessary fields\n",
    "                total_row['q_id'] = q_id\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "\n",
    "                # Populate the specified columns from the first row in the group\n",
    "                first_row = rows.iloc[0]\n",
    "                for col in columns_to_populate:\n",
    "                    if col in df.columns:\n",
    "                        total_row[col] = first_row[col]\n",
    "\n",
    "                # Sum the numeric columns\n",
    "                for col in columns_to_sum:\n",
    "                    if col in rows.columns:\n",
    "                        total_row[col] = rows[col].sum()\n",
    "                    else:\n",
    "                        total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "                new_rows.append(total_row)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 8: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "mappings = {\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"PTO's Interconnection Facilities\": \"PTO_IF\",\n",
    " \"RNUs, Estimated Costs, and Estimated Time to Construct Summary\": \"RNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    "'PTO_IF Total': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'RNU Total': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    " 'LDNU Total': 'LDNU',\n",
    " 'Total Distribution Upgrades': 'Distribution Upgrades',\n",
    " \n",
    " 'Distribution Upgrades Total': 'Distribution Upgrades',\n",
    " 'Total Potential Distribution Upgrades': 'Potential Distribution Upgrades',\n",
    " 'Total One Time Costs': 'One Time Costs',\n",
    " \n",
    "  'Total Distribution': 'Distribution Upgrades',\n",
    "}\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    \n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "     \n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "     \n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "df = df[~df['upgrade'].astype(str).isin([\n",
    "    \"CANUIRNU\", \n",
    "    \"CANUGRNU\", \n",
    "    \"SCD\",\n",
    "    \"CANU-LDNU\",\n",
    "    \"IRNUs\",\n",
    "    \"GRNUs\",\n",
    "    \"Maximum Cost Responsibility (Network Upgrades)\",\n",
    "    \"Network Upgrade\",\n",
    "    \"Plan of Service\",\n",
    "    \"Shared Upgrades\",\n",
    "    \"Element\",\n",
    "   \n",
    "    \"(Note 1)\",\n",
    "    \"(Note 2)\"\n",
    "\n",
    "\n",
    "])]\n",
    "\n",
    "#df = remove_dollar_values_and_fill_nan(df, 'max_time_to_construct')\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    " \n",
    "    itemized_df = itemized_df.drop_duplicates(subset=['q_id','type_of_upgrade','upgrade']) \n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_E_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    # Define the cost columns.\n",
    "    cost_cols = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "\n",
    "    # Build an aggregation dictionary:\n",
    "    # For columns not in grouping or cost_cols, we assume they are identical and take the first value.\n",
    "    agg_dict = {col: 'first' for col in totals_df.columns \n",
    "                if col not in ['q_id', 'type_of_upgrade'] + cost_cols}\n",
    "\n",
    "    # For the cost columns, we want to sum them.\n",
    "    agg_dict.update({col: 'sum' for col in cost_cols})\n",
    "\n",
    "    \n",
    "\n",
    "    # Group by both q_id and type_of_upgrade using the aggregation dictionary.\n",
    "    totals_df = totals_df.groupby(['q_id', 'type_of_upgrade'], as_index=False).agg(agg_dict)\n",
    "    totals_df = reorder_columns(totals_df)\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_E_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_1_cluster_14_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_1_cluster_14_total.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_E_itemized.csv')\n",
    "df2 = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/02_intermediate/costs_phase_1_cluster_5_style_E_total.csv')\n",
    "\n",
    "df1.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/01_clean/costs_phase_1_cluster_5_style_E_itemized_updated.csv', index=False)\n",
    "df2.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/01_clean/costs_phase_1_cluster_5_style_E_total_updated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipped ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects to process: [869, 870, 872, 873, 874, 875, 876, 878, 879, 880, 881, 882, 883, 884, 886, 889, 893, 895, 896, 905, 906, 907, 912, 915, 918, 919, 920, 924, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941]\n",
      "\n",
      "--- Processing project 869 ---\n",
      "No Appendix A PDF (original or addendum) found for project 869.\n",
      "\n",
      "--- Processing project 870 ---\n",
      "No Appendix A PDF (original or addendum) found for project 870.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/872\n",
      "\n",
      "--- Processing project 873 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/873/02_phase_1_study/11AS763774-C5PIMetroQ893AppA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/873/02_phase_1_study/11AS763774-C5PIMetroQ893AppA.pdf for project 873...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 873\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 9\n",
      "Base data extracted:\n",
      "{'q_id': ['873'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [9], 'point_of_interconnection': ['Participating TO’s Huntington Beach 220 kV Substation']}\n",
      "Attachment 1 header not found in PDF.\n",
      "No cost tables extracted for project 873.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/874\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/875\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/876\n",
      "\n",
      "--- Processing project 878 ---\n",
      "No Appendix A PDF (original or addendum) found for project 878.\n",
      "\n",
      "--- Processing project 879 ---\n",
      "No Appendix A PDF (original or addendum) found for project 879.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/880\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/881\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/882\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/883\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/884\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/886\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/889\n",
      "\n",
      "--- Processing project 893 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/893/02_phase_1_study/12AS780344-C5PIMetroQ893AppA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/893/02_phase_1_study/12AS780344-C5PIMetroQ893AppA.pdf for project 893...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 893\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 9\n",
      "Base data extracted:\n",
      "{'q_id': ['893'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [9], 'point_of_interconnection': ['Participating TO’s Huntington Beach 220 kV Substation']}\n",
      "Attachment 1 header not found in PDF.\n",
      "No cost tables extracted for project 893.\n",
      "\n",
      "--- Processing project 895 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/895/02_phase_1_study/12AS781853-Appendix_A__Q895_1312013.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/895/02_phase_1_study/12AS781853-Appendix_A__Q895_1312013.pdf for project 895...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 895\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Base data extracted:\n",
      "{'q_id': ['895'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Attachment 1 header not found in PDF.\n",
      "No cost tables extracted for project 895.\n",
      "\n",
      "--- Processing project 896 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/896/02_phase_1_study/12AS781741-Appendix_A__Q896_1312013.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/896/02_phase_1_study/12AS781741-Appendix_A__Q896_1312013.pdf for project 896...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 896\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Base data extracted:\n",
      "{'q_id': ['896'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Attachment 1 header not found in PDF.\n",
      "No cost tables extracted for project 896.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/905\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/906\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/907\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/912\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/915\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/918\n",
      "\n",
      "--- Processing project 919 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/919/02_phase_1_study/12AS781816-Appendix_A__Q919_1312013.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/919/02_phase_1_study/12AS781816-Appendix_A__Q919_1312013.pdf for project 919...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 919\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "Base data extracted:\n",
      "{'q_id': ['919'], 'cluster': ['5'], 'req_deliverability': ['Full'], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "Attachment 1 header not found in PDF.\n",
      "No cost tables extracted for project 919.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/920\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/924\n",
      "\n",
      "--- Processing project 928 ---\n",
      "No Appendix A PDF (original or addendum) found for project 928.\n",
      "\n",
      "--- Processing project 929 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/929/02_phase_1_study/12AS782675-QC5PIEOPQ929AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/929/02_phase_1_study/12AS782675-QC5PIEOPQ929AppendixA.pdf for project 929...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 929\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 9\n",
      "Base data extracted:\n",
      "{'q_id': ['929'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [9], 'point_of_interconnection': ['Lugo – Mohave 500 kV transmission line']}\n",
      "Found 'Attachment 1' header on page 28\n",
      "Found 'Attachment 2' header on page 32\n",
      "Detected a table that spans multiple pages. Skipping cost data for this project.\n",
      "No cost tables extracted for project 929.\n",
      "\n",
      "--- Processing project 930 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/930/02_phase_1_study/12AS783552-QC5PIEOPQ930AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/930/02_phase_1_study/12AS783552-QC5PIEOPQ930AppendixA.pdf for project 930...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 930\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 400\n",
      "Base data extracted:\n",
      "{'q_id': ['930'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [400], 'point_of_interconnection': ['Lugo – Mohave 500 kV transmission line']}\n",
      "Found 'Attachment 1' header on page 19\n",
      "Found 'Attachment 2' header on page 19\n",
      "No tables found in the specified page range.\n",
      "No cost tables extracted for project 930.\n",
      "\n",
      "--- Processing project 931 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/931/02_phase_1_study/12AS780420-QC5PIEOPQ931AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/931/02_phase_1_study/12AS780420-QC5PIEOPQ931AppendixA.pdf for project 931...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 931\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 400\n",
      "Base data extracted:\n",
      "{'q_id': ['931'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [400], 'point_of_interconnection': ['Eldorado – Mohave 500 kV transmission line']}\n",
      "Found 'Attachment 1' header on page 28\n",
      "Found 'Attachment 2' header on page 33\n",
      "Detected a table that spans multiple pages. Skipping cost data for this project.\n",
      "No cost tables extracted for project 931.\n",
      "\n",
      "--- Processing project 932 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/932/02_phase_1_study/12AS785803-QC5PIEOPQ932AppendixA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/932/02_phase_1_study/12AS785803-QC5PIEOPQ932AppendixA.pdf for project 932...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 932\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 400\n",
      "Base data extracted:\n",
      "{'q_id': ['932'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [400], 'point_of_interconnection': ['Eldorado – Mohave 500 kV transmission line']}\n",
      "Found 'Attachment 1' header on page 28\n",
      "Found 'Attachment 2' header on page 32\n",
      "Detected a table that spans multiple pages. Skipping cost data for this project.\n",
      "No cost tables extracted for project 932.\n",
      "\n",
      "--- Processing project 933 ---\n",
      "No Appendix A PDF (original or addendum) found for project 933.\n",
      "\n",
      "--- Processing project 934 ---\n",
      "No Appendix A PDF (original or addendum) found for project 934.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/935\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/936\n",
      "\n",
      "--- Processing project 937 ---\n",
      "No Appendix A PDF (original or addendum) found for project 937.\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/938\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/939\n",
      "Project folder not found: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/940\n",
      "\n",
      "--- Processing project 941 ---\n",
      "Using Appendix A PDF: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/941/02_phase_1_study/12AS783586-C5PIMetroQ941AppA.pdf\n",
      "Extracting cost data tables from /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/941/02_phase_1_study/12AS783586-C5PIMetroQ941AppA.pdf for project 941...\n",
      "Extracting base data from Appendix A PDF...\n",
      "Extracted Queue ID: 941\n",
      "Extracted Cluster Number: 5\n",
      "Extracted Deliverability Status: Partial\n",
      "Extracted Capacity: 9\n",
      "Base data extracted:\n",
      "{'q_id': ['941'], 'cluster': ['5'], 'req_deliverability': ['Partial'], 'capacity': [9], 'point_of_interconnection': ['Participating TO’s Redondo 220 kV Substation']}\n",
      "Attachment 1 header not found in PDF.\n",
      "No cost tables extracted for project 941.\n",
      "\n",
      "No cost data extracted from any project.\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 34\n",
      "Total Projects Scraped (with cost data): 0\n",
      "Total Projects Skipped: 10\n",
      "Total Projects with Multi-Page Table Skipped: 3\n",
      "Total Projects Missing: 24\n",
      "Total PDFs Accessed: 10\n",
      "\n",
      "List of Scraped Projects: []\n",
      "\n",
      "List of Skipped Projects: [873, 893, 895, 896, 919, 929, 930, 931, 932, 941]\n",
      "\n",
      "List of Projects with Multi-Page Tables Skipped: [929, 931, 932]\n",
      "\n",
      "List of Missing Projects: [872, 874, 875, 876, 880, 881, 882, 883, 884, 886, 889, 905, 906, 907, 912, 915, 918, 920, 924, 935, 936, 938, 939, 940]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import traceback\n",
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------- Configuration -------------------\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_COST = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/costdata_cluster_5_style_others.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/scraping_cluster_5_style_others_log.txt\"\n",
    "PROJECT_RANGE = range(869, 943)  # Original range\n",
    "\n",
    "# Set the page number (zero-indexed) after which to start scanning for attachments.\n",
    "# For example, if you want to start after page 12 (i.e. from page 13 onward), set it to 12.\n",
    "ATTACHMENT_SEARCH_START_PAGE = 18\n",
    "\n",
    "# Read the CSV file containing processed projects (with q_id column)\n",
    "processed_csv_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/all_clusters/costs_phase_1_all_clusters_total.csv\"\n",
    "processed_df = pd.read_csv(processed_csv_path)\n",
    "processed_q_ids = pd.to_numeric(processed_df['q_id'], errors='coerce').dropna().astype(int).unique()\n",
    "projects_to_process = sorted([q_id for q_id in PROJECT_RANGE if q_id not in processed_q_ids])\n",
    "\n",
    "# ------------------- Global Tracking Variables -------------------\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "multi_page_skipped_projects = set()\n",
    "missing_projects = set()\n",
    "total_pdfs_accessed = 0\n",
    "\n",
    "# ------------------- Helper Function for Logging -------------------\n",
    "def log_msg(msg, log_file):\n",
    "    print(msg, file=log_file)\n",
    "    print(msg)\n",
    "\n",
    "# ------------------- Other Helper Functions -------------------\n",
    "def clean_column_headers(headers):\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    elif value is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(value).replace('\\n', ' ').strip()\n",
    "\n",
    "def make_unique_headers(headers):\n",
    "    seen = {}\n",
    "    unique_headers = []\n",
    "    for header in headers:\n",
    "        if header in seen:\n",
    "            seen[header] += 1\n",
    "            unique_headers.append(f\"{header}_{seen[header]}\")\n",
    "        else:\n",
    "            seen[header] = 1\n",
    "            unique_headers.append(header)\n",
    "    return unique_headers\n",
    "\n",
    "# ------------------- Appendix PDF Check & Base Data Extraction -------------------\n",
    "def is_appendix_pdf(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "            first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "            return \"Appendix A\" in first_page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def is_addendum_pdf(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return False\n",
    "            first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "            return (\"Addendum\" in first_page_text) or (\"Revision\" in first_page_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    point_of_interconnection = None\n",
    "    poi_pattern = re.compile(r\"(Point\\s+of\\s+Interconnection|POI)\", re.IGNORECASE)\n",
    "    table_settings_list = [\n",
    "        {\"horizontal_strategy\": \"text\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 1},\n",
    "        {\"horizontal_strategy\": \"lines\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 2}\n",
    "    ]\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*[ABC]\\.1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "            extraction_successful = False\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"Attempt {attempt} with settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1}\", file=log_file)\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty.\", file=log_file)\n",
    "                            continue\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    poi_col_index = cell_index\n",
    "                                    adjacent_col_index = poi_col_index + 1\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"Found POI: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            print(f\"POI label found but adjacent value empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                    else:\n",
    "                                        print(f\"POI label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                            if extraction_successful:\n",
    "                                break\n",
    "                        if extraction_successful:\n",
    "                            break\n",
    "                    if extraction_successful:\n",
    "                        break\n",
    "                if extraction_successful:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "    if not extraction_successful:\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            print(\"POI label found but no value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            print(\"POI not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    if not is_appendix_pdf(pdf_path):\n",
    "        log_msg(f\"Skipping base extraction because {pdf_path} is not an Appendix A PDF.\", log_file)\n",
    "        return pd.DataFrame()\n",
    "    log_msg(\"Extracting base data from Appendix A PDF...\", log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "        text = clean_string_cell(text)\n",
    "        queue_id = str(project_id)\n",
    "        log_msg(f\"Extracted Queue ID: {queue_id}\", log_file)\n",
    "        clusters = re.findall(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = '5'\n",
    "        log_msg(f\"Extracted Cluster Number: {cluster_number}\", log_file)\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        log_msg(f\"Extracted Deliverability Status: {deliverability_status}\", log_file)\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        log_msg(f\"Extracted Capacity: {capacity}\", log_file)\n",
    "        poi_value = extract_table1(pdf_path, log_file)\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [poi_value]\n",
    "        }\n",
    "        log_msg(\"Base data extracted:\", log_file)\n",
    "        log_msg(str(base_data), log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "    except Exception as e:\n",
    "        log_msg(f\"Error extracting base data from {pdf_path}: {e}\", log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------------- New: Cost Data Extraction from Appendix A -------------------\n",
    "def extract_cost_data_from_appendixA(pdf_path, project_id, log_file):\n",
    "    \"\"\"\n",
    "    Extracts cost data tables from the Appendix A PDF.\n",
    "    It looks for the page where \"Attachment 1\" appears (inclusive) and then extracts\n",
    "    all tables until the page where \"Attachment 2\" appears (exclusive).\n",
    "    Only pages after ATTACHMENT_SEARCH_START_PAGE are examined.\n",
    "    If any table appears to be spread over more than one page (determined by a header\n",
    "    that appears on different pages), the project is skipped and noted.\n",
    "    The extracted tables are merged with the base data from the same PDF.\n",
    "    \"\"\"\n",
    "    log_msg(f\"Extracting cost data tables from {pdf_path} for project {project_id}...\", log_file)\n",
    "    base_data_df = extract_base_data(pdf_path, project_id, log_file)\n",
    "    if base_data_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            start_page = None\n",
    "            end_page = None\n",
    "            # Only check pages after ATTACHMENT_SEARCH_START_PAGE\n",
    "            for i in range(ATTACHMENT_SEARCH_START_PAGE, len(pdf.pages)):\n",
    "                text = pdf.pages[i].extract_text() or \"\"\n",
    "                if start_page is None and \"Attachment 1\" in text:\n",
    "                    start_page = i\n",
    "                    log_msg(f\"Found 'Attachment 1' header on page {i+1}\", log_file)\n",
    "                if \"Attachment 2\" in text:\n",
    "                    end_page = i\n",
    "                    log_msg(f\"Found 'Attachment 2' header on page {i+1}\", log_file)\n",
    "                    break\n",
    "            if start_page is None:\n",
    "                log_msg(\"Attachment 1 header not found in PDF.\", log_file)\n",
    "                return pd.DataFrame()\n",
    "            if end_page is None:\n",
    "                end_page = len(pdf.pages)\n",
    "                log_msg(\"Attachment 2 header not found; processing until end of document.\", log_file)\n",
    "            \n",
    "            cost_tables = []\n",
    "            # We'll use a dictionary to track headers and the page where they were seen.\n",
    "            headers_seen = {}\n",
    "            multi_page_flag = False\n",
    "\n",
    "            # Process pages from start_page up to (but not including) end_page\n",
    "            for i in range(start_page, end_page):\n",
    "                page = pdf.pages[i]\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    if table and any(any(cell and cell.strip() for cell in row) for row in table):\n",
    "                        # Assume the first row is the header.\n",
    "                        header = tuple(make_unique_headers(table[0]))\n",
    "                        # If we've seen this header on a different page, mark as multi-page.\n",
    "                        if header in headers_seen and headers_seen[header] != i:\n",
    "                            multi_page_flag = True\n",
    "                        else:\n",
    "                            headers_seen[header] = i\n",
    "                        df = pd.DataFrame(table)\n",
    "                        if not df.empty:\n",
    "                            headers = df.iloc[0].tolist()\n",
    "                            headers = make_unique_headers(headers)\n",
    "                            df.columns = headers\n",
    "                            df = df[1:].reset_index(drop=True)\n",
    "                            df = df.loc[:, ~df.columns.duplicated()]\n",
    "                            cost_tables.append(df)\n",
    "            \n",
    "            if multi_page_flag:\n",
    "                log_msg(\"Detected a table that spans multiple pages. Skipping cost data for this project.\", log_file)\n",
    "                multi_page_skipped_projects.add(project_id)\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            if cost_tables:\n",
    "                cost_data_df = pd.concat(cost_tables, ignore_index=True)\n",
    "                log_msg(f\"Extracted {len(cost_tables)} table(s) from pages {start_page+1} to {end_page}.\", log_file)\n",
    "            else:\n",
    "                log_msg(\"No tables found in the specified page range.\", log_file)\n",
    "                cost_data_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        log_msg(f\"Error extracting cost tables from {pdf_path}: {e}\", log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Merge the cost data with the base data (repeat base data for each row in the cost table)\n",
    "    if not cost_data_df.empty and not base_data_df.empty:\n",
    "        repeated_base = pd.concat([base_data_df] * len(cost_data_df), ignore_index=True)\n",
    "        merged_df = pd.concat([repeated_base, cost_data_df], axis=1)\n",
    "    else:\n",
    "        merged_df = cost_data_df\n",
    "    return merged_df\n",
    "\n",
    "# ------------------- New: Process Cost Data for a Project -------------------\n",
    "def process_cost_data_for_project(project_id, log_file):\n",
    "    project_folder = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "    if not os.path.exists(project_folder):\n",
    "        log_msg(f\"Project folder not found: {project_folder}\", log_file)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Gather Appendix A PDFs (original vs. addendum)\n",
    "    original_appendix_pdfs = []\n",
    "    addendum_appendix_pdf = None\n",
    "    for f in os.listdir(project_folder):\n",
    "        if not f.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_path = os.path.join(project_folder, f)\n",
    "        if is_appendix_pdf(pdf_path):\n",
    "            if is_addendum_pdf(pdf_path):\n",
    "                if not addendum_appendix_pdf:\n",
    "                    addendum_appendix_pdf = f\n",
    "            else:\n",
    "                original_appendix_pdfs.append(f)\n",
    "    \n",
    "    if not original_appendix_pdfs:\n",
    "        if addendum_appendix_pdf:\n",
    "            log_msg(f\"No original Appendix A PDF found for project {project_id}. Using addendum PDF for cost data extraction.\", log_file)\n",
    "            base_pdf = addendum_appendix_pdf\n",
    "        else:\n",
    "            log_msg(f\"No Appendix A PDF (original or addendum) found for project {project_id}.\", log_file)\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        base_pdf = original_appendix_pdfs[0]\n",
    "        log_msg(f\"Using Appendix A PDF: {os.path.join(project_folder, base_pdf)}\", log_file)\n",
    "    \n",
    "    pdf_path = os.path.join(project_folder, base_pdf)\n",
    "    global total_pdfs_accessed\n",
    "    total_pdfs_accessed += 1\n",
    "    cost_data_df = extract_cost_data_from_appendixA(pdf_path, project_id, log_file)\n",
    "    if cost_data_df.empty:\n",
    "        log_msg(f\"No cost tables extracted for project {project_id}.\", log_file)\n",
    "        skipped_projects.add(project_id)\n",
    "    else:\n",
    "        log_msg(f\"Extracted cost tables for project {project_id}.\", log_file)\n",
    "        scraped_projects.add(project_id)\n",
    "    return cost_data_df\n",
    "\n",
    "# ------------------- CSV Saving & Summary Functions -------------------\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "    df = df.applymap(clean_string_cell)\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "# ------------------- Main Processing Function -------------------\n",
    "def process_pdfs_in_folder():\n",
    "    global total_pdfs_accessed\n",
    "    all_cost_data = []\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        log_msg(f\"Projects to process: {projects_to_process}\", log_file)\n",
    "        for project_id in projects_to_process:\n",
    "            project_folder = os.path.join(BASE_DIRECTORY, str(project_id))\n",
    "            if not os.path.exists(project_folder):\n",
    "                missing_projects.add(project_id)\n",
    "                log_msg(f\"Project folder not found: {project_folder}\", log_file)\n",
    "                continue\n",
    "            log_msg(f\"\\n--- Processing project {project_id} ---\", log_file)\n",
    "            cost_data_df = process_cost_data_for_project(project_id, log_file)\n",
    "            if not cost_data_df.empty:\n",
    "                all_cost_data.append(cost_data_df)\n",
    "        \n",
    "        if all_cost_data:\n",
    "            combined_cost_data = pd.concat(all_cost_data, ignore_index=True)\n",
    "            save_to_csv(combined_cost_data, OUTPUT_CSV_PATH_COST, \"cost data\")\n",
    "        else:\n",
    "            log_msg(\"\\nNo cost data extracted from any project.\", log_file)\n",
    "        \n",
    "        total_projects_processed = len(scraped_projects) + len(skipped_projects) + len(missing_projects)\n",
    "        log_msg(\"\\n=== Scraping Summary ===\", log_file)\n",
    "        log_msg(f\"Total Projects Processed: {total_projects_processed}\", log_file)\n",
    "        log_msg(f\"Total Projects Scraped (with cost data): {len(scraped_projects)}\", log_file)\n",
    "        log_msg(f\"Total Projects Skipped: {len(skipped_projects)}\", log_file)\n",
    "        log_msg(f\"Total Projects with Multi-Page Table Skipped: {len(multi_page_skipped_projects)}\", log_file)\n",
    "        log_msg(f\"Total Projects Missing: {len(missing_projects)}\", log_file)\n",
    "        log_msg(f\"Total PDFs Accessed: {total_pdfs_accessed}\", log_file)\n",
    "        log_msg(\"\\nList of Scraped Projects: \" + str(sorted(scraped_projects)), log_file)\n",
    "        log_msg(\"\\nList of Skipped Projects: \" + str(sorted(skipped_projects)), log_file)\n",
    "        log_msg(\"\\nList of Projects with Multi-Page Tables Skipped: \" + str(sorted(multi_page_skipped_projects)), log_file)\n",
    "        log_msg(\"\\nList of Missing Projects: \" + str(sorted(missing_projects)), log_file)\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "def main():\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
