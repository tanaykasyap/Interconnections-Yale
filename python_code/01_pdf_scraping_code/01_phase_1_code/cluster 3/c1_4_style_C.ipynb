{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped PDF: Appendix A - C493_06-25-2010_Final.pdf from Project 493 (No Table 3 or Attachment data)\n",
      "Scraped PDF: Q495_ Cluster 1_Report_Appendix A_final.pdf from Project 495\n",
      "Skipped PDF: Appendix A - C510_06-25-2010_Final.pdf from Project 510 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q521 Columbia 1.pdf from Project 521 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q522 Columbia 2.pdf from Project 522 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS666541-Appendix_A__Q557_Cluster2_Phase_I_study_report_rev1.pdf from Project 557\n",
      "Scraped PDF: Appendix A - Q557 Cluster2 Phase I study report.pdf from Project 557\n",
      "Scraped PDF: 10AS665861-Appendix_A__Q559_Cluster2_Phase_I_study_report_rev1.pdf from Project 559\n",
      "Scraped PDF: Appendix A - Q559 Cluster2 Phase I study report.pdf from Project 559\n",
      "Scraped PDF: Appendix A - Q560 Cluster2 Phase I Study Report.pdf from Project 560\n",
      "Scraped PDF: 10AS665827-Appendix_A__Q560_Cluster2_Phase_I_Study_Report_rev1.pdf from Project 560\n",
      "Skipped PDF: Appendix A - C561_11-15-2010.pdf from Project 561 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - C565_11-15-2010.pdf from Project 565 (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - Q568 C2P1 Individual Report-r1.pdf from Project 568\n",
      "Scraped PDF: APPENDIX A - Q569 Individual Project Report_Final_CMB.pdf from Project 569\n",
      "Scraped PDF: Q569_Cluster 2_Phase 1_Individual_Report_Appendix A_Final_CMB.pdf from Project 569\n",
      "Skipped PDF: Appendix A - C574_11-15-2010.pdf from Project 574 (No Table 3 or Attachment data)\n",
      "Scraped PDF: QC2PI_Q576_Blythe Solar PV II Final.pdf from Project 576\n",
      "Scraped PDF: 10AS669363-Appendix_A__Q577_Cluster2_Phase_I_Study_Report_rev1.pdf from Project 577\n",
      "Scraped PDF: Appendix A - Q577 Cluster2 Phase I Study Report.pdf from Project 577\n",
      "Scraped PDF: Appendix A - Q581 Cluster2 Phase I study report.pdf from Project 581\n",
      "Skipped PDF: Appendix A - C583_11-15-2010.pdf from Project 583 (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - Q586 C2P1 Individal Report-r1.pdf from Project 586\n",
      "Scraped PDF: QC2PI_Q588_NRG Blythe Solar 2 Final.pdf from Project 588\n",
      "Scraped PDF: QC2PI Appendix A_Q589_NRG Solar Victor 1 20101112.pdf from Project 589\n",
      "Skipped PDF: 10AS667391-Appendix_A__C590_11152010.pdf from Project 590 (No Table 3 or Attachment data)\n",
      "Scraped PDF: QC2PI Appendix A_Q593_Fort Mojave Solar20101112.pdf from Project 593\n",
      "Scraped PDF: QC2PI_Q602_Enxco Avalon Solar_Appendix A.pdf from Project 602\n",
      "Scraped PDF: Appendix A - Q606 Cluster2 Phase I study report.pdf from Project 606\n",
      "Skipped PDF: 10AS669023-Appendix_A__Q607_C1C2_Phase_II_reportfinal_r1.pdf from Project 607 (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - Q607 Cluster2 Phase I study report.pdf from Project 607\n",
      "Skipped PDF: Q607 2014_Reasessment Study Report Appendix A_20140908.pdf from Project 607 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS669023-Appendix_A__Q607_Cluster2_Phase_I_study_report_rev2.pdf from Project 607\n",
      "Skipped PDF: Appendix A - C608_11-15-2010.pdf from Project 608 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 2.pdf from Project 628 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 1.pdf from Project 628 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q628 FRV Mojave Solar 4.pdf from Project 628 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Eastern-Appendix A (QC4DS) Q632AA.pdf from Project 632AA (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - Q632C C4 Phase I Study Report.pdf from Project 632C\n",
      "Scraped PDF: 10AS664977-QC3PI_Northern_Appendix_A__Q643AB_BrightSource_AV.pdf from Project 643AB\n",
      "Skipped PDF: 10AS684173-3__NRosamond_Adj_to_AttachmentA_of_Appendix1.pdf from Project 643AC (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS684173-QC3P1_Eastern_Q643AC_Palo_Verde_Solar_Final.pdf from Project 643AC\n",
      "Scraped PDF: 10AS670009-QC3PI_Eastern_Q643AE_DesertHarvest_Final.pdf from Project 643AE\n",
      "Scraped PDF: 10AS664331-QC3PI_Northern_Group_FINAL.pdf from Project 643AJ\n",
      "Skipped PDF: 10AS665249-Appendix_A__C643AM_05272011_final.pdf from Project 643AM (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS665147-Appendix_A__C643AP_05272011_final.pdf from Project 643AP (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_reportrev1.pdf from Project 643D\n",
      "Scraped PDF: 10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_report.pdf from Project 643D\n",
      "Scraped PDF: 10AS669771-Appendix_A__Final_Q643E_Cluster3_Phase_I_study_report.pdf from Project 643E\n",
      "Scraped PDF: 10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final.pdf from Project 643F\n",
      "Scraped PDF: 10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf from Project 643F\n",
      "Skipped PDF: 10AS669159-Appendix_A__Q643F.pdf from Project 643F (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669159-Placer_RioOso_Solar_PV_I__Appx_1_Attch_A_2011_02_03.pdf from Project 643F (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669431-Attachment_A_to_Appendix_1__Interconnection_Request_for_Generators_Over_20MW_REVISED.pdf from Project 643G (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS669431-Q643G_Cluster_3_Phase_1_Appendix_A_PGE_final_r1.pdf from Project 643G\n",
      "Skipped PDF: 10AS669431-Appendix_A__Q643G.pdf from Project 643G (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666507-Appendix_I_Deliverability_Assessment_Results.pdf from Project 643I (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666507-Westlands_PV_I_Appendix_1_Attch_A_2011_02_03.pdf from Project 643I (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666507-Appendix_A__Final_Q643I_Cluster3_Phase_I_study_report.pdf from Project 643I (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS666575-Appendix_A__Final_Q643J_Cluster3_Phase_I_study_report.pdf from Project 643J\n",
      "Skipped PDF: 10AS666745-Yolo_Solar_PV_I_Appendix_1_Attch_A_2012_02_03.pdf from Project 643O (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666745-Appendix_A__Q643O_Individual_Project_Report.pdf from Project 643O (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS666745-Q643O_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf from Project 643O\n",
      "Scraped PDF: 10AS664297-QC3PI_Northern_Group_FINAL.pdf from Project 643R\n",
      "Scraped PDF: 10AS665997-QC3PI_Northern_Group_FINAL.pdf from Project 643S\n",
      "Skipped PDF: 10AS666065-Appendix_A__C643T_05272011_final.pdf from Project 643T (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669533-Appendix_A__Final_Q643W_Cluster3_Phase_I_study_report.pdf from Project 643W (No Table 3 or Attachment data)\n",
      "Skipped PDF: ._PCSK_Reactive Capability Curve.pdf from Project 643W (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669533-20110826_RE_Mustang_Attachment_A_to_Appendix_1.pdf from Project 643W (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669567-20110707_RE_Tranquillity_Attachment_A_to_Appendix_1.pdf from Project 643X (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS669567-Appendix_A__Final_Q643X_Cluster3_Phase_I_study_report.pdf from Project 643X\n",
      "Scraped PDF: Appendix A - Q644 C4 Phase I Study Report- Final.pdf from Project 644\n",
      "Skipped PDF: 10AS693794-Jacumba_Solar_Farm_Q644A_Appendix_A___11092012.pdf from Project 644A (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - Q645A C4 Phase I Study Report-Final.pdf from Project 645A\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 1.pdf from Project 649B (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 2.pdf from Project 649B (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC1QC2PII_Northern_Appendix A_Q649B_Central Antelope Dry Ranch C.pdf from Project 649B (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q649B Central Antelope Dry Ranch.pdf from Project 649B (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q649C North Lancaster Ranch.pdf from Project 649C (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC1QC2PII_Northern_Appendix A_Q649C_North Lancaster Ranch.pdf from Project 649C (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q649C Attachment 1.pdf from Project 649C (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachment 2.pdf from Project 650AA (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachement 1.pdf from Project 650AA (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q650AA American Solar Greenworks.pdf from Project 650AA (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - Q650AC C4 Phase I Study Report - Final.pdf from Project 650AC\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 1.pdf from Project 651A (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 2.pdf from Project 651A (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q651A Acacia.pdf from Project 651A (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS689152-Addendum_2_to_Appendix_A__Q654.pdf from Project 653B\n",
      "Scraped PDF: Appendix A - Q653B C4 Phase I Study Report-Final.pdf from Project 653B\n",
      "Scraped PDF: Appendix A - Q653E C4 Phase I Study Report-Final.pdf from Project 653E\n",
      "Scraped PDF: Appendix A - Q653EA C4 Phase I Study Report-Final.pdf from Project 653EA\n",
      "Skipped PDF: Appendix A - S653ED_11-09-2012.pdf from Project 653ED (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - S653ED_12-30-2011_final.pdf from Project 653ED (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - Q653F C4 Phase I report-20120103.pdf from Project 653F\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 1.pdf from Project 653H (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 2.pdf from Project 653H (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q653H Western Antelope Dry Ranch.pdf from Project 653H (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - Q654 C4 Phase I Study Report-Final.pdf from Project 654\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 1.pdf from Project 658 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 2.pdf from Project 658 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q658 Antelope Big Sky Ranch.pdf from Project 658 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 1.pdf from Project 659 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 2.pdf from Project 659 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q659 Western Antelope Blue Sky Ranch B.pdf from Project 659 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q660 Western Antelope Blue Sky Ranch A.pdf from Project 660 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 1.pdf from Project 660 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 2.pdf from Project 660 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 1.pdf from Project 661 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 2.pdf from Project 661 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q661 Summer Solar.pdf from Project 661 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q662 Silver Sun Greenworks.pdf from Project 662 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC1QC2PII_Northern_Appendix A_Q662_Silver Sun Greenworks.pdf from Project 662 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 2.pdf from Project 662 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 1.pdf from Project 662 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737399-Appendix_A__Q667_11092012.pdf from Project 667 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737399-Revised_Appendix_A__Q667_11212012.pdf from Project 667 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS709953-QC4PISCENorthernAppendix_AQ670_American_Solar_Greenworks_Addition.pdf from Project 670\n",
      "Scraped PDF: 11AS709990-QC4PISCENorthernAppendix_AQ671_Apex_Greenworks.pdf from Project 671\n",
      "Scraped PDF: 11AS703463-Appendix_A__Q678_C4_Phase_I_report.pdf from Project 678\n",
      "Scraped PDF: 11AS703500-Appendix_A__Q679_C4_phase_I_report.pdf from Project 679\n",
      "Scraped PDF: 11AS699473-Appendix_A__Q680_C4_Phase_I_report20120103.pdf from Project 680\n",
      "Skipped PDF: 11AS699885-Appendix_A__Q687.pdf from Project 687 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS699885-Appendix_A__Q687_C4_Phase_I_Report.pdf from Project 687\n",
      "Skipped Addendum PDF: 11AS699885-Report_Addendum__Q687_Cluster_4_Phase_I_Appendix_A.pdf from Project 687 (Empty Data)\n",
      "Scraped PDF: 11AS703562-Appendix_A__Q688_C4_Phase_I_report.pdf from Project 688\n",
      "Scraped PDF: 11AS707788-QC4PI_Appendix_A_Q695_DixieComstock_Geothermal.pdf from Project 695\n",
      "Scraped PDF: 11AS708195-MetroQ702Appendix_A.pdf from Project 702\n",
      "Scraped PDF: 11AS700201-Appendix_A__Q705_C4_Phase_I_report__Final.pdf from Project 705\n",
      "Scraped PDF: 11AS700410-Appendix_A__Q707_C4_Phase_I_report__Final_CMB_3JAN2012.pdf from Project 707\n",
      "Scraped PDF: 11AS700272-Appendix_A__Q708_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf from Project 708\n",
      "Scraped PDF: 11AS700375-Appendix_A__Q709_C4_Phase_I_Report__Final_CMB_3JAN2012_PGE_rev1.pdf from Project 709\n",
      "Scraped PDF: 11AS708367-QC4P1_EOP_Appendix_A_Q714HiddenHillsRanch.pdf from Project 714\n",
      "Scraped PDF: 11AS701175-Appendix_A__Q720_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf from Project 720\n",
      "Scraped PDF: 11AS701347-Appendix_A__Q820_C4_Phase_I_report.pdf from Project 723\n",
      "Skipped Addendum PDF: 11AS701347-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf from Project 723 (Empty Data)\n",
      "Scraped PDF: 11AS701449-Appendix_A__Q725_C4_Phase_I_report.pdf from Project 725\n",
      "Skipped PDF: 11AS745265-Appendix_A__C737_12302011_final.pdf from Project 737 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS708680-QC4PISCENorthernAppendix_AQ738_Oasis_20_MW.pdf from Project 738\n",
      "Scraped PDF: 11AS708645-QC4P1_EOP_AppendixA_Q740PahrumpValleySEGS.pdf from Project 740\n",
      "Scraped PDF: 11AS702744-Appendix_A__Q744_C4_Phase_I_report_final.pdf from Project 744\n",
      "Scraped PDF: 11AS717303-QC4PISCENorthernAppendix_AQ746_RE_Astoria.pdf from Project 746\n",
      "Scraped PDF: 11AS702472-Appendix_A__Q751_C4_Phase_I_report__Final.pdf from Project 751\n",
      "Scraped PDF: 11AS702506-Appendix_A__Q752_C4_Phase_I_report__Final.pdf from Project 752\n",
      "Scraped PDF: 11AS702778-Appendix_A__Q762_C4_Phase_I_Report.pdf from Project 762\n",
      "Scraped PDF: 11AS702880-Appendix_A__Q765_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf from Project 765\n",
      "Scraped PDF: 11AS711971-QC4PISCENorthernAppendix_AQ768_SP_Antelope_DSR.pdf from Project 768\n",
      "Scraped PDF: 11AS712005-QC4PISCENorthernAppendix_AQ769_Springtime_Solar.pdf from Project 769\n",
      "Scraped PDF: 11AS703152-Appendix_A__Q775_C4_Phase_I_report_final_rev020312.pdf from Project 775\n",
      "Scraped PDF: 11AS709671-QC4PISCENorthernAppendix_AQ778_Western_Antelope_Dry_Ranch_Addition.pdf from Project 778\n",
      "Scraped PDF: Appendix A - Q779 C4 Phase I report - Final.pdf from Project 779\n",
      "Skipped PDF: 11AS709868-Appendix_A__C781_12302011_final.pdf from Project 781 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS708542-Appendix_A__C789_12302011_final.pdf from Project 789 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS708846-Appendix_A__C794_12302011_final.pdf from Project 794 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737469-QC4PISCENorthernGroup_Report.pdf from Project 795 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS737434-QC4PISCENorthernAppendix_AQ796_AV_Solar_Ranch_2B.pdf from Project 796\n",
      "Scraped PDF: 11AS710025-EasternQ797AppendixA.pdf from Project 797\n",
      "Scraped PDF: 10AS711623-EasternQ798AppendixA.pdf from Project 798\n",
      "Scraped PDF: 11AS699264-Appendix_A__Q799_C4_Phase_I_report__Final.pdf from Project 799\n",
      "Scraped PDF: 11AS699298-Appendix_A__Q800_C4_Phase_I_report__Final.pdf from Project 800\n",
      "Scraped PDF: 11AS699747-Appendix_A__Q805_C4_Phase_I_report__Final.pdf from Project 805\n",
      "Skipped PDF: 11AS699850-QC34PII_Appendix_A_PGE_Q806.pdf from Project 806 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS699850-Appendix_A__Q806_C4_Phase_I_report__Final.pdf from Project 806\n",
      "Scraped PDF: 11AS700096-Report_Addendum__Q809_Cluster_4_Phase_I_Appendix_A.pdf from Project 809\n",
      "Scraped PDF: 11AS700096-Appendix_A__Q809_C4_Phase_I_report__Final.pdf from Project 809\n",
      "Scraped PDF: 11AS700725-Appendix_A__Q814_C4_Phase_I_report.pdf from Project 814\n",
      "Skipped Addendum PDF: 11AS700725-Report_Addendum__Q814_Cluster_4_Phase_I_Appendix_A.pdf from Project 814 (Empty Data)\n",
      "Scraped PDF: 11AS700760-Appendix_A__Q815_C4_Phase_I_report.pdf from Project 815\n",
      "Skipped Addendum PDF: 11AS700760-Report_Addendum__Q815_Cluster_4_Phase_I_Appendix_A.pdf from Project 815 (Empty Data)\n",
      "Scraped PDF: 11AS700794-Appendix_A__Q816_C4_Phase_I_report.pdf from Project 816\n",
      "Skipped Addendum PDF: 11AS700794-Report_Addendum__Q816_Cluster_4_Phase_I_Appendix_A.pdf from Project 816 (Empty Data)\n",
      "Scraped PDF: 11AS701279-Appendix_A__Q820_C4_Phase_I_report.pdf from Project 820\n",
      "Skipped Addendum PDF: 11AS701279-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf from Project 820 (Empty Data)\n",
      "Scraped PDF: 11AS701789-Appendix_A__Q823_C4_Phase_I_report.pdf from Project 823\n",
      "Skipped PDF: 11AS701789-QC34PII_Appendix_A_PGE_Q823.pdf from Project 823 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS701823-Report_Addendum__Q824_Cluster_4_Phase_I_Appendix_A.pdf from Project 824\n",
      "Skipped PDF: 11AS701823-QC34PII_Appendix_A_PGE_Q824.pdf from Project 824 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS701823-Appendix_A__Q824_C4_Phase_I_report.pdf from Project 824\n",
      "Scraped PDF: 11AS701857-Report_Addendum__Q825_Cluster_4_Phase_I_Appendix_A.pdf from Project 825\n",
      "Scraped PDF: 11AS701857-Appendix_A__Q825_C4_Phase_I_report.pdf from Project 825\n",
      "Scraped PDF: 11AS703986-Appendix_A__Q829_C4_Phase_I_report.pdf from Project 829\n",
      "Scraped PDF: 11AS711555-EasternQ831AppendixA.pdf from Project 831\n",
      "Skipped PDF: 11AS737779-C4PI_Group_Report_Addendum_02172012.pdf from Project 837 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737779-C4PhI_Group_Report_12302011_final.pdf from Project 837 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737779-C4PI_Q837_Jacumba_II_Addendum_02172012.pdf from Project 837 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS716919-Appendix_A__C838_12302011_final.pdf from Project 838 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS741392-QC4P1_EOP_AppendixA_Q855CopperMountainSolar.pdf from Project 855\n",
      "Skipped PDF: C3C4P2-Northern-Q856-Tehachapi Wind Energy Srorage-AppendixA.pdf from Project 856 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS708812-QC4PISCENorthernAppendix_AQ856_Tehachapi_Wind_Energy_Storage.pdf from Project 856\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 3/03_raw/rawdata_cluster1_4_style_C_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 3/03_raw/rawdata_cluster1_4_style_C_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 228\n",
      "Total Projects Scraped: 87\n",
      "Total Projects Skipped: 141\n",
      "Total Projects Missing: 0\n",
      "Total PDFs Accessed: 189\n",
      "Total PDFs Scraped: 99\n",
      "Total PDFs Skipped: 90\n",
      "\n",
      "List of Scraped Projects:\n",
      "['495', '557', '559', '560', '568', '569', '576', '577', '581', '586', '588', '589', '593', '602', '606', '607', '632C', '643AB', '643AC', '643AE', '643AJ', '643D', '643E', '643F', '643G', '643J', '643O', '643R', '643S', '643X', '644', '645A', '650AC', '653B', '653E', '653EA', '653F', '654', '670', '671', '678', '679', '680', '687', '688', '695', '702', '705', '707', '708', '709', '714', '720', '723', '725', '738', '740', '744', '746', '751', '752', '762', '765', '768', '769', '775', '778', '779', '796', '797', '798', '799', '800', '805', '806', '809', '814', '815', '816', '820', '823', '824', '825', '829', '831', '855', '856']\n",
      "\n",
      "List of Skipped Projects:\n",
      "['488', '490', '493', '494', '502', '503', '506', '509', '510', '512', '513', '521', '522', '522C', '541', '552', '555', '558', '561', '565', '574', '579', '583', '585', '590', '608', '628', '632AA', '640', '642', '643', '643AA', '643AF', '643AH', '643AI', '643AK', '643AM', '643AP', '643AS', '643I', '643T', '643W', '643Z', '644A', '647', '649', '649A', '649B', '649C', '650A', '650AA', '651', '651A', '653', '653A', '653D', '653EB', '653ED', '653H', '658', '659', '660', '661', '662', '663', '664', '667', '668', '669', '674', '676', '681', '683', '684', '685', '686', '692', '696', '697', '698', '700', '703', '704', '706', '712', '716', '717', '729', '730', '732', '736', '737', '739', '741', '756', '764', '766', '767', '770', '771', '774', '781', '782', '783', '784', '785', '786', '788', '789', '790', '791', '792', '793', '794', '795', '801', '804', '807', '812', '813', '834', '836', '837', '838', '839', '840', '841', '842', '843', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '857', '858']\n",
      "\n",
      "List of Missing Projects:\n",
      "[]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['Q495_ Cluster 1_Report_Appendix A_final.pdf', '10AS666541-Appendix_A__Q557_Cluster2_Phase_I_study_report_rev1.pdf', 'Appendix A - Q557 Cluster2 Phase I study report.pdf', '10AS665861-Appendix_A__Q559_Cluster2_Phase_I_study_report_rev1.pdf', 'Appendix A - Q559 Cluster2 Phase I study report.pdf', 'Appendix A - Q560 Cluster2 Phase I Study Report.pdf', '10AS665827-Appendix_A__Q560_Cluster2_Phase_I_Study_Report_rev1.pdf', 'Appendix A - Q568 C2P1 Individual Report-r1.pdf', 'APPENDIX A - Q569 Individual Project Report_Final_CMB.pdf', 'Q569_Cluster 2_Phase 1_Individual_Report_Appendix A_Final_CMB.pdf', 'QC2PI_Q576_Blythe Solar PV II Final.pdf', '10AS669363-Appendix_A__Q577_Cluster2_Phase_I_Study_Report_rev1.pdf', 'Appendix A - Q577 Cluster2 Phase I Study Report.pdf', 'Appendix A - Q581 Cluster2 Phase I study report.pdf', 'Appendix A - Q586 C2P1 Individal Report-r1.pdf', 'QC2PI_Q588_NRG Blythe Solar 2 Final.pdf', 'QC2PI Appendix A_Q589_NRG Solar Victor 1 20101112.pdf', 'QC2PI Appendix A_Q593_Fort Mojave Solar20101112.pdf', 'QC2PI_Q602_Enxco Avalon Solar_Appendix A.pdf', 'Appendix A - Q606 Cluster2 Phase I study report.pdf', 'Appendix A - Q607 Cluster2 Phase I study report.pdf', '10AS669023-Appendix_A__Q607_Cluster2_Phase_I_study_report_rev2.pdf', 'Appendix A - Q632C C4 Phase I Study Report.pdf', '10AS664977-QC3PI_Northern_Appendix_A__Q643AB_BrightSource_AV.pdf', '10AS684173-QC3P1_Eastern_Q643AC_Palo_Verde_Solar_Final.pdf', '10AS670009-QC3PI_Eastern_Q643AE_DesertHarvest_Final.pdf', '10AS664331-QC3PI_Northern_Group_FINAL.pdf', '10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_reportrev1.pdf', '10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_report.pdf', '10AS669771-Appendix_A__Final_Q643E_Cluster3_Phase_I_study_report.pdf', '10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final.pdf', '10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf', '10AS669431-Q643G_Cluster_3_Phase_1_Appendix_A_PGE_final_r1.pdf', '10AS666575-Appendix_A__Final_Q643J_Cluster3_Phase_I_study_report.pdf', '10AS666745-Q643O_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf', '10AS664297-QC3PI_Northern_Group_FINAL.pdf', '10AS665997-QC3PI_Northern_Group_FINAL.pdf', '10AS669567-Appendix_A__Final_Q643X_Cluster3_Phase_I_study_report.pdf', 'Appendix A - Q644 C4 Phase I Study Report- Final.pdf', 'Appendix A - Q645A C4 Phase I Study Report-Final.pdf', 'Appendix A - Q650AC C4 Phase I Study Report - Final.pdf', '10AS689152-Addendum_2_to_Appendix_A__Q654.pdf', 'Appendix A - Q653B C4 Phase I Study Report-Final.pdf', 'Appendix A - Q653E C4 Phase I Study Report-Final.pdf', 'Appendix A - Q653EA C4 Phase I Study Report-Final.pdf', 'Appendix A - Q653F C4 Phase I report-20120103.pdf', 'Appendix A - Q654 C4 Phase I Study Report-Final.pdf', '11AS709953-QC4PISCENorthernAppendix_AQ670_American_Solar_Greenworks_Addition.pdf', '11AS709990-QC4PISCENorthernAppendix_AQ671_Apex_Greenworks.pdf', '11AS703463-Appendix_A__Q678_C4_Phase_I_report.pdf', '11AS703500-Appendix_A__Q679_C4_phase_I_report.pdf', '11AS699473-Appendix_A__Q680_C4_Phase_I_report20120103.pdf', '11AS699885-Appendix_A__Q687_C4_Phase_I_Report.pdf', '11AS703562-Appendix_A__Q688_C4_Phase_I_report.pdf', '11AS707788-QC4PI_Appendix_A_Q695_DixieComstock_Geothermal.pdf', '11AS708195-MetroQ702Appendix_A.pdf', '11AS700201-Appendix_A__Q705_C4_Phase_I_report__Final.pdf', '11AS700410-Appendix_A__Q707_C4_Phase_I_report__Final_CMB_3JAN2012.pdf', '11AS700272-Appendix_A__Q708_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS700375-Appendix_A__Q709_C4_Phase_I_Report__Final_CMB_3JAN2012_PGE_rev1.pdf', '11AS708367-QC4P1_EOP_Appendix_A_Q714HiddenHillsRanch.pdf', '11AS701175-Appendix_A__Q720_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS701347-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701449-Appendix_A__Q725_C4_Phase_I_report.pdf', '11AS708680-QC4PISCENorthernAppendix_AQ738_Oasis_20_MW.pdf', '11AS708645-QC4P1_EOP_AppendixA_Q740PahrumpValleySEGS.pdf', '11AS702744-Appendix_A__Q744_C4_Phase_I_report_final.pdf', '11AS717303-QC4PISCENorthernAppendix_AQ746_RE_Astoria.pdf', '11AS702472-Appendix_A__Q751_C4_Phase_I_report__Final.pdf', '11AS702506-Appendix_A__Q752_C4_Phase_I_report__Final.pdf', '11AS702778-Appendix_A__Q762_C4_Phase_I_Report.pdf', '11AS702880-Appendix_A__Q765_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS711971-QC4PISCENorthernAppendix_AQ768_SP_Antelope_DSR.pdf', '11AS712005-QC4PISCENorthernAppendix_AQ769_Springtime_Solar.pdf', '11AS703152-Appendix_A__Q775_C4_Phase_I_report_final_rev020312.pdf', '11AS709671-QC4PISCENorthernAppendix_AQ778_Western_Antelope_Dry_Ranch_Addition.pdf', 'Appendix A - Q779 C4 Phase I report - Final.pdf', '11AS737434-QC4PISCENorthernAppendix_AQ796_AV_Solar_Ranch_2B.pdf', '11AS710025-EasternQ797AppendixA.pdf', '10AS711623-EasternQ798AppendixA.pdf', '11AS699264-Appendix_A__Q799_C4_Phase_I_report__Final.pdf', '11AS699298-Appendix_A__Q800_C4_Phase_I_report__Final.pdf', '11AS699747-Appendix_A__Q805_C4_Phase_I_report__Final.pdf', '11AS699850-Appendix_A__Q806_C4_Phase_I_report__Final.pdf', '11AS700096-Report_Addendum__Q809_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700096-Appendix_A__Q809_C4_Phase_I_report__Final.pdf', '11AS700725-Appendix_A__Q814_C4_Phase_I_report.pdf', '11AS700760-Appendix_A__Q815_C4_Phase_I_report.pdf', '11AS700794-Appendix_A__Q816_C4_Phase_I_report.pdf', '11AS701279-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701789-Appendix_A__Q823_C4_Phase_I_report.pdf', '11AS701823-Report_Addendum__Q824_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701823-Appendix_A__Q824_C4_Phase_I_report.pdf', '11AS701857-Report_Addendum__Q825_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701857-Appendix_A__Q825_C4_Phase_I_report.pdf', '11AS703986-Appendix_A__Q829_C4_Phase_I_report.pdf', '11AS711555-EasternQ831AppendixA.pdf', '11AS741392-QC4P1_EOP_AppendixA_Q855CopperMountainSolar.pdf', '11AS708812-QC4PISCENorthernAppendix_AQ856_Tehachapi_Wind_Energy_Storage.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['Appendix A - C493_06-25-2010_Final.pdf', 'Appendix A - C510_06-25-2010_Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q521 Columbia 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q522 Columbia 2.pdf', 'Appendix A - C561_11-15-2010.pdf', 'Appendix A - C565_11-15-2010.pdf', 'Appendix A - C574_11-15-2010.pdf', 'Appendix A - C583_11-15-2010.pdf', '10AS667391-Appendix_A__C590_11152010.pdf', '10AS669023-Appendix_A__Q607_C1C2_Phase_II_reportfinal_r1.pdf', 'Q607 2014_Reasessment Study Report Appendix A_20140908.pdf', 'Appendix A - C608_11-15-2010.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q628 FRV Mojave Solar 4.pdf', 'Eastern-Appendix A (QC4DS) Q632AA.pdf', '10AS684173-3__NRosamond_Adj_to_AttachmentA_of_Appendix1.pdf', '10AS665249-Appendix_A__C643AM_05272011_final.pdf', '10AS665147-Appendix_A__C643AP_05272011_final.pdf', '10AS669159-Appendix_A__Q643F.pdf', '10AS669159-Placer_RioOso_Solar_PV_I__Appx_1_Attch_A_2011_02_03.pdf', '10AS669431-Attachment_A_to_Appendix_1__Interconnection_Request_for_Generators_Over_20MW_REVISED.pdf', '10AS669431-Appendix_A__Q643G.pdf', '10AS666507-Appendix_I_Deliverability_Assessment_Results.pdf', '10AS666507-Westlands_PV_I_Appendix_1_Attch_A_2011_02_03.pdf', '10AS666507-Appendix_A__Final_Q643I_Cluster3_Phase_I_study_report.pdf', '10AS666745-Yolo_Solar_PV_I_Appendix_1_Attch_A_2012_02_03.pdf', '10AS666745-Appendix_A__Q643O_Individual_Project_Report.pdf', '10AS666065-Appendix_A__C643T_05272011_final.pdf', '10AS669533-Appendix_A__Final_Q643W_Cluster3_Phase_I_study_report.pdf', '._PCSK_Reactive Capability Curve.pdf', '10AS669533-20110826_RE_Mustang_Attachment_A_to_Appendix_1.pdf', '10AS669567-20110707_RE_Tranquillity_Attachment_A_to_Appendix_1.pdf', '10AS693794-Jacumba_Solar_Farm_Q644A_Appendix_A___11092012.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 2.pdf', 'QC1QC2PII_Northern_Appendix A_Q649B_Central Antelope Dry Ranch C.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Central Antelope Dry Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649C North Lancaster Ranch.pdf', 'QC1QC2PII_Northern_Appendix A_Q649C_North Lancaster Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649C Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachement 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA American Solar Greenworks.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Acacia.pdf', 'Appendix A - S653ED_11-09-2012.pdf', 'Appendix A - S653ED_12-30-2011_final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Western Antelope Dry Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Antelope Big Sky Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Western Antelope Blue Sky Ranch B.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Western Antelope Blue Sky Ranch A.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Summer Solar.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Silver Sun Greenworks.pdf', 'QC1QC2PII_Northern_Appendix A_Q662_Silver Sun Greenworks.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 1.pdf', '11AS737399-Appendix_A__Q667_11092012.pdf', '11AS737399-Revised_Appendix_A__Q667_11212012.pdf', '11AS699885-Appendix_A__Q687.pdf', '11AS699885-Report_Addendum__Q687_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701347-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS745265-Appendix_A__C737_12302011_final.pdf', '11AS709868-Appendix_A__C781_12302011_final.pdf', '11AS708542-Appendix_A__C789_12302011_final.pdf', '11AS708846-Appendix_A__C794_12302011_final.pdf', '11AS737469-QC4PISCENorthernGroup_Report.pdf', '11AS699850-QC34PII_Appendix_A_PGE_Q806.pdf', '11AS700725-Report_Addendum__Q814_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700760-Report_Addendum__Q815_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700794-Report_Addendum__Q816_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701279-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701789-QC34PII_Appendix_A_PGE_Q823.pdf', '11AS701823-QC34PII_Appendix_A_PGE_Q824.pdf', '11AS737779-C4PI_Group_Report_Addendum_02172012.pdf', '11AS737779-C4PhI_Group_Report_12302011_final.pdf', '11AS737779-C4PI_Q837_Jacumba_II_Addendum_02172012.pdf', '11AS716919-Appendix_A__C838_12302011_final.pdf', 'C3C4P2-Northern-Q856-Tehachapi Wind Energy Srorage-AppendixA.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 2.pdf', '10AS689152-Addendum_2_to_Appendix_A__Q654.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 2.pdf', '11AS699885-Report_Addendum__Q687_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701347-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700725-Report_Addendum__Q814_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700760-Report_Addendum__Q815_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700794-Report_Addendum__Q816_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701279-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701823-Report_Addendum__Q824_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701857-Report_Addendum__Q825_Cluster_4_Phase_I_Appendix_A.pdf', '11AS737779-C4PI_Group_Report_Addendum_02172012.pdf', '11AS737779-C4PI_Q837_Jacumba_II_Addendum_02172012.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['Appendix A - C493_06-25-2010_Final.pdf', 'Q495_ Cluster 1_Report_Appendix A_final.pdf', 'Appendix A - C510_06-25-2010_Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q521 Columbia 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q522 Columbia 2.pdf', '10AS666541-Appendix_A__Q557_Cluster2_Phase_I_study_report_rev1.pdf', 'Appendix A - Q557 Cluster2 Phase I study report.pdf', '10AS665861-Appendix_A__Q559_Cluster2_Phase_I_study_report_rev1.pdf', 'Appendix A - Q559 Cluster2 Phase I study report.pdf', 'Appendix A - Q560 Cluster2 Phase I Study Report.pdf', '10AS665827-Appendix_A__Q560_Cluster2_Phase_I_Study_Report_rev1.pdf', 'Appendix A - C561_11-15-2010.pdf', 'Appendix A - C565_11-15-2010.pdf', 'Appendix A - Q568 C2P1 Individual Report-r1.pdf', 'APPENDIX A - Q569 Individual Project Report_Final_CMB.pdf', 'Q569_Cluster 2_Phase 1_Individual_Report_Appendix A_Final_CMB.pdf', 'Appendix A - C574_11-15-2010.pdf', 'QC2PI_Q576_Blythe Solar PV II Final.pdf', '10AS669363-Appendix_A__Q577_Cluster2_Phase_I_Study_Report_rev1.pdf', 'Appendix A - Q577 Cluster2 Phase I Study Report.pdf', 'Appendix A - Q581 Cluster2 Phase I study report.pdf', 'Appendix A - C583_11-15-2010.pdf', 'Appendix A - Q586 C2P1 Individal Report-r1.pdf', 'QC2PI_Q588_NRG Blythe Solar 2 Final.pdf', 'QC2PI Appendix A_Q589_NRG Solar Victor 1 20101112.pdf', '10AS667391-Appendix_A__C590_11152010.pdf', 'QC2PI Appendix A_Q593_Fort Mojave Solar20101112.pdf', 'QC2PI_Q602_Enxco Avalon Solar_Appendix A.pdf', 'Appendix A - Q606 Cluster2 Phase I study report.pdf', '10AS669023-Appendix_A__Q607_C1C2_Phase_II_reportfinal_r1.pdf', 'Appendix A - Q607 Cluster2 Phase I study report.pdf', 'Q607 2014_Reasessment Study Report Appendix A_20140908.pdf', '10AS669023-Appendix_A__Q607_Cluster2_Phase_I_study_report_rev2.pdf', 'Appendix A - C608_11-15-2010.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q628 FRV Mojave Solar 4.pdf', 'Eastern-Appendix A (QC4DS) Q632AA.pdf', 'Appendix A - Q632C C4 Phase I Study Report.pdf', '10AS664977-QC3PI_Northern_Appendix_A__Q643AB_BrightSource_AV.pdf', '10AS684173-3__NRosamond_Adj_to_AttachmentA_of_Appendix1.pdf', '10AS684173-QC3P1_Eastern_Q643AC_Palo_Verde_Solar_Final.pdf', '10AS670009-QC3PI_Eastern_Q643AE_DesertHarvest_Final.pdf', '10AS664331-QC3PI_Northern_Group_FINAL.pdf', '10AS665249-Appendix_A__C643AM_05272011_final.pdf', '10AS665147-Appendix_A__C643AP_05272011_final.pdf', '10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_reportrev1.pdf', '10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_report.pdf', '10AS669771-Appendix_A__Final_Q643E_Cluster3_Phase_I_study_report.pdf', '10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final.pdf', '10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf', '10AS669159-Appendix_A__Q643F.pdf', '10AS669159-Placer_RioOso_Solar_PV_I__Appx_1_Attch_A_2011_02_03.pdf', '10AS669431-Attachment_A_to_Appendix_1__Interconnection_Request_for_Generators_Over_20MW_REVISED.pdf', '10AS669431-Q643G_Cluster_3_Phase_1_Appendix_A_PGE_final_r1.pdf', '10AS669431-Appendix_A__Q643G.pdf', '10AS666507-Appendix_I_Deliverability_Assessment_Results.pdf', '10AS666507-Westlands_PV_I_Appendix_1_Attch_A_2011_02_03.pdf', '10AS666507-Appendix_A__Final_Q643I_Cluster3_Phase_I_study_report.pdf', '10AS666575-Appendix_A__Final_Q643J_Cluster3_Phase_I_study_report.pdf', '10AS666745-Yolo_Solar_PV_I_Appendix_1_Attch_A_2012_02_03.pdf', '10AS666745-Appendix_A__Q643O_Individual_Project_Report.pdf', '10AS666745-Q643O_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf', '10AS664297-QC3PI_Northern_Group_FINAL.pdf', '10AS665997-QC3PI_Northern_Group_FINAL.pdf', '10AS666065-Appendix_A__C643T_05272011_final.pdf', '10AS669533-Appendix_A__Final_Q643W_Cluster3_Phase_I_study_report.pdf', '._PCSK_Reactive Capability Curve.pdf', '10AS669533-20110826_RE_Mustang_Attachment_A_to_Appendix_1.pdf', '10AS669567-20110707_RE_Tranquillity_Attachment_A_to_Appendix_1.pdf', '10AS669567-Appendix_A__Final_Q643X_Cluster3_Phase_I_study_report.pdf', 'Appendix A - Q644 C4 Phase I Study Report- Final.pdf', '10AS693794-Jacumba_Solar_Farm_Q644A_Appendix_A___11092012.pdf', 'Appendix A - Q645A C4 Phase I Study Report-Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 1.pdf', 'QC1QC2PII_Northern_Appendix A_Q649B_Central Antelope Dry Ranch C.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Central Antelope Dry Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649C North Lancaster Ranch.pdf', 'QC1QC2PII_Northern_Appendix A_Q649C_North Lancaster Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649C Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachement 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA American Solar Greenworks.pdf', 'Appendix A - Q650AC C4 Phase I Study Report - Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Acacia.pdf', 'Appendix A - Q653B C4 Phase I Study Report-Final.pdf', 'Appendix A - Q653E C4 Phase I Study Report-Final.pdf', 'Appendix A - Q653EA C4 Phase I Study Report-Final.pdf', 'Appendix A - S653ED_11-09-2012.pdf', 'Appendix A - S653ED_12-30-2011_final.pdf', 'Appendix A - Q653F C4 Phase I report-20120103.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Western Antelope Dry Ranch.pdf', 'Appendix A - Q654 C4 Phase I Study Report-Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Antelope Big Sky Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Western Antelope Blue Sky Ranch B.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Western Antelope Blue Sky Ranch A.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Summer Solar.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Silver Sun Greenworks.pdf', 'QC1QC2PII_Northern_Appendix A_Q662_Silver Sun Greenworks.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 1.pdf', '11AS737399-Appendix_A__Q667_11092012.pdf', '11AS737399-Revised_Appendix_A__Q667_11212012.pdf', '11AS709953-QC4PISCENorthernAppendix_AQ670_American_Solar_Greenworks_Addition.pdf', '11AS709990-QC4PISCENorthernAppendix_AQ671_Apex_Greenworks.pdf', '11AS703463-Appendix_A__Q678_C4_Phase_I_report.pdf', '11AS703500-Appendix_A__Q679_C4_phase_I_report.pdf', '11AS699473-Appendix_A__Q680_C4_Phase_I_report20120103.pdf', '11AS699885-Appendix_A__Q687.pdf', '11AS699885-Appendix_A__Q687_C4_Phase_I_Report.pdf', '11AS703562-Appendix_A__Q688_C4_Phase_I_report.pdf', '11AS707788-QC4PI_Appendix_A_Q695_DixieComstock_Geothermal.pdf', '11AS708195-MetroQ702Appendix_A.pdf', '11AS700201-Appendix_A__Q705_C4_Phase_I_report__Final.pdf', '11AS700410-Appendix_A__Q707_C4_Phase_I_report__Final_CMB_3JAN2012.pdf', '11AS700272-Appendix_A__Q708_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS700375-Appendix_A__Q709_C4_Phase_I_Report__Final_CMB_3JAN2012_PGE_rev1.pdf', '11AS708367-QC4P1_EOP_Appendix_A_Q714HiddenHillsRanch.pdf', '11AS701175-Appendix_A__Q720_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS701347-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701449-Appendix_A__Q725_C4_Phase_I_report.pdf', '11AS745265-Appendix_A__C737_12302011_final.pdf', '11AS708680-QC4PISCENorthernAppendix_AQ738_Oasis_20_MW.pdf', '11AS708645-QC4P1_EOP_AppendixA_Q740PahrumpValleySEGS.pdf', '11AS702744-Appendix_A__Q744_C4_Phase_I_report_final.pdf', '11AS717303-QC4PISCENorthernAppendix_AQ746_RE_Astoria.pdf', '11AS702472-Appendix_A__Q751_C4_Phase_I_report__Final.pdf', '11AS702506-Appendix_A__Q752_C4_Phase_I_report__Final.pdf', '11AS702778-Appendix_A__Q762_C4_Phase_I_Report.pdf', '11AS702880-Appendix_A__Q765_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS711971-QC4PISCENorthernAppendix_AQ768_SP_Antelope_DSR.pdf', '11AS712005-QC4PISCENorthernAppendix_AQ769_Springtime_Solar.pdf', '11AS703152-Appendix_A__Q775_C4_Phase_I_report_final_rev020312.pdf', '11AS709671-QC4PISCENorthernAppendix_AQ778_Western_Antelope_Dry_Ranch_Addition.pdf', 'Appendix A - Q779 C4 Phase I report - Final.pdf', '11AS709868-Appendix_A__C781_12302011_final.pdf', '11AS708542-Appendix_A__C789_12302011_final.pdf', '11AS708846-Appendix_A__C794_12302011_final.pdf', '11AS737469-QC4PISCENorthernGroup_Report.pdf', '11AS737434-QC4PISCENorthernAppendix_AQ796_AV_Solar_Ranch_2B.pdf', '11AS710025-EasternQ797AppendixA.pdf', '10AS711623-EasternQ798AppendixA.pdf', '11AS699264-Appendix_A__Q799_C4_Phase_I_report__Final.pdf', '11AS699298-Appendix_A__Q800_C4_Phase_I_report__Final.pdf', '11AS699747-Appendix_A__Q805_C4_Phase_I_report__Final.pdf', '11AS699850-QC34PII_Appendix_A_PGE_Q806.pdf', '11AS699850-Appendix_A__Q806_C4_Phase_I_report__Final.pdf', '11AS700096-Report_Addendum__Q809_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700096-Appendix_A__Q809_C4_Phase_I_report__Final.pdf', '11AS700725-Appendix_A__Q814_C4_Phase_I_report.pdf', '11AS700760-Appendix_A__Q815_C4_Phase_I_report.pdf', '11AS700794-Appendix_A__Q816_C4_Phase_I_report.pdf', '11AS701279-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701789-Appendix_A__Q823_C4_Phase_I_report.pdf', '11AS701789-QC34PII_Appendix_A_PGE_Q823.pdf', '11AS701823-QC34PII_Appendix_A_PGE_Q824.pdf', '11AS701823-Appendix_A__Q824_C4_Phase_I_report.pdf', '11AS701857-Appendix_A__Q825_C4_Phase_I_report.pdf', '11AS703986-Appendix_A__Q829_C4_Phase_I_report.pdf', '11AS711555-EasternQ831AppendixA.pdf', '11AS737779-C4PhI_Group_Report_12302011_final.pdf', '11AS716919-Appendix_A__C838_12302011_final.pdf', '11AS741392-QC4P1_EOP_AppendixA_Q855CopperMountainSolar.pdf', 'C3C4P2-Northern-Q856-Tehachapi Wind Energy Srorage-AppendixA.pdf', '11AS708812-QC4PISCENorthernAppendix_AQ856_Tehachapi_Wind_Energy_Storage.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 96\n",
      "Number of Addendum PDFs Scraped: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "import inflect\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/03_raw/rawdata_cluster1_4_style_C_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/03_raw/rawdata_cluster1_4_style_C_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/03_raw/scraping_cluster1_4_style_C_log.txt\"\n",
    "PROJECT_RANGE = range(488, 859)  # Example range for q_ids in Clusters 4\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing, removing unwanted characters, and singularizing words.\"\"\"\n",
    "    p = inflect.engine()\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "            # Correct any mis\u2010spellings of \u201ctype of upgrade\u201d\n",
    "            header = re.sub(r'\\btype of upgr\\s*ade\\b', 'type of upgrade', header)\n",
    "            words = header.split()\n",
    "            singular_words = [p.singular_noun(word) if p.singular_noun(word) else word for word in words]\n",
    "            header = \" \".join(singular_words)\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback if none found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "    new_order = existing_desired + remaining\n",
    "    df = df[new_order]\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"\n",
    "    Ensures each row in data_rows has exactly len(headers) columns.\n",
    "    If a row is too short, it is padded with empty strings.\n",
    "    If too long, it is truncated.\n",
    "    \"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"] * (col_count - len(row)))\n",
    "            \n",
    "def extract_table2(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 2 in the provided PDF.\n",
    "    If no \"Point of Interconnection\" is found, it then searches for \n",
    "    \"Description of Interconnection\" using similar logic.\n",
    "    \"\"\"\n",
    "    import re, traceback\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 2 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "    # Pattern for fallback search:\n",
    "    desc_pattern = re.compile(r\"Description\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "    \n",
    "    table_settings_list = [\n",
    "        {\"horizontal_strategy\": \"text\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 1},\n",
    "        {\"horizontal_strategy\": \"lines\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 2},\n",
    "    ]\n",
    "    extraction_successful = False\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            table_pages = []\n",
    "            # Identify pages that mention Table 2 or B.1\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*(?:2|B\\.1)\\b\", text, re.IGNORECASE):\n",
    "                    table_pages.append(i)\n",
    "            if not table_pages:\n",
    "                print(\"No Table 2 found in the PDF.\", file=log_file)\n",
    "                return None\n",
    "            first_page = table_pages[0]\n",
    "            last_page = table_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Table 2 starts on page {scrape_start + 1} and ends on page {scrape_end}\", file=log_file)\n",
    "            \n",
    "            # ----- First search: \"Point of Interconnection\" -----\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for 'Point of Interconnection'...\", file=log_file)\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    poi_col_index = cell_index\n",
    "                                    adjacent_col_index = poi_col_index + 1\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            print(f\"\\n'Point of Interconnection' label found but adjacent value is empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            poi_value_parts = []\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "                                            if poi_value_parts:\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                            if extraction_successful:\n",
    "                                break\n",
    "                        if extraction_successful:\n",
    "                            break\n",
    "                    if extraction_successful:\n",
    "                        break\n",
    "                if extraction_successful:\n",
    "                    break\n",
    "\n",
    "            # ----- Second search: \"Description of Interconnection\" -----\n",
    "            if not extraction_successful or not (point_of_interconnection and point_of_interconnection.strip()):\n",
    "                print(\"\\nNo valid 'Point of Interconnection' found. Trying 'Description of Interconnection'...\", file=log_file)\n",
    "                extraction_successful_desc = False\n",
    "                for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                    page = pdf.pages[page_number]\n",
    "                    print(f\"\\nScraping tables on page {page_number + 1} for 'Description of Interconnection'...\", file=log_file)\n",
    "                    for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                        print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                        tables = page.find_tables(table_settings=table_settings)\n",
    "                        print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "                        for table_index, table in enumerate(tables, start=1):\n",
    "                            tab = table.extract()\n",
    "                            if not tab:\n",
    "                                print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                                continue\n",
    "                            print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                            for row_num, row in enumerate(tab, start=1):\n",
    "                                print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "                            for row_index, row in enumerate(tab, start=1):\n",
    "                                for cell_index, cell in enumerate(row, start=1):\n",
    "                                    if cell and desc_pattern.search(cell):\n",
    "                                        desc_col_index = cell_index\n",
    "                                        adjacent_col_index = desc_col_index + 1\n",
    "                                        if adjacent_col_index <= len(row):\n",
    "                                            poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                            if poi_value:\n",
    "                                                point_of_interconnection = poi_value\n",
    "                                                print(f\"\\nFound Description of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                                extraction_successful_desc = True\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                print(f\"\\n'Description of Interconnection' label found but adjacent value is empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                poi_value_parts = []\n",
    "                                                current_row_idx = row_index - 1\n",
    "                                                start_scan = max(0, current_row_idx - 2)\n",
    "                                                end_scan = min(len(tab), current_row_idx + 3)\n",
    "                                                print(f\"Scanning rows {start_scan + 1} to {end_scan} for Description value parts.\", file=log_file)\n",
    "                                                for scan_row_index in range(start_scan, end_scan):\n",
    "                                                    if scan_row_index == current_row_idx:\n",
    "                                                        continue\n",
    "                                                    scan_row = tab[scan_row_index]\n",
    "                                                    if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                        scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                        if scan_cell and not desc_pattern.search(scan_cell):\n",
    "                                                            poi_value_parts.append(scan_cell)\n",
    "                                                            print(f\"Found Description part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                        elif desc_pattern.search(scan_cell):\n",
    "                                                            print(f\"Encountered another Description label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                            continue\n",
    "                                                if poi_value_parts:\n",
    "                                                    point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                    print(f\"\\nConcatenated Description of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                    extraction_successful_desc = True\n",
    "                                                    break\n",
    "                                                else:\n",
    "                                                    print(f\"\\nNo Description value found in the surrounding rows (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        else:\n",
    "                                            print(f\"\\nDescription label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                if extraction_successful_desc:\n",
    "                                    break\n",
    "                            if extraction_successful_desc:\n",
    "                                break\n",
    "                        if extraction_successful_desc:\n",
    "                            break\n",
    "                    if extraction_successful_desc:\n",
    "                        break\n",
    "                if extraction_successful_desc:\n",
    "                    extraction_successful = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 2 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            print(\"Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            print(\"Neither 'Point of Interconnection' nor 'Description of Interconnection' found in Table 2.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "\n",
    "def fix_column_names(columns):\n",
    "    \"\"\"\n",
    "    Renames duplicate and empty column names.\n",
    "    Duplicate names are suffixed with _1, _2, etc.\n",
    "    Empty or whitespace-only names are replaced with unnamed_1, unnamed_2, etc.\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    counts = {}\n",
    "    unnamed_count = 1\n",
    "    for col in columns:\n",
    "        # Treat empty or whitespace-only names as unnamed.\n",
    "        if not col or col.strip() == \"\":\n",
    "            new_col = f\"unnamed_{unnamed_count}\"\n",
    "            unnamed_count += 1\n",
    "        else:\n",
    "            new_col = col.strip()\n",
    "        if new_col in counts:\n",
    "            new_col_with_suffix = f\"{new_col}_{counts[new_col]}\"\n",
    "            counts[new_col] += 1\n",
    "            new_cols.append(new_col_with_suffix)\n",
    "        else:\n",
    "            counts[new_col] = 1\n",
    "            new_cols.append(new_col)\n",
    "    return new_cols\n",
    "\n",
    "def post_process_columns(df, log_file):\n",
    "    \"\"\"\n",
    "    Post-processes DataFrame column names:\n",
    "      1. For any column named 'unnamed_#' (or empty), look at its first non-empty cell.\n",
    "         If that cell is not a dollar amount (i.e. does not match /^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$/)\n",
    "         and it contains 2 or 3 words, then rename the column to that value (after cleaning).\n",
    "         If a column already exists with that name, merge the data from the renamed column into the\n",
    "         existing column and drop the renamed column.\n",
    "      2. If a column is named \"Needed For\", then rename it to \"description\" (merging with an existing\n",
    "         description column if necessary).\n",
    "    \"\"\"\n",
    "    # Process unnamed columns.\n",
    "    for col in list(df.columns):\n",
    "        if col.lower().startswith(\"unnamed_\") or col.strip() == \"\":\n",
    "            # Find the first non-empty cell in this column.\n",
    "            first_non_empty = None\n",
    "            for val in df[col]:\n",
    "                cell_val = \"\"\n",
    "                if isinstance(val, str):\n",
    "                    cell_val = val.strip()\n",
    "                elif val is not None:\n",
    "                    cell_val = str(val).strip()\n",
    "                if cell_val:\n",
    "                    first_non_empty = cell_val\n",
    "                    break\n",
    "            if first_non_empty:\n",
    "                # Check if the value is a dollar amount.\n",
    "                if not re.match(r\"^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$\", first_non_empty):\n",
    "                    words = first_non_empty.split()\n",
    "                    if 2 <= len(words) <= 3:\n",
    "                        # Clean the candidate name.\n",
    "                        new_name = clean_column_headers([first_non_empty])[0]\n",
    "                        log_file.write(f\"Renaming column '{col}' to '{new_name}' based on first non-empty value '{first_non_empty}'.\\n\")\n",
    "                        if new_name in df.columns and new_name != col:\n",
    "                            # Merge the two columns: fill empty cells in existing new_name from the renamed col.\n",
    "                            for idx in df.index:\n",
    "                                existing_val = df.at[idx, new_name]\n",
    "                                candidate_val = df.at[idx, col]\n",
    "                                if (pd.isna(existing_val) or existing_val == \"\") and (not pd.isna(candidate_val) and candidate_val != \"\"):\n",
    "                                    df.at[idx, new_name] = candidate_val\n",
    "                            df.drop(columns=[col], inplace=True)\n",
    "                        else:\n",
    "                            df.rename(columns={col: new_name}, inplace=True)\n",
    "    # Process \"Needed For\" column: rename or merge it into \"description\".\n",
    "    if \"Needed For\" in df.columns:\n",
    "        if \"description\" in df.columns:\n",
    "            log_file.write(\"Merging 'Needed For' column into existing 'description' column.\\n\")\n",
    "            for idx in df.index:\n",
    "                desc_val = df.at[idx, \"description\"]\n",
    "                needed_for_val = df.at[idx, \"Needed For\"]\n",
    "                if (pd.isna(desc_val) or desc_val == \"\") and (not pd.isna(needed_for_val) and needed_for_val != \"\"):\n",
    "                    df.at[idx, \"description\"] = needed_for_val\n",
    "            df.drop(columns=[\"Needed For\"], inplace=True)\n",
    "        else:\n",
    "            log_file.write(\"Renaming 'Needed For' column to 'description'.\\n\")\n",
    "            df.rename(columns={\"Needed For\": \"description\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def extract_table3(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 3 data   the provided PDF.\n",
    "     \n",
    "      2. Renaming of duplicate/empty columns (using fix_column_names) and then post-processing\n",
    "         unnamed columns as described.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 10\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain either Table 3 patterns or Attachment 1/Attachment 2.\n",
    "            table3_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*10[-.]([1-3])\\b\", text, re.IGNORECASE):\n",
    "                    #re.search(r\"Attachment\\s*[12]\", text, re.IGNORECASE)):\n",
    "                    table3_pages.append(i)\n",
    "            if not table3_pages:\n",
    "                print(\"No Table 10  found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "            first_page = table3_pages[0]\n",
    "            last_page = table3_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Candidate pages start on {scrape_start + 1} and end on {scrape_end}\", file=log_file)\n",
    "            # Process each page that might contain table data.\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                # This variable keeps track of the bottom y-coordinate of the previous table on the page.\n",
    "                previous_table_bottom = None\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "                    table_bbox = table.bbox  # (x0, top, x1, bottom)\n",
    "                    # Define the title region for the table: above the table bounding box.\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*10[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(2).strip()\n",
    "                                break\n",
    "                        \n",
    "                  \n",
    "                    # Extract the specific phrase using the refined table title.\n",
    "                    if table_title:\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New table detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        # Rename header 'type' to 'type of upgrade' if needed.\n",
    "                        if \"type\" in headers and \"type of upgrade\" not in headers:\n",
    "                            headers = [(\"type of upgrade\" if h == \"type\" else h) for h in headers]\n",
    "                        if \"need for\" in headers:\n",
    "                            headers = [(\"description\" if h == \"need for\" else h) for h in headers]  \n",
    "                    \n",
    "                        # Apply the duplicate/empty column fixing.\n",
    "                        headers = fix_column_names(headers)\n",
    "                        data_rows = tab[1:]\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        if \"allocated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"allocated\"], inplace=True)\n",
    "                            print(f\"Dropped 'Max of' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate x \" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate x \"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file) \n",
    "\n",
    "                        if \"3339615 9\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"3339615 9\"], inplace=True)\n",
    "                            print(f\"Dropped '3339615 9' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)     \n",
    "                            \n",
    "                        if \"6 steady state reliability and posttransient voltage stability\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"6 steady state reliability and posttransient voltage stability\"], inplace=True)\n",
    "                            print(f\"Dropped '6 steady state reliability and posttransient voltage stability' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)  \n",
    "\n",
    "\n",
    "\n",
    "                        if \"escalated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"escalated\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)    \n",
    "\n",
    "\n",
    "                        # Also, if the DataFrame has a column named \"type\" (and not already \"type of upgrade\"), rename it.\n",
    "                        if 'type' in df_new.columns and 'type of upgrade' not in df_new.columns:\n",
    "                            df_new.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        # Special handling for ADNU tables if needed.\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        # Fix duplicate and unnamed columns in the new table.\n",
    "                        df_new.columns = fix_column_names(df_new.columns.tolist())\n",
    "                        # Now apply the post-processing of column names:\n",
    "                        df_new = post_process_columns(df_new, log_file)\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation table branch.\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous table title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"Continuation table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "                        # Use the number of columns from the last extracted table as expected.\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            data_rows = data_rows[1:]\n",
    "                        # Ensure every row has the same length as expected_headers.\n",
    "                        adjust_rows_length(data_rows, expected_headers)\n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "                        # Rename column 'type' if needed.\n",
    "                        if 'type' in df_continuation.columns and 'type of upgrade' not in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        if \"need for\" in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={\"need for\": \"description\"}, inplace=True)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        # Fix duplicate and unnamed columns in the continuation table.\n",
    "                        df_continuation.columns = fix_column_names(df_continuation.columns.tolist())\n",
    "                        # Post-process the columns in the continuation table.\n",
    "                        df_continuation = post_process_columns(df_continuation, log_file)\n",
    "                        # Concatenate the continuation table with the previous extracted table.\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                    # Update the previous_table_bottom for the page using the current table's bbox.\n",
    "                    previous_table_bottom = table_bbox[3]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 10 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "        print(\"\\nConcatenating all extracted Table 10/Attachment data...\", file=log_file)\n",
    "        try:\n",
    "            table3_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table3_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 10/Attachment data extracted.\", file=log_file)\n",
    "        table3_data = pd.DataFrame()\n",
    "    return table3_data\n",
    "\n",
    "\n",
    "def extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 10 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table3_data = extract_table3(pdf_path, log_file, is_addendum)\n",
    "    if table3_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        overlapping_columns = base_data.columns.intersection(table3_data.columns).difference(['point_of_interconnection'])\n",
    "        table3_data = table3_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        base_data_repeated = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "        try:\n",
    "\n",
    "                        # Concatenate base data with Table 8 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "           # if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "           #     merged_df[\"is_duplicate\"] = merged_df.duplicated(subset=[\"q_id\", \"type of upgrade\", \"upgrade\"], keep=\"first\")\n",
    "            #    merged_df = merged_df[merged_df[\"is_duplicate\"] == False].drop(columns=[\"is_duplicate\"])\n",
    "            #    print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade'.\", file=log_file)\n",
    "\n",
    "\n",
    "            if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "                # Identify rows where 'type of upgrade' and 'upgrade' are not empty\n",
    "                non_empty_rows = merged_df[\n",
    "                    merged_df[\"type of upgrade\"].notna() & merged_df[\"upgrade\"].notna() &\n",
    "                    (merged_df[\"type of upgrade\"].str.strip() != \"\") & (merged_df[\"upgrade\"].str.strip() != \"\")\n",
    "                ]\n",
    "\n",
    "                # Group by q_id, type of upgrade, and upgrade, keeping the first occurrence\n",
    "                grouped_df = non_empty_rows.groupby([\"q_id\", \"type of upgrade\", \"upgrade\"], as_index=False).first()\n",
    "\n",
    "                # Get the original order of the rows in merged_df before filtering\n",
    "                merged_df[\"original_index\"] = merged_df.index\n",
    "\n",
    "                # Combine unique grouped rows with originally empty rows\n",
    "                final_df = pd.concat([\n",
    "                    grouped_df,\n",
    "                    merged_df[merged_df[\"type of upgrade\"].isna() | (merged_df[\"type of upgrade\"].str.strip() == \"\") |\n",
    "                            merged_df[\"upgrade\"].isna() | (merged_df[\"upgrade\"].str.strip() == \"\")]\n",
    "                ], ignore_index=True, sort=False)\n",
    "\n",
    "                # Restore the original order of the rows based on the saved index\n",
    "                final_df.sort_values(by=\"original_index\", inplace=True)\n",
    "                final_df.drop(columns=[\"original_index\"], inplace=True)\n",
    "                merged_df = final_df\n",
    "\n",
    "                print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade', excluding empty rows while preserving order.\", file=log_file)\n",
    "\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            print(f\"Merged base data with Table 3 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 3 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data\n",
    "\n",
    "def check_has_table3(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 3 or Attachment 1/Attachment 2.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*10[-.]?\\d*\", text, re.IGNORECASE):\n",
    "                    #re.search(r\"Attachment\\s*[12]\", text, re.IGNORECASE))\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            return \"Addendum\" in text\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "        text = clean_string_cell(text)\n",
    "        #queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = str(project_id)\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "        point_of_interconnection = extract_table2(pdf_path, log_file)\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "    df = df.map(clean_string_cell)\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "    #if 'q_id' in df.columns:\n",
    "    #   df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "import os\n",
    "import re\n",
    "# (Other imports remain the same)\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"\n",
    "    Processes all PDFs in the directories within BASE_DIRECTORY whose numeric prefix is in PROJECT_RANGE.\n",
    "    This allows folders like '641' and '641AA' (if 641 is in the PROJECT_RANGE) to be processed,\n",
    "    and uses the full folder name (e.g. '641AA') as the project id (q_id).\n",
    "    \"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    " \n",
    "\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        # List all subdirectories in BASE_DIRECTORY that have a numeric prefix.\n",
    "        folders = [\n",
    "            folder for folder in os.listdir(BASE_DIRECTORY)\n",
    "            if os.path.isdir(os.path.join(BASE_DIRECTORY, folder)) and re.match(r'^(\\d+)', folder)\n",
    "        ]\n",
    "    \n",
    "\n",
    "\n",
    "        def sort_key(folder):\n",
    "            match = re.match(r'^(\\d+)', folder)\n",
    "            if match:\n",
    "                numeric = int(match.group(1))\n",
    "                return (numeric, folder)\n",
    "            return (float('inf'), folder)\n",
    "\n",
    "        # Sort the folders in ascending order.\n",
    "        sorted_folders = sorted(folders, key=sort_key)\n",
    "\n",
    "        # Process each folder in sorted order.\n",
    "        for folder in sorted_folders:\n",
    "            folder_path = os.path.join(BASE_DIRECTORY, folder)\n",
    "            match = re.match(r'^(\\d+)', folder)\n",
    "            if not match:\n",
    "                continue  # Skip if there is no numeric prefix.\n",
    "            numeric_part = int(match.group(1))\n",
    "            # Process the folder only if its numeric part is in the desired range.\n",
    "            if numeric_part not in PROJECT_RANGE:\n",
    "                continue\n",
    "\n",
    "            # Use the full folder name as the project identifier (q_id).\n",
    "            project_id = folder  # e.g., \"641AA\" or \"641\"\n",
    "            project_path = os.path.join(folder_path, \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    try:\n",
    "                        has_table3 = check_has_table3(pdf_path)\n",
    "                        if not has_table3:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\", file=log_file)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "                        if is_add and base_data_extracted:\n",
    "                            table3_data = extract_table3(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table3_data.empty:\n",
    "                                merged_df = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table3_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            df = extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "    # Save results and print summary as before.\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total projects (from 488 to 859) with a missing or empty '02_phase_1_study' folder: 94\n",
      "\n",
      "Folders with an empty '02_phase_1_study' subfolder (full paths):\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/488\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/490\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/494\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/502\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/503\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/509\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/512\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/555\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/579\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/585\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/640\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/642\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/643\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/643AK\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/643AS\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/643Z\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/647\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/649\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/649A\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/650A\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/651\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/653\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/653A\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/653D\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/653EB\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/663\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/664\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/668\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/669\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/674\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/676\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/681\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/683\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/684\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/685\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/686\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/692\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/696\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/697\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/698\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/700\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/703\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/704\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/706\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/712\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/716\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/717\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/729\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/730\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/732\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/736\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/739\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/741\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/756\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/764\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/766\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/767\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/770\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/771\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/774\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/782\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/783\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/784\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/785\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/786\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/788\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/790\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/791\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/792\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/793\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/801\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/804\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/807\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/812\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/813\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/834\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/836\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/839\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/840\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/841\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/842\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/843\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/845\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/846\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/847\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/848\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/849\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/850\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/851\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/852\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/853\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/854\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/857\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/858\n",
      "\n",
      "=== Projects Missing '02_phase_1_study' (Missing or Empty) Grouped by Cluster ===\n",
      "\n",
      "Cluster C01:\n",
      "  488\n",
      "  490\n",
      "  494\n",
      "  502\n",
      "  503\n",
      "  509\n",
      "  512\n",
      "\n",
      "Cluster C02:\n",
      "  555\n",
      "  585\n",
      "\n",
      "Cluster Unknown:\n",
      "  579\n",
      "  653EB\n",
      "  674\n",
      "\n",
      "Cluster SGIP-TC:\n",
      "  640\n",
      "  642\n",
      "  647\n",
      "  649\n",
      "  649A\n",
      "  650A\n",
      "  651\n",
      "  653\n",
      "  653A\n",
      "  653D\n",
      "  663\n",
      "  664\n",
      "\n",
      "Cluster C03:\n",
      "  643\n",
      "  643AK\n",
      "  643AS\n",
      "  643Z\n",
      "\n",
      "Cluster C04:\n",
      "  668\n",
      "  669\n",
      "  676\n",
      "  681\n",
      "  683\n",
      "  684\n",
      "  685\n",
      "  686\n",
      "  692\n",
      "  696\n",
      "  697\n",
      "  698\n",
      "  700\n",
      "  703\n",
      "  704\n",
      "  706\n",
      "  712\n",
      "  716\n",
      "  717\n",
      "  729\n",
      "  730\n",
      "  732\n",
      "  736\n",
      "  739\n",
      "  741\n",
      "  756\n",
      "  764\n",
      "  766\n",
      "  767\n",
      "  770\n",
      "  771\n",
      "  774\n",
      "  782\n",
      "  783\n",
      "  784\n",
      "  785\n",
      "  786\n",
      "  788\n",
      "  790\n",
      "  791\n",
      "  792\n",
      "  793\n",
      "  807\n",
      "  812\n",
      "  813\n",
      "  834\n",
      "  836\n",
      "  839\n",
      "  840\n",
      "  841\n",
      "  842\n",
      "  843\n",
      "  845\n",
      "  846\n",
      "  847\n",
      "  848\n",
      "  849\n",
      "  850\n",
      "  851\n",
      "  852\n",
      "  853\n",
      "  854\n",
      "  857\n",
      "  858\n",
      "\n",
      "Cluster nan:\n",
      "  801\n",
      "  804\n",
      "\n",
      "=== Projects with Empty '02_phase_1_study' but with Non-Empty '03_phase_2_study' or '05_reassesment' (Grouped by Cluster) ===\n",
      "\n",
      "Cluster C01:\n",
      "  494\n",
      "  502\n",
      "  503\n",
      "  512\n",
      "\n",
      "=== All Project Folder Names (Ascending Order) ===\n",
      "Project folder names (ascending order):\n",
      "'488', '490', '493', '494', '495', '502', '503', '506', '509', '510'\n",
      "'512', '513', '521', '522', '522C', '541', '552', '555', '557', '558'\n",
      "'559', '560', '561', '565', '568', '569', '574', '576', '577', '579'\n",
      "'581', '583', '585', '586', '588', '589', '590', '593', '602', '606'\n",
      "'607', '608', '628', '632AA', '632C', '640', '642', '643', '643AA', '643AB'\n",
      "'643AC', '643AE', '643AF', '643AH', '643AI', '643AJ', '643AK', '643AM', '643AP', '643AS'\n",
      "'643D', '643E', '643F', '643G', '643I', '643J', '643O', '643R', '643S', '643T'\n",
      "'643W', '643X', '643Z', '644', '644A', '645A', '647', '649', '649A', '649B'\n",
      "'649C', '650A', '650AA', '650AC', '651', '651A', '653', '653A', '653B', '653D'\n",
      "'653E', '653EA', '653EB', '653ED', '653F', '653H', '654', '658', '659', '660'\n",
      "'661', '662', '663', '664', '667', '668', '669', '670', '671', '674'\n",
      "'676', '678', '679', '680', '681', '683', '684', '685', '686', '687'\n",
      "'688', '692', '695', '696', '697', '698', '700', '702', '703', '704'\n",
      "'705', '706', '707', '708', '709', '712', '714', '716', '717', '720'\n",
      "'723', '725', '729', '730', '732', '736', '737', '738', '739', '740'\n",
      "'741', '744', '746', '751', '752', '756', '762', '764', '765', '766'\n",
      "'767', '768', '769', '770', '771', '774', '775', '778', '779', '781'\n",
      "'782', '783', '784', '785', '786', '788', '789', '790', '791', '792'\n",
      "'793', '794', '795', '796', '797', '798', '799', '800', '801', '804'\n",
      "'805', '806', '807', '809', '812', '813', '814', '815', '816', '820'\n",
      "'823', '824', '825', '829', '831', '834', '836', '837', '838', '839'\n",
      "'840', '841', '842', '843', '845', '846', '847', '848', '849', '850'\n",
      "'851', '852', '853', '854', '855', '856', '857', '858'\n",
      "\n",
      "=== Total Count by Cluster ===\n",
      "Cluster C01:\n",
      "  Missing or Empty '02_phase_1_study': 7\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 4\n",
      "Cluster C02:\n",
      "  Missing or Empty '02_phase_1_study': 2\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster C03:\n",
      "  Missing or Empty '02_phase_1_study': 4\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster C04:\n",
      "  Missing or Empty '02_phase_1_study': 64\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster SGIP-TC:\n",
      "  Missing or Empty '02_phase_1_study': 12\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster Unknown:\n",
      "  Missing or Empty '02_phase_1_study': 3\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster nan:\n",
      "  Missing or Empty '02_phase_1_study': 2\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# =====================================================\n",
    "# User Settings and File Paths (update these as needed)\n",
    "# =====================================================\n",
    "\n",
    "# Base folder containing all project folders\n",
    "base_folder = r'/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data'\n",
    "\n",
    "# Numeric range to consider (based on the numeric prefix of folder names)\n",
    "range_start = 488   # (Update these values if needed)\n",
    "range_end   = 859\n",
    "\n",
    "# Path to your CSV file (which must have columns \"q_id\" and \"cluster_number\")\n",
    "phase_status_csv = r'/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/phase_status.csv'\n",
    "\n",
    "# =====================================================\n",
    "# Read CSV and Build Cluster Mapping\n",
    "# =====================================================\n",
    "\n",
    "# Read the CSV ensuring that the q_id is treated as a string\n",
    "phase_status = pd.read_csv(phase_status_csv, dtype={'q_id': str})\n",
    "# Create a mapping from q_id to cluster_number\n",
    "cluster_mapping = dict(zip(phase_status['q_id'], phase_status['cluster_number']))\n",
    "\n",
    "# =====================================================\n",
    "# Initialize Data Structures for Results\n",
    "# =====================================================\n",
    "\n",
    "empty_phase1_count = 0          # Count of projects with an empty \"02_phase_1_study\"\n",
    "empty_phase1_folders = []       # List of folder paths with an empty \"02_phase_1_study\" subfolder\n",
    "\n",
    "# Dictionaries to group projects by cluster:\n",
    "# 1. Projects missing Phase 1 (i.e. the folder is missing or exists but is empty)\n",
    "missing_phase1_by_cluster = {}  \n",
    "\n",
    "# 2. Projects that have an empty Phase 1 folder but have non-empty \"03_phase_2_study\" or \"05_reassesment\"\n",
    "empty_phase1_with_following_by_cluster = {}  \n",
    "\n",
    "# List to hold all project folder names (for final sorted list)\n",
    "project_folders = []\n",
    "\n",
    "# =====================================================\n",
    "# Gather and Sort Candidate Folders\n",
    "# =====================================================\n",
    "\n",
    "# We use a list of tuples (folder_name, numeric_prefix) so we can sort by the numeric value\n",
    "folder_candidates = []\n",
    "for item in os.listdir(base_folder):\n",
    "    item_path = os.path.join(base_folder, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        # Use regex to extract the numeric prefix (works for names like \"641\" or \"641AA\")\n",
    "        m = re.match(r'^(\\d+)', item)\n",
    "        if m:\n",
    "            numeric_part = int(m.group(1))\n",
    "            if range_start <= numeric_part <= range_end:\n",
    "                folder_candidates.append((item, numeric_part))\n",
    "\n",
    "# Sort candidates in ascending order first by the numeric prefix, then alphabetically by the full folder name\n",
    "folder_candidates.sort(key=lambda x: (x[1], x[0]))\n",
    "\n",
    "# =====================================================\n",
    "# Process Each Candidate Folder\n",
    "# =====================================================\n",
    "\n",
    "for folder_name, num in folder_candidates:\n",
    "    # Add folder name to final list (for reporting later)\n",
    "    project_folders.append(folder_name)\n",
    "    \n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "    # Build subfolder paths\n",
    "    phase1_path = os.path.join(folder_path, \"02_phase_1_study\")\n",
    "    phase2_path = os.path.join(folder_path, \"03_phase_2_study\")\n",
    "    reassessment_path = os.path.join(folder_path, \"05_reassesment\")\n",
    "    \n",
    "    # Determine the cluster for this project using the CSV mapping; default to \"Unknown\" if not found.\n",
    "    cluster = cluster_mapping.get(folder_name, \"Unknown\")\n",
    "    \n",
    "    # Check for \"missing\" Phase 1:\n",
    "    # Here, we define \"missing\" as: either the Phase 1 folder does not exist OR it exists but is empty.\n",
    "    phase1_missing = False\n",
    "    if not os.path.isdir(phase1_path):\n",
    "        phase1_missing = True\n",
    "    else:\n",
    "        # The folder exists; check if it is empty.\n",
    "        if not os.listdir(phase1_path):\n",
    "            phase1_missing = True\n",
    "            empty_phase1_count += 1\n",
    "            empty_phase1_folders.append(folder_path)\n",
    "    \n",
    "    if phase1_missing:\n",
    "        missing_phase1_by_cluster.setdefault(cluster, []).append(folder_name)\n",
    "    \n",
    "        # Additionally, if the folder exists (but is empty) and at least one of the follow-up folders is non-empty,\n",
    "        # then add it to the separate grouping.\n",
    "        if os.path.isdir(phase1_path):  # only check follow-ups if the folder exists (even though it's empty)\n",
    "            phase2_non_empty = os.path.isdir(phase2_path) and bool(os.listdir(phase2_path))\n",
    "            reassessment_non_empty = os.path.isdir(reassessment_path) and bool(os.listdir(reassessment_path))\n",
    "            if phase2_non_empty or reassessment_non_empty:\n",
    "                empty_phase1_with_following_by_cluster.setdefault(cluster, []).append(folder_name)\n",
    "\n",
    "# =====================================================\n",
    "# Output the Results\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\nTotal projects (from {range_start} to {range_end}) with a missing or empty '02_phase_1_study' folder: {len(sum(missing_phase1_by_cluster.values(), []))}\")\n",
    "\n",
    "if empty_phase1_folders:\n",
    "    print(\"\\nFolders with an empty '02_phase_1_study' subfolder (full paths):\")\n",
    "    for folder in empty_phase1_folders:\n",
    "        print(folder)\n",
    "\n",
    "print(\"\\n=== Projects Missing '02_phase_1_study' (Missing or Empty) Grouped by Cluster ===\")\n",
    "if missing_phase1_by_cluster:\n",
    "    for clust, folders in missing_phase1_by_cluster.items():\n",
    "        print(f\"\\nCluster {clust}:\")\n",
    "        for folder in folders:\n",
    "            print(f\"  {folder}\")\n",
    "else:\n",
    "    print(\"No projects found missing the '02_phase_1_study' folder.\")\n",
    "\n",
    "print(\"\\n=== Projects with Empty '02_phase_1_study' but with Non-Empty '03_phase_2_study' or '05_reassesment' (Grouped by Cluster) ===\")\n",
    "if empty_phase1_with_following_by_cluster:\n",
    "    for clust, folders in empty_phase1_with_following_by_cluster.items():\n",
    "        print(f\"\\nCluster {clust}:\")\n",
    "        for folder in folders:\n",
    "            print(f\"  {folder}\")\n",
    "else:\n",
    "    print(\"No projects found with empty '02_phase_1_study' but non-empty subsequent phases.\")\n",
    "\n",
    "print(\"\\n=== All Project Folder Names (Ascending Order) ===\")\n",
    "# Build a list of just the folder names\n",
    "project_folders = [folder for folder, num in folder_candidates]\n",
    "\n",
    "# Print the list in a format you can copy and paste\n",
    "# Print the list with 10 items per line\n",
    "print(\"Project folder names (ascending order):\")\n",
    "line_length = 10\n",
    "for i in range(0, len(project_folders), line_length):\n",
    "    line_items = project_folders[i:i+line_length]\n",
    "    # Join each item wrapped in quotes and separated by a comma and space\n",
    "    line = \", \".join(f\"'{item}'\" for item in line_items)\n",
    "    print(line)\n",
    "\n",
    "# =====================================================\n",
    "# Print Total Counts by Cluster\n",
    "# =====================================================\n",
    "\n",
    "# Get all clusters that appear in either dictionary.\n",
    "all_clusters = set(missing_phase1_by_cluster.keys()).union(set(empty_phase1_with_following_by_cluster.keys()))\n",
    "\n",
    "print(\"\\n=== Total Count by Cluster ===\")\n",
    "for clust in sorted(all_clusters, key=lambda x: str(x)):\n",
    "    count_missing = len(missing_phase1_by_cluster.get(clust, []))\n",
    "    count_following = len(empty_phase1_with_following_by_cluster.get(clust, []))\n",
    "    print(f\"Cluster {clust}:\")\n",
    "    print(f\"  Missing or Empty '02_phase_1_study': {count_missing}\")\n",
    "    print(f\"  Empty '02_phase_1_study' with Follow-up Data: {count_following}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skipped by both styles but not present in the missing list:\n",
    " [521, a combined report for phase 1 and phase 2 present under phase 2\n",
    "  522, \n",
    "  \n",
    "  541, missing\n",
    "  552, missing\n",
    "  558, missing\n",
    "  667, was scraped\n",
    "    737, no table\n",
    "    795] group report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparison for Scraped Projects ===\n",
      "Total Style C Scraped Projects: 87\n",
      "Total Style R Scraped Projects: 30\n",
      "Common Scraped Projects (count = 0):\n",
      "[]\n",
      "Projects unique to Style C (count = 87):\n",
      "['495', '557', '559', '560', '568', '569', '576', '577', '581', '586', '588', '589', '593', '602', '606', '607', '632C', '643AB', '643AC', '643AE', '643AJ', '643D', '643E', '643F', '643G', '643J', '643O', '643R', '643S', '643X', '644', '645A', '650AC', '653B', '653E', '653EA', '653F', '654', '670', '671', '678', '679', '680', '687', '688', '695', '702', '705', '707', '708', '709', '714', '720', '723', '725', '738', '740', '744', '746', '751', '752', '762', '765', '768', '769', '775', '778', '779', '796', '797', '798', '799', '800', '805', '806', '809', '814', '815', '816', '820', '823', '824', '825', '829', '831', '855', '856']\n",
      "Projects unique to Style R (count = 30):\n",
      "['493', '510', '561', '565', '574', '583', '590', '608', '628', '643AM', '643AP', '643T', '644A', '649B', '649C', '650AA', '651A', '653ED', '653H', '658', '659', '660', '661', '662', '667', '781', '789', '794', '837', '838']\n",
      "\n",
      "=== Comparison for Skipped Projects ===\n",
      "Total Style C Skipped Projects: 141\n",
      "Total Style R Skipped Projects: 198\n",
      "Common Skipped Projects (count = 111):\n",
      "['488', '490', '494', '502', '503', '506', '509', '512', '513', '521', '522', '522C', '541', '552', '555', '558', '579', '585', '632AA', '640', '642', '643', '643AA', '643AF', '643AH', '643AI', '643AK', '643AS', '643I', '643W', '643Z', '647', '649', '649A', '650A', '651', '653', '653A', '653D', '653EB', '663', '664', '668', '669', '674', '676', '681', '683', '684', '685', '686', '692', '696', '697', '698', '700', '703', '704', '706', '712', '716', '717', '729', '730', '732', '736', '737', '739', '741', '756', '764', '766', '767', '770', '771', '774', '782', '783', '784', '785', '786', '788', '790', '791', '792', '793', '795', '801', '804', '807', '812', '813', '834', '836', '839', '840', '841', '842', '843', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '857', '858']\n",
      "Common Skipped Projects Missing (count = 93):\n",
      "['494', '502', '503', '506', '509', '521', '522', '522C', '541', '552', '555', '558', '579', '585', '632AA', '640', '642', '643', '643AA', '643AF', '643AH', '643AI', '643AK', '643AS', '643I', '643Z', '647', '649', '649A', '651', '653', '653A', '653D', '653EB', '663', '664', '668', '669', '674', '681', '683', '684', '685', '686', '696', '697', '698', '700', '703', '704', '712', '716', '717', '729', '730', '732', '736', '737', '739', '756', '764', '766', '770', '771', '774', '784', '785', '786', '788', '790', '791', '792', '795', '801', '804', '807', '812', '813', '834', '836', '839', '842', '843', '845', '846', '847', '848', '849', '850', '853', '854', '857', '858']\n",
      "Skipped Projects Not Missing (count = 18):\n",
      "['488', '490', '512', '513', '643W', '650A', '676', '692', '706', '741', '767', '782', '783', '793', '840', '841', '851', '852']\n",
      "Projects unique to Style C (count = 30):\n",
      "['493', '510', '561', '565', '574', '583', '590', '608', '628', '643AM', '643AP', '643T', '644A', '649B', '649C', '650AA', '651A', '653ED', '653H', '658', '659', '660', '661', '662', '667', '781', '789', '794', '837', '838']\n",
      "Projects unique to Style R (count = 87):\n",
      "['495', '557', '559', '560', '568', '569', '576', '577', '581', '586', '588', '589', '593', '602', '606', '607', '632C', '643AB', '643AC', '643AE', '643AJ', '643D', '643E', '643F', '643G', '643J', '643O', '643R', '643S', '643X', '644', '645A', '650AC', '653B', '653E', '653EA', '653F', '654', '670', '671', '678', '679', '680', '687', '688', '695', '702', '705', '707', '708', '709', '714', '720', '723', '725', '738', '740', '744', '746', '751', '752', '762', '765', '768', '769', '775', '778', '779', '796', '797', '798', '799', '800', '805', '806', '809', '814', '815', '816', '820', '823', '824', '825', '829', '831', '855', '856']\n",
      "Projects unique to Empty (count = 112):\n",
      "['488490', '493', '495', '510', '512513', '557', '559560', '561', '565', '568', '569', '574', '576', '577', '581583', '586', '588', '589', '590', '593', '602', '606', '607608', '628', '632C', '643AB', '643AC643AE', '643AJ', '643AM', '643AP', '643D643E', '643F', '643G', '643J', '643O', '643R', '643S', '643T', '643W643X', '644', '644A', '645A', '649B', '649C650A', '650AA', '650AC', '651A', '653B', '653E653EA', '653ED', '653F', '653H', '654', '658', '659', '660', '661662', '667', '670', '671', '676678', '679', '680', '687', '688692', '695', '702', '705706', '707', '708', '709', '714', '720', '723725', '738', '740', '741744', '746', '751', '752', '762', '765', '767768', '769', '775', '778', '779', '781', '782783', '789', '793794', '796', '797', '798', '799', '800', '805806', '809', '814', '815', '816', '820', '823824', '825', '829', '831', '837', '838', '840841', '851852', '855', '856']\n"
     ]
    }
   ],
   "source": [
    "# Define the lists for Style C\n",
    "styleC_scraped = ['495', '557', '559', '560', '568', '569', '576', '577', '581', '586', '588', '589', '593', '602', '606', '607', '632C', '643AB', '643AC', '643AE', '643AJ', '643D', '643E', '643F', '643G', '643J', '643O', '643R', '643S', '643X', '644', '645A', '650AC', '653B', '653E', '653EA', '653F', '654', '670', '671', '678', '679', '680', '687', '688', '695', '702', '705', '707', '708', '709', '714', '720', '723', '725', '738', '740', '744', '746', '751', '752', '762', '765', '768', '769', '775', '778', '779', '796', '797', '798', '799', '800', '805', '806', '809', '814', '815', '816', '820', '823', '824', '825', '829', '831', '855', '856']\n",
    "\n",
    "styleC_skipped = ['488', '490', '493', '494', '502', '503', '506', '509', '510', '512', '513', '521', '522', '522C', '541', '552', '555', '558', '561', '565', '574', '579', '583', '585', '590', '608', '628', '632AA', '640', '642', '643', '643AA', '643AF', '643AH', '643AI', '643AK', '643AM', '643AP', '643AS', '643I', '643T', '643W', '643Z', '644A', '647', '649', '649A', '649B', '649C', '650A', '650AA', '651', '651A', '653', '653A', '653D', '653EB', '653ED', '653H', '658', '659', '660', '661', '662', '663', '664', '667', '668', '669', '674', '676', '681', '683', '684', '685', '686', '692', '696', '697', '698', '700', '703', '704', '706', '712', '716', '717', '729', '730', '732', '736', '737', '739', '741', '756', '764', '766', '767', '770', '771', '774', '781', '782', '783', '784', '785', '786', '788', '789', '790', '791', '792', '793', '794', '795', '801', '804', '807', '812', '813', '834', '836', '837', '838', '839', '840', '841', '842', '843', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '857', '858']\n",
    "\n",
    "# Define the lists for Style R\n",
    "styleR_scraped = ['493', '510', '561', '565', '574', '583', '590', '608', '628', '643AM', '643AP', '643T', '644A', '649B', '649C', '650AA', '651A', '653ED', '653H', '658', '659', '660', '661', '662', '667', '781', '789', '794', '837', '838']\n",
    "\n",
    "styleR_skipped = ['488', '490', '494', '495', '502', '503', '506', '509', '512', '513', '521', '522', '522C', '541', '552', '555', '557', '558', '559', '560', '568', '569', '576', '577', '579', '581', '585', '586', '588', '589', '593', '602', '606', '607', '632AA', '632C', '640', '642', '643', '643AA', '643AB', '643AC', '643AE', '643AF', '643AH', '643AI', '643AJ', '643AK', '643AS', '643D', '643E', '643F', '643G', '643I', '643J', '643O', '643R', '643S', '643W', '643X', '643Z', '644', '645A', '647', '649', '649A', '650A', '650AC', '651', '653', '653A', '653B', '653D', '653E', '653EA', '653EB', '653F', '654', '663', '664', '668', '669', '670', '671', '674', '676', '678', '679', '680', '681', '683', '684', '685', '686', '687', '688', '692', '695', '696', '697', '698', '700', '702', '703', '704', '705', '706', '707', '708', '709', '712', '714', '716', '717', '720', '723', '725', '729', '730', '732', '736', '737', '738', '739', '740', '741', '744', '746', '751', '752', '756', '762', '764', '765', '766', '767', '768', '769', '770', '771', '774', '775', '778', '779', '782', '783', '784', '785', '786', '788', '790', '791', '792', '793', '795', '796', '797', '798', '799', '800', '801', '804', '805', '806', '807', '809', '812', '813', '814', '815', '816', '820', '823', '824', '825', '829', '831', '834', '836', '839', '840', '841', '842', '843', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '857', '858']\n",
    "\n",
    "\n",
    "\n",
    "empty= ['488'\n",
    "'490', '493', '494', '495', '502', '503', '506', '509', '510', '512'\n",
    "'513', '521', '522', '522C', '541', '552', '555', '557', '558', '559'\n",
    "'560', '561', '565', '568', '569', '574', '576', '577', '579', '581'\n",
    "'583', '585', '586', '588', '589', '590', '593', '602', '606', '607'\n",
    "'608', '628', '632AA', '632C', '640', '642', '643', '643AA', '643AB', '643AC'\n",
    "'643AE', '643AF', '643AH', '643AI', '643AJ', '643AK', '643AM', '643AP', '643AS', '643D'\n",
    "'643E', '643F', '643G', '643I', '643J', '643O', '643R', '643S', '643T', '643W'\n",
    "'643X', '643Z', '644', '644A', '645A', '647', '649', '649A', '649B', '649C'\n",
    "'650A', '650AA', '650AC', '651', '651A', '653', '653A', '653B', '653D', '653E'\n",
    "'653EA', '653EB', '653ED', '653F', '653H', '654', '658', '659', '660', '661'\n",
    "'662', '663', '664', '667', '668', '669', '670', '671', '674', '676'\n",
    "'678', '679', '680', '681', '683', '684', '685', '686', '687', '688'\n",
    "'692', '695', '696', '697', '698', '700', '702', '703', '704', '705'\n",
    "'706', '707', '708', '709', '712', '714', '716', '717', '720', '723'\n",
    "'725', '729', '730', '732', '736', '737', '738', '739', '740', '741'\n",
    "'744', '746', '751', '752', '756', '762', '764', '765', '766', '767'\n",
    "'768', '769', '770', '771', '774', '775', '778', '779', '781', '782'\n",
    "'783', '784', '785', '786', '788', '789', '790', '791', '792', '793'\n",
    "'794', '795', '796', '797', '798', '799', '800', '801', '804', '805'\n",
    "'806', '807', '809', '812', '813', '814', '815', '816', '820', '823'\n",
    "'824', '825', '829', '831', '834', '836', '837', '838', '839', '840'\n",
    "'841', '842', '843', '845', '846', '847', '848', '849', '850', '851'\n",
    "'852', '853', '854', '855', '856', '857', '858']\n",
    "# ------------------------------\n",
    "# Comparison for Scraped Projects\n",
    "# ------------------------------\n",
    "\n",
    "# Convert lists to sets for set operations\n",
    "set_styleC_scraped = set(styleC_scraped)\n",
    "set_styleR_scraped = set(styleR_scraped)\n",
    "set_empty = set(empty)\n",
    "\n",
    "# Common projects in scraped lists (intersection)\n",
    "common_scraped = set_styleC_scraped & set_styleR_scraped\n",
    "\n",
    "# Projects unique to each style\n",
    "unique_to_styleC_scraped = set_styleC_scraped - set_styleR_scraped\n",
    "unique_to_styleR_scraped = set_styleR_scraped - set_styleC_scraped\n",
    "\n",
    "# ------------------------------\n",
    "# Comparison for Skipped Projects\n",
    "# ------------------------------\n",
    "\n",
    "set_styleC_skipped = set(styleC_skipped)\n",
    "set_styleR_skipped = set(styleR_skipped)\n",
    "set_empty = set(empty)\n",
    "\n",
    "# Common projects in skipped lists (intersection)\n",
    "common_skipped = set_styleC_skipped & set_styleR_skipped\n",
    "common_skipped_missing = common_skipped & set_empty\n",
    "skipped_but_not_missing = common_skipped - common_skipped_missing\n",
    "\n",
    "# Projects unique to each style\n",
    "unique_to_styleC_skipped = set_styleC_skipped - set_styleR_skipped\n",
    "unique_to_styleR_skipped = set_styleR_skipped - set_styleC_skipped\n",
    "unique_to_empty = set_empty - common_skipped\n",
    "\n",
    "# ------------------------------\n",
    "# Output the Results\n",
    "# ------------------------------\n",
    "\n",
    "print(\"=== Comparison for Scraped Projects ===\")\n",
    "print(\"Total Style C Scraped Projects:\", len(styleC_scraped))\n",
    "print(\"Total Style R Scraped Projects:\", len(styleR_scraped))\n",
    "print(\"Common Scraped Projects (count = {}):\".format(len(common_scraped)))\n",
    "print(sorted(common_scraped))\n",
    "print(\"Projects unique to Style C (count = {}):\".format(len(unique_to_styleC_scraped)))\n",
    "print(sorted(unique_to_styleC_scraped))\n",
    "print(\"Projects unique to Style R (count = {}):\".format(len(unique_to_styleR_scraped)))\n",
    "print(sorted(unique_to_styleR_scraped))\n",
    "\n",
    "print(\"\\n=== Comparison for Skipped Projects ===\")\n",
    "print(\"Total Style C Skipped Projects:\", len(styleC_skipped))\n",
    "print(\"Total Style R Skipped Projects:\", len(styleR_skipped))\n",
    "print(\"Common Skipped Projects (count = {}):\".format(len(common_skipped)))\n",
    "print(sorted(common_skipped))\n",
    "print(\"Common Skipped Projects Missing (count = {}):\".format(len(common_skipped_missing)))\n",
    "print(sorted(common_skipped_missing))\n",
    "print(\"Skipped Projects Not Missing (count = {}):\".format(len(skipped_but_not_missing)))\n",
    "print(sorted(skipped_but_not_missing))\n",
    "\n",
    "print(\"Projects unique to Style C (count = {}):\".format(len(unique_to_styleC_skipped)))\n",
    "print(sorted(unique_to_styleC_skipped))\n",
    "print(\"Projects unique to Style R (count = {}):\".format(len(unique_to_styleR_skipped)))\n",
    "print(sorted(unique_to_styleR_skipped))\n",
    "print(\"Projects unique to Empty (count = {}):\".format(len(unique_to_empty)))\n",
    "print(sorted(unique_to_empty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 643A has a table 10, but was not scraped. other 17 are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Itemzied and Merged datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 495\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 557\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 559\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 560\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 568\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Processing q_id: 569\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 576\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 577\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 581\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Processing q_id: 586\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 588\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 589\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 593\n",
      "\n",
      "Checking Upgrade: ADNU, Total Rows Present?: False\n",
      "Creating Total row for ADNU\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 602\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 606\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 607\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Processing q_id: 632C\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 643AB\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643AC\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643AE\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643D\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643E\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643F\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643G\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643J\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643O\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643X\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Processing q_id: 644\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 645A\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 650AC\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 653B\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 653E\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 653EA\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 653F\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 654\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Processing q_id: 670\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 671\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 678\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 679\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 680\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Processing q_id: 687\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 688\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 695\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 702\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 705\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 707\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Processing q_id: 708\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 709\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 714\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 720\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 725\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 738\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 740\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 744\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 746\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 751\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Processing q_id: 752\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 762\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 765\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 768\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 769\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 775\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 778\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 779\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 796\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 799\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 800\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 805\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 806\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 809\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 814\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 815\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 816\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 823\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 825\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 829\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 831\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 855\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 856\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "     q_id  cluster req_deliverability  latitude longitude  capacity  \\\n",
      "0    495      NaN               None       NaN      None       NaN   \n",
      "1    495      NaN               None       NaN      None       NaN   \n",
      "2    557      NaN               None       NaN      None       NaN   \n",
      "3    557      NaN               None       NaN      None       NaN   \n",
      "4    557      NaN               None       NaN      None       NaN   \n",
      "..   ...      ...                ...       ...       ...       ...   \n",
      "198  855      4.0               Full       NaN      None     150.0   \n",
      "199  855      4.0               Full       NaN      None     150.0   \n",
      "200  856      4.0               None       NaN      None       NaN   \n",
      "201  856      4.0               None       NaN      None       NaN   \n",
      "202  856      4.0               None       NaN      None       NaN   \n",
      "\n",
      "                              point_of_interconnection type_of_upgrade  \\\n",
      "0                       Melones  Riverbank 115 kV Line    Total PTO_IF   \n",
      "1                       Melones  Riverbank 115 kV Line       Total RNU   \n",
      "2    Interconnect to Smyrna -Alpaugh115 kV Line via...      Total LDNU   \n",
      "3    Interconnect to Smyrna -Alpaugh115 kV Line via...    Total PTO_IF   \n",
      "4    Interconnect to Smyrna -Alpaugh115 kV Line via...       Total RNU   \n",
      "..                                                 ...             ...   \n",
      "198            Sempra Owned Merchant 220 kV Substation    Total PTO_IF   \n",
      "199            Sempra Owned Merchant 220 kV Substation       Total RNU   \n",
      "200                                 Monolith 66 kV Bus      Total LDNU   \n",
      "201                                 Monolith 66 kV Bus    Total PTO_IF   \n",
      "202                                 Monolith 66 kV Bus       Total RNU   \n",
      "\n",
      "    upgrade description cost_allocation_factor  estimated_cost_x_1000  \\\n",
      "0                                                             45000.0   \n",
      "1                                                            160000.0   \n",
      "2                                                              1900.0   \n",
      "3                                                               400.0   \n",
      "4                                                               310.0   \n",
      "..      ...         ...                    ...                    ...   \n",
      "198                                                             259.0   \n",
      "199                                                            3555.0   \n",
      "200                                                           20864.0   \n",
      "201                                                             590.0   \n",
      "202                                                              11.0   \n",
      "\n",
      "    estimated_time_to_construct item  \n",
      "0                                 no  \n",
      "1                                 no  \n",
      "2                                 no  \n",
      "3                                 no  \n",
      "4                                 no  \n",
      "..                          ...  ...  \n",
      "198                               no  \n",
      "199                               no  \n",
      "200                               no  \n",
      "201                               no  \n",
      "202                               no  \n",
      "\n",
      "[203 rows x 14 columns]\n",
      "Itemized rows saved to 'costs_phase_1_cluster_5_style_D_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_1_cluster_5_style_D_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU' 'ADNU']\n",
      "['495' '557' '559' '560' '568' '569' '576' '577' '581' '586' '588' '589'\n",
      " '593' '602' '606' '607' '632C' '643AB' '643AC' '643AE' '643D' '643E'\n",
      " '643F' '643G' '643J' '643O' '643X' '644' '645A' '650AC' '653B' '653E'\n",
      " '653EA' '653F' '654' '670' '671' '678' '679' '680' '687' '688' '695'\n",
      " '702' '705' '707' '708' '709' '714' '720' '725' '738' '740' '744' '746'\n",
      " '751' '752' '762' '765' '768' '769' '775' '778' '779' '796' '799' '800'\n",
      " '805' '806' '809' '814' '815' '816' '823' '825' '829' '831' '855' '856']\n",
      "[nan  2.  4.  3.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/03_raw/rawdata_cluster1_4_style_C_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "#df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'cost allocation factor' in df.columns:\n",
    "#    df['description'] = df['cost allocation factor'].apply(extract_non_numeric_text)\n",
    "#    df['cost_allocation_factor'] = df['cost allocation factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "             \n",
    "            ],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"MW\",\n",
    "            \n",
    "        ],   \n",
    "\n",
    "\n",
    "\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \n",
    "\n",
    "            \"estimated cost x 1000\",\n",
    "            \"estimated_cost\",\n",
    "            \"estimated cost\",\n",
    "            \n",
    "            \"estimated cost x 1000 constant dollar_1\",\n",
    "            \"estimated cost x 1000 constant dollar\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"estimated cost x\",\n",
    "            \"allocated_cost\",\n",
    "            \"assigned cost\",\n",
    "            \"allocated cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "             \n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \n",
    "            \"escalated cost x 1000 constant dollar\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"assigned cost escalated\",\n",
    "            \n",
    "             \n",
    "\n",
    "        ],\n",
    "\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "            \n",
    "\n",
    "\n",
    "        ],\n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "       \n",
    "         \n",
    "\n",
    "        \"description\": [\"description\",  ],\n",
    "\n",
    "        \n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocation\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "            \"project allocation\",\n",
    "            \"percent allocation\",\n",
    "\n",
    "        ],\n",
    "       \n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "#if 'upgrade' in df.columns:\n",
    " #   df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    "\n",
    "#df.drop(['unnamed_3', 'unnamed_15', 'unnamed_18', 'unnamed_16', 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "df.drop(['cost', 'total cost','cost_1', 'interconnection facility element', '11 technicalrequirement', '11 technical requirement', 'unnamed_5','unnamed_2','unnamed_3',\n",
    "         'preqc3', '4937 a', 'unnamed_6', 'unnamed_1'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "#df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "df['tot'] = df.apply(\n",
    "    lambda row: 'yes' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) \n",
    "        ) else 'no',\n",
    "    axis=1\n",
    ") \n",
    "\n",
    "# Now extract ONLY \"Total\" rows with a foolproof match\n",
    "total_rows_df = df[df['tot'] == 'yes']\n",
    "\n",
    "total_rows_df = total_rows_df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "total_rows_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/02_intermediate/costs_phase_1_cluster_1_4_style_C_total_network.csv', index=False) \n",
    "df = df[df['cost_allocation_factor'].str.strip().str.lower() != 'total']\n",
    " \n",
    "df.drop('tot', axis=1, inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)  \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    \"PTO\u2019s Interconnection Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTO\u2019s Interconnectio n Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTOs Interconnection Facilities\": \"PTO_IF\",\n",
    "    \"PTOs Interconnectio n Facilities\": \"PTO_IF\",\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Delivery Network\": \"ADNU\",\n",
    " \"Plan of Service Reliability Network Upgrades\": \"RNU\",\n",
    " \"Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Reliability Network Upgrades\": \"RNU\",\n",
    " \"SDG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Upgrades\": \"LDNU\",\n",
    " \"SCE Distribution Upgrades\": \"LDNU\",\n",
    " \"SCE Reliability Network Upgrades for Short Circuit duty\": \"RNU\",\n",
    " \"SCE Network Upgrades\": \"RNU\",\n",
    " \"Plan of Service Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Network Upgrades\": \"LDNU\",\n",
    "\n",
    " \"Reliability Network Upgrades\": \"RNU\",\n",
    "    \"Local Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Area Deliverability Upgrades\": \"ADNU\",\n",
    "    \"Escalated Cost and Time to Construct for Interconnection Facilities, Reliability Network Upgrades, and Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Distribution\": \"ADNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    "}\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()    \n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/03_raw/cluster_5_style_D.csv', index=False)\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "#df = stable_sort_by_type_of_upgrade(df)  \n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000',]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ( (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    \"\"\"\n",
    "    Removes the word 'month' or 'months' (case insensitive) from the value.\n",
    "    Leaves behind any numbers or number ranges (e.g. \"6\", \"6-12\").\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Remove 'month' or 'months' (case-insensitive), optionally with spaces around them.\n",
    "        cleaned_value = re.sub(r'(?i)\\s*months?\\s*', '', value)\n",
    "        return cleaned_value.strip()\n",
    "    return value\n",
    "\n",
    "# Then apply it to your column, for example with Pandas:\n",
    "df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "#df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df = itemized_df.drop_duplicates(subset=['q_id', 'type_of_upgrade', 'upgrade'])\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/02_intermediate/costs_phase_1_cluster_1_4_style_C_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    #totals_df = totals_df.drop_duplicates(subset=['q_id', 'type_of_upgrade', 'upgrade'])\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/02_intermediate/costs_phase_1_cluster_1_4_style_C_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_1_cluster_5_style_D_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_1_cluster_5_style_D_total.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n",
    "\n",
    "\n",
    "#df.to_csv('Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 14/03_raw/rawdata_cluster14_style_Q.csv')\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}