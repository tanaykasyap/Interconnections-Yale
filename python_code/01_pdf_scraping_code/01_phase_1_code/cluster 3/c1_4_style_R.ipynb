{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: Appendix A - C493_06-25-2010_Final.pdf from Project 493\n",
      "Skipped PDF: Q495_ Cluster 1_Report_Appendix A_final.pdf from Project 495 (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - C510_06-25-2010_Final.pdf from Project 510\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q521 Columbia 1.pdf from Project 521 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q522 Columbia 2.pdf from Project 522 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666541-Appendix_A__Q557_Cluster2_Phase_I_study_report_rev1.pdf from Project 557 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q557 Cluster2 Phase I study report.pdf from Project 557 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS665861-Appendix_A__Q559_Cluster2_Phase_I_study_report_rev1.pdf from Project 559 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q559 Cluster2 Phase I study report.pdf from Project 559 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q560 Cluster2 Phase I Study Report.pdf from Project 560 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS665827-Appendix_A__Q560_Cluster2_Phase_I_Study_Report_rev1.pdf from Project 560 (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - C561_11-15-2010.pdf from Project 561\n",
      "Scraped PDF: Appendix A - C565_11-15-2010.pdf from Project 565\n",
      "Skipped PDF: Appendix A - Q568 C2P1 Individual Report-r1.pdf from Project 568 (No Table 3 or Attachment data)\n",
      "Skipped PDF: APPENDIX A - Q569 Individual Project Report_Final_CMB.pdf from Project 569 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Q569_Cluster 2_Phase 1_Individual_Report_Appendix A_Final_CMB.pdf from Project 569 (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - C574_11-15-2010.pdf from Project 574\n",
      "Skipped PDF: QC2PI_Q576_Blythe Solar PV II Final.pdf from Project 576 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669363-Appendix_A__Q577_Cluster2_Phase_I_Study_Report_rev1.pdf from Project 577 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q577 Cluster2 Phase I Study Report.pdf from Project 577 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q581 Cluster2 Phase I study report.pdf from Project 581 (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - C583_11-15-2010.pdf from Project 583\n",
      "Skipped PDF: Appendix A - Q586 C2P1 Individal Report-r1.pdf from Project 586 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC2PI_Q588_NRG Blythe Solar 2 Final.pdf from Project 588 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC2PI Appendix A_Q589_NRG Solar Victor 1 20101112.pdf from Project 589 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS667391-Appendix_A__C590_11152010.pdf from Project 590\n",
      "Skipped PDF: QC2PI Appendix A_Q593_Fort Mojave Solar20101112.pdf from Project 593 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC2PI_Q602_Enxco Avalon Solar_Appendix A.pdf from Project 602 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q606 Cluster2 Phase I study report.pdf from Project 606 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669023-Appendix_A__Q607_C1C2_Phase_II_reportfinal_r1.pdf from Project 607 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q607 Cluster2 Phase I study report.pdf from Project 607 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Q607 2014_Reasessment Study Report Appendix A_20140908.pdf from Project 607 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669023-Appendix_A__Q607_Cluster2_Phase_I_study_report_rev2.pdf from Project 607 (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - C608_11-15-2010.pdf from Project 608\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 2.pdf from Project 628\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 1.pdf from Project 628\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q628 FRV Mojave Solar 4.pdf from Project 628 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Eastern-Appendix A (QC4DS) Q632AA.pdf from Project 632AA (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q632C C4 Phase I Study Report.pdf from Project 632C (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS664977-QC3PI_Northern_Appendix_A__Q643AB_BrightSource_AV.pdf from Project 643AB (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS684173-3__NRosamond_Adj_to_AttachmentA_of_Appendix1.pdf from Project 643AC (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS684173-QC3P1_Eastern_Q643AC_Palo_Verde_Solar_Final.pdf from Project 643AC (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS670009-QC3PI_Eastern_Q643AE_DesertHarvest_Final.pdf from Project 643AE (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS664331-QC3PI_Northern_Group_FINAL.pdf from Project 643AJ (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS665249-Appendix_A__C643AM_05272011_final.pdf from Project 643AM\n",
      "Scraped PDF: 10AS665147-Appendix_A__C643AP_05272011_final.pdf from Project 643AP\n",
      "Skipped PDF: 10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_reportrev1.pdf from Project 643D (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_report.pdf from Project 643D (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669771-Appendix_A__Final_Q643E_Cluster3_Phase_I_study_report.pdf from Project 643E (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final.pdf from Project 643F (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf from Project 643F (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669159-Appendix_A__Q643F.pdf from Project 643F (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669159-Placer_RioOso_Solar_PV_I__Appx_1_Attch_A_2011_02_03.pdf from Project 643F (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669431-Attachment_A_to_Appendix_1__Interconnection_Request_for_Generators_Over_20MW_REVISED.pdf from Project 643G (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669431-Q643G_Cluster_3_Phase_1_Appendix_A_PGE_final_r1.pdf from Project 643G (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669431-Appendix_A__Q643G.pdf from Project 643G (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666507-Appendix_I_Deliverability_Assessment_Results.pdf from Project 643I (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666507-Westlands_PV_I_Appendix_1_Attch_A_2011_02_03.pdf from Project 643I (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666507-Appendix_A__Final_Q643I_Cluster3_Phase_I_study_report.pdf from Project 643I (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666575-Appendix_A__Final_Q643J_Cluster3_Phase_I_study_report.pdf from Project 643J (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666745-Yolo_Solar_PV_I_Appendix_1_Attch_A_2012_02_03.pdf from Project 643O (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666745-Appendix_A__Q643O_Individual_Project_Report.pdf from Project 643O (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS666745-Q643O_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf from Project 643O (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS664297-QC3PI_Northern_Group_FINAL.pdf from Project 643R (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS665997-QC3PI_Northern_Group_FINAL.pdf from Project 643S (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS666065-Appendix_A__C643T_05272011_final.pdf from Project 643T\n",
      "Skipped PDF: 10AS669533-Appendix_A__Final_Q643W_Cluster3_Phase_I_study_report.pdf from Project 643W (No Table 3 or Attachment data)\n",
      "Skipped PDF: ._PCSK_Reactive Capability Curve.pdf from Project 643W (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669533-20110826_RE_Mustang_Attachment_A_to_Appendix_1.pdf from Project 643W (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669567-20110707_RE_Tranquillity_Attachment_A_to_Appendix_1.pdf from Project 643X (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS669567-Appendix_A__Final_Q643X_Cluster3_Phase_I_study_report.pdf from Project 643X (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q644 C4 Phase I Study Report- Final.pdf from Project 644 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 10AS693794-Jacumba_Solar_Farm_Q644A_Appendix_A___11092012.pdf from Project 644A\n",
      "Skipped PDF: Appendix A - Q645A C4 Phase I Study Report-Final.pdf from Project 645A (No Table 3 or Attachment data)\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 1.pdf from Project 649B\n",
      "Scraped Addendum PDF: QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 2.pdf from Project 649B\n",
      "Scraped PDF: QC1QC2PII_Northern_Appendix A_Q649B_Central Antelope Dry Ranch C.pdf from Project 649B\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q649B Central Antelope Dry Ranch.pdf from Project 649B (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q649C North Lancaster Ranch.pdf from Project 649C (No Table 3 or Attachment data)\n",
      "Scraped PDF: QC1QC2PII_Northern_Appendix A_Q649C_North Lancaster Ranch.pdf from Project 649C\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q649C Attachment 1.pdf from Project 649C\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachment 2.pdf from Project 650AA\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachement 1.pdf from Project 650AA\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q650AA American Solar Greenworks.pdf from Project 650AA (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q650AC C4 Phase I Study Report - Final.pdf from Project 650AC (No Table 3 or Attachment data)\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 1.pdf from Project 651A\n",
      "Scraped Addendum PDF: QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 2.pdf from Project 651A\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q651A Acacia.pdf from Project 651A (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS689152-Addendum_2_to_Appendix_A__Q654.pdf from Project 653B (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q653B C4 Phase I Study Report-Final.pdf from Project 653B (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q653E C4 Phase I Study Report-Final.pdf from Project 653E (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q653EA C4 Phase I Study Report-Final.pdf from Project 653EA (No Table 3 or Attachment data)\n",
      "Scraped PDF: Appendix A - S653ED_11-09-2012.pdf from Project 653ED\n",
      "Scraped PDF: Appendix A - S653ED_12-30-2011_final.pdf from Project 653ED\n",
      "Skipped PDF: Appendix A - Q653F C4 Phase I report-20120103.pdf from Project 653F (No Table 3 or Attachment data)\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 1.pdf from Project 653H\n",
      "Scraped Addendum PDF: QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 2.pdf from Project 653H\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q653H Western Antelope Dry Ranch.pdf from Project 653H (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q654 C4 Phase I Study Report-Final.pdf from Project 654 (No Table 3 or Attachment data)\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 1.pdf from Project 658\n",
      "Scraped Addendum PDF: QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 2.pdf from Project 658\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q658 Antelope Big Sky Ranch.pdf from Project 658 (No Table 3 or Attachment data)\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 1.pdf from Project 659\n",
      "Scraped Addendum PDF: QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 2.pdf from Project 659\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q659 Western Antelope Blue Sky Ranch B.pdf from Project 659 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q660 Western Antelope Blue Sky Ranch A.pdf from Project 660 (No Table 3 or Attachment data)\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 1.pdf from Project 660\n",
      "Scraped Addendum PDF: QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 2.pdf from Project 660\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 1.pdf from Project 661\n",
      "Scraped Addendum PDF: QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 2.pdf from Project 661\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q661 Summer Solar.pdf from Project 661 (No Table 3 or Attachment data)\n",
      "Skipped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q662 Silver Sun Greenworks.pdf from Project 662 (No Table 3 or Attachment data)\n",
      "Scraped PDF: QC1QC2PII_Northern_Appendix A_Q662_Silver Sun Greenworks.pdf from Project 662\n",
      "Scraped Addendum PDF: QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 2.pdf from Project 662\n",
      "Scraped PDF: QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 1.pdf from Project 662\n",
      "Scraped PDF: 11AS737399-Appendix_A__Q667_11092012.pdf from Project 667\n",
      "Scraped PDF: 11AS737399-Revised_Appendix_A__Q667_11212012.pdf from Project 667\n",
      "Skipped PDF: 11AS709953-QC4PISCENorthernAppendix_AQ670_American_Solar_Greenworks_Addition.pdf from Project 670 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS709990-QC4PISCENorthernAppendix_AQ671_Apex_Greenworks.pdf from Project 671 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS703463-Appendix_A__Q678_C4_Phase_I_report.pdf from Project 678 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS703500-Appendix_A__Q679_C4_phase_I_report.pdf from Project 679 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699473-Appendix_A__Q680_C4_Phase_I_report20120103.pdf from Project 680 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699885-Appendix_A__Q687.pdf from Project 687 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699885-Appendix_A__Q687_C4_Phase_I_Report.pdf from Project 687 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699885-Report_Addendum__Q687_Cluster_4_Phase_I_Appendix_A.pdf from Project 687 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS703562-Appendix_A__Q688_C4_Phase_I_report.pdf from Project 688 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS707788-QC4PI_Appendix_A_Q695_DixieComstock_Geothermal.pdf from Project 695 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS708195-MetroQ702Appendix_A.pdf from Project 702 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700201-Appendix_A__Q705_C4_Phase_I_report__Final.pdf from Project 705 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700410-Appendix_A__Q707_C4_Phase_I_report__Final_CMB_3JAN2012.pdf from Project 707 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700272-Appendix_A__Q708_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf from Project 708 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700375-Appendix_A__Q709_C4_Phase_I_Report__Final_CMB_3JAN2012_PGE_rev1.pdf from Project 709 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS708367-QC4P1_EOP_Appendix_A_Q714HiddenHillsRanch.pdf from Project 714 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701175-Appendix_A__Q720_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf from Project 720 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701347-Appendix_A__Q820_C4_Phase_I_report.pdf from Project 723 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701347-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf from Project 723 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701449-Appendix_A__Q725_C4_Phase_I_report.pdf from Project 725 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS745265-Appendix_A__C737_12302011_final.pdf from Project 737 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS708680-QC4PISCENorthernAppendix_AQ738_Oasis_20_MW.pdf from Project 738 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS708645-QC4P1_EOP_AppendixA_Q740PahrumpValleySEGS.pdf from Project 740 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS702744-Appendix_A__Q744_C4_Phase_I_report_final.pdf from Project 744 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS717303-QC4PISCENorthernAppendix_AQ746_RE_Astoria.pdf from Project 746 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS702472-Appendix_A__Q751_C4_Phase_I_report__Final.pdf from Project 751 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS702506-Appendix_A__Q752_C4_Phase_I_report__Final.pdf from Project 752 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS702778-Appendix_A__Q762_C4_Phase_I_Report.pdf from Project 762 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS702880-Appendix_A__Q765_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf from Project 765 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS711971-QC4PISCENorthernAppendix_AQ768_SP_Antelope_DSR.pdf from Project 768 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS712005-QC4PISCENorthernAppendix_AQ769_Springtime_Solar.pdf from Project 769 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS703152-Appendix_A__Q775_C4_Phase_I_report_final_rev020312.pdf from Project 775 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS709671-QC4PISCENorthernAppendix_AQ778_Western_Antelope_Dry_Ranch_Addition.pdf from Project 778 (No Table 3 or Attachment data)\n",
      "Skipped PDF: Appendix A - Q779 C4 Phase I report - Final.pdf from Project 779 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS709868-Appendix_A__C781_12302011_final.pdf from Project 781\n",
      "Scraped PDF: 11AS708542-Appendix_A__C789_12302011_final.pdf from Project 789\n",
      "Scraped PDF: 11AS708846-Appendix_A__C794_12302011_final.pdf from Project 794\n",
      "Skipped PDF: 11AS737469-QC4PISCENorthernGroup_Report.pdf from Project 795 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737434-QC4PISCENorthernAppendix_AQ796_AV_Solar_Ranch_2B.pdf from Project 796 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS710025-EasternQ797AppendixA.pdf from Project 797 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 10AS711623-EasternQ798AppendixA.pdf from Project 798 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699264-Appendix_A__Q799_C4_Phase_I_report__Final.pdf from Project 799 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699298-Appendix_A__Q800_C4_Phase_I_report__Final.pdf from Project 800 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699747-Appendix_A__Q805_C4_Phase_I_report__Final.pdf from Project 805 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699850-QC34PII_Appendix_A_PGE_Q806.pdf from Project 806 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS699850-Appendix_A__Q806_C4_Phase_I_report__Final.pdf from Project 806 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700096-Report_Addendum__Q809_Cluster_4_Phase_I_Appendix_A.pdf from Project 809 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700096-Appendix_A__Q809_C4_Phase_I_report__Final.pdf from Project 809 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700725-Appendix_A__Q814_C4_Phase_I_report.pdf from Project 814 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700725-Report_Addendum__Q814_Cluster_4_Phase_I_Appendix_A.pdf from Project 814 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700760-Appendix_A__Q815_C4_Phase_I_report.pdf from Project 815 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700760-Report_Addendum__Q815_Cluster_4_Phase_I_Appendix_A.pdf from Project 815 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700794-Appendix_A__Q816_C4_Phase_I_report.pdf from Project 816 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS700794-Report_Addendum__Q816_Cluster_4_Phase_I_Appendix_A.pdf from Project 816 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701279-Appendix_A__Q820_C4_Phase_I_report.pdf from Project 820 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701279-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf from Project 820 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701789-Appendix_A__Q823_C4_Phase_I_report.pdf from Project 823 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701789-QC34PII_Appendix_A_PGE_Q823.pdf from Project 823 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701823-Report_Addendum__Q824_Cluster_4_Phase_I_Appendix_A.pdf from Project 824 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701823-QC34PII_Appendix_A_PGE_Q824.pdf from Project 824 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701823-Appendix_A__Q824_C4_Phase_I_report.pdf from Project 824 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701857-Report_Addendum__Q825_Cluster_4_Phase_I_Appendix_A.pdf from Project 825 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS701857-Appendix_A__Q825_C4_Phase_I_report.pdf from Project 825 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS703986-Appendix_A__Q829_C4_Phase_I_report.pdf from Project 829 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS711555-EasternQ831AppendixA.pdf from Project 831 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737779-C4PI_Group_Report_Addendum_02172012.pdf from Project 837 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737779-C4PhI_Group_Report_12302011_final.pdf from Project 837 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS737779-C4PI_Q837_Jacumba_II_Addendum_02172012.pdf from Project 837\n",
      "Scraped PDF: 11AS716919-Appendix_A__C838_12302011_final.pdf from Project 838\n",
      "Skipped PDF: 11AS741392-QC4P1_EOP_AppendixA_Q855CopperMountainSolar.pdf from Project 855 (No Table 3 or Attachment data)\n",
      "Skipped PDF: C3C4P2-Northern-Q856-Tehachapi Wind Energy Srorage-AppendixA.pdf from Project 856 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS708812-QC4PISCENorthernAppendix_AQ856_Tehachapi_Wind_Energy_Storage.pdf from Project 856 (No Table 3 or Attachment data)\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 3/03_raw/rawdata_cluster1_4_style_R_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 3/03_raw/rawdata_cluster1_4_style_R_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 228\n",
      "Total Projects Scraped: 30\n",
      "Total Projects Skipped: 198\n",
      "Total Projects Missing: 0\n",
      "Total PDFs Accessed: 189\n",
      "Total PDFs Scraped: 45\n",
      "Total PDFs Skipped: 144\n",
      "\n",
      "List of Scraped Projects:\n",
      "['493', '510', '561', '565', '574', '583', '590', '608', '628', '643AM', '643AP', '643T', '644A', '649B', '649C', '650AA', '651A', '653ED', '653H', '658', '659', '660', '661', '662', '667', '781', '789', '794', '837', '838']\n",
      "\n",
      "List of Skipped Projects:\n",
      "['488', '490', '494', '495', '502', '503', '506', '509', '512', '513', '521', '522', '522C', '541', '552', '555', '557', '558', '559', '560', '568', '569', '576', '577', '579', '581', '585', '586', '588', '589', '593', '602', '606', '607', '632AA', '632C', '640', '642', '643', '643AA', '643AB', '643AC', '643AE', '643AF', '643AH', '643AI', '643AJ', '643AK', '643AS', '643D', '643E', '643F', '643G', '643I', '643J', '643O', '643R', '643S', '643W', '643X', '643Z', '644', '645A', '647', '649', '649A', '650A', '650AC', '651', '653', '653A', '653B', '653D', '653E', '653EA', '653EB', '653F', '654', '663', '664', '668', '669', '670', '671', '674', '676', '678', '679', '680', '681', '683', '684', '685', '686', '687', '688', '692', '695', '696', '697', '698', '700', '702', '703', '704', '705', '706', '707', '708', '709', '712', '714', '716', '717', '720', '723', '725', '729', '730', '732', '736', '737', '738', '739', '740', '741', '744', '746', '751', '752', '756', '762', '764', '765', '766', '767', '768', '769', '770', '771', '774', '775', '778', '779', '782', '783', '784', '785', '786', '788', '790', '791', '792', '793', '795', '796', '797', '798', '799', '800', '801', '804', '805', '806', '807', '809', '812', '813', '814', '815', '816', '820', '823', '824', '825', '829', '831', '834', '836', '839', '840', '841', '842', '843', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '857', '858']\n",
      "\n",
      "List of Missing Projects:\n",
      "[]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['Appendix A - C493_06-25-2010_Final.pdf', 'Appendix A - C510_06-25-2010_Final.pdf', 'Appendix A - C561_11-15-2010.pdf', 'Appendix A - C565_11-15-2010.pdf', 'Appendix A - C574_11-15-2010.pdf', 'Appendix A - C583_11-15-2010.pdf', '10AS667391-Appendix_A__C590_11152010.pdf', 'Appendix A - C608_11-15-2010.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 1.pdf', '10AS665249-Appendix_A__C643AM_05272011_final.pdf', '10AS665147-Appendix_A__C643AP_05272011_final.pdf', '10AS666065-Appendix_A__C643T_05272011_final.pdf', '10AS693794-Jacumba_Solar_Farm_Q644A_Appendix_A___11092012.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 2.pdf', 'QC1QC2PII_Northern_Appendix A_Q649B_Central Antelope Dry Ranch C.pdf', 'QC1QC2PII_Northern_Appendix A_Q649C_North Lancaster Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649C Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachement 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 2.pdf', 'Appendix A - S653ED_11-09-2012.pdf', 'Appendix A - S653ED_12-30-2011_final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 2.pdf', 'QC1QC2PII_Northern_Appendix A_Q662_Silver Sun Greenworks.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 1.pdf', '11AS737399-Appendix_A__Q667_11092012.pdf', '11AS737399-Revised_Appendix_A__Q667_11212012.pdf', '11AS709868-Appendix_A__C781_12302011_final.pdf', '11AS708542-Appendix_A__C789_12302011_final.pdf', '11AS708846-Appendix_A__C794_12302011_final.pdf', '11AS737779-C4PI_Q837_Jacumba_II_Addendum_02172012.pdf', '11AS716919-Appendix_A__C838_12302011_final.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['Q495_ Cluster 1_Report_Appendix A_final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q521 Columbia 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q522 Columbia 2.pdf', '10AS666541-Appendix_A__Q557_Cluster2_Phase_I_study_report_rev1.pdf', 'Appendix A - Q557 Cluster2 Phase I study report.pdf', '10AS665861-Appendix_A__Q559_Cluster2_Phase_I_study_report_rev1.pdf', 'Appendix A - Q559 Cluster2 Phase I study report.pdf', 'Appendix A - Q560 Cluster2 Phase I Study Report.pdf', '10AS665827-Appendix_A__Q560_Cluster2_Phase_I_Study_Report_rev1.pdf', 'Appendix A - Q568 C2P1 Individual Report-r1.pdf', 'APPENDIX A - Q569 Individual Project Report_Final_CMB.pdf', 'Q569_Cluster 2_Phase 1_Individual_Report_Appendix A_Final_CMB.pdf', 'QC2PI_Q576_Blythe Solar PV II Final.pdf', '10AS669363-Appendix_A__Q577_Cluster2_Phase_I_Study_Report_rev1.pdf', 'Appendix A - Q577 Cluster2 Phase I Study Report.pdf', 'Appendix A - Q581 Cluster2 Phase I study report.pdf', 'Appendix A - Q586 C2P1 Individal Report-r1.pdf', 'QC2PI_Q588_NRG Blythe Solar 2 Final.pdf', 'QC2PI Appendix A_Q589_NRG Solar Victor 1 20101112.pdf', 'QC2PI Appendix A_Q593_Fort Mojave Solar20101112.pdf', 'QC2PI_Q602_Enxco Avalon Solar_Appendix A.pdf', 'Appendix A - Q606 Cluster2 Phase I study report.pdf', '10AS669023-Appendix_A__Q607_C1C2_Phase_II_reportfinal_r1.pdf', 'Appendix A - Q607 Cluster2 Phase I study report.pdf', 'Q607 2014_Reasessment Study Report Appendix A_20140908.pdf', '10AS669023-Appendix_A__Q607_Cluster2_Phase_I_study_report_rev2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q628 FRV Mojave Solar 4.pdf', 'Eastern-Appendix A (QC4DS) Q632AA.pdf', 'Appendix A - Q632C C4 Phase I Study Report.pdf', '10AS664977-QC3PI_Northern_Appendix_A__Q643AB_BrightSource_AV.pdf', '10AS684173-3__NRosamond_Adj_to_AttachmentA_of_Appendix1.pdf', '10AS684173-QC3P1_Eastern_Q643AC_Palo_Verde_Solar_Final.pdf', '10AS670009-QC3PI_Eastern_Q643AE_DesertHarvest_Final.pdf', '10AS664331-QC3PI_Northern_Group_FINAL.pdf', '10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_reportrev1.pdf', '10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_report.pdf', '10AS669771-Appendix_A__Final_Q643E_Cluster3_Phase_I_study_report.pdf', '10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final.pdf', '10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf', '10AS669159-Appendix_A__Q643F.pdf', '10AS669159-Placer_RioOso_Solar_PV_I__Appx_1_Attch_A_2011_02_03.pdf', '10AS669431-Attachment_A_to_Appendix_1__Interconnection_Request_for_Generators_Over_20MW_REVISED.pdf', '10AS669431-Q643G_Cluster_3_Phase_1_Appendix_A_PGE_final_r1.pdf', '10AS669431-Appendix_A__Q643G.pdf', '10AS666507-Appendix_I_Deliverability_Assessment_Results.pdf', '10AS666507-Westlands_PV_I_Appendix_1_Attch_A_2011_02_03.pdf', '10AS666507-Appendix_A__Final_Q643I_Cluster3_Phase_I_study_report.pdf', '10AS666575-Appendix_A__Final_Q643J_Cluster3_Phase_I_study_report.pdf', '10AS666745-Yolo_Solar_PV_I_Appendix_1_Attch_A_2012_02_03.pdf', '10AS666745-Appendix_A__Q643O_Individual_Project_Report.pdf', '10AS666745-Q643O_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf', '10AS664297-QC3PI_Northern_Group_FINAL.pdf', '10AS665997-QC3PI_Northern_Group_FINAL.pdf', '10AS669533-Appendix_A__Final_Q643W_Cluster3_Phase_I_study_report.pdf', '._PCSK_Reactive Capability Curve.pdf', '10AS669533-20110826_RE_Mustang_Attachment_A_to_Appendix_1.pdf', '10AS669567-20110707_RE_Tranquillity_Attachment_A_to_Appendix_1.pdf', '10AS669567-Appendix_A__Final_Q643X_Cluster3_Phase_I_study_report.pdf', 'Appendix A - Q644 C4 Phase I Study Report- Final.pdf', 'Appendix A - Q645A C4 Phase I Study Report-Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Central Antelope Dry Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649C North Lancaster Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA American Solar Greenworks.pdf', 'Appendix A - Q650AC C4 Phase I Study Report - Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Acacia.pdf', '10AS689152-Addendum_2_to_Appendix_A__Q654.pdf', 'Appendix A - Q653B C4 Phase I Study Report-Final.pdf', 'Appendix A - Q653E C4 Phase I Study Report-Final.pdf', 'Appendix A - Q653EA C4 Phase I Study Report-Final.pdf', 'Appendix A - Q653F C4 Phase I report-20120103.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Western Antelope Dry Ranch.pdf', 'Appendix A - Q654 C4 Phase I Study Report-Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Antelope Big Sky Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Western Antelope Blue Sky Ranch B.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Western Antelope Blue Sky Ranch A.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Summer Solar.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Silver Sun Greenworks.pdf', '11AS709953-QC4PISCENorthernAppendix_AQ670_American_Solar_Greenworks_Addition.pdf', '11AS709990-QC4PISCENorthernAppendix_AQ671_Apex_Greenworks.pdf', '11AS703463-Appendix_A__Q678_C4_Phase_I_report.pdf', '11AS703500-Appendix_A__Q679_C4_phase_I_report.pdf', '11AS699473-Appendix_A__Q680_C4_Phase_I_report20120103.pdf', '11AS699885-Appendix_A__Q687.pdf', '11AS699885-Appendix_A__Q687_C4_Phase_I_Report.pdf', '11AS699885-Report_Addendum__Q687_Cluster_4_Phase_I_Appendix_A.pdf', '11AS703562-Appendix_A__Q688_C4_Phase_I_report.pdf', '11AS707788-QC4PI_Appendix_A_Q695_DixieComstock_Geothermal.pdf', '11AS708195-MetroQ702Appendix_A.pdf', '11AS700201-Appendix_A__Q705_C4_Phase_I_report__Final.pdf', '11AS700410-Appendix_A__Q707_C4_Phase_I_report__Final_CMB_3JAN2012.pdf', '11AS700272-Appendix_A__Q708_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS700375-Appendix_A__Q709_C4_Phase_I_Report__Final_CMB_3JAN2012_PGE_rev1.pdf', '11AS708367-QC4P1_EOP_Appendix_A_Q714HiddenHillsRanch.pdf', '11AS701175-Appendix_A__Q720_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS701347-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701347-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701449-Appendix_A__Q725_C4_Phase_I_report.pdf', '11AS745265-Appendix_A__C737_12302011_final.pdf', '11AS708680-QC4PISCENorthernAppendix_AQ738_Oasis_20_MW.pdf', '11AS708645-QC4P1_EOP_AppendixA_Q740PahrumpValleySEGS.pdf', '11AS702744-Appendix_A__Q744_C4_Phase_I_report_final.pdf', '11AS717303-QC4PISCENorthernAppendix_AQ746_RE_Astoria.pdf', '11AS702472-Appendix_A__Q751_C4_Phase_I_report__Final.pdf', '11AS702506-Appendix_A__Q752_C4_Phase_I_report__Final.pdf', '11AS702778-Appendix_A__Q762_C4_Phase_I_Report.pdf', '11AS702880-Appendix_A__Q765_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS711971-QC4PISCENorthernAppendix_AQ768_SP_Antelope_DSR.pdf', '11AS712005-QC4PISCENorthernAppendix_AQ769_Springtime_Solar.pdf', '11AS703152-Appendix_A__Q775_C4_Phase_I_report_final_rev020312.pdf', '11AS709671-QC4PISCENorthernAppendix_AQ778_Western_Antelope_Dry_Ranch_Addition.pdf', 'Appendix A - Q779 C4 Phase I report - Final.pdf', '11AS737469-QC4PISCENorthernGroup_Report.pdf', '11AS737434-QC4PISCENorthernAppendix_AQ796_AV_Solar_Ranch_2B.pdf', '11AS710025-EasternQ797AppendixA.pdf', '10AS711623-EasternQ798AppendixA.pdf', '11AS699264-Appendix_A__Q799_C4_Phase_I_report__Final.pdf', '11AS699298-Appendix_A__Q800_C4_Phase_I_report__Final.pdf', '11AS699747-Appendix_A__Q805_C4_Phase_I_report__Final.pdf', '11AS699850-QC34PII_Appendix_A_PGE_Q806.pdf', '11AS699850-Appendix_A__Q806_C4_Phase_I_report__Final.pdf', '11AS700096-Report_Addendum__Q809_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700096-Appendix_A__Q809_C4_Phase_I_report__Final.pdf', '11AS700725-Appendix_A__Q814_C4_Phase_I_report.pdf', '11AS700725-Report_Addendum__Q814_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700760-Appendix_A__Q815_C4_Phase_I_report.pdf', '11AS700760-Report_Addendum__Q815_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700794-Appendix_A__Q816_C4_Phase_I_report.pdf', '11AS700794-Report_Addendum__Q816_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701279-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701279-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701789-Appendix_A__Q823_C4_Phase_I_report.pdf', '11AS701789-QC34PII_Appendix_A_PGE_Q823.pdf', '11AS701823-Report_Addendum__Q824_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701823-QC34PII_Appendix_A_PGE_Q824.pdf', '11AS701823-Appendix_A__Q824_C4_Phase_I_report.pdf', '11AS701857-Report_Addendum__Q825_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701857-Appendix_A__Q825_C4_Phase_I_report.pdf', '11AS703986-Appendix_A__Q829_C4_Phase_I_report.pdf', '11AS711555-EasternQ831AppendixA.pdf', '11AS737779-C4PI_Group_Report_Addendum_02172012.pdf', '11AS737779-C4PhI_Group_Report_12302011_final.pdf', '11AS741392-QC4P1_EOP_AppendixA_Q855CopperMountainSolar.pdf', 'C3C4P2-Northern-Q856-Tehachapi Wind Energy Srorage-AppendixA.pdf', '11AS708812-QC4PISCENorthernAppendix_AQ856_Tehachapi_Wind_Energy_Storage.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 2.pdf', '10AS689152-Addendum_2_to_Appendix_A__Q654.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 2.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 2.pdf', '11AS699885-Report_Addendum__Q687_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701347-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700725-Report_Addendum__Q814_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700760-Report_Addendum__Q815_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700794-Report_Addendum__Q816_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701279-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701823-Report_Addendum__Q824_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701857-Report_Addendum__Q825_Cluster_4_Phase_I_Appendix_A.pdf', '11AS737779-C4PI_Group_Report_Addendum_02172012.pdf', '11AS737779-C4PI_Q837_Jacumba_II_Addendum_02172012.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['Appendix A - C493_06-25-2010_Final.pdf', 'Q495_ Cluster 1_Report_Appendix A_final.pdf', 'Appendix A - C510_06-25-2010_Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q521 Columbia 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q522 Columbia 2.pdf', '10AS666541-Appendix_A__Q557_Cluster2_Phase_I_study_report_rev1.pdf', 'Appendix A - Q557 Cluster2 Phase I study report.pdf', '10AS665861-Appendix_A__Q559_Cluster2_Phase_I_study_report_rev1.pdf', 'Appendix A - Q559 Cluster2 Phase I study report.pdf', 'Appendix A - Q560 Cluster2 Phase I Study Report.pdf', '10AS665827-Appendix_A__Q560_Cluster2_Phase_I_Study_Report_rev1.pdf', 'Appendix A - C561_11-15-2010.pdf', 'Appendix A - C565_11-15-2010.pdf', 'Appendix A - Q568 C2P1 Individual Report-r1.pdf', 'APPENDIX A - Q569 Individual Project Report_Final_CMB.pdf', 'Q569_Cluster 2_Phase 1_Individual_Report_Appendix A_Final_CMB.pdf', 'Appendix A - C574_11-15-2010.pdf', 'QC2PI_Q576_Blythe Solar PV II Final.pdf', '10AS669363-Appendix_A__Q577_Cluster2_Phase_I_Study_Report_rev1.pdf', 'Appendix A - Q577 Cluster2 Phase I Study Report.pdf', 'Appendix A - Q581 Cluster2 Phase I study report.pdf', 'Appendix A - C583_11-15-2010.pdf', 'Appendix A - Q586 C2P1 Individal Report-r1.pdf', 'QC2PI_Q588_NRG Blythe Solar 2 Final.pdf', 'QC2PI Appendix A_Q589_NRG Solar Victor 1 20101112.pdf', '10AS667391-Appendix_A__C590_11152010.pdf', 'QC2PI Appendix A_Q593_Fort Mojave Solar20101112.pdf', 'QC2PI_Q602_Enxco Avalon Solar_Appendix A.pdf', 'Appendix A - Q606 Cluster2 Phase I study report.pdf', '10AS669023-Appendix_A__Q607_C1C2_Phase_II_reportfinal_r1.pdf', 'Appendix A - Q607 Cluster2 Phase I study report.pdf', 'Q607 2014_Reasessment Study Report Appendix A_20140908.pdf', '10AS669023-Appendix_A__Q607_Cluster2_Phase_I_study_report_rev2.pdf', 'Appendix A - C608_11-15-2010.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q628 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q628 FRV Mojave Solar 4.pdf', 'Eastern-Appendix A (QC4DS) Q632AA.pdf', 'Appendix A - Q632C C4 Phase I Study Report.pdf', '10AS664977-QC3PI_Northern_Appendix_A__Q643AB_BrightSource_AV.pdf', '10AS684173-3__NRosamond_Adj_to_AttachmentA_of_Appendix1.pdf', '10AS684173-QC3P1_Eastern_Q643AC_Palo_Verde_Solar_Final.pdf', '10AS670009-QC3PI_Eastern_Q643AE_DesertHarvest_Final.pdf', '10AS664331-QC3PI_Northern_Group_FINAL.pdf', '10AS665249-Appendix_A__C643AM_05272011_final.pdf', '10AS665147-Appendix_A__C643AP_05272011_final.pdf', '10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_reportrev1.pdf', '10AS668683-Appendix_A__Final_Q643D_Cluster3_Phase_I_study_report.pdf', '10AS669771-Appendix_A__Final_Q643E_Cluster3_Phase_I_study_report.pdf', '10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final.pdf', '10AS669159-Q643F_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf', '10AS669159-Appendix_A__Q643F.pdf', '10AS669159-Placer_RioOso_Solar_PV_I__Appx_1_Attch_A_2011_02_03.pdf', '10AS669431-Attachment_A_to_Appendix_1__Interconnection_Request_for_Generators_Over_20MW_REVISED.pdf', '10AS669431-Q643G_Cluster_3_Phase_1_Appendix_A_PGE_final_r1.pdf', '10AS669431-Appendix_A__Q643G.pdf', '10AS666507-Appendix_I_Deliverability_Assessment_Results.pdf', '10AS666507-Westlands_PV_I_Appendix_1_Attch_A_2011_02_03.pdf', '10AS666507-Appendix_A__Final_Q643I_Cluster3_Phase_I_study_report.pdf', '10AS666575-Appendix_A__Final_Q643J_Cluster3_Phase_I_study_report.pdf', '10AS666745-Yolo_Solar_PV_I_Appendix_1_Attch_A_2012_02_03.pdf', '10AS666745-Appendix_A__Q643O_Individual_Project_Report.pdf', '10AS666745-Q643O_Cluster_3_Phase_1_Appendix_A_PGE_final_rev1.pdf', '10AS664297-QC3PI_Northern_Group_FINAL.pdf', '10AS665997-QC3PI_Northern_Group_FINAL.pdf', '10AS666065-Appendix_A__C643T_05272011_final.pdf', '10AS669533-Appendix_A__Final_Q643W_Cluster3_Phase_I_study_report.pdf', '._PCSK_Reactive Capability Curve.pdf', '10AS669533-20110826_RE_Mustang_Attachment_A_to_Appendix_1.pdf', '10AS669567-20110707_RE_Tranquillity_Attachment_A_to_Appendix_1.pdf', '10AS669567-Appendix_A__Final_Q643X_Cluster3_Phase_I_study_report.pdf', 'Appendix A - Q644 C4 Phase I Study Report- Final.pdf', '10AS693794-Jacumba_Solar_Farm_Q644A_Appendix_A___11092012.pdf', 'Appendix A - Q645A C4 Phase I Study Report-Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Attachment 1.pdf', 'QC1QC2PII_Northern_Appendix A_Q649B_Central Antelope Dry Ranch C.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649B Central Antelope Dry Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649C North Lancaster Ranch.pdf', 'QC1QC2PII_Northern_Appendix A_Q649C_North Lancaster Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q649C Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA Attachement 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q650AA American Solar Greenworks.pdf', 'Appendix A - Q650AC C4 Phase I Study Report - Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q651A Acacia.pdf', 'Appendix A - Q653B C4 Phase I Study Report-Final.pdf', 'Appendix A - Q653E C4 Phase I Study Report-Final.pdf', 'Appendix A - Q653EA C4 Phase I Study Report-Final.pdf', 'Appendix A - S653ED_11-09-2012.pdf', 'Appendix A - S653ED_12-30-2011_final.pdf', 'Appendix A - Q653F C4 Phase I report-20120103.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q653H Western Antelope Dry Ranch.pdf', 'Appendix A - Q654 C4 Phase I Study Report-Final.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q658 Antelope Big Sky Ranch.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q659 Western Antelope Blue Sky Ranch B.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Western Antelope Blue Sky Ranch A.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q660 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Attachment 1.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q661 Summer Solar.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Silver Sun Greenworks.pdf', 'QC1QC2PII_Northern_Appendix A_Q662_Silver Sun Greenworks.pdf', 'QC4PI-SCE-Northern-Appendix A-DS-Q662 Attachment 1.pdf', '11AS737399-Appendix_A__Q667_11092012.pdf', '11AS737399-Revised_Appendix_A__Q667_11212012.pdf', '11AS709953-QC4PISCENorthernAppendix_AQ670_American_Solar_Greenworks_Addition.pdf', '11AS709990-QC4PISCENorthernAppendix_AQ671_Apex_Greenworks.pdf', '11AS703463-Appendix_A__Q678_C4_Phase_I_report.pdf', '11AS703500-Appendix_A__Q679_C4_phase_I_report.pdf', '11AS699473-Appendix_A__Q680_C4_Phase_I_report20120103.pdf', '11AS699885-Appendix_A__Q687.pdf', '11AS699885-Appendix_A__Q687_C4_Phase_I_Report.pdf', '11AS703562-Appendix_A__Q688_C4_Phase_I_report.pdf', '11AS707788-QC4PI_Appendix_A_Q695_DixieComstock_Geothermal.pdf', '11AS708195-MetroQ702Appendix_A.pdf', '11AS700201-Appendix_A__Q705_C4_Phase_I_report__Final.pdf', '11AS700410-Appendix_A__Q707_C4_Phase_I_report__Final_CMB_3JAN2012.pdf', '11AS700272-Appendix_A__Q708_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS700375-Appendix_A__Q709_C4_Phase_I_Report__Final_CMB_3JAN2012_PGE_rev1.pdf', '11AS708367-QC4P1_EOP_Appendix_A_Q714HiddenHillsRanch.pdf', '11AS701175-Appendix_A__Q720_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS701347-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701449-Appendix_A__Q725_C4_Phase_I_report.pdf', '11AS745265-Appendix_A__C737_12302011_final.pdf', '11AS708680-QC4PISCENorthernAppendix_AQ738_Oasis_20_MW.pdf', '11AS708645-QC4P1_EOP_AppendixA_Q740PahrumpValleySEGS.pdf', '11AS702744-Appendix_A__Q744_C4_Phase_I_report_final.pdf', '11AS717303-QC4PISCENorthernAppendix_AQ746_RE_Astoria.pdf', '11AS702472-Appendix_A__Q751_C4_Phase_I_report__Final.pdf', '11AS702506-Appendix_A__Q752_C4_Phase_I_report__Final.pdf', '11AS702778-Appendix_A__Q762_C4_Phase_I_Report.pdf', '11AS702880-Appendix_A__Q765_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS711971-QC4PISCENorthernAppendix_AQ768_SP_Antelope_DSR.pdf', '11AS712005-QC4PISCENorthernAppendix_AQ769_Springtime_Solar.pdf', '11AS703152-Appendix_A__Q775_C4_Phase_I_report_final_rev020312.pdf', '11AS709671-QC4PISCENorthernAppendix_AQ778_Western_Antelope_Dry_Ranch_Addition.pdf', 'Appendix A - Q779 C4 Phase I report - Final.pdf', '11AS709868-Appendix_A__C781_12302011_final.pdf', '11AS708542-Appendix_A__C789_12302011_final.pdf', '11AS708846-Appendix_A__C794_12302011_final.pdf', '11AS737469-QC4PISCENorthernGroup_Report.pdf', '11AS737434-QC4PISCENorthernAppendix_AQ796_AV_Solar_Ranch_2B.pdf', '11AS710025-EasternQ797AppendixA.pdf', '10AS711623-EasternQ798AppendixA.pdf', '11AS699264-Appendix_A__Q799_C4_Phase_I_report__Final.pdf', '11AS699298-Appendix_A__Q800_C4_Phase_I_report__Final.pdf', '11AS699747-Appendix_A__Q805_C4_Phase_I_report__Final.pdf', '11AS699850-QC34PII_Appendix_A_PGE_Q806.pdf', '11AS699850-Appendix_A__Q806_C4_Phase_I_report__Final.pdf', '11AS700096-Report_Addendum__Q809_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700096-Appendix_A__Q809_C4_Phase_I_report__Final.pdf', '11AS700725-Appendix_A__Q814_C4_Phase_I_report.pdf', '11AS700760-Appendix_A__Q815_C4_Phase_I_report.pdf', '11AS700794-Appendix_A__Q816_C4_Phase_I_report.pdf', '11AS701279-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701789-Appendix_A__Q823_C4_Phase_I_report.pdf', '11AS701789-QC34PII_Appendix_A_PGE_Q823.pdf', '11AS701823-QC34PII_Appendix_A_PGE_Q824.pdf', '11AS701823-Appendix_A__Q824_C4_Phase_I_report.pdf', '11AS701857-Appendix_A__Q825_C4_Phase_I_report.pdf', '11AS703986-Appendix_A__Q829_C4_Phase_I_report.pdf', '11AS711555-EasternQ831AppendixA.pdf', '11AS737779-C4PhI_Group_Report_12302011_final.pdf', '11AS716919-Appendix_A__C838_12302011_final.pdf', '11AS741392-QC4P1_EOP_AppendixA_Q855CopperMountainSolar.pdf', 'C3C4P2-Northern-Q856-Tehachapi Wind Energy Srorage-AppendixA.pdf', '11AS708812-QC4PISCENorthernAppendix_AQ856_Tehachapi_Wind_Energy_Storage.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 34\n",
      "Number of Addendum PDFs Scraped: 11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "import inflect\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/03_raw/rawdata_cluster1_4_style_R_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/03_raw/rawdata_cluster1_4_style_R_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/03_raw/scraping_cluster1_4_style_R_log.txt\"\n",
    "PROJECT_RANGE = range(488, 859)  # Example range for q_ids in Clusters 4\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing, removing unwanted characters, and singularizing words.\"\"\"\n",
    "    p = inflect.engine()\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "            # Correct any mis\u2010spellings of \u201ctype of upgrade\u201d\n",
    "            header = re.sub(r'\\btype of upgr\\s*ade\\b', 'type of upgrade', header)\n",
    "            words = header.split()\n",
    "            singular_words = [p.singular_noun(word) if p.singular_noun(word) else word for word in words]\n",
    "            header = \" \".join(singular_words)\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback if none found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "    new_order = existing_desired + remaining\n",
    "    df = df[new_order]\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"\n",
    "    Ensures each row in data_rows has exactly len(headers) columns.\n",
    "    If a row is too short, it is padded with empty strings.\n",
    "    If too long, it is truncated.\n",
    "    \"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"] * (col_count - len(row)))\n",
    "\n",
    "def extract_table2(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "    table_settings_list = [\n",
    "        {\"horizontal_strategy\": \"text\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 1},\n",
    "        {\"horizontal_strategy\": \"lines\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 2},\n",
    "    ]\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*(?:2|B\\.1)\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 2 found in the PDF.\", file=log_file)\n",
    "                return None\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Table 2 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "            extraction_successful = False\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 2...\", file=log_file)\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    poi_col_index = cell_index\n",
    "                                    adjacent_col_index = poi_col_index + 1\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            poi_value_parts = []\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "                                            if poi_value_parts:\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                            if extraction_successful:\n",
    "                                break\n",
    "                        if extraction_successful:\n",
    "                            break\n",
    "                    if extraction_successful:\n",
    "                        break\n",
    "                if extraction_successful:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 2 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "    if not extraction_successful:\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            print(\"Point of Interconnection not found in Table 2.\", file=log_file)\n",
    "            return None\n",
    "    return point_of_interconnection\n",
    "\n",
    "def fix_column_names(columns):\n",
    "    \"\"\"\n",
    "    Renames duplicate and empty column names.\n",
    "    Duplicate names are suffixed with _1, _2, etc.\n",
    "    Empty or whitespace-only names are replaced with unnamed_1, unnamed_2, etc.\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    counts = {}\n",
    "    unnamed_count = 1\n",
    "    for col in columns:\n",
    "        # Treat empty or whitespace-only names as unnamed.\n",
    "        if not col or col.strip() == \"\":\n",
    "            new_col = f\"unnamed_{unnamed_count}\"\n",
    "            unnamed_count += 1\n",
    "        else:\n",
    "            new_col = col.strip()\n",
    "        if new_col in counts:\n",
    "            new_col_with_suffix = f\"{new_col}_{counts[new_col]}\"\n",
    "            counts[new_col] += 1\n",
    "            new_cols.append(new_col_with_suffix)\n",
    "        else:\n",
    "            counts[new_col] = 1\n",
    "            new_cols.append(new_col)\n",
    "    return new_cols\n",
    "\n",
    "def post_process_columns(df, log_file):\n",
    "    \"\"\"\n",
    "    Post-processes DataFrame column names:\n",
    "      1. For any column named 'unnamed_#' (or empty), look at its first non-empty cell.\n",
    "         If that cell is not a dollar amount (i.e. does not match /^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$/)\n",
    "         and it contains 2 or 3 words, then rename the column to that value (after cleaning).\n",
    "         If a column already exists with that name, merge the data from the renamed column into the\n",
    "         existing column and drop the renamed column.\n",
    "      2. If a column is named \"Needed For\", then rename it to \"description\" (merging with an existing\n",
    "         description column if necessary).\n",
    "    \"\"\"\n",
    "    # Process unnamed columns.\n",
    "    for col in list(df.columns):\n",
    "        if col.lower().startswith(\"unnamed_\") or col.strip() == \"\":\n",
    "            # Find the first non-empty cell in this column.\n",
    "            first_non_empty = None\n",
    "            for val in df[col]:\n",
    "                cell_val = \"\"\n",
    "                if isinstance(val, str):\n",
    "                    cell_val = val.strip()\n",
    "                elif val is not None:\n",
    "                    cell_val = str(val).strip()\n",
    "                if cell_val:\n",
    "                    first_non_empty = cell_val\n",
    "                    break\n",
    "            if first_non_empty:\n",
    "                # Check if the value is a dollar amount.\n",
    "                if not re.match(r\"^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$\", first_non_empty):\n",
    "                    words = first_non_empty.split()\n",
    "                    if 2 <= len(words) <= 3:\n",
    "                        # Clean the candidate name.\n",
    "                        new_name = clean_column_headers([first_non_empty])[0]\n",
    "                        log_file.write(f\"Renaming column '{col}' to '{new_name}' based on first non-empty value '{first_non_empty}'.\\n\")\n",
    "                        if new_name in df.columns and new_name != col:\n",
    "                            # Merge the two columns: fill empty cells in existing new_name from the renamed col.\n",
    "                            for idx in df.index:\n",
    "                                existing_val = df.at[idx, new_name]\n",
    "                                candidate_val = df.at[idx, col]\n",
    "                                if (pd.isna(existing_val) or existing_val == \"\") and (not pd.isna(candidate_val) and candidate_val != \"\"):\n",
    "                                    df.at[idx, new_name] = candidate_val\n",
    "                            df.drop(columns=[col], inplace=True)\n",
    "                        else:\n",
    "                            df.rename(columns={col: new_name}, inplace=True)\n",
    "    # Process \"Needed For\" column: rename or merge it into \"description\".\n",
    "    if \"Needed For\" in df.columns:\n",
    "        if \"description\" in df.columns:\n",
    "            log_file.write(\"Merging 'Needed For' column into existing 'description' column.\\n\")\n",
    "            for idx in df.index:\n",
    "                desc_val = df.at[idx, \"description\"]\n",
    "                needed_for_val = df.at[idx, \"Needed For\"]\n",
    "                if (pd.isna(desc_val) or desc_val == \"\") and (not pd.isna(needed_for_val) and needed_for_val != \"\"):\n",
    "                    df.at[idx, \"description\"] = needed_for_val\n",
    "            df.drop(columns=[\"Needed For\"], inplace=True)\n",
    "        else:\n",
    "            log_file.write(\"Renaming 'Needed For' column to 'description'.\\n\")\n",
    "            df.rename(columns={\"Needed For\": \"description\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def extract_table3(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 3 data   the provided PDF.\n",
    "     \n",
    "      2. Renaming of duplicate/empty columns (using fix_column_names) and then post-processing\n",
    "         unnamed columns as described.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 10\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain either Table 3 patterns or Attachment 1/Attachment 2.\n",
    "            table3_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*11[-.]([1-3])\\b\", text, re.IGNORECASE):\n",
    "                    #re.search(r\"Attachment\\s*[12]\", text, re.IGNORECASE)):\n",
    "                    table3_pages.append(i)\n",
    "            if not table3_pages:\n",
    "                print(\"No Table 10  found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "            first_page = table3_pages[0]\n",
    "            last_page = table3_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Candidate pages start on {scrape_start + 1} and end on {scrape_end}\", file=log_file)\n",
    "            # Process each page that might contain table data.\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                # This variable keeps track of the bottom y-coordinate of the previous table on the page.\n",
    "                previous_table_bottom = None\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "                    table_bbox = table.bbox  # (x0, top, x1, bottom)\n",
    "                    # Define the title region for the table: above the table bounding box.\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*11[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(2).strip()\n",
    "                                break\n",
    "                        \n",
    "                  \n",
    "                    # Extract the specific phrase using the refined table title.\n",
    "                    if table_title:\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New table detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        # Rename header 'type' to 'type of upgrade' if needed.\n",
    "                        if \"type\" in headers and \"type of upgrade\" not in headers:\n",
    "                            headers = [(\"type of upgrade\" if h == \"type\" else h) for h in headers]\n",
    "                        if \"need for\" in headers:\n",
    "                            headers = [(\"description\" if h == \"need for\" else h) for h in headers]  \n",
    "                    \n",
    "                        # Apply the duplicate/empty column fixing.\n",
    "                        headers = fix_column_names(headers)\n",
    "                        data_rows = tab[1:]\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        if \"allocated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"allocated\"], inplace=True)\n",
    "                            print(f\"Dropped 'Max of' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate x \" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate x \"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file) \n",
    "\n",
    "                        if \"3339615 9\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"3339615 9\"], inplace=True)\n",
    "                            print(f\"Dropped '3339615 9' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)     \n",
    "                            \n",
    "                        if \"6 steady state reliability and posttransient voltage stability\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"6 steady state reliability and posttransient voltage stability\"], inplace=True)\n",
    "                            print(f\"Dropped '6 steady state reliability and posttransient voltage stability' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)  \n",
    "\n",
    "\n",
    "\n",
    "                        if \"escalated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"escalated\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)    \n",
    "\n",
    "\n",
    "                        # Also, if the DataFrame has a column named \"type\" (and not already \"type of upgrade\"), rename it.\n",
    "                        if 'type' in df_new.columns and 'type of upgrade' not in df_new.columns:\n",
    "                            df_new.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        # Special handling for ADNU tables if needed.\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        # Fix duplicate and unnamed columns in the new table.\n",
    "                        df_new.columns = fix_column_names(df_new.columns.tolist())\n",
    "                        # Now apply the post-processing of column names:\n",
    "                        df_new = post_process_columns(df_new, log_file)\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation table branch.\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous table title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"Continuation table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "                        # Use the number of columns from the last extracted table as expected.\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            data_rows = data_rows[1:]\n",
    "                        # Ensure every row has the same length as expected_headers.\n",
    "                        adjust_rows_length(data_rows, expected_headers)\n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "                        # Rename column 'type' if needed.\n",
    "                        if 'type' in df_continuation.columns and 'type of upgrade' not in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        if \"need for\" in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={\"need for\": \"description\"}, inplace=True)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        # Fix duplicate and unnamed columns in the continuation table.\n",
    "                        df_continuation.columns = fix_column_names(df_continuation.columns.tolist())\n",
    "                        # Post-process the columns in the continuation table.\n",
    "                        df_continuation = post_process_columns(df_continuation, log_file)\n",
    "                        # Concatenate the continuation table with the previous extracted table.\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                    # Update the previous_table_bottom for the page using the current table's bbox.\n",
    "                    previous_table_bottom = table_bbox[3]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 10 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "        print(\"\\nConcatenating all extracted Table 10/Attachment data...\", file=log_file)\n",
    "        try:\n",
    "            table3_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table3_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 10/Attachment data extracted.\", file=log_file)\n",
    "        table3_data = pd.DataFrame()\n",
    "    return table3_data\n",
    "\n",
    "\n",
    "def extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 10 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table3_data = extract_table3(pdf_path, log_file, is_addendum)\n",
    "    if table3_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        overlapping_columns = base_data.columns.intersection(table3_data.columns).difference(['point_of_interconnection'])\n",
    "        table3_data = table3_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        base_data_repeated = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "        try:\n",
    "\n",
    "                        # Concatenate base data with Table 8 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "           # if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "           #     merged_df[\"is_duplicate\"] = merged_df.duplicated(subset=[\"q_id\", \"type of upgrade\", \"upgrade\"], keep=\"first\")\n",
    "            #    merged_df = merged_df[merged_df[\"is_duplicate\"] == False].drop(columns=[\"is_duplicate\"])\n",
    "            #    print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade'.\", file=log_file)\n",
    "\n",
    "\n",
    "            if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "                # Identify rows where 'type of upgrade' and 'upgrade' are not empty\n",
    "                non_empty_rows = merged_df[\n",
    "                    merged_df[\"type of upgrade\"].notna() & merged_df[\"upgrade\"].notna() &\n",
    "                    (merged_df[\"type of upgrade\"].str.strip() != \"\") & (merged_df[\"upgrade\"].str.strip() != \"\")\n",
    "                ]\n",
    "\n",
    "                # Group by q_id, type of upgrade, and upgrade, keeping the first occurrence\n",
    "                grouped_df = non_empty_rows.groupby([\"q_id\", \"type of upgrade\", \"upgrade\"], as_index=False).first()\n",
    "\n",
    "                # Get the original order of the rows in merged_df before filtering\n",
    "                merged_df[\"original_index\"] = merged_df.index\n",
    "\n",
    "                # Combine unique grouped rows with originally empty rows\n",
    "                final_df = pd.concat([\n",
    "                    grouped_df,\n",
    "                    merged_df[merged_df[\"type of upgrade\"].isna() | (merged_df[\"type of upgrade\"].str.strip() == \"\") |\n",
    "                            merged_df[\"upgrade\"].isna() | (merged_df[\"upgrade\"].str.strip() == \"\")]\n",
    "                ], ignore_index=True, sort=False)\n",
    "\n",
    "                # Restore the original order of the rows based on the saved index\n",
    "                final_df.sort_values(by=\"original_index\", inplace=True)\n",
    "                final_df.drop(columns=[\"original_index\"], inplace=True)\n",
    "                merged_df = final_df\n",
    "\n",
    "                print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade', excluding empty rows while preserving order.\", file=log_file)\n",
    "\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            print(f\"Merged base data with Table 3 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 3 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data\n",
    "\n",
    "def check_has_table3(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 3 or Attachment 1/Attachment 2.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*11[-.]?\\d*\", text, re.IGNORECASE):\n",
    "                    #re.search(r\"Attachment\\s*[12]\", text, re.IGNORECASE))\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            return \"Addendum\" in text\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "        text = clean_string_cell(text)\n",
    "        #queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = str(project_id) #queue_id.group(1) if queue_id else \n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "        point_of_interconnection = extract_table2(pdf_path, log_file)\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "    df = df.map(clean_string_cell)\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "    #if 'q_id' in df.columns:\n",
    "    #    df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "'''\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "            project_scraped = False\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    try:\n",
    "                        has_table3 = check_has_table3(pdf_path)\n",
    "                        if not has_table3:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\", file=log_file)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "                        if is_add and base_data_extracted:\n",
    "                            table3_data = extract_table3(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table3_data.empty:\n",
    "                                merged_df = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table3_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            df = extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "'''\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"\n",
    "    Processes all PDFs in the directories within BASE_DIRECTORY whose numeric prefix is in PROJECT_RANGE.\n",
    "    This allows folders like '641' and '641AA' (if 641 is in the PROJECT_RANGE) to be processed,\n",
    "    and uses the full folder name (e.g. '641AA') as the project id (q_id).\n",
    "    \"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    " \n",
    "\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        # List all subdirectories in BASE_DIRECTORY that have a numeric prefix.\n",
    "        folders = [\n",
    "            folder for folder in os.listdir(BASE_DIRECTORY)\n",
    "            if os.path.isdir(os.path.join(BASE_DIRECTORY, folder)) and re.match(r'^(\\d+)', folder)\n",
    "        ]\n",
    "    \n",
    "\n",
    "\n",
    "        def sort_key(folder):\n",
    "            match = re.match(r'^(\\d+)', folder)\n",
    "            if match:\n",
    "                numeric = int(match.group(1))\n",
    "                return (numeric, folder)\n",
    "            return (float('inf'), folder)\n",
    "\n",
    "        # Sort the folders in ascending order.\n",
    "        sorted_folders = sorted(folders, key=sort_key)\n",
    "\n",
    "        # Process each folder in sorted order.\n",
    "        for folder in sorted_folders:\n",
    "            folder_path = os.path.join(BASE_DIRECTORY, folder)\n",
    "            match = re.match(r'^(\\d+)', folder)\n",
    "            if not match:\n",
    "                continue  # Skip if there is no numeric prefix.\n",
    "            numeric_part = int(match.group(1))\n",
    "            # Process the folder only if its numeric part is in the desired range.\n",
    "            if numeric_part not in PROJECT_RANGE:\n",
    "                continue\n",
    "\n",
    "            # Use the full folder name as the project identifier (q_id).\n",
    "            project_id = folder  # e.g., \"641AA\" or \"641\"\n",
    "            project_path = os.path.join(folder_path, \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    try:\n",
    "                        has_table3 = check_has_table3(pdf_path)\n",
    "                        if not has_table3:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\", file=log_file)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "                        if is_add and base_data_extracted:\n",
    "                            table3_data = extract_table3(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table3_data.empty:\n",
    "                                merged_df = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table3_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            df = extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "    # Save results and print summary as before.\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total folders (from 488 to 859) with an empty '02_phase_1_study' subfolder: 86\n",
      "\n",
      "Folders with empty '02_phase_1_study':\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/764\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/790\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/739\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/706\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/730\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/502\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/791\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/736\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/503\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/700\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/686\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/681\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/643\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/845\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/842\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/642\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/674\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/843\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/857\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/850\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/804\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/651\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/669\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/490\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/851\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/858\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/692\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/834\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/668\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/785\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/771\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/782\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/712\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/783\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/770\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/784\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/741\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/767\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/555\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/793\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/756\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/732\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/703\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/509\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/704\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/792\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/766\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/841\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/846\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/848\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/676\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/649\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/685\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/488\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/647\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/812\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/640\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/849\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/847\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/840\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/813\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/684\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/683\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/807\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/494\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/664\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/836\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/697\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/663\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/853\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/854\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/696\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/698\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/653\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/839\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/801\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/852\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/788\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/585\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/786\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/717\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/579\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/774\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/716\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/729\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/512\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Update this to the path of your main folder\n",
    "base_folder = r'/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data'  \n",
    "\n",
    "# Set your numeric range\n",
    "range_start = 488\n",
    "range_end = 859\n",
    "\n",
    "# Counter for folders with empty '02_phase_1_study'\n",
    "empty_count = 0\n",
    "\n",
    "# List to hold paths that meet the condition (optional)\n",
    "empty_folders = []\n",
    "\n",
    "# Loop through each item in the main folder\n",
    "for item in os.listdir(base_folder):\n",
    "    item_path = os.path.join(base_folder, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        # Try to interpret the folder name as a number\n",
    "        try:\n",
    "            folder_number = int(item)\n",
    "        except ValueError:\n",
    "            continue  # Skip folders that don't have a numeric name\n",
    "\n",
    "        # Check if the folder's number is in the desired range\n",
    "        if range_start <= folder_number <= range_end:\n",
    "            # Build the path for the '02_phase_1_study' subfolder\n",
    "            phase1_path = os.path.join(item_path, \"02_phase_1_study\")\n",
    "            if os.path.isdir(phase1_path):\n",
    "                # Check if the subfolder is empty\n",
    "                if not os.listdir(phase1_path):\n",
    "                    empty_count += 1\n",
    "                    empty_folders.append(item_path)\n",
    "            else:\n",
    "                # If you want to log folders missing the subfolder, you can print or handle that here.\n",
    "                print(f\"Folder {item_path} does not have a '02_phase_1_study' subfolder.\")\n",
    "\n",
    "print(f\"\\nTotal folders (from {range_start} to {range_end}) with an empty '02_phase_1_study' subfolder: {empty_count}\")\n",
    "\n",
    "# (Optional) List the folders that met the condition\n",
    "if empty_folders:\n",
    "    print(\"\\nFolders with empty '02_phase_1_study':\")\n",
    "    for folder in empty_folders:\n",
    "        print(folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# counts projects with missing phase 1 and classifies them by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total projects (from 154 to 2192) with a missing or empty '02_phase_1_study' folder: 121\n",
      "\n",
      "Folders with an empty '02_phase_1_study' subfolder (full paths):\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/417\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/488\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/490\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/494\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/502\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/503\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/509\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/512\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/555\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/579\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/585\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/640\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/642\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/643\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/643AK\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/643AS\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/643Z\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/647\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/649\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/649A\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/650A\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/651\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/653\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/653A\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/653D\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/653EB\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/663\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/664\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/668\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/669\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/674\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/676\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/681\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/683\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/684\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/685\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/686\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/692\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/696\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/697\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/698\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/700\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/703\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/704\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/706\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/712\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/716\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/717\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/729\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/730\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/732\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/736\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/739\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/741\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/756\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/764\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/766\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/767\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/770\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/771\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/774\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/782\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/783\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/784\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/785\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/786\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/788\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/790\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/791\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/792\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/793\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/801\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/804\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/807\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/812\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/813\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/834\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/836\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/839\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/840\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/841\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/842\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/843\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/845\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/846\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/847\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/848\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/849\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/850\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/851\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/852\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/853\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/854\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/857\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/858\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/878\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/879\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/928\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/933\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/934\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/937\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/944\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/948\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/952\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/959\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/968\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/969\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/977\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/985\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/998\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1002\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1003\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1231\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1257\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1362\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1556\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1723\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1735\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1833\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/2039\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/2100\n",
      "\n",
      "=== Projects Missing '02_phase_1_study' (Missing or Empty) Grouped by Cluster ===\n",
      "\n",
      "Cluster TC:\n",
      "  417\n",
      "\n",
      "Cluster C01:\n",
      "  488\n",
      "  490\n",
      "  494\n",
      "  502\n",
      "  503\n",
      "  509\n",
      "  512\n",
      "\n",
      "Cluster C02:\n",
      "  555\n",
      "  585\n",
      "\n",
      "Cluster Unknown:\n",
      "  579\n",
      "  653EB\n",
      "  674\n",
      "  1723\n",
      "  1735\n",
      "  1833\n",
      "  2100\n",
      "\n",
      "Cluster SGIP-TC:\n",
      "  640\n",
      "  642\n",
      "  647\n",
      "  649\n",
      "  649A\n",
      "  650A\n",
      "  651\n",
      "  653\n",
      "  653A\n",
      "  653D\n",
      "  663\n",
      "  664\n",
      "\n",
      "Cluster C03:\n",
      "  643\n",
      "  643AK\n",
      "  643AS\n",
      "  643Z\n",
      "\n",
      "Cluster C04:\n",
      "  668\n",
      "  669\n",
      "  676\n",
      "  681\n",
      "  683\n",
      "  684\n",
      "  685\n",
      "  686\n",
      "  692\n",
      "  696\n",
      "  697\n",
      "  698\n",
      "  700\n",
      "  703\n",
      "  704\n",
      "  706\n",
      "  712\n",
      "  716\n",
      "  717\n",
      "  729\n",
      "  730\n",
      "  732\n",
      "  736\n",
      "  739\n",
      "  741\n",
      "  756\n",
      "  764\n",
      "  766\n",
      "  767\n",
      "  770\n",
      "  771\n",
      "  774\n",
      "  782\n",
      "  783\n",
      "  784\n",
      "  785\n",
      "  786\n",
      "  788\n",
      "  790\n",
      "  791\n",
      "  792\n",
      "  793\n",
      "  807\n",
      "  812\n",
      "  813\n",
      "  834\n",
      "  836\n",
      "  839\n",
      "  840\n",
      "  841\n",
      "  842\n",
      "  843\n",
      "  845\n",
      "  846\n",
      "  847\n",
      "  848\n",
      "  849\n",
      "  850\n",
      "  851\n",
      "  852\n",
      "  853\n",
      "  854\n",
      "  857\n",
      "  858\n",
      "\n",
      "Cluster nan:\n",
      "  801\n",
      "  804\n",
      "\n",
      "Cluster C05:\n",
      "  878\n",
      "  879\n",
      "  928\n",
      "  933\n",
      "  934\n",
      "  937\n",
      "\n",
      "Cluster C06:\n",
      "  944\n",
      "  948\n",
      "  952\n",
      "  959\n",
      "  968\n",
      "  969\n",
      "  977\n",
      "  985\n",
      "  998\n",
      "  1002\n",
      "  1003\n",
      "\n",
      "Cluster C09:\n",
      "  1231\n",
      "  1257\n",
      "\n",
      "Cluster C10:\n",
      "  1362\n",
      "\n",
      "Cluster C12:\n",
      "  1556\n",
      "\n",
      "Cluster C14:\n",
      "  2039\n",
      "\n",
      "=== Projects with Empty '02_phase_1_study' but with Non-Empty '03_phase_2_study' or '05_reassesment' (Grouped by Cluster) ===\n",
      "\n",
      "Cluster C01:\n",
      "  494\n",
      "  502\n",
      "  503\n",
      "  512\n",
      "\n",
      "Cluster C05:\n",
      "  928\n",
      "\n",
      "Cluster C06:\n",
      "  952\n",
      "  1002\n",
      "\n",
      "=== All Project Folder Names (Ascending Order) ===\n",
      "Project folder names (ascending order):\n",
      "'154', '163', '175', '183', '188', '193', '205', '215', '222', '239'\n",
      "'242', '250', '254', '258', '272', '282', '294', '297', '304', '320'\n",
      "'334', '337', '342', '348', '349', '356', '365', '378', '383', '407'\n",
      "'408', '412', '417', '421', '429', '442', '467', '468', '485', '488'\n",
      "'490', '493', '494', '495', '502', '503', '506', '509', '510', '512'\n",
      "'513', '521', '522', '522C', '541', '552', '555', '557', '558', '559'\n",
      "'560', '561', '565', '568', '569', '574', '576', '577', '579', '581'\n",
      "'583', '585', '586', '588', '589', '590', '593', '602', '606', '607'\n",
      "'608', '628', '632AA', '632C', '640', '642', '643', '643AA', '643AB', '643AC'\n",
      "'643AE', '643AF', '643AH', '643AI', '643AJ', '643AK', '643AM', '643AP', '643AS', '643D'\n",
      "'643E', '643F', '643G', '643I', '643J', '643O', '643R', '643S', '643T', '643W'\n",
      "'643X', '643Z', '644', '644A', '645A', '647', '649', '649A', '649B', '649C'\n",
      "'650A', '650AA', '650AC', '651', '651A', '653', '653A', '653B', '653D', '653E'\n",
      "'653EA', '653EB', '653ED', '653F', '653H', '654', '658', '659', '660', '661'\n",
      "'662', '663', '664', '667', '668', '669', '670', '671', '674', '676'\n",
      "'678', '679', '680', '681', '683', '684', '685', '686', '687', '688'\n",
      "'692', '695', '696', '697', '698', '700', '702', '703', '704', '705'\n",
      "'706', '707', '708', '709', '712', '714', '716', '717', '720', '723'\n",
      "'725', '729', '730', '732', '736', '737', '738', '739', '740', '741'\n",
      "'744', '746', '751', '752', '756', '762', '764', '765', '766', '767'\n",
      "'768', '769', '770', '771', '774', '775', '778', '779', '781', '782'\n",
      "'783', '784', '785', '786', '788', '789', '790', '791', '792', '793'\n",
      "'794', '795', '796', '797', '798', '799', '800', '801', '804', '805'\n",
      "'806', '807', '809', '812', '813', '814', '815', '816', '820', '823'\n",
      "'824', '825', '829', '831', '834', '836', '837', '838', '839', '840'\n",
      "'841', '842', '843', '845', '846', '847', '848', '849', '850', '851'\n",
      "'852', '853', '854', '855', '856', '857', '858', '869', '870', '871'\n",
      "'873', '877', '878', '879', '885', '887', '888', '890', '891', '892'\n",
      "'893', '894', '895', '896', '897', '898', '899', '900', '901', '902'\n",
      "'903', '904', '908', '909', '910', '911', '913', '914', '916', '917'\n",
      "'919', '921', '922', '923', '925', '926', '927', '928', '929', '930'\n",
      "'931', '932', '933', '934', '937', '941', '942', '943', '944', '945'\n",
      "'946', '947', '948', '950', '951', '952', '953', '954', '955', '956'\n",
      "'957', '959', '960', '961', '962', '963', '964', '965', '966', '967'\n",
      "'968', '969', '970', '971', '972', '974', '975', '976', '977', '985'\n",
      "'986', '987', '988', '989', '990', '991', '992', '993', '994', '997'\n",
      "'998', '1000', '1001', '1002', '1003', '1007', '1010', '1011', '1013', '1014'\n",
      "'1015', '1016', '1018', '1019', '1020', '1021', '1023', '1024', '1026', '1027'\n",
      "'1028', '1029', '1030', '1031', '1032', '1033', '1035', '1036', '1037', '1038'\n",
      "'1040', '1043', '1045', '1046', '1047', '1048', '1049', '1050', '1051', '1052'\n",
      "'1053', '1055', '1056', '1057', '1058', '1059', '1060', '1061', '1062', '1063'\n",
      "'1064', '1065', '1066', '1067', '1068', '1069', '1070', '1071', '1072', '1073'\n",
      "'1074', '1075', '1076', '1077', '1080', '1083', '1084', '1087', '1088', '1089'\n",
      "'1097', '1098', '1099', '1101', '1102', '1103', '1104', '1105', '1106', '1107'\n",
      "'1108', '1109', '1111', '1112', '1113', '1114', '1115', '1116', '1117', '1118'\n",
      "'1119', '1120', '1121', '1122', '1123', '1126', '1127', '1128', '1129', '1130'\n",
      "'1131', '1132', '1133', '1135', '1136', '1137', '1138', '1139', '1140', '1141'\n",
      "'1142', '1143', '1144', '1146', '1147', '1148', '1149', '1152', '1153', '1154'\n",
      "'1156', '1157', '1158', '1159', '1160', '1162', '1163', '1164', '1165', '1166'\n",
      "'1168', '1169', '1170', '1171', '1174', '1175', '1179', '1180', '1184', '1186'\n",
      "'1187', '1189', '1191', '1192', '1193', '1194', '1196', '1197', '1198', '1200'\n",
      "'1202', '1203', '1204', '1205', '1206', '1207', '1208', '1209', '1210', '1211'\n",
      "'1212', '1214', '1215', '1216', '1217', '1218', '1219', '1220', '1223', '1224'\n",
      "'1225', '1226', '1227', '1228', '1229', '1230', '1231', '1232', '1233', '1234'\n",
      "'1235', '1236', '1237', '1238', '1239', '1240', '1241', '1242', '1243', '1244'\n",
      "'1245', '1246', '1248', '1249', '1250', '1251', '1252', '1253', '1254', '1255'\n",
      "'1256', '1257', '1258', '1259', '1260', '1262', '1263', '1264', '1265', '1267'\n",
      "'1268', '1269', '1270', '1271', '1272', '1273', '1275', '1277', '1278', '1281'\n",
      "'1282', '1283', '1284', '1285', '1286', '1287', '1288', '1291', '1292', '1293'\n",
      "'1294', '1295', '1296', '1297', '1299', '1300', '1301', '1302', '1305', '1306'\n",
      "'1307', '1308', '1309', '1310', '1311', '1312', '1313', '1314', '1315', '1316'\n",
      "'1317', '1318', '1319', '1320', '1321', '1322', '1323', '1324', '1325', '1326'\n",
      "'1327', '1328', '1329', '1330', '1331', '1332', '1333', '1334', '1335', '1336'\n",
      "'1338', '1339', '1341', '1344', '1345', '1347', '1349', '1350', '1351', '1353'\n",
      "'1354', '1357', '1358', '1359', '1360', '1361', '1362', '1363', '1364', '1366'\n",
      "'1367', '1368', '1372', '1377', '1378', '1381', '1382', '1383', '1384', '1385'\n",
      "'1389', '1390', '1391', '1392', '1394', '1395', '1397', '1398', '1400', '1401'\n",
      "'1402', '1403', '1404', '1405', '1406', '1407', '1409', '1410', '1411', '1412'\n",
      "'1413', '1414', '1415', '1416', '1417', '1418', '1419', '1420', '1421', '1423'\n",
      "'1424', '1425', '1426', '1427', '1428', '1429', '1430', '1431', '1432', '1434'\n",
      "'1435', '1437', '1442', '1443', '1444', '1445', '1447', '1449', '1450', '1451'\n",
      "'1452', '1453', '1454', '1455', '1456', '1457', '1458', '1459', '1460', '1461'\n",
      "'1463', '1464', '1465', '1466', '1470', '1471', '1472', '1473', '1474', '1475'\n",
      "'1476', '1477', '1478', '1479', '1481', '1482', '1483', '1484', '1485', '1488'\n",
      "'1490', '1491', '1492', '1493', '1494', '1495', '1496', '1498', '1499', '1500'\n",
      "'1501', '1502', '1503', '1504', '1505', '1506', '1507', '1508', '1509', '1510'\n",
      "'1511', '1512', '1513', '1514', '1515', '1516', '1517', '1518', '1519', '1521'\n",
      "'1522', '1523', '1524', '1525', '1526', '1527', '1528', '1529', '1530', '1531'\n",
      "'1532', '1533', '1534', '1541', '1542', '1543', '1546', '1548', '1549', '1550'\n",
      "'1552', '1553', '1554', '1555', '1556', '1557', '1558', '1559', '1561', '1564'\n",
      "'1565', '1566', '1568', '1573', '1574', '1578', '1581', '1582', '1584', '1586'\n",
      "'1587', '1590', '1591', '1592', '1593', '1594', '1595', '1596', '1597', '1598'\n",
      "'1599', '1600', '1601', '1603', '1604', '1605', '1608', '1609', '1610', '1611'\n",
      "'1612', '1613', '1615', '1616', '1617', '1618', '1619', '1620', '1621', '1622'\n",
      "'1625', '1626', '1628', '1629', '1631', '1632', '1635', '1636', '1637', '1640'\n",
      "'1641', '1642', '1643', '1644', '1645', '1646', '1647', '1648', '1649', '1650'\n",
      "'1653', '1654', '1655', '1656', '1657', '1658', '1661', '1662', '1663', '1664'\n",
      "'1665', '1666', '1667', '1668', '1669', '1670', '1671', '1672', '1673', '1674'\n",
      "'1675', '1678', '1682', '1683', '1684', '1685', '1686', '1687', '1688', '1689'\n",
      "'1690', '1691', '1692', '1694', '1695', '1696', '1699', '1700', '1702', '1703'\n",
      "'1705', '1709', '1710', '1712', '1713', '1714', '1715', '1718', '1719', '1721'\n",
      "'1722', '1723', '1724', '1726', '1728', '1729', '1731', '1732', '1733', '1735'\n",
      "'1736', '1737', '1738', '1739', '1740', '1741', '1742', '1743', '1744', '1745'\n",
      "'1747', '1748', '1749', '1750', '1751', '1752', '1754', '1756', '1757', '1758'\n",
      "'1759', '1760', '1761', '1762', '1763', '1764', '1766', '1768', '1770', '1774'\n",
      "'1775', '1776', '1778', '1779', '1780', '1782', '1783', '1784', '1786', '1787'\n",
      "'1788', '1789', '1790', '1791', '1792', '1793', '1794', '1795', '1796', '1797'\n",
      "'1798', '1799', '1800', '1801', '1802', '1803', '1805', '1806', '1810', '1811'\n",
      "'1812', '1814', '1815', '1817', '1818', '1820', '1821', '1822', '1823', '1824'\n",
      "'1825', '1827', '1829', '1831', '1832', '1833', '1835', '1837', '1838', '1840'\n",
      "'1842', '1843', '1844', '1845', '1846', '1847', '1848', '1849', '1850', '1851'\n",
      "'1852', '1853', '1854', '1855', '1856', '1857', '1858', '1859', '1860', '1861'\n",
      "'1862', '1863', '1864', '1865', '1866', '1867', '1871', '1872', '1874', '1875'\n",
      "'1876', '1877', '1878', '1879', '1880', '1881', '1882', '1883', '1884', '1886'\n",
      "'1887', '1888', '1889', '1890', '1891', '1892', '1893', '1894', '1895', '1896'\n",
      "'1897', '1899', '1900', '1901', '1902', '1903', '1904', '1905', '1906', '1907'\n",
      "'1908', '1909', '1910', '1911', '1912', '1913', '1914', '1915', '1916', '1917'\n",
      "'1918', '1919', '1920', '1921', '1922', '1923', '1924', '1925', '1926', '1927'\n",
      "'1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1940', '1941'\n",
      "'1942', '1943', '1944', '1945', '1946', '1949', '1950', '1951', '1952', '1953'\n",
      "'1954', '1955', '1956', '1957', '1958', '1959', '1960', '1962', '1963', '1964'\n",
      "'1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974'\n",
      "'1975', '1976', '1977', '1978', '1979', '1980', '1983', '1987', '1988', '1992'\n",
      "'1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003'\n",
      "'2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013'\n",
      "'2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023'\n",
      "'2025', '2026', '2027', '2029', '2030', '2031', '2032', '2033', '2034', '2035'\n",
      "'2036', '2037', '2039', '2041', '2042', '2043', '2045', '2046', '2047', '2048'\n",
      "'2049', '2050', '2051', '2052', '2054', '2055', '2056', '2057', '2060', '2061'\n",
      "'2062', '2064', '2065', '2066', '2068', '2069', '2070', '2071', '2072', '2073'\n",
      "'2075', '2078', '2079', '2080', '2081', '2083', '2084', '2085', '2086', '2089'\n",
      "'2090', '2091', '2092', '2093', '2096', '2097', '2098', '2100', '2101', '2103'\n",
      "'2104', '2105', '2106', '2108', '2109', '2110', '2111', '2112', '2113', '2114'\n",
      "'2115', '2116', '2117', '2118', '2119', '2120', '2121', '2122', '2123', '2124'\n",
      "'2125', '2126', '2127', '2128', '2129', '2131', '2134', '2135', '2136', '2137'\n",
      "'2138', '2140', '2141', '2142', '2143', '2144', '2145', '2146', '2147', '2148'\n",
      "'2149', '2150', '2151', '2152', '2153', '2154', '2155', '2156', '2157', '2161'\n",
      "'2162', '2163', '2165', '2166', '2167', '2168', '2169', '2170', '2172', '2173'\n",
      "'2175', '2176', '2177', '2178', '2180', '2181', '2182', '2184', '2185', '2186'\n",
      "'2187', '2188', '2192'\n",
      "\n",
      "=== Total Count by Cluster ===\n",
      "Cluster C01:\n",
      "  Missing or Empty '02_phase_1_study': 7\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 4\n",
      "Cluster C02:\n",
      "  Missing or Empty '02_phase_1_study': 2\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster C03:\n",
      "  Missing or Empty '02_phase_1_study': 4\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster C04:\n",
      "  Missing or Empty '02_phase_1_study': 64\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster C05:\n",
      "  Missing or Empty '02_phase_1_study': 6\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 1\n",
      "Cluster C06:\n",
      "  Missing or Empty '02_phase_1_study': 11\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 2\n",
      "Cluster C09:\n",
      "  Missing or Empty '02_phase_1_study': 2\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster C10:\n",
      "  Missing or Empty '02_phase_1_study': 1\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster C12:\n",
      "  Missing or Empty '02_phase_1_study': 1\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster C14:\n",
      "  Missing or Empty '02_phase_1_study': 1\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster SGIP-TC:\n",
      "  Missing or Empty '02_phase_1_study': 12\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster TC:\n",
      "  Missing or Empty '02_phase_1_study': 1\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster Unknown:\n",
      "  Missing or Empty '02_phase_1_study': 7\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n",
      "Cluster nan:\n",
      "  Missing or Empty '02_phase_1_study': 2\n",
      "  Empty '02_phase_1_study' with Follow-up Data: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# =====================================================\n",
    "# User Settings and File Paths (update these as needed)\n",
    "# =====================================================\n",
    "\n",
    "# Base folder containing all project folders\n",
    "base_folder = r'/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data'\n",
    "\n",
    "# Numeric range to consider (based on the numeric prefix of folder names)\n",
    "range_start = 154   # (Update these values if needed)\n",
    "range_end   = 2192\n",
    "\n",
    "# Path to your CSV file (which must have columns \"q_id\" and \"cluster_number\")\n",
    "phase_status_csv = r'/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/phase_status.csv'\n",
    "\n",
    "# =====================================================\n",
    "# Read CSV and Build Cluster Mapping\n",
    "# =====================================================\n",
    "\n",
    "# Read the CSV ensuring that the q_id is treated as a string\n",
    "phase_status = pd.read_csv(phase_status_csv, dtype={'q_id': str})\n",
    "# Create a mapping from q_id to cluster_number\n",
    "cluster_mapping = dict(zip(phase_status['q_id'], phase_status['cluster_number']))\n",
    "\n",
    "# =====================================================\n",
    "# Initialize Data Structures for Results\n",
    "# =====================================================\n",
    "\n",
    "empty_phase1_count = 0          # Count of projects with an empty \"02_phase_1_study\"\n",
    "empty_phase1_folders = []       # List of folder paths with an empty \"02_phase_1_study\" subfolder\n",
    "\n",
    "# Dictionaries to group projects by cluster:\n",
    "# 1. Projects missing Phase 1 (i.e. the folder is missing or exists but is empty)\n",
    "missing_phase1_by_cluster = {}  \n",
    "\n",
    "# 2. Projects that have an empty Phase 1 folder but have non-empty \"03_phase_2_study\" or \"05_reassesment\"\n",
    "empty_phase1_with_following_by_cluster = {}  \n",
    "\n",
    "# List to hold all project folder names (for final sorted list)\n",
    "project_folders = []\n",
    "\n",
    "# =====================================================\n",
    "# Gather and Sort Candidate Folders\n",
    "# =====================================================\n",
    "\n",
    "# We use a list of tuples (folder_name, numeric_prefix) so we can sort by the numeric value\n",
    "folder_candidates = []\n",
    "for item in os.listdir(base_folder):\n",
    "    item_path = os.path.join(base_folder, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        # Use regex to extract the numeric prefix (works for names like \"641\" or \"641AA\")\n",
    "        m = re.match(r'^(\\d+)', item)\n",
    "        if m:\n",
    "            numeric_part = int(m.group(1))\n",
    "            if range_start <= numeric_part <= range_end:\n",
    "                folder_candidates.append((item, numeric_part))\n",
    "\n",
    "# Sort candidates in ascending order first by the numeric prefix, then alphabetically by the full folder name\n",
    "folder_candidates.sort(key=lambda x: (x[1], x[0]))\n",
    "\n",
    "# =====================================================\n",
    "# Process Each Candidate Folder\n",
    "# =====================================================\n",
    "\n",
    "for folder_name, num in folder_candidates:\n",
    "    # Add folder name to final list (for reporting later)\n",
    "    project_folders.append(folder_name)\n",
    "    \n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "    # Build subfolder paths\n",
    "    phase1_path = os.path.join(folder_path, \"02_phase_1_study\")\n",
    "    phase2_path = os.path.join(folder_path, \"03_phase_2_study\")\n",
    "    reassessment_path = os.path.join(folder_path, \"05_reassesment\")\n",
    "    \n",
    "    # Determine the cluster for this project using the CSV mapping; default to \"Unknown\" if not found.\n",
    "    cluster = cluster_mapping.get(folder_name, \"Unknown\")\n",
    "    \n",
    "    # Check for \"missing\" Phase 1:\n",
    "    # Here, we define \"missing\" as: either the Phase 1 folder does not exist OR it exists but is empty.\n",
    "    phase1_missing = False\n",
    "    if not os.path.isdir(phase1_path):\n",
    "        phase1_missing = True\n",
    "    else:\n",
    "        # The folder exists; check if it is empty.\n",
    "        if not os.listdir(phase1_path):\n",
    "            phase1_missing = True\n",
    "            empty_phase1_count += 1\n",
    "            empty_phase1_folders.append(folder_path)\n",
    "    \n",
    "    if phase1_missing:\n",
    "        missing_phase1_by_cluster.setdefault(cluster, []).append(folder_name)\n",
    "    \n",
    "        # Additionally, if the folder exists (but is empty) and at least one of the follow-up folders is non-empty,\n",
    "        # then add it to the separate grouping.\n",
    "        if os.path.isdir(phase1_path):  # only check follow-ups if the folder exists (even though it's empty)\n",
    "            phase2_non_empty = os.path.isdir(phase2_path) and bool(os.listdir(phase2_path))\n",
    "            reassessment_non_empty = os.path.isdir(reassessment_path) and bool(os.listdir(reassessment_path))\n",
    "            if phase2_non_empty or reassessment_non_empty:\n",
    "                empty_phase1_with_following_by_cluster.setdefault(cluster, []).append(folder_name)\n",
    "\n",
    "# =====================================================\n",
    "# Output the Results\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\nTotal projects (from {range_start} to {range_end}) with a missing or empty '02_phase_1_study' folder: {len(sum(missing_phase1_by_cluster.values(), []))}\")\n",
    "\n",
    "if empty_phase1_folders:\n",
    "    print(\"\\nFolders with an empty '02_phase_1_study' subfolder (full paths):\")\n",
    "    for folder in empty_phase1_folders:\n",
    "        print(folder)\n",
    "\n",
    "print(\"\\n=== Projects Missing '02_phase_1_study' (Missing or Empty) Grouped by Cluster ===\")\n",
    "if missing_phase1_by_cluster:\n",
    "    for clust, folders in missing_phase1_by_cluster.items():\n",
    "        print(f\"\\nCluster {clust}:\")\n",
    "        for folder in folders:\n",
    "            print(f\"  {folder}\")\n",
    "else:\n",
    "    print(\"No projects found missing the '02_phase_1_study' folder.\")\n",
    "\n",
    "print(\"\\n=== Projects with Empty '02_phase_1_study' but with Non-Empty '03_phase_2_study' or '05_reassesment' (Grouped by Cluster) ===\")\n",
    "if empty_phase1_with_following_by_cluster:\n",
    "    for clust, folders in empty_phase1_with_following_by_cluster.items():\n",
    "        print(f\"\\nCluster {clust}:\")\n",
    "        for folder in folders:\n",
    "            print(f\"  {folder}\")\n",
    "else:\n",
    "    print(\"No projects found with empty '02_phase_1_study' but non-empty subsequent phases.\")\n",
    "\n",
    " \n",
    "\n",
    "print(\"\\n=== All Project Folder Names (Ascending Order) ===\")\n",
    "# Build a list of just the folder names\n",
    "project_folders = [folder for folder, num in folder_candidates]\n",
    "\n",
    "# Print the list in a format you can copy and paste\n",
    "# Print the list with 10 items per line\n",
    "print(\"Project folder names (ascending order):\")\n",
    "line_length = 10\n",
    "for i in range(0, len(project_folders), line_length):\n",
    "    line_items = project_folders[i:i+line_length]\n",
    "    # Join each item wrapped in quotes and separated by a comma and space\n",
    "    line = \", \".join(f\"'{item}'\" for item in line_items)\n",
    "    print(line)\n",
    "\n",
    "# =====================================================\n",
    "# Print Total Counts by Cluster\n",
    "# =====================================================\n",
    "\n",
    "# Get all clusters that appear in either dictionary.\n",
    "all_clusters = set(missing_phase1_by_cluster.keys()).union(set(empty_phase1_with_following_by_cluster.keys()))\n",
    "\n",
    "print(\"\\n=== Total Count by Cluster ===\")\n",
    "for clust in sorted(all_clusters, key=lambda x: str(x)):\n",
    "    count_missing = len(missing_phase1_by_cluster.get(clust, []))\n",
    "    count_following = len(empty_phase1_with_following_by_cluster.get(clust, []))\n",
    "    print(f\"Cluster {clust}:\")\n",
    "    print(f\"  Missing or Empty '02_phase_1_study': {count_missing}\")\n",
    "    print(f\"  Empty '02_phase_1_study' with Follow-up Data: {count_following}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creating itemized and total datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 667 extracted phase 2 by mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_96622/1382837374.py:352: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_96622/1382837374.py:352: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing q_id: 493\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 510\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 561\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 565\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 574\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 583\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 590\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 608\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 628\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643AM\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643AP\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 643T\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 649B\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 649C\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 650AA\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 651A\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 653ED\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 653H\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 658\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 659\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 660\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 661\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 662\n",
      "\n",
      "Checking Upgrade: LDNU, Total Rows Present?: False\n",
      "Creating Total row for LDNU\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 781\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 789\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 794\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "Processing q_id: 838\n",
      "\n",
      "Checking Upgrade: PTO_IF, Total Rows Present?: False\n",
      "Creating Total row for PTO_IF\n",
      "\n",
      "Checking Upgrade: RNU, Total Rows Present?: False\n",
      "Creating Total row for RNU\n",
      "\n",
      "New Total Rows Created:\n",
      "    q_id  cluster req_deliverability  latitude  longitude  capacity  \\\n",
      "0   493      NaN               Full       NaN        NaN       NaN   \n",
      "1   493      NaN               Full       NaN        NaN       NaN   \n",
      "2   493      NaN               Full       NaN        NaN       NaN   \n",
      "3   510      NaN               None       NaN        NaN       NaN   \n",
      "4   510      NaN               None       NaN        NaN       NaN   \n",
      "..  ...      ...                ...       ...        ...       ...   \n",
      "71  789      NaN               Full       NaN        NaN       NaN   \n",
      "72  794      NaN               Full       NaN        NaN       NaN   \n",
      "73  794      NaN               Full       NaN        NaN       NaN   \n",
      "74  838      NaN               Full       NaN        NaN       NaN   \n",
      "75  838      NaN               Full       NaN        NaN       NaN   \n",
      "\n",
      "                             point_of_interconnection type_of_upgrade  \\\n",
      "0   500 kV Bus at new switchyard looped into propo...      Total LDNU   \n",
      "1   500 kV Bus at new switchyard looped into propo...    Total PTO_IF   \n",
      "2   500 kV Bus at new switchyard looped into propo...       Total RNU   \n",
      "3            230 kV Bus at Imperial Valley Substation      Total LDNU   \n",
      "4            230 kV Bus at Imperial Valley Substation    Total PTO_IF   \n",
      "..                                                ...             ...   \n",
      "71             69 kV bus at Boulevard East Substation       Total RNU   \n",
      "72                 138 kV bus at Boulevard Substation    Total PTO_IF   \n",
      "73                 138 kV bus at Boulevard Substation       Total RNU   \n",
      "74     230 kV bus at the proposed New Imperial Valley    Total PTO_IF   \n",
      "75     230 kV bus at the proposed New Imperial Valley       Total RNU   \n",
      "\n",
      "   type_of_upgrade_2 upgrade description cost_allocation_factor  \\\n",
      "0                                                                 \n",
      "1                                                                 \n",
      "2                                                                 \n",
      "3                                                                 \n",
      "4                                                                 \n",
      "..               ...     ...         ...                    ...   \n",
      "71                                                                \n",
      "72                                                                \n",
      "73                                                                \n",
      "74                                                                \n",
      "75                                                                \n",
      "\n",
      "    estimated_cost_x_1000 estimated_time_to_construct item  \n",
      "0                 49715.0                               no  \n",
      "1                   341.0                               no  \n",
      "2                 73489.0                               no  \n",
      "3                 16495.0                               no  \n",
      "4                   250.0                               no  \n",
      "..                    ...                         ...  ...  \n",
      "71                 4827.0                               no  \n",
      "72                 1999.0                               no  \n",
      "73                 2153.0                               no  \n",
      "74                  193.0                               no  \n",
      "75               148108.0                               no  \n",
      "\n",
      "[76 rows x 15 columns]\n",
      "Itemized rows saved to 'costs_phase_1_cluster_5_style_D_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_1_cluster_5_style_D_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU']\n",
      "['493' '510' '561' '565' '574' '583' '590' '608' '628' '643AM' '643AP'\n",
      " '643T' '649B' '649C' '650AA' '651A' '653ED' '653H' '658' '659' '660'\n",
      " '661' '662' '781' '789' '794' '838']\n",
      "[nan  1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_96622/1382837374.py:352: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_96622/1382837374.py:352: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_96622/1382837374.py:453: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean[required_cols] = df_clean[required_cols].applymap(lambda x: str(x).strip())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_96622/1382837374.py:892: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/03_raw/rawdata_cluster1_4_style_R_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "#df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 0: CREATE DESCRIPTION COLUMN FROM COST ALLOCATION FACTOR\n",
    "\n",
    "\n",
    "def move_non_numeric_text(value):\n",
    "    \"\"\"Move non-numeric, non-percentage text from cost allocation factor to description.\n",
    "       If a value is moved, return None for cost allocation factor.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return value  # Keep numeric or percentage values\n",
    "        return None  # Clear the value if it's text (moved to description)\n",
    "    return value  # Return as is for non-string values\n",
    "\n",
    "\n",
    "def extract_non_numeric_text(value):\n",
    "    \"\"\"Extract non-numeric, non-percentage text from the cost allocation factor column.\"\"\"\n",
    "    if isinstance(value, str):  # Ensure it's a string before processing\n",
    "        if re.fullmatch(r\"[\\d,.]+%?\", value):  # Check if it's numeric or a percentage\n",
    "            return None\n",
    "        return value.strip()  # Return text entries as is\n",
    "    return None  # Return None for non-string values\n",
    "\n",
    "\n",
    "\n",
    "def clean_total_entries(value):\n",
    "    \"\"\"If the value starts with 'Total', remove numbers, commas, and percentage signs, keeping only 'Total'.\"\"\"\n",
    "    if isinstance(value, str) and value.startswith(\"Total\"):\n",
    "        return \"Total\"  # Keep only \"Total\"\n",
    "    return value  # Leave other values unchanged\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_cost_allocation(df, source_col, target_col=\"cost_allocation_factor\"):\n",
    "    \"\"\"\n",
    "    Extracts percentage values from a specified source column and moves them into a target column.\n",
    "    \n",
    "    - A percentage value is defined as a string that, when stripped of whitespace,\n",
    "      fully matches a pattern of digits (with optional commas or periods) followed by a percent sign.\n",
    "    - If a cell in the source column matches this pattern, its value is placed into the target column,\n",
    "      and the source column cell is cleared (set to an empty string).\n",
    "    - If the cell does not match a percentage pattern, it is left untouched in the source column.\n",
    "    \n",
    "    Parameters:\n",
    "      df         : pandas DataFrame.\n",
    "      source_col : string, the name of the column to scan for percentage values.\n",
    "      target_col : string, the name of the column to store the extracted percentage values.\n",
    "                   Defaults to \"cost_allocation_factor\".\n",
    "    \n",
    "    Returns:\n",
    "      The DataFrame with the updated columns.\n",
    "    \"\"\"\n",
    "    # Define a regex pattern to match a percentage value (e.g., \"78.25%\").\n",
    "    # The pattern allows digits, commas, and periods, followed immediately by a \"%\" (ignoring leading/trailing spaces).\n",
    "    pattern = r\"^\\s*[\\d,\\.]+%\\s*$\"\n",
    "    \n",
    "    def extract_percentage(text):\n",
    "        # If text matches the percentage pattern, return the stripped text; otherwise, return None.\n",
    "        if isinstance(text, str) and re.fullmatch(pattern, text):\n",
    "            return text.strip()\n",
    "        return None\n",
    "\n",
    "    def clear_percentage(text):\n",
    "        # If text matches the percentage pattern, clear it (return an empty string).\n",
    "        # Otherwise, return the text stripped of surrounding whitespace.\n",
    "        if isinstance(text, str) and re.fullmatch(pattern, text):\n",
    "            return \"\"\n",
    "        if isinstance(text, str):\n",
    "            return text.strip()\n",
    "        return text\n",
    "\n",
    "    # Create (or overwrite) the target column with extracted percentage values from the source column.\n",
    "    df[target_col] = df[source_col].apply(extract_percentage)\n",
    "    # In the source column, remove any percentage values (leaving other text intact).\n",
    "    df[source_col] = df[source_col].apply(clear_percentage)\n",
    "    \n",
    "    return df\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "def filter_numeric_costs(df, col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame and column name, this function extracts the numeric cost from each cell,\n",
    "    converting values with an optional '$' sign (and possible commas) to floats.\n",
    "    If a valid numeric cost cannot be extracted, the cell is set to NaN.\n",
    "    \n",
    "    Parameters:\n",
    "      df  : pandas DataFrame.\n",
    "      col : string, the name of the column to process.\n",
    "      \n",
    "    Returns:\n",
    "      The original DataFrame with the specified column converted to numeric values (or NaN if conversion fails).\n",
    "    \"\"\"\n",
    "    def extract_numeric(value):\n",
    "        value_str = str(value)\n",
    "        # This regex matches an optional '$', optional spaces, and a number with commas and an optional decimal part.\n",
    "        match = re.search(r'\\$?\\s*([\\d,]+(?:\\.\\d+)?)', value_str)\n",
    "        if match:\n",
    "            num_str = match.group(1).replace(',', '')\n",
    "            try:\n",
    "                return float(num_str)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    # Apply the extraction function to the specified column.\n",
    "    df[col] = df[col].apply(extract_numeric)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def extract_months_values(df, col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame and column name, this function extracts text patterns matching\n",
    "    durations expressed in months (e.g., \"32 months\", \"43 Month\", \"23 Months\", \"22Months\", or \"21-29months\").\n",
    "    If a valid pattern is found, it returns the matched text; otherwise, it returns an empty string.\n",
    "    \n",
    "    Parameters:\n",
    "        df  : pandas DataFrame.\n",
    "        col : string, the name of the column to process.\n",
    "        \n",
    "    Returns:\n",
    "        The DataFrame with the specified column updated.\n",
    "    \"\"\"\n",
    "    def extract_months(text):\n",
    "        text = str(text)\n",
    "        # Pattern explanation:\n",
    "        #   \\d+          : one or more digits\n",
    "        #   (?:-\\d+)?    : optionally, a hyphen followed by one or more digits (to capture ranges like 21-29)\n",
    "        #   \\s*          : optional whitespace\n",
    "        #   [Mm]onths?   : \"month\" or \"months\" (case insensitive for the first letter)\n",
    "        pattern = r'(\\d+(?:-\\d+)?\\s*[Mm]onths?)'\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group(1) if match else \"\"\n",
    "    \n",
    "    df[col] = df[col].apply(extract_months)\n",
    "    return df\n",
    "\n",
    "def move_months_values(df, source_col, target_col):\n",
    "    \"\"\"\n",
    "    For a given DataFrame, this function extracts text patterns matching durations expressed in months\n",
    "    (e.g., \"32 months\", \"43 Month\", \"23 Months\", \"22Months\", or \"21-29months\") from the source column,\n",
    "    moves the extracted text to the target column, and removes it from the source column.\n",
    "    \n",
    "    Parameters:\n",
    "        df         : pandas DataFrame.\n",
    "        source_col : string, the name of the column to extract the month text from.\n",
    "        target_col : string, the name of the column where the extracted month text will be moved.\n",
    "        \n",
    "    Returns:\n",
    "        The updated DataFrame with the month values moved.\n",
    "    \"\"\"\n",
    "    # Pattern explanation:\n",
    "    #   \\d+          : one or more digits\n",
    "    #   (?:-\\d+)?    : optionally, a hyphen and one or more digits (to capture ranges like 21-29)\n",
    "    #   \\s*          : optional whitespace\n",
    "    #   [Mm]onths?   : \"month\" or \"months\" (case insensitive for the first letter)\n",
    "    pattern = r'(\\d+(?:-\\d+)?\\s*[Mm]onths?)'\n",
    "    \n",
    "    def process_text(text):\n",
    "        text = str(text)\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            extracted = match.group(1)\n",
    "            # Remove the extracted text from the source text and clean up extra spaces\n",
    "            updated_text = re.sub(pattern, \"\", text).strip()\n",
    "            return extracted, updated_text\n",
    "        else:\n",
    "            return \"\", text\n",
    "\n",
    "    # Prepare lists to store the extracted month text and the updated source text\n",
    "    extracted_vals = []\n",
    "    updated_source_vals = []\n",
    "    \n",
    "    for val in df[source_col]:\n",
    "        ext, updated = process_text(val)\n",
    "        extracted_vals.append(ext)\n",
    "        updated_source_vals.append(updated)\n",
    "    \n",
    "    # Create/update the target column with the extracted month text\n",
    "    df[target_col] = extracted_vals\n",
    "    # Replace the source column values with the text after removal of the month text\n",
    "    df[source_col] = updated_source_vals\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Filter numeric costs in 'estimated_cost_x_1000' and 'escalated_cost_x_1000' columns\n",
    "df = filter_numeric_costs(df, 'unnamed_11')\n",
    "\n",
    "df = filter_numeric_costs(df, 'unnamed_10')\n",
    "\n",
    "df = filter_numeric_costs(df, 'estimated')\n",
    "\n",
    "df = filter_numeric_costs(df, 'estimated cost x')\n",
    "\n",
    "\n",
    "df = extract_months_values(df, 'estimated_1')\n",
    "#df = move_months_values(df, 'unnamed_13', 'estimated time to construct')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['cost_allocation_factor']= None\n",
    "\n",
    "\n",
    "df = extract_cost_allocation(df, \"unnamed_8\", \"cost_allocation_factor\")\n",
    "\n",
    "df = extract_cost_allocation(df, \"unnamed_9\", \"cost_allocation_factor\")\n",
    "\n",
    "# Create the 'description' column from 'cost allocation factor'\n",
    "#if 'unnamed_9' in df.columns:\n",
    " #  df['unnamed_9'] = df['unnamed_9'].apply(extract_non_numeric_text)\n",
    "  # df['unnamed_9'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values\n",
    "\n",
    "\n",
    "#if 'unnamed_8' in df.columns:\n",
    " #  df['unnamed_8'] = df['unnamed_8'].apply(extract_non_numeric_text)\n",
    "  # df['unnamed_8'] = df['cost_allocation_factor'].apply(move_non_numeric_text)  # Clear moved values   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 1 MERGE COLUMNS\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "\n",
    "        \"upgrade\": [\n",
    "            \"upgrade\",\n",
    "             \"unnamed_4\",\n",
    "             \"eastern area sp\",\n",
    "            ],\n",
    "\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"MW\",\n",
    "            \n",
    "        ],   \n",
    "\n",
    "        \"description\": [\"description\",\n",
    "                         \"unnamed_6\" , \"unnamed_7\"],\n",
    "\n",
    "        \"estimated_time_to_construct\": [ \n",
    "            \n",
    "            \"estimated time to construct\", \"6 month\", \"unnamed_17\",\n",
    "                                         \"84 month\", \"24 month\", \"48 month\", \"estimated_1\",\"60 month\",\"12 month\", \n",
    "                                         \"3648 month\"\n",
    "                                         ],\n",
    "\n",
    "        \"type_of_upgrade_2\": [\"unnamed_1\", \"delivery network upgrade\", \"reliability network upgrade\",],\n",
    "\n",
    "\n",
    "\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \n",
    "\n",
    "            \"estimated cost x 1000\",\n",
    "            \"estimated_cost\",\n",
    "            \"estimated cost\",\n",
    "            \"estimated\",\n",
    "            \"estimated cost x\",\n",
    "            \n",
    "            \"estimated cost x 1000 constant dollar_1\",\n",
    "            \"estimated cost x 1000 constant dollar\",\n",
    "            \"estimated cost x 1000 constant\",\n",
    "            \"estimated cost x 1000 constant dollar_1\",\n",
    "            \"estimated cost x\",\n",
    "            \"allocated_cost\",\n",
    "            \"assigned cost\",\n",
    "            \"allocated cost\",\n",
    "            \"sum of allocated constant cost\",\n",
    "            \"unnamed_13\",\n",
    "            \"unnamed_11\",\n",
    "            \"unnamed_10\",\n",
    "\n",
    "             \n",
    "        ],    \n",
    "\n",
    "\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \n",
    "            \"escalated cost x 1000 constant dollar\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"allocated cost escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\",\n",
    "            \"assigned cost escalated\",\n",
    "            \n",
    "             \n",
    "\n",
    "        ],\n",
    "\n",
    "         \n",
    "\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "       \n",
    "         \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost_allocation_factor\",\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocation\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "            \"project allocation\",\n",
    "            \"percent allocation\",\n",
    "           \n",
    "\n",
    "        ],\n",
    "       \n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 2: REMOVE DOLLAR SIGNED VALUES FROM 'estimated_time_to_construct'\n",
    "######## Other clean up\n",
    "\n",
    "def remove_dollar_values(value):\n",
    "    \"\"\"Remove dollar amounts (e.g., $3625.89, $3300) from 'estimated_time_to_construct'.\"\"\"\n",
    "    if isinstance(value, str) and re.search(r\"^\\$\\d+(\\.\\d{1,2})?$\", value.strip()):\n",
    "        return None  # Replace with None if it's a dollar-signed number\n",
    "    return value.strip() if isinstance(value, str) else value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(remove_dollar_values)\n",
    "\n",
    "\n",
    "## Remove ranodm number in Total row:    \n",
    "# Apply cleaning function to \"upgrade\" column after merging\n",
    "#if 'upgrade' in df.columns:\n",
    " #   df['upgrade'] = df['upgrade'].apply(clean_total_entries)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 3: DROP UNNEEDED COLUMNS\n",
    "\n",
    "#df.drop(['unnamed_3', 'unnamed_15', 'unnamed_18', 'unnamed_16', 'estimated cost x 1000 escalated with itcca'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "df.drop(['unnamed_3','unnamed_5', 'unnamed_8', 'unnamed_9','unnamed_12', 'unnamed_14', 'unnamed_15', 'unnamed_16','unnamed_18','unnamed_19' , 'see prior study','type of'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 4: NAMING CONVENTION\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "\n",
    "# Convert estimated_time_to_construct to integer (remove decimals) and keep NaNs as empty\n",
    "#df['estimated_time_to_construct'] = pd.to_numeric(df['estimated_time_to_construct'], errors='coerce').apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "\n",
    " \n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Processes the DataFrame as follows:\n",
    "    \n",
    "    1. Drops any rows where any of these columns are empty or blank:\n",
    "       - 'upgrade', 'description', 'cost_allocation_factor',\n",
    "         'estimated_time_to_construct', 'type_of_upgrade_2', 'estimated_cost_x_1000'\n",
    "    \n",
    "    2. For each remaining row, if the value in 'type_of_upgrade' starts with\n",
    "       'SCE', 'SDG&E', or 'PG&E' (or is empty after stripping),\n",
    "       then the value in 'type_of_upgrade_2' is replaced with the value from 'type_of_upgrade'.\n",
    "       \n",
    "    Parameters:\n",
    "        df: pandas DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        A cleaned DataFrame with the above processing applied.\n",
    "    \"\"\"\n",
    "    # Define the required columns\n",
    "    required_cols = [\n",
    "        \"upgrade\", \"description\", \"cost_allocation_factor\",\n",
    "        \"estimated_time_to_construct\", \"type_of_upgrade_2\", \"estimated_cost_x_1000\"\n",
    "    ]\n",
    "    \n",
    "       # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Replace NaN with empty strings for checking emptiness\n",
    "    df_clean[required_cols] = df_clean[required_cols].fillna(\"\")\n",
    "\n",
    "    # Convert all required columns to strings and strip whitespace\n",
    "    df_clean[required_cols] = df_clean[required_cols].applymap(lambda x: str(x).strip())\n",
    "    \n",
    "    \n",
    " # Drop rows where all required columns are empty\n",
    "    df_clean = df_clean[~(df_clean[required_cols].apply(lambda row: all(row == \"\"), axis=1))]\n",
    "    \n",
    " \n",
    "    \n",
    "    # Define a function to update type_of_upgrade_2 if needed.\n",
    "    def update_type(row):\n",
    "        # Get the value from type_of_upgrade (converted to string and stripped)\n",
    "        val = str(row.get(\"type_of_upgrade\", \"\")).strip()\n",
    "        # If the value is empty or starts with SCE, SDG&E, or PG&E, then update type_of_upgrade_2\n",
    "        if val == \"\" or re.match(r'^(SCE|SDG&E|PG&E)', val):\n",
    "            row[\"type_of_upgrade\"] = row[\"type_of_upgrade_2\"]\n",
    "        return row\n",
    "\n",
    "\n",
    "    # Apply the function row-wise\n",
    "    df_clean = df_clean.apply(update_type, axis=1)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "df = process_dataframe(df)\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"type_of_upgrade_2\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df     \n",
    "\n",
    "\n",
    "\n",
    "df = reorder_columns(df)\n",
    "\n",
    "df= df[df['q_id']!= '667']\n",
    " \n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].replace(\"\", np.nan).ffill() \n",
    "\n",
    "\n",
    "df= df[df['type_of_upgrade']!= '12. Local Furnishing Bonds']\n",
    "df= df[df['type_of_upgrade']!= '(when applicable):']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/03_raw/cluster_1_4_style_R.csv', index=False)\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 5: REMOVING TOTAL ROW, AS THE PDFS GIVE TOTAL NETWORK COST RATHER THAN BY RNU, LDNU AS WE HAD BEFORE\n",
    "# Remove rows where upgrade is \"Total\" (case-insensitive)\n",
    "\n",
    "df['tot'] = df.apply(\n",
    "    lambda row: 'yes' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) \n",
    "        ) else 'no',\n",
    "    axis=1\n",
    ") \n",
    "\n",
    "# Now extract ONLY \"Total\" rows with a foolproof match\n",
    "total_rows_df = df[df['tot'] == 'yes']\n",
    "\n",
    "total_rows_df = total_rows_df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "total_rows_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/02_intermediate/costs_phase_1_cluster_1_4_style_R_total_network.csv', index=False) \n",
    "df = df[df['type_of_upgrade'].str.strip().str.lower() != 'total']\n",
    "df = df[df['type_of_upgrade'].str.strip().str.lower() != 'total cost']\n",
    " \n",
    "df.drop('tot', axis=1, inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 6: Move upgrade phrases like IRNU from upgrade column to a new column upgrade_classificatio and also replace type_of_upgrade with LDNU, CANU\n",
    "\n",
    "\n",
    "\n",
    "# Define the list of phrases for upgrade classification\n",
    "upgrade_phrases = [\"IRNU\", \"GRNU\", \"CANU-D\", \"IRNU-A\", \"LDNU\", \"CANU-GR\", \"PNU\", \"CANU\"]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)  \n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    \"PTO\u2019s Interconnection Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTO\u2019s Interconnectio n Facilities (Note 2)\": \"PTO_IF\",\n",
    "    \"PTOs Interconnection Facilities\": \"PTO_IF\",\n",
    "    \"PTOs Interconnectio n Facilities\": \"PTO_IF\",\n",
    " \"PTO\": 'PTO_IF',\n",
    " \"PNU\": \"OPNU\",\n",
    " \"Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Delivery Network\": \"ADNU\",\n",
    " \"Plan of Service Reliability Network Upgrades\": \"RNU\",\n",
    " \"Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Reliability Network Upgrades\": \"RNU\",\n",
    " \"SDG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Upgrades\": \"LDNU\",\n",
    " \"SCE Distribution Upgrades\": \"LDNU\",\n",
    " \"SCE Reliability Network Upgrades for Short Circuit duty\": \"RNU\",\n",
    " \"SCE Network Upgrades\": \"RNU\",\n",
    " \"Plan of Service Distribution Upgrades\": \"LDNU\",\n",
    " \"PG&E Delivery Network Upgrades\": \"LDNU\",\n",
    " \"SCE Delivery Network Upgrades\": \"LDNU\",\n",
    " \"Upgrades, Estimated Costs, and Estimated Time to Construct Summary for C565 - Continued\": \"LDNU\",\n",
    " \"Upgrades, Estimated Costs, and Estimated Time to Construct Summary for C565 -\": \"LDNU\",\n",
    " \"Reliability Network Upgrades to Physically Interconnect\": \"RNU\",\n",
    "\n",
    " \"Reliability Network Upgrades\": \"RNU\",\n",
    "    \"Local Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Area Deliverability Upgrades\": \"ADNU\",\n",
    "    \"Escalated Cost and Time to Construct for Interconnection Facilities, Reliability Network Upgrades, and Delivery Network Upgrades\": \"LDNU\",\n",
    "    \"Distribution\": \"ADNU\",\n",
    "'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " 'Total CANU': 'CANU',\n",
    " 'Total LOPNU': 'LOPNU',\n",
    " 'Total ADNU': 'ADNU',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "#STEP 7: Stable sort type of upgrade\n",
    "\n",
    "def stable_sort_by_type_of_upgrade(df):\n",
    "    \"\"\"Performs a stable sort within each q_id to order type_of_upgrade while preserving row order in other columns.\"\"\"\n",
    "    \n",
    "    # Define the custom sorting order for type_of_upgrade\n",
    "    type_order = {\"PTO_IF\": 1, \"RNU\": 2, \"LDNU\": 3, \"PNU\": 4, \"ADNU\": 5}\n",
    "\n",
    "    # Assign a numerical sorting key; use a high number if type_of_upgrade is missing\n",
    "    df['sort_key'] = df['type_of_upgrade'].map(lambda x: type_order.get(x, 99))\n",
    "\n",
    "    # Perform a stable sort by q_id first, then by type_of_upgrade using the custom order\n",
    "    df = df.sort_values(by=['q_id', 'sort_key'], kind='stable').drop(columns=['sort_key'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply stable sorting\n",
    "  \n",
    "\n",
    "\n",
    "df = df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], as_index=False).first()\n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "#df = stable_sort_by_type_of_upgrade(df)  \n",
    "#df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/03_raw/cluster_14_style_Q.csv', index=False)\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 8: Remove $ signs and convert to numeric\n",
    " \n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000',]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "########################################\n",
    "# STEP 9: Create Total rows\n",
    "\n",
    "\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000']\n",
    "columns_to_populate = ['cluster', 'req_deliverability', 'latitude', 'longitude', 'capacity', 'point_of_interconnection']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    print(f\"\\nProcessing q_id: {q_id}\")  # Debug print\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade):\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        # Debug: Print current group\n",
    "        print(f\"\\nChecking Upgrade: {upgrade}, Total Rows Present?:\", \n",
    "              ( (group['item'] == 'no')).any())\n",
    "\n",
    "        # Check if a Total row already exists for this (q_id, upgrade)\n",
    "        total_exists = ((group['item'] == 'no')).any()\n",
    "        \n",
    "        if total_exists:\n",
    "            print(f\"Skipping Total row for {upgrade} (already exists).\")\n",
    "            continue\n",
    "        \n",
    "        total_row = {col: '' for col in df.columns}  # Initialize all columns as empty strings\n",
    "        total_row['q_id'] = q_id\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "\n",
    "        # Populate specified columns from the existing row\n",
    "        first_row = rows.iloc[0]\n",
    "        for col in columns_to_populate:\n",
    "            if col in df.columns:\n",
    "                total_row[col] = first_row[col]\n",
    "\n",
    "        # Sum the numeric columns\n",
    "        for col in columns_to_sum:\n",
    "            if col in rows.columns:\n",
    "                total_row[col] = rows[col].sum()\n",
    "            else:\n",
    "                total_row[col] = 0  # Default to 0 if column is missing\n",
    "\n",
    "        print(f\"Creating Total row for {upgrade}\")  # Debug print\n",
    "        new_rows.append(total_row)\n",
    "\n",
    "# Convert list to DataFrame and append\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    print(\"\\nNew Total Rows Created:\\n\", total_rows_df)  # Debug print\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "  \n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "df = stable_sort_by_type_of_upgrade(df)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    \"\"\"\n",
    "    Removes the word 'month' or 'months' (case insensitive) from the value.\n",
    "    Leaves behind any numbers or number ranges (e.g. \"6\", \"6-12\").\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Remove 'month' or 'months' (case-insensitive), optionally with spaces around them.\n",
    "        cleaned_value = re.sub(r'(?i)\\s*months?\\s*', '', value)\n",
    "        \n",
    "        return cleaned_value.strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Then apply it to your column, for example with Pandas:\n",
    "df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "         \n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "if 'upgrade' in df.columns:\n",
    "    df['upgrade'] = df['upgrade'].ffill()      \n",
    "\n",
    "\n",
    "df.drop('type_of_upgrade_2', axis=1, inplace=True, errors='ignore') \n",
    "\n",
    "#df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df = itemized_df.drop_duplicates(subset=['q_id', 'type_of_upgrade', 'upgrade'])\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/02_intermediate/costs_phase_1_cluster_1_4_style_R_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    #totals_df = totals_df.drop_duplicates(subset=['q_id', 'type_of_upgrade', 'upgrade'])\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/02_intermediate/costs_phase_1_cluster_1_4_style_R_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_1_cluster_5_style_D_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_1_cluster_5_style_D_total.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n",
    "\n",
    "\n",
    "#df.to_csv('Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 14/03_raw/rawdata_cluster14_style_Q.csv')\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded itemized data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 3/02_intermediate/costs_phase_1_cluster_1_4_style_R_itemized.csv\n",
      "Loaded totals data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 3/02_intermediate/costs_phase_1_cluster_1_4_style_R_total.csv\n",
      "Warning: 'escalated_cost_x_1000' column is missing in the itemized dataset.\n",
      "\n",
      "Q_ids with missing upgrades:\n",
      "  Q_id 493 is missing upgrades: ADNU\n",
      "  Q_id 510 is missing upgrades: ADNU\n",
      "  Q_id 561 is missing upgrades: ADNU\n",
      "  Q_id 565 is missing upgrades: ADNU\n",
      "  Q_id 574 is missing upgrades: ADNU\n",
      "  Q_id 583 is missing upgrades: ADNU\n",
      "  Q_id 590 is missing upgrades: ADNU\n",
      "  Q_id 608 is missing upgrades: ADNU\n",
      "  Q_id 628 is missing upgrades: ADNU\n",
      "  Q_id 643AM is missing upgrades: ADNU\n",
      "  Q_id 643AP is missing upgrades: ADNU\n",
      "  Q_id 643T is missing upgrades: ADNU\n",
      "  Q_id 649B is missing upgrades: ADNU\n",
      "  Q_id 649C is missing upgrades: ADNU\n",
      "  Q_id 650AA is missing upgrades: ADNU\n",
      "  Q_id 651A is missing upgrades: ADNU\n",
      "  Q_id 653ED is missing upgrades: LDNU, ADNU\n",
      "  Q_id 653H is missing upgrades: ADNU\n",
      "  Q_id 658 is missing upgrades: ADNU\n",
      "  Q_id 659 is missing upgrades: ADNU\n",
      "  Q_id 660 is missing upgrades: ADNU\n",
      "  Q_id 661 is missing upgrades: ADNU\n",
      "  Q_id 662 is missing upgrades: ADNU\n",
      "  Q_id 781 is missing upgrades: LDNU, ADNU\n",
      "  Q_id 789 is missing upgrades: LDNU, ADNU\n",
      "  Q_id 794 is missing upgrades: LDNU, ADNU\n",
      "  Q_id 838 is missing upgrades: LDNU, ADNU\n",
      "\n",
      "No type of upgrade is repeated for any Q_id.\n",
      "\n",
      "Mismatches saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 3/mismatches.csv'.\n",
      "\n",
      "Direct Matches (Exact Point of Interconnection Names):\n",
      "                    point_of_interconnection                         q_id\n",
      "1   230 KV BUS AT IMPERIAL VALLEY SUBSTATION  [510, 561, 590, 608, 643AM]\n",
      "11                        ANTELOPE 66 KV BUS            [651A, 653H, 662]\n",
      "12             ANTELOPE SUBSTATION 66 KV BUS                   [659, 660]\n",
      "\n",
      "Fuzzy Matches (>=80% Similarity in Point of Interconnection):\n",
      "                           point_of_interconnection_1  \\\n",
      "0   500 KV BUS AT NEW SWITCHYARD LOOPED INTO PROPO...   \n",
      "1            230 KV BUS AT IMPERIAL VALLEY SUBSTATION   \n",
      "2         PROPOSED 138 KV BUS AT BOULEVARD SUBSTATION   \n",
      "3         PROPOSED 138 KV BUS AT BOULEVARD SUBSTATION   \n",
      "4         PROPOSED 138 KV BUS AT BOULEVARD SUBSTATION   \n",
      "5             ANTELOPE-CAL CEMENT-ROSAMOND 66 KV LINE   \n",
      "6             ANTELOPE-CAL CEMENT-ROSAMOND 66 KV LINE   \n",
      "7             ANTELOPE-CAL CEMENT-ROSAMOND 66 KV LINE   \n",
      "8             ANTELOPE-CAL CEMENT-ROSAMOND 66 KV LINE   \n",
      "9             ANTELOPE-CAL CEMENT-ROSAMOND 66 KV LINE   \n",
      "10  PROPOSED NEW 500 KV SWITCHYARD BETWEEN NORTH G...   \n",
      "11           ANTELOPE - DEL SUR - ROSAMOND 66 KV LINE   \n",
      "12           ANTELOPE - DEL SUR - ROSAMOND 66 KV LINE   \n",
      "13           ANTELOPE - DEL SUR - ROSAMOND 66 KV LINE   \n",
      "14           ANTELOPE - DEL SUR - ROSAMOND 66 KV LINE   \n",
      "15        ANTELOPE - CAL CEMENT - ROSAMOND 66 KV LINE   \n",
      "16        ANTELOPE - CAL CEMENT - ROSAMOND 66 KV LINE   \n",
      "17        ANTELOPE - CAL CEMENT - ROSAMOND 66 KV LINE   \n",
      "18               ANTELOPE-DEL SUR-ROSAMOND 66 KV LINE   \n",
      "19               ANTELOPE-DEL SUR-ROSAMOND 66 KV LINE   \n",
      "20                                 ANTELOPE 66 KV BUS   \n",
      "21                                 ANTELOPE 66 KV BUS   \n",
      "22                                 ANTELOPE 66 KV BUS   \n",
      "23                           BOULEVARD EAST 69 KV BUS   \n",
      "24                           BOULEVARD EAST 69 KV BUS   \n",
      "25  ANTELOPE - LANCASTER - LANPRI - SHUTTLE 66 KV ...   \n",
      "26             69 KV BUS AT BOULEVARD EAST SUBSTATION   \n",
      "\n",
      "                        q_ids_1  \\\n",
      "0                         [493]   \n",
      "1   [510, 561, 590, 608, 643AM]   \n",
      "2                         [583]   \n",
      "3                         [583]   \n",
      "4                         [583]   \n",
      "5                         [628]   \n",
      "6                         [628]   \n",
      "7                         [628]   \n",
      "8                         [628]   \n",
      "9                         [628]   \n",
      "10                       [643T]   \n",
      "11                       [649B]   \n",
      "12                       [649B]   \n",
      "13                       [649B]   \n",
      "14                       [649B]   \n",
      "15                       [649C]   \n",
      "16                       [649C]   \n",
      "17                       [649C]   \n",
      "18                      [650AA]   \n",
      "19                      [650AA]   \n",
      "20            [651A, 653H, 662]   \n",
      "21            [651A, 653H, 662]   \n",
      "22            [651A, 653H, 662]   \n",
      "23                      [653ED]   \n",
      "24                      [653ED]   \n",
      "25                        [658]   \n",
      "26                        [789]   \n",
      "\n",
      "                           point_of_interconnection_2            q_ids_2  \\\n",
      "0   500 KV BUS AT PROPOSED SWITCHYARD WITH SUNRISE...            [643AP]   \n",
      "1      230 KV BUS AT THE PROPOSED NEW IMPERIAL VALLEY              [838]   \n",
      "2                  138 KV BUS AT BOULEVARD SUBSTATION              [794]   \n",
      "3              69 KV BUS AT BOULEVARD EAST SUBSTATION              [789]   \n",
      "4                            BOULEVARD EAST 69 KV BUS            [653ED]   \n",
      "5         ANTELOPE - CAL CEMENT - ROSAMOND 66 KV LINE             [649C]   \n",
      "6                      ANTELOPE - ROSAMOND 66 KV LINE              [661]   \n",
      "7            ANTELOPE - DEL SUR - ROSAMOND 66 KV LINE             [649B]   \n",
      "8                ANTELOPE-DEL SUR-ROSAMOND 66 KV LINE            [650AA]   \n",
      "9                                  ANTELOPE 66 KV BUS  [651A, 653H, 662]   \n",
      "10  NEW 69 KV SWITCHYARD BETWEEN CAMERON AND BARRE...              [781]   \n",
      "11               ANTELOPE-DEL SUR-ROSAMOND 66 KV LINE            [650AA]   \n",
      "12                     ANTELOPE - ROSAMOND 66 KV LINE              [661]   \n",
      "13        ANTELOPE - CAL CEMENT - ROSAMOND 66 KV LINE             [649C]   \n",
      "14                                 ANTELOPE 66 KV BUS  [651A, 653H, 662]   \n",
      "15                     ANTELOPE - ROSAMOND 66 KV LINE              [661]   \n",
      "16               ANTELOPE-DEL SUR-ROSAMOND 66 KV LINE            [650AA]   \n",
      "17                                 ANTELOPE 66 KV BUS  [651A, 653H, 662]   \n",
      "18                     ANTELOPE - ROSAMOND 66 KV LINE              [661]   \n",
      "19                                 ANTELOPE 66 KV BUS  [651A, 653H, 662]   \n",
      "20                      ANTELOPE SUBSTATION 66 KV BUS         [659, 660]   \n",
      "21  ANTELOPE - LANCASTER - LANPRI - SHUTTLE 66 KV ...              [658]   \n",
      "22                     ANTELOPE - ROSAMOND 66 KV LINE              [661]   \n",
      "23             69 KV BUS AT BOULEVARD EAST SUBSTATION              [789]   \n",
      "24                 138 KV BUS AT BOULEVARD SUBSTATION              [794]   \n",
      "25                     ANTELOPE - ROSAMOND 66 KV LINE              [661]   \n",
      "26                 138 KV BUS AT BOULEVARD SUBSTATION              [794]   \n",
      "\n",
      "    similarity_score  \n",
      "0                 97  \n",
      "1                 84  \n",
      "2                100  \n",
      "3                 88  \n",
      "4                 80  \n",
      "5                100  \n",
      "6                100  \n",
      "7                 88  \n",
      "8                 88  \n",
      "9                 88  \n",
      "10                81  \n",
      "11               100  \n",
      "12               100  \n",
      "13                88  \n",
      "14                88  \n",
      "15               100  \n",
      "16                88  \n",
      "17                88  \n",
      "18               100  \n",
      "19                88  \n",
      "20               100  \n",
      "21                88  \n",
      "22                88  \n",
      "23               100  \n",
      "24                80  \n",
      "25                81  \n",
      "26                94  \n",
      "Matched Q_ids saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 3/matched_qids.csv'.\n",
      "\n",
      "Total checks performed: 76\n",
      "Total mismatches found: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "# Paths to the CSV files\n",
    "ITEMIZED_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/02_intermediate/costs_phase_1_cluster_1_4_style_R_itemized.csv'\n",
    "TOTALS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/02_intermediate/costs_phase_1_cluster_1_4_style_R_total.csv'\n",
    "\n",
    "# Columns in totals_df that hold the reported total costs\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'estimated_cost_x_1000'\n",
    "\n",
    "# Upgrade types to check\n",
    "REQUIRED_UPGRADES = ['PTO_IF', 'RNU', 'LDNU', 'ADNU']\n",
    "\n",
    "# Output paths\n",
    "MISMATCHES_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/mismatches.csv'\n",
    "MATCHED_QIDS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/matched_qids.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "def load_csv(path, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {dataset_name} from {path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Load datasets\n",
    "itemized_df = load_csv(ITEMIZED_CSV_PATH, \"itemized data\")\n",
    "totals_df = load_csv(TOTALS_CSV_PATH, \"totals data\")\n",
    "\n",
    "# ---------------------- Data Cleaning ---------------------- #\n",
    "\n",
    "def clean_text(df, column):\n",
    "    \"\"\"\n",
    "    Cleans text data by stripping leading/trailing spaces and converting to uppercase.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        df[column] = df[column].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        print(f\"Warning: '{column}' column is missing in the dataset. Filling with 'UNKNOWN'.\")\n",
    "        df[column] = 'UNKNOWN'\n",
    "    return df\n",
    "\n",
    "# Clean 'type_of_upgrade' and 'point_of_interconnection' in both datasets\n",
    "itemized_df = clean_text(itemized_df, 'type_of_upgrade')\n",
    "itemized_df = clean_text(itemized_df, 'point_of_interconnection')\n",
    "\n",
    "totals_df = clean_text(totals_df, 'type_of_upgrade')\n",
    "totals_df = clean_text(totals_df, 'point_of_interconnection')\n",
    "\n",
    "# ---------------------- Data Preparation ---------------------- #\n",
    "\n",
    "# Ensure necessary columns exist in itemized_df\n",
    "required_itemized_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', 'estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in required_itemized_columns:\n",
    "    if col not in itemized_df.columns:\n",
    "        print(f\"Warning: '{col}' column is missing in the itemized dataset.\")\n",
    "        if col in ['q_id', 'type_of_upgrade', 'point_of_interconnection']:\n",
    "            itemized_df[col] = 'UNKNOWN'\n",
    "        else:\n",
    "            itemized_df[col] = 0\n",
    "\n",
    "# Ensure necessary columns exist in totals_df\n",
    "required_totals_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in required_totals_columns:\n",
    "    if col not in totals_df.columns:\n",
    "        print(f\"Error: '{col}' column is missing in the totals dataset. Cannot proceed.\")\n",
    "        exit(1)\n",
    "\n",
    "# Convert cost columns to numeric, coercing errors to NaN and filling with 0\n",
    "cost_columns_itemized = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in cost_columns_itemized:\n",
    "    itemized_df[col] = pd.to_numeric(itemized_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "cost_columns_totals = [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in cost_columns_totals:\n",
    "    totals_df[col] = pd.to_numeric(totals_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# ---------------------- Calculate Manual Totals ---------------------- #\n",
    "\n",
    "# Group itemized data by q_id and type_of_upgrade and calculate sums\n",
    "itemized_grouped = itemized_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    'estimated_cost_x_1000': 'sum',\n",
    "    'escalated_cost_x_1000': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "itemized_grouped['manual_total'] = itemized_grouped.apply(\n",
    "    lambda row: row['estimated_cost_x_1000'] if row['estimated_cost_x_1000'] > 0 else row['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Prepare Totals Data ---------------------- #\n",
    "\n",
    "# Group totals data by q_id and type_of_upgrade and calculate sums\n",
    "totals_grouped = totals_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "    TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "totals_grouped['reported_total'] = totals_grouped.apply(\n",
    "    lambda row: row[TOTALS_ESTIMATED_COLUMN] if row[TOTALS_ESTIMATED_COLUMN] > 0 else row[TOTALS_ESCALATED_COLUMN],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Merge Data ---------------------- #\n",
    "\n",
    "# Merge the itemized and totals data on q_id and type_of_upgrade\n",
    "comparison_df = pd.merge(\n",
    "    itemized_grouped,\n",
    "    totals_grouped[['q_id', 'type_of_upgrade', 'reported_total']],\n",
    "    on=['q_id', 'type_of_upgrade'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---------------------- Check for Missing Upgrades ---------------------- #\n",
    "\n",
    "# Identify q_ids that are missing any of the required upgrades\n",
    "missing_upgrades_report = []\n",
    "for q_id in comparison_df['q_id'].unique():\n",
    "    upgrades_present = comparison_df[comparison_df['q_id'] == q_id]['type_of_upgrade'].unique()\n",
    "    missing_upgrades = [upgrade for upgrade in REQUIRED_UPGRADES if upgrade not in upgrades_present]\n",
    "    if missing_upgrades:\n",
    "        missing_upgrades_report.append((q_id, missing_upgrades))\n",
    "\n",
    "# Report missing upgrades\n",
    "if missing_upgrades_report:\n",
    "    print(\"\\nQ_ids with missing upgrades:\")\n",
    "    for q_id, missing in missing_upgrades_report:\n",
    "        print(f\"  Q_id {q_id} is missing upgrades: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nAll q_ids have all required upgrades.\")\n",
    "\n",
    "\n",
    "# ------------------------ Check for no duplicates in type of upgrade in total data ------------------------ #\n",
    "\n",
    " \n",
    "\n",
    "# Identify duplicates by grouping by q_id and type_of_upgrade\n",
    "duplicates = totals_df[totals_df.duplicated(subset=['q_id', 'type_of_upgrade'], keep=False)]\n",
    "\n",
    "if not duplicates.empty:\n",
    "    print(\"\\nDuplicate upgrade types detected:\")\n",
    "    for q_id, group in duplicates.groupby('q_id'):\n",
    "        upgrade_types = group['type_of_upgrade'].unique()\n",
    "        print(f\"  Q_id {q_id} has repeated upgrade types: {', '.join(upgrade_types)}\")\n",
    "else:\n",
    "    print(\"\\nNo type of upgrade is repeated for any Q_id.\")    \n",
    "\n",
    "# ---------------------- Compare Totals and Identify Mismatches ---------------------- #\n",
    "\n",
    "# Initialize list to store mismatches\n",
    "mismatches = []\n",
    "\n",
    "# Iterate through each row to compare manual_total with reported_total\n",
    "for index, row in comparison_df.iterrows():\n",
    "    q_id = row['q_id']\n",
    "    upgrade = row['type_of_upgrade']\n",
    "    manual_total = row['manual_total']\n",
    "    reported_total = row['reported_total']\n",
    "    \n",
    "    # Determine if both manual_total and reported_total are zero\n",
    "    if manual_total == 0.0 and reported_total == 0.0:\n",
    "        continue  # No mismatch\n",
    "    # Determine if manual_total is zero and reported_total is missing or zero\n",
    "    elif manual_total == 0.0 and (pd.isna(row['reported_total']) or reported_total == 0.0):\n",
    "        continue  # No mismatch\n",
    "    # If reported_total is missing (NaN) and manual_total is not zero\n",
    "    elif pd.isna(row['reported_total']) and manual_total != 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: Missing\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': 'Missing'\n",
    "        })\n",
    "    # If manual_total is not zero and reported_total is zero\n",
    "    elif manual_total != 0.0 and reported_total == 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: 0.0\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # If both totals are non-zero but differ beyond tolerance\n",
    "    elif abs(manual_total - reported_total) > 1e+1:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: {reported_total}\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # Else, totals match; do nothing\n",
    "\n",
    "# Create a DataFrame for mismatches\n",
    "mismatches_df = pd.DataFrame(mismatches, columns=['q_id', 'type_of_upgrade', 'manual_total', 'reported_total'])\n",
    "\n",
    "# Save mismatches to a CSV file\n",
    "try:\n",
    "    mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "    print(f\"\\nMismatches saved to '{MISMATCHES_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving mismatches CSV: {e}\")\n",
    "\n",
    "# ---------------------- Point of Interconnection Matching ---------------------- #\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from itemized dataset\n",
    "itemized_poi = itemized_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from totals dataset\n",
    "totals_poi = totals_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Merge both to have a complete list of q_id and point_of_interconnection\n",
    "all_poi = pd.concat([itemized_poi, totals_poi]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ---------------------- Direct Match Identification ---------------------- #\n",
    "\n",
    "# Group by point_of_interconnection to find q_ids sharing the same point_of_interconnection\n",
    "direct_matches = all_poi.groupby('point_of_interconnection')['q_id'].apply(list).reset_index()\n",
    "\n",
    "# Filter groups with more than one q_id (i.e., shared points_of_interconnection)\n",
    "direct_matches = direct_matches[direct_matches['q_id'].apply(len) > 1]\n",
    "\n",
    "print(\"\\nDirect Matches (Exact Point of Interconnection Names):\")\n",
    "if not direct_matches.empty:\n",
    "    print(direct_matches)\n",
    "else:\n",
    "    print(\"No direct matches found.\")\n",
    "\n",
    "# ---------------------- Fuzzy Match Identification ---------------------- #\n",
    "\n",
    "# Prepare list of points_of_interconnection for fuzzy matching\n",
    "poi_list = all_poi['point_of_interconnection'].unique().tolist()\n",
    "\n",
    "# Initialize list to store fuzzy matches\n",
    "fuzzy_matches = []\n",
    "\n",
    "# Iterate through each point_of_interconnection to find similar ones\n",
    "for i, poi in enumerate(poi_list):\n",
    "    # Compare with the rest of the points to avoid redundant comparisons\n",
    "    similar_pois = process.extract(poi, poi_list[i+1:], scorer=fuzz.token_set_ratio)\n",
    "    \n",
    "    # Filter matches with similarity >= 80%\n",
    "    for match_poi, score in similar_pois:\n",
    "        if score >= 80:\n",
    "            # Retrieve q_ids for both points_of_interconnection\n",
    "            qids_poi1 = all_poi[all_poi['point_of_interconnection'] == poi]['q_id'].tolist()\n",
    "            qids_poi2 = all_poi[all_poi['point_of_interconnection'] == match_poi]['q_id'].tolist()\n",
    "            \n",
    "            # Append the matched pairs with their points_of_interconnection and similarity score\n",
    "            fuzzy_matches.append({\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_ids_1': qids_poi1,\n",
    "                'point_of_interconnection_2': match_poi,\n",
    "                'q_ids_2': qids_poi2,\n",
    "                'similarity_score': score\n",
    "            })\n",
    "\n",
    "# Convert fuzzy matches to DataFrame\n",
    "fuzzy_matches_df = pd.DataFrame(fuzzy_matches)\n",
    "\n",
    "print(\"\\nFuzzy Matches (>=80% Similarity in Point of Interconnection):\")\n",
    "if not fuzzy_matches_df.empty:\n",
    "    print(fuzzy_matches_df)\n",
    "else:\n",
    "    print(\"No fuzzy matches found.\")\n",
    "\n",
    "# ---------------------- Save Matched Q_ids to CSV ---------------------- #\n",
    "\n",
    "# For clarity, create a combined DataFrame for direct and fuzzy matches\n",
    "\n",
    "# Direct matches: list each pair of q_ids sharing the same point_of_interconnection\n",
    "direct_matches_expanded = []\n",
    "for _, row in direct_matches.iterrows():\n",
    "    qids = row['q_id']\n",
    "    poi = row['point_of_interconnection']\n",
    "    # Generate all possible unique pairs\n",
    "    for i in range(len(qids)):\n",
    "        for j in range(i+1, len(qids)):\n",
    "            direct_matches_expanded.append({\n",
    "                'match_type': 'Direct',\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_id_1': qids[i],\n",
    "                'point_of_interconnection_2': poi,\n",
    "                'q_id_2': qids[j],\n",
    "                'similarity_score': 100\n",
    "            })\n",
    "\n",
    "# Fuzzy matches: already have pairs\n",
    "fuzzy_matches_expanded = []\n",
    "for _, row in fuzzy_matches_df.iterrows():\n",
    "    fuzzy_matches_expanded.append({\n",
    "        'match_type': 'Fuzzy',\n",
    "        'point_of_interconnection_1': row['point_of_interconnection_1'],\n",
    "        'q_id_1': row['q_ids_1'],\n",
    "        'point_of_interconnection_2': row['point_of_interconnection_2'],\n",
    "        'q_id_2': row['q_ids_2'],\n",
    "        'similarity_score': row['similarity_score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "matched_qids_df = pd.DataFrame(direct_matches_expanded + fuzzy_matches_expanded)\n",
    "\n",
    "# Save matched q_ids to CSV\n",
    "try:\n",
    "    matched_qids_df.to_csv(MATCHED_QIDS_CSV_PATH, index=False)\n",
    "    print(f\"Matched Q_ids saved to '{MATCHED_QIDS_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving matched Q_ids CSV: {e}\")\n",
    "\n",
    "# ---------------------- Summary ---------------------- #\n",
    "\n",
    "# Print a summary\n",
    "total_checked = len(comparison_df)\n",
    "total_mismatches = len(mismatches_df)\n",
    "print(f\"\\nTotal checks performed: {total_checked}\")\n",
    "print(f\"Total mismatches found: {total_mismatches}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}