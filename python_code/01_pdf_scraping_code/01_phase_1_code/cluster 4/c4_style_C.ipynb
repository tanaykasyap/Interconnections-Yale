{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: 11AS737399-Appendix_A__Q667_11092012.pdf from Project 667\n",
      "Scraped PDF: 11AS737399-Revised_Appendix_A__Q667_11212012.pdf from Project 667\n",
      "Scraped PDF: 11AS709953-QC4PISCENorthernAppendix_AQ670_American_Solar_Greenworks_Addition.pdf from Project 670\n",
      "Scraped PDF: 11AS709990-QC4PISCENorthernAppendix_AQ671_Apex_Greenworks.pdf from Project 671\n",
      "Scraped PDF: 11AS703463-Appendix_A__Q678_C4_Phase_I_report.pdf from Project 678\n",
      "Scraped PDF: 11AS703500-Appendix_A__Q679_C4_phase_I_report.pdf from Project 679\n",
      "Scraped PDF: 11AS699473-Appendix_A__Q680_C4_Phase_I_report20120103.pdf from Project 680\n",
      "Skipped PDF: 11AS699885-Appendix_A__Q687.pdf from Project 687 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS699885-Appendix_A__Q687_C4_Phase_I_Report.pdf from Project 687\n",
      "Skipped Addendum PDF: 11AS699885-Report_Addendum__Q687_Cluster_4_Phase_I_Appendix_A.pdf from Project 687 (Empty Data)\n",
      "Scraped PDF: 11AS703562-Appendix_A__Q688_C4_Phase_I_report.pdf from Project 688\n",
      "Scraped PDF: 11AS707788-QC4PI_Appendix_A_Q695_DixieComstock_Geothermal.pdf from Project 695\n",
      "Scraped PDF: 11AS708195-MetroQ702Appendix_A.pdf from Project 702\n",
      "Scraped PDF: 11AS700201-Appendix_A__Q705_C4_Phase_I_report__Final.pdf from Project 705\n",
      "Scraped PDF: 11AS700410-Appendix_A__Q707_C4_Phase_I_report__Final_CMB_3JAN2012.pdf from Project 707\n",
      "Scraped PDF: 11AS700272-Appendix_A__Q708_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf from Project 708\n",
      "Scraped PDF: 11AS700375-Appendix_A__Q709_C4_Phase_I_Report__Final_CMB_3JAN2012_PGE_rev1.pdf from Project 709\n",
      "Scraped PDF: 11AS708367-QC4P1_EOP_Appendix_A_Q714HiddenHillsRanch.pdf from Project 714\n",
      "Scraped PDF: 11AS701175-Appendix_A__Q720_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf from Project 720\n",
      "Scraped PDF: 11AS701347-Appendix_A__Q820_C4_Phase_I_report.pdf from Project 723\n",
      "Skipped Addendum PDF: 11AS701347-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf from Project 723 (Empty Data)\n",
      "Scraped PDF: 11AS701449-Appendix_A__Q725_C4_Phase_I_report.pdf from Project 725\n",
      "Skipped PDF: 11AS745265-Appendix_A__C737_12302011_final.pdf from Project 737 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS708680-QC4PISCENorthernAppendix_AQ738_Oasis_20_MW.pdf from Project 738\n",
      "Scraped PDF: 11AS708645-QC4P1_EOP_AppendixA_Q740PahrumpValleySEGS.pdf from Project 740\n",
      "Scraped PDF: 11AS702744-Appendix_A__Q744_C4_Phase_I_report_final.pdf from Project 744\n",
      "Scraped PDF: 11AS717303-QC4PISCENorthernAppendix_AQ746_RE_Astoria.pdf from Project 746\n",
      "Scraped PDF: 11AS702472-Appendix_A__Q751_C4_Phase_I_report__Final.pdf from Project 751\n",
      "Scraped PDF: 11AS702506-Appendix_A__Q752_C4_Phase_I_report__Final.pdf from Project 752\n",
      "Scraped PDF: 11AS702778-Appendix_A__Q762_C4_Phase_I_Report.pdf from Project 762\n",
      "Scraped PDF: 11AS702880-Appendix_A__Q765_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf from Project 765\n",
      "Scraped PDF: 11AS711971-QC4PISCENorthernAppendix_AQ768_SP_Antelope_DSR.pdf from Project 768\n",
      "Scraped PDF: 11AS712005-QC4PISCENorthernAppendix_AQ769_Springtime_Solar.pdf from Project 769\n",
      "Scraped PDF: 11AS703152-Appendix_A__Q775_C4_Phase_I_report_final_rev020312.pdf from Project 775\n",
      "Scraped PDF: 11AS709671-QC4PISCENorthernAppendix_AQ778_Western_Antelope_Dry_Ranch_Addition.pdf from Project 778\n",
      "Scraped PDF: Appendix A - Q779 C4 Phase I report - Final.pdf from Project 779\n",
      "Scraped PDF: 11AS709868-Appendix_A__C781_12302011_final.pdf from Project 781\n",
      "Scraped PDF: 11AS708542-Appendix_A__C789_12302011_final.pdf from Project 789\n",
      "Scraped PDF: 11AS708846-Appendix_A__C794_12302011_final.pdf from Project 794\n",
      "Skipped PDF: 11AS737469-QC4PISCENorthernGroup_Report.pdf from Project 795 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS737434-QC4PISCENorthernAppendix_AQ796_AV_Solar_Ranch_2B.pdf from Project 796\n",
      "Scraped PDF: 11AS710025-EasternQ797AppendixA.pdf from Project 797\n",
      "Scraped PDF: 10AS711623-EasternQ798AppendixA.pdf from Project 798\n",
      "Scraped PDF: 11AS699264-Appendix_A__Q799_C4_Phase_I_report__Final.pdf from Project 799\n",
      "Scraped PDF: 11AS699298-Appendix_A__Q800_C4_Phase_I_report__Final.pdf from Project 800\n",
      "Scraped PDF: 11AS699747-Appendix_A__Q805_C4_Phase_I_report__Final.pdf from Project 805\n",
      "Skipped PDF: 11AS699850-QC34PII_Appendix_A_PGE_Q806.pdf from Project 806 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS699850-Appendix_A__Q806_C4_Phase_I_report__Final.pdf from Project 806\n",
      "Scraped PDF: 11AS700096-Report_Addendum__Q809_Cluster_4_Phase_I_Appendix_A.pdf from Project 809\n",
      "Scraped PDF: 11AS700096-Appendix_A__Q809_C4_Phase_I_report__Final.pdf from Project 809\n",
      "Scraped PDF: 11AS700725-Appendix_A__Q814_C4_Phase_I_report.pdf from Project 814\n",
      "Skipped Addendum PDF: 11AS700725-Report_Addendum__Q814_Cluster_4_Phase_I_Appendix_A.pdf from Project 814 (Empty Data)\n",
      "Scraped PDF: 11AS700760-Appendix_A__Q815_C4_Phase_I_report.pdf from Project 815\n",
      "Skipped Addendum PDF: 11AS700760-Report_Addendum__Q815_Cluster_4_Phase_I_Appendix_A.pdf from Project 815 (Empty Data)\n",
      "Scraped PDF: 11AS700794-Appendix_A__Q816_C4_Phase_I_report.pdf from Project 816\n",
      "Skipped Addendum PDF: 11AS700794-Report_Addendum__Q816_Cluster_4_Phase_I_Appendix_A.pdf from Project 816 (Empty Data)\n",
      "Scraped PDF: 11AS701279-Appendix_A__Q820_C4_Phase_I_report.pdf from Project 820\n",
      "Skipped Addendum PDF: 11AS701279-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf from Project 820 (Empty Data)\n",
      "Scraped PDF: 11AS701789-Appendix_A__Q823_C4_Phase_I_report.pdf from Project 823\n",
      "Skipped PDF: 11AS701789-QC34PII_Appendix_A_PGE_Q823.pdf from Project 823 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS701823-Report_Addendum__Q824_Cluster_4_Phase_I_Appendix_A.pdf from Project 824\n",
      "Skipped PDF: 11AS701823-QC34PII_Appendix_A_PGE_Q824.pdf from Project 824 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS701823-Appendix_A__Q824_C4_Phase_I_report.pdf from Project 824\n",
      "Scraped PDF: 11AS701857-Report_Addendum__Q825_Cluster_4_Phase_I_Appendix_A.pdf from Project 825\n",
      "Scraped PDF: 11AS701857-Appendix_A__Q825_C4_Phase_I_report.pdf from Project 825\n",
      "Scraped PDF: 11AS703986-Appendix_A__Q829_C4_Phase_I_report.pdf from Project 829\n",
      "Scraped PDF: 11AS711555-EasternQ831AppendixA.pdf from Project 831\n",
      "Skipped PDF: 11AS737779-C4PI_Group_Report_Addendum_02172012.pdf from Project 837 (No Table 3 or Attachment data)\n",
      "Skipped PDF: 11AS737779-C4PhI_Group_Report_12302011_final.pdf from Project 837 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS737779-C4PI_Q837_Jacumba_II_Addendum_02172012.pdf from Project 837\n",
      "Scraped PDF: 11AS716919-Appendix_A__C838_12302011_final.pdf from Project 838\n",
      "Scraped PDF: 11AS741392-QC4P1_EOP_AppendixA_Q855CopperMountainSolar.pdf from Project 855\n",
      "Skipped PDF: C3C4P2-Northern-Q856-Tehachapi Wind Energy Srorage-AppendixA.pdf from Project 856 (No Table 3 or Attachment data)\n",
      "Scraped PDF: 11AS708812-QC4PISCENorthernAppendix_AQ856_Tehachapi_Wind_Energy_Storage.pdf from Project 856\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 4/03_raw/rawdata_cluster4_style_C_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 4/03_raw/rawdata_cluster4_style_C_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 124\n",
      "Total Projects Scraped: 55\n",
      "Total Projects Skipped: 69\n",
      "Total Projects Missing: 68\n",
      "Total PDFs Accessed: 74\n",
      "Total PDFs Scraped: 59\n",
      "Total PDFs Skipped: 15\n",
      "\n",
      "List of Scraped Projects:\n",
      "[667, 670, 671, 678, 679, 680, 687, 688, 695, 702, 705, 707, 708, 709, 714, 720, 723, 725, 738, 740, 744, 746, 751, 752, 762, 765, 768, 769, 775, 778, 779, 781, 789, 794, 796, 797, 798, 799, 800, 805, 806, 809, 814, 815, 816, 820, 823, 824, 825, 829, 831, 837, 838, 855, 856]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[668, 669, 674, 676, 681, 683, 684, 685, 686, 692, 696, 697, 698, 700, 703, 704, 706, 712, 716, 717, 729, 730, 732, 736, 737, 739, 741, 756, 764, 766, 767, 770, 771, 774, 782, 783, 784, 785, 786, 788, 790, 791, 792, 793, 795, 801, 804, 807, 812, 813, 834, 836, 839, 840, 841, 842, 843, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 857, 858]\n",
      "\n",
      "List of Missing Projects:\n",
      "[672, 673, 675, 677, 682, 689, 690, 691, 693, 694, 699, 701, 710, 711, 713, 715, 718, 719, 721, 722, 724, 726, 727, 728, 731, 733, 734, 735, 742, 743, 745, 747, 748, 749, 750, 753, 754, 755, 757, 758, 759, 760, 761, 763, 772, 773, 776, 777, 780, 787, 802, 803, 808, 810, 811, 817, 818, 819, 821, 822, 826, 827, 828, 830, 832, 833, 835, 844]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['11AS737399-Appendix_A__Q667_11092012.pdf', '11AS737399-Revised_Appendix_A__Q667_11212012.pdf', '11AS709953-QC4PISCENorthernAppendix_AQ670_American_Solar_Greenworks_Addition.pdf', '11AS709990-QC4PISCENorthernAppendix_AQ671_Apex_Greenworks.pdf', '11AS703463-Appendix_A__Q678_C4_Phase_I_report.pdf', '11AS703500-Appendix_A__Q679_C4_phase_I_report.pdf', '11AS699473-Appendix_A__Q680_C4_Phase_I_report20120103.pdf', '11AS699885-Appendix_A__Q687_C4_Phase_I_Report.pdf', '11AS703562-Appendix_A__Q688_C4_Phase_I_report.pdf', '11AS707788-QC4PI_Appendix_A_Q695_DixieComstock_Geothermal.pdf', '11AS708195-MetroQ702Appendix_A.pdf', '11AS700201-Appendix_A__Q705_C4_Phase_I_report__Final.pdf', '11AS700410-Appendix_A__Q707_C4_Phase_I_report__Final_CMB_3JAN2012.pdf', '11AS700272-Appendix_A__Q708_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS700375-Appendix_A__Q709_C4_Phase_I_Report__Final_CMB_3JAN2012_PGE_rev1.pdf', '11AS708367-QC4P1_EOP_Appendix_A_Q714HiddenHillsRanch.pdf', '11AS701175-Appendix_A__Q720_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS701347-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701449-Appendix_A__Q725_C4_Phase_I_report.pdf', '11AS708680-QC4PISCENorthernAppendix_AQ738_Oasis_20_MW.pdf', '11AS708645-QC4P1_EOP_AppendixA_Q740PahrumpValleySEGS.pdf', '11AS702744-Appendix_A__Q744_C4_Phase_I_report_final.pdf', '11AS717303-QC4PISCENorthernAppendix_AQ746_RE_Astoria.pdf', '11AS702472-Appendix_A__Q751_C4_Phase_I_report__Final.pdf', '11AS702506-Appendix_A__Q752_C4_Phase_I_report__Final.pdf', '11AS702778-Appendix_A__Q762_C4_Phase_I_Report.pdf', '11AS702880-Appendix_A__Q765_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS711971-QC4PISCENorthernAppendix_AQ768_SP_Antelope_DSR.pdf', '11AS712005-QC4PISCENorthernAppendix_AQ769_Springtime_Solar.pdf', '11AS703152-Appendix_A__Q775_C4_Phase_I_report_final_rev020312.pdf', '11AS709671-QC4PISCENorthernAppendix_AQ778_Western_Antelope_Dry_Ranch_Addition.pdf', 'Appendix A - Q779 C4 Phase I report - Final.pdf', '11AS709868-Appendix_A__C781_12302011_final.pdf', '11AS708542-Appendix_A__C789_12302011_final.pdf', '11AS708846-Appendix_A__C794_12302011_final.pdf', '11AS737434-QC4PISCENorthernAppendix_AQ796_AV_Solar_Ranch_2B.pdf', '11AS710025-EasternQ797AppendixA.pdf', '10AS711623-EasternQ798AppendixA.pdf', '11AS699264-Appendix_A__Q799_C4_Phase_I_report__Final.pdf', '11AS699298-Appendix_A__Q800_C4_Phase_I_report__Final.pdf', '11AS699747-Appendix_A__Q805_C4_Phase_I_report__Final.pdf', '11AS699850-Appendix_A__Q806_C4_Phase_I_report__Final.pdf', '11AS700096-Report_Addendum__Q809_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700096-Appendix_A__Q809_C4_Phase_I_report__Final.pdf', '11AS700725-Appendix_A__Q814_C4_Phase_I_report.pdf', '11AS700760-Appendix_A__Q815_C4_Phase_I_report.pdf', '11AS700794-Appendix_A__Q816_C4_Phase_I_report.pdf', '11AS701279-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701789-Appendix_A__Q823_C4_Phase_I_report.pdf', '11AS701823-Report_Addendum__Q824_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701823-Appendix_A__Q824_C4_Phase_I_report.pdf', '11AS701857-Report_Addendum__Q825_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701857-Appendix_A__Q825_C4_Phase_I_report.pdf', '11AS703986-Appendix_A__Q829_C4_Phase_I_report.pdf', '11AS711555-EasternQ831AppendixA.pdf', '11AS737779-C4PI_Q837_Jacumba_II_Addendum_02172012.pdf', '11AS716919-Appendix_A__C838_12302011_final.pdf', '11AS741392-QC4P1_EOP_AppendixA_Q855CopperMountainSolar.pdf', '11AS708812-QC4PISCENorthernAppendix_AQ856_Tehachapi_Wind_Energy_Storage.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['11AS699885-Appendix_A__Q687.pdf', '11AS699885-Report_Addendum__Q687_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701347-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS745265-Appendix_A__C737_12302011_final.pdf', '11AS737469-QC4PISCENorthernGroup_Report.pdf', '11AS699850-QC34PII_Appendix_A_PGE_Q806.pdf', '11AS700725-Report_Addendum__Q814_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700760-Report_Addendum__Q815_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700794-Report_Addendum__Q816_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701279-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701789-QC34PII_Appendix_A_PGE_Q823.pdf', '11AS701823-QC34PII_Appendix_A_PGE_Q824.pdf', '11AS737779-C4PI_Group_Report_Addendum_02172012.pdf', '11AS737779-C4PhI_Group_Report_12302011_final.pdf', 'C3C4P2-Northern-Q856-Tehachapi Wind Energy Srorage-AppendixA.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['11AS699885-Report_Addendum__Q687_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701347-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700725-Report_Addendum__Q814_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700760-Report_Addendum__Q815_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700794-Report_Addendum__Q816_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701279-Report_Addendum__Q820_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701823-Report_Addendum__Q824_Cluster_4_Phase_I_Appendix_A.pdf', '11AS701857-Report_Addendum__Q825_Cluster_4_Phase_I_Appendix_A.pdf', '11AS737779-C4PI_Group_Report_Addendum_02172012.pdf', '11AS737779-C4PI_Q837_Jacumba_II_Addendum_02172012.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['11AS737399-Appendix_A__Q667_11092012.pdf', '11AS737399-Revised_Appendix_A__Q667_11212012.pdf', '11AS709953-QC4PISCENorthernAppendix_AQ670_American_Solar_Greenworks_Addition.pdf', '11AS709990-QC4PISCENorthernAppendix_AQ671_Apex_Greenworks.pdf', '11AS703463-Appendix_A__Q678_C4_Phase_I_report.pdf', '11AS703500-Appendix_A__Q679_C4_phase_I_report.pdf', '11AS699473-Appendix_A__Q680_C4_Phase_I_report20120103.pdf', '11AS699885-Appendix_A__Q687.pdf', '11AS699885-Appendix_A__Q687_C4_Phase_I_Report.pdf', '11AS703562-Appendix_A__Q688_C4_Phase_I_report.pdf', '11AS707788-QC4PI_Appendix_A_Q695_DixieComstock_Geothermal.pdf', '11AS708195-MetroQ702Appendix_A.pdf', '11AS700201-Appendix_A__Q705_C4_Phase_I_report__Final.pdf', '11AS700410-Appendix_A__Q707_C4_Phase_I_report__Final_CMB_3JAN2012.pdf', '11AS700272-Appendix_A__Q708_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS700375-Appendix_A__Q709_C4_Phase_I_Report__Final_CMB_3JAN2012_PGE_rev1.pdf', '11AS708367-QC4P1_EOP_Appendix_A_Q714HiddenHillsRanch.pdf', '11AS701175-Appendix_A__Q720_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS701347-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701449-Appendix_A__Q725_C4_Phase_I_report.pdf', '11AS745265-Appendix_A__C737_12302011_final.pdf', '11AS708680-QC4PISCENorthernAppendix_AQ738_Oasis_20_MW.pdf', '11AS708645-QC4P1_EOP_AppendixA_Q740PahrumpValleySEGS.pdf', '11AS702744-Appendix_A__Q744_C4_Phase_I_report_final.pdf', '11AS717303-QC4PISCENorthernAppendix_AQ746_RE_Astoria.pdf', '11AS702472-Appendix_A__Q751_C4_Phase_I_report__Final.pdf', '11AS702506-Appendix_A__Q752_C4_Phase_I_report__Final.pdf', '11AS702778-Appendix_A__Q762_C4_Phase_I_Report.pdf', '11AS702880-Appendix_A__Q765_C4_Phase_I_Report__Final_CMB_3JAN2012.pdf', '11AS711971-QC4PISCENorthernAppendix_AQ768_SP_Antelope_DSR.pdf', '11AS712005-QC4PISCENorthernAppendix_AQ769_Springtime_Solar.pdf', '11AS703152-Appendix_A__Q775_C4_Phase_I_report_final_rev020312.pdf', '11AS709671-QC4PISCENorthernAppendix_AQ778_Western_Antelope_Dry_Ranch_Addition.pdf', 'Appendix A - Q779 C4 Phase I report - Final.pdf', '11AS709868-Appendix_A__C781_12302011_final.pdf', '11AS708542-Appendix_A__C789_12302011_final.pdf', '11AS708846-Appendix_A__C794_12302011_final.pdf', '11AS737469-QC4PISCENorthernGroup_Report.pdf', '11AS737434-QC4PISCENorthernAppendix_AQ796_AV_Solar_Ranch_2B.pdf', '11AS710025-EasternQ797AppendixA.pdf', '10AS711623-EasternQ798AppendixA.pdf', '11AS699264-Appendix_A__Q799_C4_Phase_I_report__Final.pdf', '11AS699298-Appendix_A__Q800_C4_Phase_I_report__Final.pdf', '11AS699747-Appendix_A__Q805_C4_Phase_I_report__Final.pdf', '11AS699850-QC34PII_Appendix_A_PGE_Q806.pdf', '11AS699850-Appendix_A__Q806_C4_Phase_I_report__Final.pdf', '11AS700096-Report_Addendum__Q809_Cluster_4_Phase_I_Appendix_A.pdf', '11AS700096-Appendix_A__Q809_C4_Phase_I_report__Final.pdf', '11AS700725-Appendix_A__Q814_C4_Phase_I_report.pdf', '11AS700760-Appendix_A__Q815_C4_Phase_I_report.pdf', '11AS700794-Appendix_A__Q816_C4_Phase_I_report.pdf', '11AS701279-Appendix_A__Q820_C4_Phase_I_report.pdf', '11AS701789-Appendix_A__Q823_C4_Phase_I_report.pdf', '11AS701789-QC34PII_Appendix_A_PGE_Q823.pdf', '11AS701823-QC34PII_Appendix_A_PGE_Q824.pdf', '11AS701823-Appendix_A__Q824_C4_Phase_I_report.pdf', '11AS701857-Appendix_A__Q825_C4_Phase_I_report.pdf', '11AS703986-Appendix_A__Q829_C4_Phase_I_report.pdf', '11AS711555-EasternQ831AppendixA.pdf', '11AS737779-C4PhI_Group_Report_12302011_final.pdf', '11AS716919-Appendix_A__C838_12302011_final.pdf', '11AS741392-QC4P1_EOP_AppendixA_Q855CopperMountainSolar.pdf', 'C3C4P2-Northern-Q856-Tehachapi Wind Energy Srorage-AppendixA.pdf', '11AS708812-QC4PISCENorthernAppendix_AQ856_Tehachapi_Wind_Energy_Storage.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 56\n",
      "Number of Addendum PDFs Scraped: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "import inflect\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 4/03_raw/rawdata_cluster4_style_C_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 4/03_raw/rawdata_cluster4_style_C_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 4/03_raw/scraping_cluster4_style_C_log.txt\"\n",
    "PROJECT_RANGE = range(667, 859)  # Example range for q_ids in Clusters 4\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing, removing unwanted characters, and singularizing words.\"\"\"\n",
    "    p = inflect.engine()\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "            # Correct any mis\u2010spellings of \u201ctype of upgrade\u201d\n",
    "            header = re.sub(r'\\btype of upgr\\s*ade\\b', 'type of upgrade', header)\n",
    "            words = header.split()\n",
    "            singular_words = [p.singular_noun(word) if p.singular_noun(word) else word for word in words]\n",
    "            header = \" \".join(singular_words)\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback if none found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "    new_order = existing_desired + remaining\n",
    "    df = df[new_order]\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"\n",
    "    Ensures each row in data_rows has exactly len(headers) columns.\n",
    "    If a row is too short, it is padded with empty strings.\n",
    "    If too long, it is truncated.\n",
    "    \"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"] * (col_count - len(row)))\n",
    "\n",
    "def extract_table2(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "    table_settings_list = [\n",
    "        {\"horizontal_strategy\": \"text\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 1},\n",
    "        {\"horizontal_strategy\": \"lines\", \"vertical_strategy\": \"lines\", \"snap_tolerance\": 2},\n",
    "    ]\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*(?:2|B\\.1)\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 2 found in the PDF.\", file=log_file)\n",
    "                return None\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Table 2 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "            extraction_successful = False\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 2...\", file=log_file)\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    poi_col_index = cell_index\n",
    "                                    adjacent_col_index = poi_col_index + 1\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            poi_value_parts = []\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "                                            if poi_value_parts:\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' (Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column (Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                            if extraction_successful:\n",
    "                                break\n",
    "                        if extraction_successful:\n",
    "                            break\n",
    "                    if extraction_successful:\n",
    "                        break\n",
    "                if extraction_successful:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 2 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "    if not extraction_successful:\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            print(\"Point of Interconnection not found in Table 2.\", file=log_file)\n",
    "            return None\n",
    "    return point_of_interconnection\n",
    "\n",
    "def fix_column_names(columns):\n",
    "    \"\"\"\n",
    "    Renames duplicate and empty column names.\n",
    "    Duplicate names are suffixed with _1, _2, etc.\n",
    "    Empty or whitespace-only names are replaced with unnamed_1, unnamed_2, etc.\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    counts = {}\n",
    "    unnamed_count = 1\n",
    "    for col in columns:\n",
    "        # Treat empty or whitespace-only names as unnamed.\n",
    "        if not col or col.strip() == \"\":\n",
    "            new_col = f\"unnamed_{unnamed_count}\"\n",
    "            unnamed_count += 1\n",
    "        else:\n",
    "            new_col = col.strip()\n",
    "        if new_col in counts:\n",
    "            new_col_with_suffix = f\"{new_col}_{counts[new_col]}\"\n",
    "            counts[new_col] += 1\n",
    "            new_cols.append(new_col_with_suffix)\n",
    "        else:\n",
    "            counts[new_col] = 1\n",
    "            new_cols.append(new_col)\n",
    "    return new_cols\n",
    "\n",
    "def post_process_columns(df, log_file):\n",
    "    \"\"\"\n",
    "    Post-processes DataFrame column names:\n",
    "      1. For any column named 'unnamed_#' (or empty), look at its first non-empty cell.\n",
    "         If that cell is not a dollar amount (i.e. does not match /^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$/)\n",
    "         and it contains 2 or 3 words, then rename the column to that value (after cleaning).\n",
    "         If a column already exists with that name, merge the data from the renamed column into the\n",
    "         existing column and drop the renamed column.\n",
    "      2. If a column is named \"Needed For\", then rename it to \"description\" (merging with an existing\n",
    "         description column if necessary).\n",
    "    \"\"\"\n",
    "    # Process unnamed columns.\n",
    "    for col in list(df.columns):\n",
    "        if col.lower().startswith(\"unnamed_\") or col.strip() == \"\":\n",
    "            # Find the first non-empty cell in this column.\n",
    "            first_non_empty = None\n",
    "            for val in df[col]:\n",
    "                cell_val = \"\"\n",
    "                if isinstance(val, str):\n",
    "                    cell_val = val.strip()\n",
    "                elif val is not None:\n",
    "                    cell_val = str(val).strip()\n",
    "                if cell_val:\n",
    "                    first_non_empty = cell_val\n",
    "                    break\n",
    "            if first_non_empty:\n",
    "                # Check if the value is a dollar amount.\n",
    "                if not re.match(r\"^\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?$\", first_non_empty):\n",
    "                    words = first_non_empty.split()\n",
    "                    if 2 <= len(words) <= 3:\n",
    "                        # Clean the candidate name.\n",
    "                        new_name = clean_column_headers([first_non_empty])[0]\n",
    "                        log_file.write(f\"Renaming column '{col}' to '{new_name}' based on first non-empty value '{first_non_empty}'.\\n\")\n",
    "                        if new_name in df.columns and new_name != col:\n",
    "                            # Merge the two columns: fill empty cells in existing new_name from the renamed col.\n",
    "                            for idx in df.index:\n",
    "                                existing_val = df.at[idx, new_name]\n",
    "                                candidate_val = df.at[idx, col]\n",
    "                                if (pd.isna(existing_val) or existing_val == \"\") and (not pd.isna(candidate_val) and candidate_val != \"\"):\n",
    "                                    df.at[idx, new_name] = candidate_val\n",
    "                            df.drop(columns=[col], inplace=True)\n",
    "                        else:\n",
    "                            df.rename(columns={col: new_name}, inplace=True)\n",
    "    # Process \"Needed For\" column: rename or merge it into \"description\".\n",
    "    if \"Needed For\" in df.columns:\n",
    "        if \"description\" in df.columns:\n",
    "            log_file.write(\"Merging 'Needed For' column into existing 'description' column.\\n\")\n",
    "            for idx in df.index:\n",
    "                desc_val = df.at[idx, \"description\"]\n",
    "                needed_for_val = df.at[idx, \"Needed For\"]\n",
    "                if (pd.isna(desc_val) or desc_val == \"\") and (not pd.isna(needed_for_val) and needed_for_val != \"\"):\n",
    "                    df.at[idx, \"description\"] = needed_for_val\n",
    "            df.drop(columns=[\"Needed For\"], inplace=True)\n",
    "        else:\n",
    "            log_file.write(\"Renaming 'Needed For' column to 'description'.\\n\")\n",
    "            df.rename(columns={\"Needed For\": \"description\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def extract_table3(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 3 data   the provided PDF.\n",
    "     \n",
    "      2. Renaming of duplicate/empty columns (using fix_column_names) and then post-processing\n",
    "         unnamed columns as described.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 10\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain either Table 3 patterns or Attachment 1/Attachment 2.\n",
    "            table3_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*10[-.]([1-3])\\b\", text, re.IGNORECASE) or re.search(r\"Table\\s*11[-.]([1-2])\\b\", text, re.IGNORECASE):\n",
    "                    #re.search(r\"Attachment\\s*[12]\", text, re.IGNORECASE)):\n",
    "                    table3_pages.append(i)\n",
    "            if not table3_pages:\n",
    "                print(\"No Table 10  found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "            first_page = table3_pages[0]\n",
    "            last_page = table3_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "            print(f\"Candidate pages start on {scrape_start + 1} and end on {scrape_end}\", file=log_file)\n",
    "            # Process each page that might contain table data.\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                # This variable keeps track of the bottom y-coordinate of the previous table on the page.\n",
    "                previous_table_bottom = None\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "                    table_bbox = table.bbox  # (x0, top, x1, bottom)\n",
    "                    # Define the title region for the table: above the table bounding box.\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*10[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE) or re.search(r\"Table\\s*11\\.[12][:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(2).strip()\n",
    "                                break\n",
    "                        \n",
    "                  \n",
    "                    # Extract the specific phrase using the refined table title.\n",
    "                    if table_title:\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New table detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        # Rename header 'type' to 'type of upgrade' if needed.\n",
    "                        if \"type\" in headers and \"type of upgrade\" not in headers:\n",
    "                            headers = [(\"type of upgrade\" if h == \"type\" else h) for h in headers]\n",
    "                        if \"need for\" in headers:\n",
    "                            headers = [(\"description\" if h == \"need for\" else h) for h in headers]  \n",
    "                    \n",
    "                        # Apply the duplicate/empty column fixing.\n",
    "                        headers = fix_column_names(headers)\n",
    "                        data_rows = tab[1:]\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        if \"allocated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"allocated\"], inplace=True)\n",
    "                            print(f\"Dropped 'Max of' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate x \" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate x \"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)\n",
    "\n",
    "                        if \"cost rate\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"cost rate\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file) \n",
    "\n",
    "                        if \"3339615 9\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"3339615 9\"], inplace=True)\n",
    "                            print(f\"Dropped '3339615 9' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)     \n",
    "                            \n",
    "                        if \"6 steady state reliability and posttransient voltage stability\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"6 steady state reliability and posttransient voltage stability\"], inplace=True)\n",
    "                            print(f\"Dropped '6 steady state reliability and posttransient voltage stability' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)  \n",
    "\n",
    "\n",
    "\n",
    "                        if \"escalated\" in df_new.columns:\n",
    "                            df_new.drop(columns=[\"escalated\"], inplace=True)\n",
    "                            print(f\"Dropped 'cost rate x' column in table on page {page_number + 1}, table {table_index}.\", file=log_file)    \n",
    "\n",
    "\n",
    "                        # Also, if the DataFrame has a column named \"type\" (and not already \"type of upgrade\"), rename it.\n",
    "                        if 'type' in df_new.columns and 'type of upgrade' not in df_new.columns:\n",
    "                            df_new.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        # Special handling for ADNU tables if needed.\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        # Fix duplicate and unnamed columns in the new table.\n",
    "                        df_new.columns = fix_column_names(df_new.columns.tolist())\n",
    "                        # Now apply the post-processing of column names:\n",
    "                        df_new = post_process_columns(df_new, log_file)\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation table branch.\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous table title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        print(f\"Continuation table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "                        # Use the number of columns from the last extracted table as expected.\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            data_rows = data_rows[1:]\n",
    "                        # Ensure every row has the same length as expected_headers.\n",
    "                        adjust_rows_length(data_rows, expected_headers)\n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "                        # Rename column 'type' if needed.\n",
    "                        if 'type' in df_continuation.columns and 'type of upgrade' not in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={'type': 'type of upgrade'}, inplace=True)\n",
    "                        if \"need for\" in df_continuation.columns:\n",
    "                            df_continuation.rename(columns={\"need for\": \"description\"}, inplace=True)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation ADNU table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        else:\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing None in 'type of upgrade' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        # Fix duplicate and unnamed columns in the continuation table.\n",
    "                        df_continuation.columns = fix_column_names(df_continuation.columns.tolist())\n",
    "                        # Post-process the columns in the continuation table.\n",
    "                        df_continuation = post_process_columns(df_continuation, log_file)\n",
    "                        # Concatenate the continuation table with the previous extracted table.\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "                    # Update the previous_table_bottom for the page using the current table's bbox.\n",
    "                    previous_table_bottom = table_bbox[3]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 10 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "        print(\"\\nConcatenating all extracted Table 10/Attachment data...\", file=log_file)\n",
    "        try:\n",
    "            table3_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table3_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 10/Attachment data extracted.\", file=log_file)\n",
    "        table3_data = pd.DataFrame()\n",
    "    return table3_data\n",
    "\n",
    "\n",
    "def extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 10 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table3_data = extract_table3(pdf_path, log_file, is_addendum)\n",
    "    if table3_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        overlapping_columns = base_data.columns.intersection(table3_data.columns).difference(['point_of_interconnection'])\n",
    "        table3_data = table3_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        base_data_repeated = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "        try:\n",
    "\n",
    "                        # Concatenate base data with Table 8 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "           # if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "           #     merged_df[\"is_duplicate\"] = merged_df.duplicated(subset=[\"q_id\", \"type of upgrade\", \"upgrade\"], keep=\"first\")\n",
    "            #    merged_df = merged_df[merged_df[\"is_duplicate\"] == False].drop(columns=[\"is_duplicate\"])\n",
    "            #    print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade'.\", file=log_file)\n",
    "\n",
    "\n",
    "            if \"q_id\" in merged_df.columns and \"type of upgrade\" in merged_df.columns and \"upgrade\" in merged_df.columns:\n",
    "                # Identify rows where 'type of upgrade' and 'upgrade' are not empty\n",
    "                non_empty_rows = merged_df[\n",
    "                    merged_df[\"type of upgrade\"].notna() & merged_df[\"upgrade\"].notna() &\n",
    "                    (merged_df[\"type of upgrade\"].str.strip() != \"\") & (merged_df[\"upgrade\"].str.strip() != \"\")\n",
    "                ]\n",
    "\n",
    "                # Group by q_id, type of upgrade, and upgrade, keeping the first occurrence\n",
    "                grouped_df = non_empty_rows.groupby([\"q_id\", \"type of upgrade\", \"upgrade\"], as_index=False).first()\n",
    "\n",
    "                # Get the original order of the rows in merged_df before filtering\n",
    "                merged_df[\"original_index\"] = merged_df.index\n",
    "\n",
    "                # Combine unique grouped rows with originally empty rows\n",
    "                final_df = pd.concat([\n",
    "                    grouped_df,\n",
    "                    merged_df[merged_df[\"type of upgrade\"].isna() | (merged_df[\"type of upgrade\"].str.strip() == \"\") |\n",
    "                            merged_df[\"upgrade\"].isna() | (merged_df[\"upgrade\"].str.strip() == \"\")]\n",
    "                ], ignore_index=True, sort=False)\n",
    "\n",
    "                # Restore the original order of the rows based on the saved index\n",
    "                final_df.sort_values(by=\"original_index\", inplace=True)\n",
    "                final_df.drop(columns=[\"original_index\"], inplace=True)\n",
    "                merged_df = final_df\n",
    "\n",
    "                print(f\"Removed duplicate rows based on 'q_id', 'type of upgrade', and 'upgrade', excluding empty rows while preserving order.\", file=log_file)\n",
    "\n",
    "            merged_df = pd.concat([base_data_repeated, table3_data], axis=1, sort=False)\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            print(f\"Merged base data with Table 3 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 3 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data\n",
    "\n",
    "def check_has_table3(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 3 or Attachment 1/Attachment 2.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*10[-.]?\\d*\", text, re.IGNORECASE) or re.search(r\"Table\\s*11[-.]([1-2])\\b\", text, re.IGNORECASE):\n",
    "                    #re.search(r\"Attachment\\s*[12]\", text, re.IGNORECASE))\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            return \"Addendum\" in text\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "        text = clean_string_cell(text)\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "        point_of_interconnection = extract_table2(pdf_path, log_file)\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "    df = df.map(clean_string_cell)\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "            project_scraped = False\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    try:\n",
    "                        has_table3 = check_has_table3(pdf_path)\n",
    "                        if not has_table3:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\", file=log_file)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 3 or Attachment data)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "                        if is_add and base_data_extracted:\n",
    "                            table3_data = extract_table3(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table3_data.empty:\n",
    "                                merged_df = pd.concat([base_data] * len(table3_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table3_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            df = extract_table3_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total folders (from 667 to 859) with an empty '02_phase_1_study' subfolder: 67\n",
      "\n",
      "Folders with empty '02_phase_1_study':\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/764\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/790\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/739\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/706\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/730\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/791\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/736\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/700\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/686\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/681\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/845\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/842\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/674\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/843\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/857\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/850\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/804\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/669\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/851\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/858\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/692\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/834\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/668\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/785\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/771\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/782\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/712\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/783\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/770\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/784\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/741\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/767\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/793\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/756\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/732\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/703\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/704\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/792\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/766\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/841\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/846\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/848\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/676\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/685\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/812\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/849\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/847\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/840\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/813\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/684\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/683\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/807\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/836\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/697\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/853\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/854\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/696\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/698\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/839\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/801\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/852\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/788\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/786\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/717\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/774\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/716\n",
      "/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/729\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Update this to the path of your main folder\n",
    "base_folder = r'/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data'  \n",
    "\n",
    "# Set your numeric range\n",
    "range_start = 667\n",
    "range_end = 859\n",
    "\n",
    "# Counter for folders with empty '02_phase_1_study'\n",
    "empty_count = 0\n",
    "\n",
    "# List to hold paths that meet the condition (optional)\n",
    "empty_folders = []\n",
    "\n",
    "# Loop through each item in the main folder\n",
    "for item in os.listdir(base_folder):\n",
    "    item_path = os.path.join(base_folder, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        # Try to interpret the folder name as a number\n",
    "        try:\n",
    "            folder_number = int(item)\n",
    "        except ValueError:\n",
    "            continue  # Skip folders that don't have a numeric name\n",
    "\n",
    "        # Check if the folder's number is in the desired range\n",
    "        if range_start <= folder_number <= range_end:\n",
    "            # Build the path for the '02_phase_1_study' subfolder\n",
    "            phase1_path = os.path.join(item_path, \"02_phase_1_study\")\n",
    "            if os.path.isdir(phase1_path):\n",
    "                # Check if the subfolder is empty\n",
    "                if not os.listdir(phase1_path):\n",
    "                    empty_count += 1\n",
    "                    empty_folders.append(item_path)\n",
    "            else:\n",
    "                # If you want to log folders missing the subfolder, you can print or handle that here.\n",
    "                print(f\"Folder {item_path} does not have a '02_phase_1_study' subfolder.\")\n",
    "\n",
    "print(f\"\\nTotal folders (from {range_start} to {range_end}) with an empty '02_phase_1_study' subfolder: {empty_count}\")\n",
    "\n",
    "# (Optional) List the folders that met the condition\n",
    "if empty_folders:\n",
    "    print(\"\\nFolders with empty '02_phase_1_study':\")\n",
    "    for folder in empty_folders:\n",
    "        print(folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# list of non missing pdfs not scraped by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "667 - table 11.1,11.2\n",
    "737,- no table\n",
    "781,table 11.1,11.2\n",
    "789, table 11.1,11.2\n",
    "794,table 11.1,11.2\n",
    "795, no table\n",
    "837, table 14.1\n",
    "838 table 11.1,11.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}