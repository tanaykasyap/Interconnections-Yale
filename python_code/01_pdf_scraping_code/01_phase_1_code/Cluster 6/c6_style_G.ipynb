{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All cluster 6 projects code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: Appendix A - Q945 LECU.pdf from Project 945\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 6/rawdata_cluster6_originals.csv\n",
      "No data to save for addendums.\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 1\n",
      "Total Projects Scraped: 1\n",
      "Total Projects Skipped: 0\n",
      "Total Projects Missing: 0\n",
      "Total PDFs Accessed: 1\n",
      "Total PDFs Scraped: 1\n",
      "Total PDFs Skipped: 0\n",
      "\n",
      "List of Scraped Projects:\n",
      "[945]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[]\n",
      "\n",
      "List of Missing Projects:\n",
      "[]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['Appendix A - Q945 LECU.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "[]\n",
      "\n",
      "List of Addendum PDFs:\n",
      "[]\n",
      "\n",
      "List of Original PDFs:\n",
      "['Appendix A - Q945 LECU.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 1\n",
      "Number of Addendum PDFs Scraped: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/03_raw/rawdata_cluster6_style_G_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/03_raw/rawdata_cluster6_style_G_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/03_raw/scraping_cluster6_style_G_log.txt\"\n",
    "PROJECT_RANGE = range(943, 1003)  # Example range for q_ids in Clusters  8\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 7\"\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*7?\\d*\", text, re.IGNORECASE):\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 7 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "\n",
    "            print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end}\", file=log_file)\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(2).strip()\n",
    "                                break\n",
    "\n",
    "                    if table_title:\n",
    "                        # New Table 7 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        # Handle new ADNU tables (grouping logic)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    # Group all adnu rows into one 'upgrade' row\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    # If 'type of upgrade' exists, just rename adnu if needed\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row none, original logic replaced only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' first row for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            # Non-ADNU new tables\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If exist and none in first row original logic replaced only first row\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for first row in new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(\"Duplicate columns detected in new table. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous Table 7 title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "\n",
    "                        # Check if the number of columns matches\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\")\n",
    "                            continue  # No table to continue with\n",
    "\n",
    "                        # Define expected columns based on the last extracted table\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "\n",
    "                        # Detect header row in continuation\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            actual_header_row = data_rows[0]\n",
    "                            actual_headers = clean_column_headers(actual_header_row)\n",
    "                             \n",
    "\n",
    "                            # Go back to previous page bounding box to update table title if needed\n",
    "                            if page_number > 0:\n",
    "                                previous_page = pdf.pages[page_number - 1]\n",
    "                                bbox_lower_region = (0, table_bbox[1], previous_page.width, previous_page.height)\n",
    "                                title_text_previous = previous_page.within_bbox(bbox_lower_region).extract_text() or \"\"\n",
    "                                new_table_title = None\n",
    "                                if title_text_previous:\n",
    "                                    title_lines_prev = title_text_previous.split('\\n')[::-1]\n",
    "                                    for line in title_lines_prev:\n",
    "                                        line = line.strip()\n",
    "                                        match_prev = re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                                        if match_prev:\n",
    "                                            new_table_title = match_prev.group(2).strip()\n",
    "                                            break\n",
    "                                if new_table_title:\n",
    "                                    specific_phrase = extract_specific_phrase(new_table_title)\n",
    "                                    print(f\"Updated table title from previous page: '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                else:\n",
    "                                    print(\"No table title found in previous page region. Using existing specific_phrase.\", file=log_file)\n",
    "                            else:\n",
    "                                print(\"No previous page available for title extraction for continuation table. Using existing specific_phrase.\", file=log_file)\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                            # Handle continuation ADNU or non-ADNU\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                    \n",
    "                                print(\"Continuation ADNU table detected. No grouping, just rename and type of upgrade handling.\", file=log_file)\n",
    "                                  \n",
    "                                    \n",
    "                                #if \"adnu\" in data_rows[0]:\n",
    "                                    #\n",
    "                                 #   print(\"Handling continuation for 'Area Delivery Netowrk Upgrade' table\")\n",
    "                                 #   print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\")\n",
    "                                 #   \n",
    "                                 #   data_rows = [ [\"upgrade\"] + row[1:] for row in data_rows ]\n",
    "                                    \n",
    "\n",
    "                                if \"type of upgrade\" not in data_rows[0]:\n",
    "                                    # Insert 'type of upgrade' column at the beginning\n",
    "                                    print(\"Inserting 'type of upgrade' column with specific phrase in continuation table.\",file=log_file)\n",
    "                                    data_rows = [ [specific_phrase] + row for row in data_rows ]\n",
    "\n",
    "\n",
    "                                    if \"ADNU\" in data_rows[0]:\n",
    "                                        print(\"Handling continuation for 'Area Delivery Network Upgrade' table\",file=log_file)\n",
    "                                        print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\",file=log_file)\n",
    "                                        # Find the index where \"ADNU\" occurs in the first row\n",
    "                                        adnu_idx = data_rows[0].index(\"ADNU\")\n",
    "                                        # Replace \"ADNU\" with \"upgrade\" in that column for every row\n",
    "                                        #for r in range(len(data_rows)):\n",
    "                                        data_rows[0][adnu_idx] = \"upgrade\"\n",
    "\n",
    "\n",
    "\n",
    "                                    \n",
    "                                \n",
    "                        # Handle missing or extra columns\n",
    "                        if len(data_rows[0]) < expected_columns:\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                    # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                print(f\"Detected missing 'upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'upgrade' column.\",file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                            else:\n",
    "                                    # For other tables, assume missing \"Type of Upgrade\" column\n",
    "                                print(f\"Detected missing 'Type of Upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'Type of Upgrade' column.\",file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                        elif len(data_rows[0]) > expected_columns:\n",
    "                            # Extra columns detected; adjust accordingly\n",
    "                            print(f\"Detected extra columns in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping extra columns.\",file=log_file)\n",
    "                            data_rows = [row[:expected_columns] for row in data_rows]\n",
    "                            \n",
    "                        \n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "                        # Create DataFrame for the continuation table\n",
    "\n",
    "                        if is_header_row:    \n",
    "                            data_rows = data_rows[1:]\n",
    "                            print(f\"Dropped header row from data_rows after modifications for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "    \n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\")\n",
    "                            continue  # Skip this table due to error\n",
    "\n",
    "                            # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"type of upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                        else:\n",
    "                            # General Handling for other tables\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\",file=log_file)\n",
    "                            df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)            \n",
    "\n",
    "\n",
    "\n",
    "                                         \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 7 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After processing all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 7 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 7 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 7 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 7.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            return \"Addendum\" in text\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "\n",
    "                    # Determine output DataFrame and CSV path based on addendum status\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "\n",
    "                    try:\n",
    "                        has_table7 = check_has_table7(pdf_path)\n",
    "                        if not has_table7:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                            # Print to ipynb output\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            # Extract base data from original PDF\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                        if is_add and base_data_extracted:\n",
    "                            # For addendums, use the extracted base data\n",
    "                            table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table7_data.empty:\n",
    "                                # Merge base data with Table 7 data\n",
    "                                merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                # Optionally, print to ipynb\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            # For originals, extract Table 7 data\n",
    "                            df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                # Optionally, print to ipynb\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        # Optionally, print to ipynb\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.map(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missed 945 so doing it individually and manually adding it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just had to add an extra line to check if the data frame is empty before accessing the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: Appendix A - Q945 LECU.pdf from Project 945\n",
      "Scraped PDF: 13AS837386-Q946_Northern_Orchard_Solar_C6Ph_I_Appendix_AIndividual_ReportRev1.pdf from Project 946\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 6/03_raw/rawdata_cluster6_style_G_945.csv\n",
      "No data to save for addendums.\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 2\n",
      "Total Projects Scraped: 2\n",
      "Total Projects Skipped: 0\n",
      "Total Projects Missing: 0\n",
      "Total PDFs Accessed: 2\n",
      "Total PDFs Scraped: 2\n",
      "Total PDFs Skipped: 0\n",
      "\n",
      "List of Scraped Projects:\n",
      "[945, 946]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[]\n",
      "\n",
      "List of Missing Projects:\n",
      "[]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['Appendix A - Q945 LECU.pdf', '13AS837386-Q946_Northern_Orchard_Solar_C6Ph_I_Appendix_AIndividual_ReportRev1.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "[]\n",
      "\n",
      "List of Addendum PDFs:\n",
      "[]\n",
      "\n",
      "List of Original PDFs:\n",
      "['Appendix A - Q945 LECU.pdf', '13AS837386-Q946_Northern_Orchard_Solar_C6Ph_I_Appendix_AIndividual_ReportRev1.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 2\n",
      "Number of Addendum PDFs Scraped: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/03_raw/rawdata_cluster6_style_G_945.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/03_raw/rawdata_cluster6_style_G_945_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/03_raw/scraping_cluster6_style_G_log_945.txt\"\n",
    "PROJECT_RANGE = range(945, 947)  # Example range for q_ids in Clusters  8\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 7\"\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*7?\\d*\", text, re.IGNORECASE):\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 7 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1\n",
    "\n",
    "            print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end}\", file=log_file)\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(2).strip()\n",
    "                                break\n",
    "\n",
    "                    if table_title:\n",
    "                        # New Table 7 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        if df_new.empty:\n",
    "                            print(f\"The extracted DataFrame for 'Area Delivery Network Upgrade' is empty. Skipping this table.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        # Handle new ADNU tables (grouping logic)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    # Group all adnu rows into one 'upgrade' row\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    # If 'type of upgrade' exists, just rename adnu if needed\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row none, original logic replaced only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' first row for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            # Non-ADNU new tables\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If exist and none in first row original logic replaced only first row\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for first row in new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(\"Duplicate columns detected in new table. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous Table 7 title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "\n",
    "                        # Check if the number of columns matches\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\")\n",
    "                            continue  # No table to continue with\n",
    "\n",
    "                        # Define expected columns based on the last extracted table\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "\n",
    "                        # Detect header row in continuation\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            actual_header_row = data_rows[0]\n",
    "                            actual_headers = clean_column_headers(actual_header_row)\n",
    "                             \n",
    "\n",
    "                            # Go back to previous page bounding box to update table title if needed\n",
    "                            if page_number > 0:\n",
    "                                previous_page = pdf.pages[page_number - 1]\n",
    "                                bbox_lower_region = (0, table_bbox[1], previous_page.width, previous_page.height)\n",
    "                                title_text_previous = previous_page.within_bbox(bbox_lower_region).extract_text() or \"\"\n",
    "                                new_table_title = None\n",
    "                                if title_text_previous:\n",
    "                                    title_lines_prev = title_text_previous.split('\\n')[::-1]\n",
    "                                    for line in title_lines_prev:\n",
    "                                        line = line.strip()\n",
    "                                        match_prev = re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                                        if match_prev:\n",
    "                                            new_table_title = match_prev.group(2).strip()\n",
    "                                            break\n",
    "                                if new_table_title:\n",
    "                                    specific_phrase = extract_specific_phrase(new_table_title)\n",
    "                                    print(f\"Updated table title from previous page: '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                else:\n",
    "                                    print(\"No table title found in previous page region. Using existing specific_phrase.\", file=log_file)\n",
    "                            else:\n",
    "                                print(\"No previous page available for title extraction for continuation table. Using existing specific_phrase.\", file=log_file)\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                            # Handle continuation ADNU or non-ADNU\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                    \n",
    "                                print(\"Continuation ADNU table detected. No grouping, just rename and type of upgrade handling.\", file=log_file)\n",
    "                                  \n",
    "                                    \n",
    "                                #if \"adnu\" in data_rows[0]:\n",
    "                                    #\n",
    "                                 #   print(\"Handling continuation for 'Area Delivery Netowrk Upgrade' table\")\n",
    "                                 #   print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\")\n",
    "                                 #   \n",
    "                                 #   data_rows = [ [\"upgrade\"] + row[1:] for row in data_rows ]\n",
    "                                    \n",
    "\n",
    "                                if \"type of upgrade\" not in data_rows[0]:\n",
    "                                    # Insert 'type of upgrade' column at the beginning\n",
    "                                    print(\"Inserting 'type of upgrade' column with specific phrase in continuation table.\",file=log_file)\n",
    "                                    data_rows = [ [specific_phrase] + row for row in data_rows ]\n",
    "\n",
    "\n",
    "                                    if \"ADNU\" in data_rows[0]:\n",
    "                                        print(\"Handling continuation for 'Area Delivery Network Upgrade' table\",file=log_file)\n",
    "                                        print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\",file=log_file)\n",
    "                                        # Find the index where \"ADNU\" occurs in the first row\n",
    "                                        adnu_idx = data_rows[0].index(\"ADNU\")\n",
    "                                        # Replace \"ADNU\" with \"upgrade\" in that column for every row\n",
    "                                        #for r in range(len(data_rows)):\n",
    "                                        data_rows[0][adnu_idx] = \"upgrade\"\n",
    "\n",
    "\n",
    "\n",
    "                                    \n",
    "                                \n",
    "                        # Handle missing or extra columns\n",
    "                        if len(data_rows[0]) < expected_columns:\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                    # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                print(f\"Detected missing 'upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'upgrade' column.\",file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                            else:\n",
    "                                    # For other tables, assume missing \"Type of Upgrade\" column\n",
    "                                print(f\"Detected missing 'Type of Upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'Type of Upgrade' column.\",file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                        elif len(data_rows[0]) > expected_columns:\n",
    "                            # Extra columns detected; adjust accordingly\n",
    "                            print(f\"Detected extra columns in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping extra columns.\",file=log_file)\n",
    "                            data_rows = [row[:expected_columns] for row in data_rows]\n",
    "                            \n",
    "                        \n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "                        # Create DataFrame for the continuation table\n",
    "\n",
    "                        if is_header_row:    \n",
    "                            data_rows = data_rows[1:]\n",
    "                            print(f\"Dropped header row from data_rows after modifications for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "    \n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\")\n",
    "                            continue  # Skip this table due to error\n",
    "\n",
    "                        if df_continuation.empty:\n",
    "                            print(f\"The continuation DataFrame on page {page_number + 1}, table {table_index + 1} is empty. Skipping this table.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                            # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"type of upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                        else:\n",
    "                            # General Handling for other tables\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\",file=log_file)\n",
    "                            df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)            \n",
    "\n",
    "\n",
    "\n",
    "                                         \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 7 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After processing all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 7 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 7 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 7 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 7.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            return \"Addendum\" in text\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "\n",
    "                    # Determine output DataFrame and CSV path based on addendum status\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "\n",
    "                    try:\n",
    "                        has_table7 = check_has_table7(pdf_path)\n",
    "                        if not has_table7:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                            # Print to ipynb output\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            # Extract base data from original PDF\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                        if is_add and base_data_extracted:\n",
    "                            # For addendums, use the extracted base data\n",
    "                            table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table7_data.empty:\n",
    "                                # Merge base data with Table 7 data\n",
    "                                merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                # Optionally, print to ipynb\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            # For originals, extract Table 7 data\n",
    "                            df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                # Optionally, print to ipynb\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        # Optionally, print to ipynb\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.map(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Itemized and total datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_1_cluster_8_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_1_cluster_8_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU' 'ADNU']\n",
      "[ 943  945  946  947  951  954  955  956  962  963  964  966  972  974\n",
      "  975  976  987  988  989  992  997 1000 1001]\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/03_raw/rawdata_cluster6_style_G_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\"\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"estimated cost x 1000\",\n",
    "            \"allocated_cost\",\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\"\n",
    "        ],\n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\"\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\"\n",
    "\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "#df.drop('incremental mw', axis=1, inplace=True)\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "df = reorder_columns(df)\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "required_columns = ['type_of_upgrade', 'cost_allocation_factor']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "\n",
    "'''\n",
    "# ## ADNU GROUPING LOGIC START\n",
    "# If type_of_upgrade == 'ADNU' and multiple rows have the same 'upgrade' with no estimated_cost_x_1000,\n",
    "# we group them into a single row by keeping the first and dropping the rest.\n",
    "if 'type_of_upgrade' in df.columns and 'upgrade' in df.columns and 'estimated_cost_x_1000' in df.columns:\n",
    "    to_drop = []\n",
    "    for (qid, toup, upg), grp in df.groupby(['q_id', 'type_of_upgrade', 'upgrade'], sort=False):\n",
    "        if isinstance(toup, str) and toup.lower() == 'adnu':\n",
    "            # Check if all estimated_cost_x_1000 are empty/None/'None'\n",
    "            is_empty_est = grp['estimated_cost_x_1000'].apply(lambda x: pd.isna(x) or x == '' or x == 'None')\n",
    "            if len(grp) > 1 and is_empty_est.all():\n",
    "                # Keep the first row, drop the rest\n",
    "                idxs = grp.index.tolist()\n",
    "                drop_idxs = idxs[1:]\n",
    "                to_drop.extend(drop_idxs)\n",
    "    if to_drop:\n",
    "        df.drop(to_drop, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "# ## ADNU GROUPING LOGIC END\n",
    "'''\n",
    "\n",
    "# Step 1: Create the 'item' column\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 2: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "# Step 3: Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Step 4: For each q_id and type_of_upgrade, if only one row and no total present, create new Total row\n",
    "new_rows = []\n",
    "for q_id, group in df.groupby('q_id'):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "\n",
    "    if any('Total' in str(x) for x in group.get('cost_allocation_factor', [])):\n",
    "        continue\n",
    "\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade) or 'Total' in str(upgrade):\n",
    "            continue\n",
    "\n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "        if len(rows) == 1:\n",
    "            original_row = rows.iloc[0].copy()\n",
    "            total_row = original_row.copy()\n",
    "\n",
    "            total_row['type_of_upgrade'] = 'Total'\n",
    "            total_row['item'] = 'no'\n",
    "\n",
    "            original_index = df[(df['q_id'] == q_id) & (df['type_of_upgrade'] == upgrade)].index[0]\n",
    "            new_rows.append((original_index + 1, total_row))\n",
    "\n",
    "for idx, row in sorted(new_rows, reverse=True):\n",
    "    if idx < 0 or idx > len(df):\n",
    "        continue\n",
    "    df = pd.concat([df.iloc[:idx], pd.DataFrame([row]), df.iloc[idx:]]).reset_index(drop=True)\n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "mappings = {\n",
    "    'Reliability Network Upgrade': 'RNU',\n",
    "    'Reliability Network upgrade': 'RNU',\n",
    "    'Local Delivery Network Upgrade': 'LDNU',\n",
    "    'Local Delivery Network': 'LDNU',\n",
    "    \"PTOs Interconnection Facilities\": 'PTO_IF',\n",
    "    'Area Delivery Network Upgrade': 'ADNU',\n",
    "    'Reliability Network upgrade to Physically Interconnect': 'RNU',\n",
    "    'PTO': 'PTO_IF',\n",
    "}\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()\n",
    "\n",
    "if 'upgrade' in df.columns and 'type_of_upgrade' in df.columns and 'q_id' in df.columns:\n",
    "    df['upgrade'] = df.groupby(['q_id', 'type_of_upgrade'])['upgrade'].ffill()\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/02_intermediate/costs_phase_1_cluster_6_style_G_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/02_intermediate/costs_phase_1_cluster_6_style_G_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_1_cluster_8_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_1_cluster_8_total.csv'.\")\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded itemized data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 6//02_intermediate/costs_phase_1_cluster_6_style_G_itemized.csv\n",
      "Loaded totals data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 6//02_intermediate/costs_phase_1_cluster_6_style_G_total.csv\n",
      "\n",
      "All q_ids have all required upgrades.\n",
      "\n",
      "Mismatches saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 6/mismatches.csv'.\n",
      "\n",
      "Direct Matches (Exact Point of Interconnection Names):\n",
      "No direct matches found.\n",
      "\n",
      "Fuzzy Matches (>=80% Similarity in Point of Interconnection):\n",
      "             point_of_interconnection_1 q_ids_1  \\\n",
      "0               LERDO 115 KV SUBSTATION   [943]   \n",
      "1  MIDWAY  WHEELER RIDGE 230 KV LINE #2   [946]   \n",
      "2                 VASCO 60KV SUBSTATION   [951]   \n",
      "3                 VASCO 60KV SUBSTATION   [951]   \n",
      "4                HALSEY 60KV SUBSTATION   [962]   \n",
      "5              MIDWAY-MIDSUN 115KV LINE   [966]   \n",
      "\n",
      "         point_of_interconnection_2 q_ids_2  similarity_score  \n",
      "0            OLIVE 115KV SUBSTATION   [947]                84  \n",
      "1  MIDWAY - WHEELER RIDGE 230KV BUS  [1000]                84  \n",
      "2            HALSEY 60KV SUBSTATION   [962]                84  \n",
      "3     EAST NICOLAUS 60KV SUBSTATION   [992]                83  \n",
      "4     EAST NICOLAUS 60KV SUBSTATION   [992]                81  \n",
      "5         MIDWAY-TEMBLOR 115KV LINE   [972]                83  \n",
      "Matched Q_ids saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 6/matched_qids.csv'.\n",
      "\n",
      "Total checks performed: 92\n",
      "Total mismatches found: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "# Paths to the CSV files\n",
    "ITEMIZED_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6//02_intermediate/costs_phase_1_cluster_6_style_G_itemized.csv'\n",
    "TOTALS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6//02_intermediate/costs_phase_1_cluster_6_style_G_total.csv'\n",
    "\n",
    "# Columns in totals_df that hold the reported total costs\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "# Upgrade types to check\n",
    "REQUIRED_UPGRADES = ['PTO_IF', 'RNU', 'LDNU', 'ADNU']\n",
    "\n",
    "# Output paths\n",
    "MISMATCHES_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/mismatches.csv'\n",
    "MATCHED_QIDS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/matched_qids.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "def load_csv(path, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {dataset_name} from {path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Load datasets\n",
    "itemized_df = load_csv(ITEMIZED_CSV_PATH, \"itemized data\")\n",
    "totals_df = load_csv(TOTALS_CSV_PATH, \"totals data\")\n",
    "\n",
    "# ---------------------- Data Cleaning ---------------------- #\n",
    "\n",
    "def clean_text(df, column):\n",
    "    \"\"\"\n",
    "    Cleans text data by stripping leading/trailing spaces and converting to uppercase.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        df[column] = df[column].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        print(f\"Warning: '{column}' column is missing in the dataset. Filling with 'UNKNOWN'.\")\n",
    "        df[column] = 'UNKNOWN'\n",
    "    return df\n",
    "\n",
    "# Clean 'type_of_upgrade' and 'point_of_interconnection' in both datasets\n",
    "itemized_df = clean_text(itemized_df, 'type_of_upgrade')\n",
    "itemized_df = clean_text(itemized_df, 'point_of_interconnection')\n",
    "\n",
    "totals_df = clean_text(totals_df, 'type_of_upgrade')\n",
    "totals_df = clean_text(totals_df, 'point_of_interconnection')\n",
    "\n",
    "# ---------------------- Data Preparation ---------------------- #\n",
    "\n",
    "# Ensure necessary columns exist in itemized_df\n",
    "required_itemized_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', 'estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in required_itemized_columns:\n",
    "    if col not in itemized_df.columns:\n",
    "        print(f\"Warning: '{col}' column is missing in the itemized dataset.\")\n",
    "        if col in ['q_id', 'type_of_upgrade', 'point_of_interconnection']:\n",
    "            itemized_df[col] = 'UNKNOWN'\n",
    "        else:\n",
    "            itemized_df[col] = 0\n",
    "\n",
    "# Ensure necessary columns exist in totals_df\n",
    "required_totals_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in required_totals_columns:\n",
    "    if col not in totals_df.columns:\n",
    "        print(f\"Error: '{col}' column is missing in the totals dataset. Cannot proceed.\")\n",
    "        exit(1)\n",
    "\n",
    "# Convert cost columns to numeric, coercing errors to NaN and filling with 0\n",
    "cost_columns_itemized = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in cost_columns_itemized:\n",
    "    itemized_df[col] = pd.to_numeric(itemized_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "cost_columns_totals = [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in cost_columns_totals:\n",
    "    totals_df[col] = pd.to_numeric(totals_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# ---------------------- Calculate Manual Totals ---------------------- #\n",
    "\n",
    "# Group itemized data by q_id and type_of_upgrade and calculate sums\n",
    "itemized_grouped = itemized_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    'estimated_cost_x_1000': 'sum',\n",
    "    'escalated_cost_x_1000': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "itemized_grouped['manual_total'] = itemized_grouped.apply(\n",
    "    lambda row: row['estimated_cost_x_1000'] if row['estimated_cost_x_1000'] > 0 else row['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Prepare Totals Data ---------------------- #\n",
    "\n",
    "# Group totals data by q_id and type_of_upgrade and calculate sums\n",
    "totals_grouped = totals_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "    TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "totals_grouped['reported_total'] = totals_grouped.apply(\n",
    "    lambda row: row[TOTALS_ESTIMATED_COLUMN] if row[TOTALS_ESTIMATED_COLUMN] > 0 else row[TOTALS_ESCALATED_COLUMN],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Merge Data ---------------------- #\n",
    "\n",
    "# Merge the itemized and totals data on q_id and type_of_upgrade\n",
    "comparison_df = pd.merge(\n",
    "    itemized_grouped,\n",
    "    totals_grouped[['q_id', 'type_of_upgrade', 'reported_total']],\n",
    "    on=['q_id', 'type_of_upgrade'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---------------------- Check for Missing Upgrades ---------------------- #\n",
    "\n",
    "# Identify q_ids that are missing any of the required upgrades\n",
    "missing_upgrades_report = []\n",
    "for q_id in comparison_df['q_id'].unique():\n",
    "    upgrades_present = comparison_df[comparison_df['q_id'] == q_id]['type_of_upgrade'].unique()\n",
    "    missing_upgrades = [upgrade for upgrade in REQUIRED_UPGRADES if upgrade not in upgrades_present]\n",
    "    if missing_upgrades:\n",
    "        missing_upgrades_report.append((q_id, missing_upgrades))\n",
    "\n",
    "# Report missing upgrades\n",
    "if missing_upgrades_report:\n",
    "    print(\"\\nQ_ids with missing upgrades:\")\n",
    "    for q_id, missing in missing_upgrades_report:\n",
    "        print(f\"  Q_id {q_id} is missing upgrades: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nAll q_ids have all required upgrades.\")\n",
    "\n",
    "# ---------------------- Compare Totals and Identify Mismatches ---------------------- #\n",
    "\n",
    "# Initialize list to store mismatches\n",
    "mismatches = []\n",
    "\n",
    "# Iterate through each row to compare manual_total with reported_total\n",
    "for index, row in comparison_df.iterrows():\n",
    "    q_id = row['q_id']\n",
    "    upgrade = row['type_of_upgrade']\n",
    "    manual_total = row['manual_total']\n",
    "    reported_total = row['reported_total']\n",
    "    \n",
    "    # Determine if both manual_total and reported_total are zero\n",
    "    if manual_total == 0.0 and reported_total == 0.0:\n",
    "        continue  # No mismatch\n",
    "    # Determine if manual_total is zero and reported_total is missing or zero\n",
    "    elif manual_total == 0.0 and (pd.isna(row['reported_total']) or reported_total == 0.0):\n",
    "        continue  # No mismatch\n",
    "    # If reported_total is missing (NaN) and manual_total is not zero\n",
    "    elif pd.isna(row['reported_total']) and manual_total != 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: Missing\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': 'Missing'\n",
    "        })\n",
    "    # If manual_total is not zero and reported_total is zero\n",
    "    elif manual_total != 0.0 and reported_total == 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: 0.0\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # If both totals are non-zero but differ beyond tolerance\n",
    "    elif abs(manual_total - reported_total) > 1e-2:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: {reported_total}\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # Else, totals match; do nothing\n",
    "\n",
    "# Create a DataFrame for mismatches\n",
    "mismatches_df = pd.DataFrame(mismatches, columns=['q_id', 'type_of_upgrade', 'manual_total', 'reported_total'])\n",
    "\n",
    "# Save mismatches to a CSV file\n",
    "try:\n",
    "    mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "    print(f\"\\nMismatches saved to '{MISMATCHES_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving mismatches CSV: {e}\")\n",
    "\n",
    "# ---------------------- Point of Interconnection Matching ---------------------- #\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from itemized dataset\n",
    "itemized_poi = itemized_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from totals dataset\n",
    "totals_poi = totals_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Merge both to have a complete list of q_id and point_of_interconnection\n",
    "all_poi = pd.concat([itemized_poi, totals_poi]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ---------------------- Direct Match Identification ---------------------- #\n",
    "\n",
    "# Group by point_of_interconnection to find q_ids sharing the same point_of_interconnection\n",
    "direct_matches = all_poi.groupby('point_of_interconnection')['q_id'].apply(list).reset_index()\n",
    "\n",
    "# Filter groups with more than one q_id (i.e., shared points_of_interconnection)\n",
    "direct_matches = direct_matches[direct_matches['q_id'].apply(len) > 1]\n",
    "\n",
    "print(\"\\nDirect Matches (Exact Point of Interconnection Names):\")\n",
    "if not direct_matches.empty:\n",
    "    print(direct_matches)\n",
    "else:\n",
    "    print(\"No direct matches found.\")\n",
    "\n",
    "# ---------------------- Fuzzy Match Identification ---------------------- #\n",
    "\n",
    "# Prepare list of points_of_interconnection for fuzzy matching\n",
    "poi_list = all_poi['point_of_interconnection'].unique().tolist()\n",
    "\n",
    "# Initialize list to store fuzzy matches\n",
    "fuzzy_matches = []\n",
    "\n",
    "# Iterate through each point_of_interconnection to find similar ones\n",
    "for i, poi in enumerate(poi_list):\n",
    "    # Compare with the rest of the points to avoid redundant comparisons\n",
    "    similar_pois = process.extract(poi, poi_list[i+1:], scorer=fuzz.token_set_ratio)\n",
    "    \n",
    "    # Filter matches with similarity >= 80%\n",
    "    for match_poi, score in similar_pois:\n",
    "        if score >= 80:\n",
    "            # Retrieve q_ids for both points_of_interconnection\n",
    "            qids_poi1 = all_poi[all_poi['point_of_interconnection'] == poi]['q_id'].tolist()\n",
    "            qids_poi2 = all_poi[all_poi['point_of_interconnection'] == match_poi]['q_id'].tolist()\n",
    "            \n",
    "            # Append the matched pairs with their points_of_interconnection and similarity score\n",
    "            fuzzy_matches.append({\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_ids_1': qids_poi1,\n",
    "                'point_of_interconnection_2': match_poi,\n",
    "                'q_ids_2': qids_poi2,\n",
    "                'similarity_score': score\n",
    "            })\n",
    "\n",
    "# Convert fuzzy matches to DataFrame\n",
    "fuzzy_matches_df = pd.DataFrame(fuzzy_matches)\n",
    "\n",
    "print(\"\\nFuzzy Matches (>=80% Similarity in Point of Interconnection):\")\n",
    "if not fuzzy_matches_df.empty:\n",
    "    print(fuzzy_matches_df)\n",
    "else:\n",
    "    print(\"No fuzzy matches found.\")\n",
    "\n",
    "# ---------------------- Save Matched Q_ids to CSV ---------------------- #\n",
    "\n",
    "# For clarity, create a combined DataFrame for direct and fuzzy matches\n",
    "\n",
    "# Direct matches: list each pair of q_ids sharing the same point_of_interconnection\n",
    "direct_matches_expanded = []\n",
    "for _, row in direct_matches.iterrows():\n",
    "    qids = row['q_id']\n",
    "    poi = row['point_of_interconnection']\n",
    "    # Generate all possible unique pairs\n",
    "    for i in range(len(qids)):\n",
    "        for j in range(i+1, len(qids)):\n",
    "            direct_matches_expanded.append({\n",
    "                'match_type': 'Direct',\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_id_1': qids[i],\n",
    "                'point_of_interconnection_2': poi,\n",
    "                'q_id_2': qids[j],\n",
    "                'similarity_score': 100\n",
    "            })\n",
    "\n",
    "# Fuzzy matches: already have pairs\n",
    "fuzzy_matches_expanded = []\n",
    "for _, row in fuzzy_matches_df.iterrows():\n",
    "    fuzzy_matches_expanded.append({\n",
    "        'match_type': 'Fuzzy',\n",
    "        'point_of_interconnection_1': row['point_of_interconnection_1'],\n",
    "        'q_id_1': row['q_ids_1'],\n",
    "        'point_of_interconnection_2': row['point_of_interconnection_2'],\n",
    "        'q_id_2': row['q_ids_2'],\n",
    "        'similarity_score': row['similarity_score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "matched_qids_df = pd.DataFrame(direct_matches_expanded + fuzzy_matches_expanded)\n",
    "\n",
    "# Save matched q_ids to CSV\n",
    "try:\n",
    "    matched_qids_df.to_csv(MATCHED_QIDS_CSV_PATH, index=False)\n",
    "    print(f\"Matched Q_ids saved to '{MATCHED_QIDS_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving matched Q_ids CSV: {e}\")\n",
    "\n",
    "# ---------------------- Summary ---------------------- #\n",
    "\n",
    "# Print a summary\n",
    "total_checked = len(comparison_df)\n",
    "total_mismatches = len(mismatches_df)\n",
    "print(f\"\\nTotal checks performed: {total_checked}\")\n",
    "print(f\"Total mismatches found: {total_mismatches}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}