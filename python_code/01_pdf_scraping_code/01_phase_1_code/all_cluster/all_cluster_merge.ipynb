{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of q_ids under each style:\n",
      "Style\n",
      "G    383\n",
      "O    160\n",
      "J     85\n",
      "H     42\n",
      "M     41\n",
      "N     31\n",
      "Q     29\n",
      "I      5\n",
      "K      4\n",
      "L      3\n",
      "Name: count, dtype: int64\n",
      "Phase status updated and saved as 'phase_status_updated.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the phase_status CSV file\n",
    "file_path = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/phase_status.csv'\n",
    "df = pd.read_csv(file_path, dtype={'q_id': str})\n",
    "\n",
    " \n",
    "\n",
    "# Define the lists of q_ids for each style (remove duplicates)\n",
    "style_g_qids = set(map(str, [\n",
    "943, 945, 946, 947, 951, 954, 955, 956, 962, 963, 964, 966, 972, 974, 975, 976, 987, 988, 989, 992, 997,\n",
    "1000, 1001, 1007, 1010, 1011, 1013, 1014, 1015, 1016, 1018, 1019, 1020, 1021, 1023, 1024, 1026, 1027, 1028,\n",
    "1029, 1030, 1031, 1032, 1033, 1035, 1036, 1037, 1038, 1040, 1043, 1045, 1046, 1047, 1048, 1049, 1050, 1051,\n",
    "1052, 1053, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1097, 1098, 1099, 1101, 1102, 1103, 1104,\n",
    "1105, 1106, 1107, 1108, 1109, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123,\n",
    "1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144,\n",
    "1146, 1147, 1148, 1149, 1152, 1153, 1154, 1156, 1157, 1158, 1159, 1160, 1162, 1163, 1164, 1165, 1168, 1169,\n",
    "1170, 1171, 1174, 1175, 1179, 1180, 1184, 1186, 1187, 1189, 1191, 1223, 1224, 1225, 1226, 1227, 1228, 1229,\n",
    "1230, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1248, 1249,\n",
    "1250, 1251, 1252, 1253, 1254, 1255, 1256, 1258, 1259, 1260, 1262, 1263, 1264, 1265, 1267, 1268, 1269, 1270,\n",
    "1271, 1272, 1273, 1275, 1277, 1278, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1291, 1292, 1293, 1294,\n",
    "1349, 1350, 1351, 1353, 1354, 1357, 1358, 1359, 1361, 1363, 1364, 1366, 1367, 1368, 1372, 1377, 1378, 1381,\n",
    "1382, 1383, 1384, 1385, 1389, 1390, 1391, 1392, 1394, 1395, 1397, 1398, 1427, 1428, 1429, 1430, 1431, 1432,\n",
    "1434, 1435, 1442, 1443, 1444, 1445, 1447, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459,\n",
    "1460, 1461, 1463, 1464, 1465, 1466, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1481, 1482,\n",
    "1483, 1484, 1485, 1488, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1498, 1499, 1500, 1501, 1502, 1503, 1504,\n",
    "1505, 1506, 1507, 1531, 1532, 1533, 1541, 1542, 1543, 1546, 1548, 1549, 1550, 1552, 1553, 1554, 1555, 1557,\n",
    "1558, 1559, 1561, 1564, 1565, 1566, 1568, 1573, 1574, 1578, 1581, 1582, 1584, 1586, 1587, 1590, 1591, 1592,\n",
    "1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690,\n",
    "1691, 1692, 1694, 1695, 1696, 1700, 1702, 1703, 1705, 1709, 1710, 1712, 1713, 1714, 1715, 1718, 1721, 1722,\n",
    "1724, 1726, 1728, 1729, 1731, 1732, 1733, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1747,\n",
    "1748, 1749, 1750, 1751\n",
    "\n",
    "]))\n",
    "\n",
    "style_n_qids = set(map(str, [\n",
    "    1656, 1657, 1658, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675,\n",
    "    1802, 1803, 1805, 1806, 1810, 1811, 1814, 1815, 1817, 1820, 1821, 1824, 1825, 1827\n",
    "]))\n",
    "\n",
    "style_o_qids = set(map(str, [\n",
    "    1831, 1837, 1838, 1840, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, \n",
    "    1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1871, 1872, 1874, 1875, 1876, 1877, \n",
    "    1878, 1879, 1880, 1881, 1882, 1883, 1884, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, \n",
    "    1897, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, \n",
    "    1916, 1917, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, \n",
    "    1935, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, \n",
    "    1959, 1960, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, \n",
    "    1978, 1979, 1980, 1983, 1987, 1988, 1992, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, \n",
    "    2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n",
    "]))\n",
    "\n",
    "style_q_qids = set(map(str, [\n",
    "    1832, 2153, 2154, 2155, 2156, 2157, 2161, 2162, 2163, 2165, 2166, 2167, 2168, 2169, 2170, 2172, 2173, \n",
    "    2175, 2176, 2177, 2178, 2180, 2181, 2182, 2184, 2185, 2186, 2187, 2188, 2192\n",
    "]))\n",
    "\n",
    " \n",
    "# Remove any previous G and O classifications that aren't in the provided lists\n",
    "df.loc[~df['q_id'].astype(str).isin(style_g_qids) & (df['Style'] == 'G'), 'Style'] = None\n",
    "df.loc[~df['q_id'].astype(str).isin(style_o_qids) & (df['Style'] == 'O'), 'Style'] = None\n",
    "\n",
    "# Update style values only for matching q_ids\n",
    "# Update the Style column only for matching q_ids\n",
    "df.loc[df['q_id'].astype(str).isin(style_g_qids), 'Style'] = 'G'\n",
    "df.loc[df['q_id'].astype(str).isin(style_n_qids), 'Style'] = 'N'\n",
    "df.loc[df['q_id'].astype(str).isin(style_o_qids), 'Style'] = 'O'\n",
    "df.loc[df['q_id'].astype(str).isin(style_q_qids), 'Style'] = 'Q'\n",
    "\n",
    "style_counts = df['Style'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Count of q_ids under each style:\")\n",
    "print(style_counts)\n",
    "\n",
    "# Save the updated dataframe back to CSV\n",
    "df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/phase_status_updated.csv', index=False)\n",
    "\n",
    "print(\"Phase status updated and saved as 'phase_status_updated.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_H_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 13/02_intermediate/costs_phase_1_cluster_13_style_G_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 13/02_intermediate/costs_phase_1_cluster_13_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/02_intermediate/costs_phase_1_cluster_14_style_others_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/02_intermediate/costs_phase_1_cluster_14_style_O_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/02_intermediate/costs_phase_1_cluster_14_style_missed_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 9/02_intermediate/costs_phase_1_cluster_9_style_G_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 9/02_intermediate/costs_phase_1_cluster_9_style_J_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 8/02_intermediate/costs_phase_1_cluster_8_style_G_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 12/02_intermediate/costs_phase_1_cluster_12_style_M_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 12/02_intermediate/costs_phase_1_cluster_12_style_G_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 10/02_intermediate/costs_phase_1_cluster_10_style_J_itemized_addendums.csv\n",
      "Processing: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_itemized_addendums.csv\n",
      "Addendum projects list saved to: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/all_clusters/addendum_projects_list.csv\n",
      "Total number of projects in addendum_projects_list: 144\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/01_clean/costs_phase_1_cluster_7_style_H_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/01_clean/costs_phase_1_cluster_7_style_G_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/01_clean/costs_phase_1_cluster_7_style_H_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/01_clean/costs_phase_1_cluster_7_style_G_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 13/01_clean/costs_phase_1_cluster_13_style_N_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 13/01_clean/costs_phase_1_cluster_13_style_N_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 13/01_clean/costs_phase_1_cluster_13_style_G_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 13/01_clean/costs_phase_1_cluster_13_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 13/01_clean/costs_phase_1_cluster_13_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 13/01_clean/costs_phase_1_cluster_13_style_G_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/01_clean/costs_phase_1_cluster_14_style_O_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/01_clean/costs_phase_1_cluster_14_style_Q_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/01_clean/costs_phase_1_cluster_14_style_others_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/01_clean/costs_phase_1_cluster_14_style_missed_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/01_clean/costs_phase_1_cluster_14_style_O_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/01_clean/costs_phase_1_cluster_14_style_missed_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/01_clean/costs_phase_1_cluster_14_style_others_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 14/01_clean/costs_phase_1_cluster_14_style_Q_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 9/01_clean/costs_phase_1_cluster_9_style_G_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 9/01_clean/costs_phase_1_cluster_9_style_G_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 9/01_clean/costs_phase_1_cluster_9_style_J_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 9/01_clean/costs_phase_1_cluster_9_style_J_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 8/01_clean/costs_phase_1_cluster_8_style_H_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 8/01_clean/costs_phase_1_cluster_8_style_H_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 8/01_clean/costs_phase_1_cluster_8_style_G_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 8/01_clean/costs_phase_1_cluster_8_style_G_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/01_clean/costs_phase_1_cluster_6_style_G_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/01_clean/costs_phase_1_cluster_6_style_H_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/01_clean/costs_phase_1_cluster_6_style_H_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6/01_clean/costs_phase_1_cluster_6_style_G_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 12/01_clean/costs_phase_1_cluster_12_style_G_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 12/01_clean/costs_phase_1_cluster_12_style_M_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 12/01_clean/costs_phase_1_cluster_12_style_G_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 12/01_clean/costs_phase_1_cluster_12_style_M_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 12/01_clean/costs_phase_1_cluster_12_style_N_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 12/01_clean/costs_phase_1_cluster_12_style_N_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/01_clean/costs_phase_1_cluster_1_4_style_R_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/01_clean/costs_phase_1_cluster_1_4_style_C_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/01_clean/costs_phase_1_cluster_1_4_style_R_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 3/01_clean/costs_phase_1_cluster_1_4_style_C_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 10/01_clean/costs_phase_1_cluster_10_style_J_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 10/01_clean/costs_phase_1_cluster_10_style_G_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 10/01_clean/costs_phase_1_cluster_10_style_G_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 10/01_clean/costs_phase_1_cluster_10_style_J_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 10/01_clean/costs_phase_1_cluster_10_style_10_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/01_clean/costs_phase_1_cluster_5_style_D_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/01_clean/costs_phase_1_cluster_5_style_E_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/01_clean/costs_phase_1_cluster_5_style_E_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 5/01_clean/costs_phase_1_cluster_5_style_D_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/01_clean/costs_phase_1_cluster_11_style_G_itemized_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/01_clean/costs_phase_1_cluster_11_style_J_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/01_clean/costs_phase_1_cluster_11_style_G_total_updated.csv\n",
      "Loading: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/01_clean/costs_phase_1_cluster_11_style_J_itemized_updated.csv\n",
      "Combined itemized dataset with 'original' column saved to: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/all_clusters/costs_phase_1_all_clusters_itemized.csv\n",
      "Combined total dataset with 'original' column saved to: /Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/all_clusters/costs_phase_1_all_clusters_total.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"original\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_time_to_construct\",\n",
    "        \"estimated_cost\",\n",
    "        \"escalated_cost\",\n",
    "        \"total_estimated_cost\",\n",
    "        \"total_escalated_cost\",\n",
    "        \n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df  \n",
    "\n",
    "\n",
    "def replace_text_with_zero(value):\n",
    "    \"\"\"\n",
    "    Cleans a value by removing percentage symbols, converting numeric values, and replacing text with zero.\n",
    "    Ignores NA values and retains original values if conversion fails.\n",
    "\n",
    "    Args:\n",
    "        value (str/int/float): The value to be processed.\n",
    "\n",
    "    Returns:\n",
    "        int/float/str: Cleaned numeric value, original value if conversion fails, or NA if value is missing.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):  # Ignore NA values and return as is\n",
    "        return value  \n",
    "\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "\n",
    "        # Remove percentage symbols and convert to a numeric value\n",
    "        if value.endswith('%'):\n",
    "            value = value.replace('%', '')  # Remove % symbol\n",
    "\n",
    "        # If value contains alphabetic characters, consider it as non-numeric and return 0\n",
    "        if re.search(r'[a-zA-Z]', value):\n",
    "            return 0\n",
    "        \n",
    " \n",
    "\n",
    "    try:\n",
    "        return pd.to_numeric(value, errors='coerce') if value != '' else value\n",
    "    except (ValueError, TypeError):\n",
    "        return value  # Keep the original value if conversion fails\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "columns_to_clean = ['cost_allocation_factor', 'estimated_cost']        \n",
    "\n",
    "\n",
    "def clean_text(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing unwanted characters and patterns,\n",
    "    such as '$', '*', and text like '6months' while keeping numeric ranges (e.g., '6-24') intact.\n",
    "    \n",
    "    Args:\n",
    "        value (str): The value to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "        float/int/str: Cleaned numeric value or original string if numeric patterns are detected.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Remove unwanted characters like $, * and \"(Note 2)\" references\n",
    "        \n",
    "     \n",
    "        \n",
    "        # Replace \"6months\", \"12months\" etc., while preserving numeric ranges like \"6-24\"\n",
    "        value = re.sub(r'(\\d+)months', r'\\1', value, flags=re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        return pd.to_numeric(value)  # Convert to numeric type where possible\n",
    "    except ValueError:\n",
    "        return value  # Return the cleaned string if conversion fails\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        #value = re.sub(r'months','', value)\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "\n",
    "\n",
    "def create_addendum_list_and_mark_original(root_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Collects all q_ids from itemized_addendums files in each cluster folder, creates an addendum_projects_list.csv,\n",
    "    and adds an 'original' column to the combined itemized and total datasets based on the presence in the addendum list.\n",
    "\n",
    "    Args:\n",
    "        root_folder (str): Path to the root folder containing cluster folders.\n",
    "        output_folder (str): Path to save the addendum_projects_list.csv and combined datasets.\n",
    "    \"\"\"\n",
    "    addendum_qids = []\n",
    "\n",
    "    # Step 1: Collect all q_ids from itemized_addendums files\n",
    "    for cluster_folder in os.listdir(root_folder):\n",
    "        cluster_path = os.path.join(root_folder, cluster_folder)\n",
    "        if os.path.isdir(cluster_path):  # Ensure it's a directory\n",
    "            intermediate_folder = os.path.join(cluster_path, \"02_intermediate\")\n",
    "            if os.path.exists(intermediate_folder):  # Check if 02_intermediate exists\n",
    "                for file_name in os.listdir(intermediate_folder):\n",
    "                    if \"itemized_addendums.csv\" in file_name:\n",
    "                        file_path = os.path.join(intermediate_folder, file_name)\n",
    "                        print(f\"Processing: {file_path}\")\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        if 'q_id' in df.columns:\n",
    "                            addendum_qids.extend(df['q_id'].dropna().astype(int).unique())  # Convert to integers\n",
    "\n",
    "    # Remove duplicates from addendum_qids and sort them\n",
    "    addendum_qids = sorted(set(addendum_qids))\n",
    "\n",
    "    # Save addendum_projects_list.csv\n",
    "    addendum_list_df = pd.DataFrame({'q_id': addendum_qids})\n",
    "    addendum_list_file = os.path.join(output_folder, \"addendum_projects_list.csv\")\n",
    "    addendum_list_df.to_csv(addendum_list_file, index=False)\n",
    "    print(f\"Addendum projects list saved to: {addendum_list_file}\")\n",
    "    print(f\"Total number of projects in addendum_projects_list: {len(addendum_qids)}\")\n",
    "\n",
    "    # Step 2: Combine all itemized_updated and total_updated files\n",
    "    combined_itemized = []\n",
    "    combined_total = []\n",
    "\n",
    "    for cluster_folder in os.listdir(root_folder):\n",
    "        cluster_path = os.path.join(root_folder, cluster_folder)\n",
    "        if os.path.isdir(cluster_path):  # Ensure it's a directory\n",
    "            clean_folder = os.path.join(cluster_path, \"01_clean\")\n",
    "            if os.path.exists(clean_folder):  # Check if 01_clean exists\n",
    "                for file_name in os.listdir(clean_folder):\n",
    "                    file_path = os.path.join(clean_folder, file_name)\n",
    "                    if \"itemized_updated.csv\" in file_name:\n",
    "                        print(f\"Loading: {file_path}\")\n",
    "                        itemized_df = pd.read_csv(file_path)\n",
    "                        combined_itemized.append(itemized_df)\n",
    "                    elif \"total_updated.csv\" in file_name:\n",
    "                        print(f\"Loading: {file_path}\")\n",
    "                        total_df = pd.read_csv(file_path)\n",
    "                        combined_total.append(total_df)\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "    # Combine all itemized datasets\n",
    "    if combined_itemized:\n",
    "        combined_itemized_df = pd.concat(combined_itemized, ignore_index=True)\n",
    "\n",
    "        # Convert q_id to integers\n",
    "        combined_itemized_df['q_id'] = combined_itemized_df['q_id'].fillna(0).astype(str)\n",
    "        \n",
    "\n",
    "        # Add 'original' column to itemized dataset\n",
    "        combined_itemized_df['original'] = combined_itemized_df['q_id'].apply(\n",
    "            lambda qid: 'no' if qid in addendum_qids else 'yes'\n",
    "        )\n",
    "\n",
    "        # Reorder columns to place 'original' next to 'q_id'\n",
    "        if 'q_id' in combined_itemized_df.columns and 'original' in combined_itemized_df.columns:\n",
    "            cols = list(combined_itemized_df.columns)\n",
    "            cols.insert(cols.index('q_id') + 1, cols.pop(cols.index('original')))\n",
    "            combined_itemized_df = combined_itemized_df[cols]\n",
    "\n",
    "        # Sort by q_id\n",
    "        combined_itemized_df.drop(['estimate_d_time_to_construc_t','estimated_cost_x_1000_escalated_with_itcca' ,'adnu_cost_rate_x_1000_escalated',  'potential_duration_months',\n",
    "                                   'none_7', 'none_8', 'network_upgrade_type', 'adnu_cost_rate_escalated_x_1000','upgrade_classification', 'sum_of_reallocated_share',\t'sum_of_reallocated_cost_x_1000_constant_dollar_2022',\n",
    "                                       \t'sum_of_reallocated_costs_x_1000_escalated_constant_dollars_od_year',\n",
    "                                   'ttype_of_upgrade','adnu_cost_rate_x_1000'], axis=1, inplace=True)\n",
    "        combined_itemized_df.rename(columns={'estimated_cost_x_1000': 'estimated_cost', 'escalated_cost_x_1000': 'escalated_cost', 'total_estimated_cost_x_1000': 'total_estimated_cost',\n",
    "                                             'total_estimated_cost_x_1000_escalated':'total_escalated_cost',}, inplace=True)\n",
    "        \n",
    "        for col in ['estimated_cost', 'escalated_cost', 'total_estimated_cost', 'total_escalated_cost', ]:\n",
    "            if col in combined_itemized_df.columns:\n",
    "                combined_itemized_df[col] = combined_itemized_df[col].apply(clean_currency)\n",
    "\n",
    "\n",
    "        for col in ['estimated_time_to_construct']:\n",
    "            if col in combined_itemized_df.columns:\n",
    "                combined_itemized_df[col]=combined_itemized_df[col].apply(clean_text)   \n",
    "\n",
    "        for col in columns_to_clean:\n",
    "            if col in combined_itemized_df.columns:\n",
    "                combined_itemized_df[col] = combined_itemized_df[col].apply(replace_text_with_zero)      \n",
    " \n",
    "        \n",
    "        combined_itemized_df=reorder_columns(combined_itemized_df)\n",
    "\n",
    "\n",
    "        combined_itemized_df = combined_itemized_df.sort_values(by=\"q_id\", kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "        # Save the combined itemized dataset\n",
    "        combined_itemized_file = os.path.join(output_folder, \"costs_phase_1_all_clusters_itemized.csv\")\n",
    "        combined_itemized_df.to_csv(combined_itemized_file, index=False)\n",
    "        print(f\"Combined itemized dataset with 'original' column saved to: {combined_itemized_file}\")\n",
    "\n",
    "    # Combine all total datasets\n",
    "    if combined_total:\n",
    "        combined_total_df = pd.concat(combined_total, ignore_index=True)\n",
    "\n",
    "        # Convert q_id to integers\n",
    "        combined_total_df['q_id'] = combined_total_df['q_id'].fillna(0).astype(str)\n",
    "\n",
    "        # Add 'original' column to total dataset\n",
    "        combined_total_df['original'] = combined_total_df['q_id'].apply(\n",
    "            lambda qid: 'no' if qid in addendum_qids else 'yes'\n",
    "        )\n",
    "\n",
    "        # Reorder columns to place 'original' next to 'q_id'\n",
    "        if 'q_id' in combined_total_df.columns and 'original' in combined_total_df.columns:\n",
    "            cols = list(combined_total_df.columns)\n",
    "            cols.insert(cols.index('q_id') + 1, cols.pop(cols.index('original')))\n",
    "            combined_total_df = combined_total_df[cols]\n",
    "\n",
    "        combined_total_df.drop(['estimate_d_time_to_construc_t','estimated_cost_x_1000_escalated_with_itcca' ,'adnu_cost_rate_x_1000_escalated',  'potential_duration_months',\n",
    "                                   'none_7', 'none_8', 'network_upgrade_type', 'adnu_cost_rate_escalated_x_1000','upgrade_classification', 'sum_of_reallocated_share',\t'sum_of_reallocated_cost_x_1000_constant_dollar_2022',\n",
    "                                       \t'sum_of_reallocated_costs_x_1000_escalated_constant_dollars_od_year',\n",
    "                                   'ttype_of_upgrade','adnu_cost_rate_x_1000'], axis=1, inplace=True)\n",
    "        combined_total_df.rename(columns={'estimated_cost_x_1000': 'estimated_cost', 'escalated_cost_x_1000': 'escalated_cost', 'total_estimated_cost_x_1000': 'total_estimated_cost',\n",
    "                                             'total_estimated_cost_x_1000_escalated':'total_escalated_cost',}, inplace=True)\n",
    "        \n",
    "        for col in ['estimated_cost', 'escalated_cost', 'total_estimated_cost', 'total_escalated_cost', ]:\n",
    "            if col in combined_total_df.columns:\n",
    "                combined_total_df[col] = combined_total_df[col].apply(clean_currency)\n",
    "\n",
    "        for col in ['estimated_time_to_construct']:\n",
    "            if col in combined_total_df.columns:\n",
    "                combined_total_df[col]=combined_total_df[col].apply(clean_text)  \n",
    "                \n",
    "        for col in columns_to_clean:\n",
    "            if col in combined_total_df.columns:\n",
    "                combined_total_df[col] = combined_total_df[col].apply(replace_text_with_zero)      \n",
    "\n",
    "         \n",
    "\n",
    "        # Columns to clean by converting text to 0\n",
    "        #columns_to_clean = ['cost_allocation_factor', 'estimated_cost']\n",
    "\n",
    "        # Convert non-numeric values to 0\n",
    "        #combined_total_df[columns_to_clean] = combined_total_df[columns_to_clean].apply(pd.to_numeric, errors='coerce').fillna(0)        \n",
    "        \n",
    " \n",
    "        \n",
    "        combined_total_df=reorder_columns(combined_total_df)    \n",
    "\n",
    "        # Sort by q_id\n",
    "        combined_total_df = combined_total_df.sort_values(by=\"q_id\", kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "        # Save the combined total dataset\n",
    "        combined_total_file = os.path.join(output_folder, \"costs_phase_1_all_clusters_total.csv\")\n",
    "        combined_total_df.to_csv(combined_total_file, index=False)\n",
    "        print(f\"Combined total dataset with 'original' column saved to: {combined_total_file}\")\n",
    "\n",
    "# Define the root folder and output folder\n",
    "root_folder = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/\"  # Update with your root folder path\n",
    "output_folder = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/all_clusters\"  # Update with your output folder path\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Run the function\n",
    "create_addendum_list_and_mark_original(root_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
