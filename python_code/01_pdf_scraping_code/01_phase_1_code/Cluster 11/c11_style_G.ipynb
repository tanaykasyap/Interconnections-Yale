{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for a subset of cluster 11 projects to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing PDF: /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1459/02_phase_1_study/Q1459_Mulqueeney_Ranch_Wind_OCR.pdf\n",
      "Extracting base data from PDF...\n",
      "Extracted Queue ID: 1459\n",
      "Extracted Cluster Number: 11\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: 60\n",
      "\n",
      "Processing /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1459/02_phase_1_study/Q1459_Mulqueeney_Ranch_Wind_OCR.pdf for Table 1 extraction...\n",
      "Table 1 starts on page 5 and ends on page 6\n",
      "\n",
      "Scraping tables on page 5 for Table 1...\n",
      "\n",
      "Attempt 1 with table settings: {'horizontal_strategy': 'text', 'vertical_strategy': 'lines', 'snap_tolerance': 1}\n",
      "Found 0 table(s) on page 5 with current settings.\n",
      "\n",
      "Attempt 2 with table settings: {'horizontal_strategy': 'lines', 'vertical_strategy': 'lines', 'snap_tolerance': 2}\n",
      "Found 0 table(s) on page 5 with current settings.\n",
      "\n",
      "Scraping tables on page 6 for Table 1...\n",
      "\n",
      "Attempt 1 with table settings: {'horizontal_strategy': 'text', 'vertical_strategy': 'lines', 'snap_tolerance': 1}\n",
      "Found 0 table(s) on page 6 with current settings.\n",
      "\n",
      "Attempt 2 with table settings: {'horizontal_strategy': 'lines', 'vertical_strategy': 'lines', 'snap_tolerance': 2}\n",
      "Found 0 table(s) on page 6 with current settings.\n",
      "Point of Interconnection not found in Table 1.\n",
      "Searching for GPS coordinates...\n",
      "Found directional GPS coordinates: ('37.708941', '-121.583837')\n",
      "Base data extracted: {'q_id': ['1459'], 'cluster': ['11'], 'req_deliverability': ['Full'], 'latitude': ['37.708941'], 'longitude': ['-121.583837'], 'capacity': [60], 'point_of_interconnection': [None]}\n",
      "\n",
      "Processing /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1459/02_phase_1_study/Q1459_Mulqueeney_Ranch_Wind_OCR.pdf for Table 7 extraction...\n",
      "Table 7 starts on page 15 and ends on page 19\n",
      "\n",
      "Scraping tables on page 15...\n",
      "\n",
      "Scraping tables on page 16...\n",
      "\n",
      "Scraping tables on page 17...\n",
      "\n",
      "Scraping tables on page 18...\n",
      "\n",
      "Scraping tables on page 19...\n",
      "No Table 7 data extracted.\n",
      "No Table 7 data found in /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1459/02_phase_1_study/Q1459_Mulqueeney_Ranch_Wind_OCR.pdf. Using base data only.\n",
      "\n",
      "Columns reordered as per specification.\n",
      "\n",
      "Final combined DataFrame:\n",
      "\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n",
      "\u2502   q_id \u2502   cluster \u2502 req_deliverability   \u2502   latitude \u2502   longitude \u2502   capacity \u2502 point_of_interconnection   \u2502\n",
      "\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n",
      "\u2502   1459 \u2502        11 \u2502 Full                 \u2502    37.7089 \u2502    -121.584 \u2502         60 \u2502                            \u2502\n",
      "\u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_19256/1105327861.py:651: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  core = core.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "from tabulate import tabulate\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-10\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def search_gps_coordinates(text):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    print(\"Searching for GPS coordinates...\")\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\")\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\")\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\")\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\")\n",
    "    return (None, None)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def extract_table1(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\")\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Identify all pages that contain \"Table 1\"\n",
    "        table1_pages = []\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text() or \"\"\n",
    "            if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                table1_pages.append(i)\n",
    "\n",
    "        if not table1_pages:\n",
    "            print(\"No Table 1 found in the PDF.\")\n",
    "            return None  # Return None if no Table 1 found\n",
    "\n",
    "        first_page = table1_pages[0]\n",
    "        last_page = table1_pages[-1]\n",
    "        scrape_start = first_page\n",
    "        scrape_end = last_page + 1  # Plus one to include the next page if needed\n",
    "\n",
    "        print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\")\n",
    "\n",
    "        # Flag to indicate if extraction was successful\n",
    "        extraction_successful = False\n",
    "\n",
    "        # Iterate through the specified page range\n",
    "        for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "            page = pdf.pages[page_number]\n",
    "            print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\")\n",
    "\n",
    "            for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                print(f\"\\nAttempt {attempt} with table settings: {table_settings}\")\n",
    "                tables = page.find_tables(table_settings=table_settings)\n",
    "                print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\")\n",
    "\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\")\n",
    "                        continue  # Skip empty tables\n",
    "\n",
    "                    print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\")\n",
    "                    for row_num, row in enumerate(tab, start=1):\n",
    "                        print(f\"Row {row_num}: {row}\")\n",
    "\n",
    "                    # Iterate through each row in the table\n",
    "                    for row_index, row in enumerate(tab, start=1):\n",
    "                        # Iterate through each cell in the row\n",
    "                        for cell_index, cell in enumerate(row, start=1):\n",
    "                            if cell and poi_pattern.search(cell):\n",
    "                                # Assuming the next column contains the value\n",
    "                                poi_col_index = cell_index  # 1-based index\n",
    "                                adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                if adjacent_col_index <= len(row):\n",
    "                                    poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                    if poi_value:  # Check if the value is not empty\n",
    "                                        point_of_interconnection = poi_value\n",
    "                                        print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\")\n",
    "                                        extraction_successful = True\n",
    "                                        break  # Exit the cell loop\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\")\n",
    "                                        # Proceed to scan surrounding rows for the value\n",
    "                                        poi_value_parts = []\n",
    "\n",
    "                                        # Define the range to scan: two rows above and two rows below\n",
    "                                        # Convert to 0-based index\n",
    "                                        current_row_idx = row_index - 1\n",
    "                                        start_scan = max(0, current_row_idx - 2)\n",
    "                                        end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                        print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\")\n",
    "\n",
    "                                        for scan_row_index in range(start_scan, end_scan):\n",
    "                                            # Skip the current row where the label was found\n",
    "                                            if scan_row_index == current_row_idx:\n",
    "                                                continue\n",
    "\n",
    "                                            scan_row = tab[scan_row_index]\n",
    "                                            # Ensure the adjacent column exists in the scan row\n",
    "                                            if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                    poi_value_parts.append(scan_cell)\n",
    "                                                    print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\")\n",
    "                                                elif poi_pattern.search(scan_cell):\n",
    "                                                    # If another POI label is found, skip it\n",
    "                                                    print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\")\n",
    "                                                    continue\n",
    "\n",
    "                                        if poi_value_parts:\n",
    "                                            # Concatenate the parts to form the complete POI value\n",
    "                                            point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                            print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index})\")\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\")\n",
    "                                            # Do not return immediately; proceed to retry\n",
    "                                else:\n",
    "                                    print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                          f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\")\n",
    "                                    # Do not return immediately; proceed to retry\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the row loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the table loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the attempt loop\n",
    "            if extraction_successful:\n",
    "                break  # Exit the page loop\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\")\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\")\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_base_data(pdf_path, project_id):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\")\n",
    "    with open(pdf_path, 'rb') as pdf:\n",
    "        reader = PyPDF2.PdfReader(pdf)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text\n",
    "\n",
    "    text = clean_string_cell(text)\n",
    "\n",
    "    queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "    queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "    print(f\"Extracted Queue ID: {queue_id}\")\n",
    "\n",
    "    cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "    cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "    print(f\"Extracted Cluster Number: {cluster_number}\")\n",
    "\n",
    "    deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "    deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "    print(f\"Extracted Deliverability Status: {deliverability_status}\")\n",
    "\n",
    "    # Extract Capacity\n",
    "    capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "    if capacity:\n",
    "        capacity = int(capacity.group(1))\n",
    "    else:\n",
    "        capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "        capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "    print(f\"Extracted Capacity: {capacity}\")\n",
    "\n",
    "    # Removed Point of Interconnection extraction as per instruction\n",
    "    point_of_interconnection = extract_table1(pdf_path)\n",
    "    #point_of_interconnection = None\n",
    "    latitude, longitude = search_gps_coordinates(text)\n",
    "\n",
    "    # Initialize base data dictionary\n",
    "    base_data = {\n",
    "        \"q_id\": [queue_id],\n",
    "        \"cluster\": [cluster_number],\n",
    "        \"req_deliverability\": [deliverability_status],\n",
    "        \"latitude\": [latitude],\n",
    "        \"longitude\": [longitude],\n",
    "        \"capacity\": [capacity],\n",
    "        \"point_of_interconnection\": [point_of_interconnection]\n",
    "    }\n",
    "\n",
    "    print(\"Base data extracted:\", base_data)\n",
    "    return pd.DataFrame(base_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_table7(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\")\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None  # To store the specific phrase from the table title\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Identify all pages that contain \"Table 7\" or \"Table 7-\"\n",
    "        table7_pages = []\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text() or \"\"\n",
    "            if re.search(r\"Table\\s*7[-\\s]\", text, re.IGNORECASE): #[-\\s]\n",
    "                table7_pages.append(i)\n",
    "\n",
    "        if not table7_pages:\n",
    "            print(\"No Table 7 found in the PDF.\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame if no Table 7 found\n",
    "\n",
    "        first_page = table7_pages[0]\n",
    "        last_page = table7_pages[-1]\n",
    "        scrape_start = first_page\n",
    "        scrape_end = last_page + 1  # Plus one to include the next page\n",
    "\n",
    "        print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\")\n",
    "\n",
    "        # Iterate through the specified page range\n",
    "        for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "            page = pdf.pages[page_number]\n",
    "            print(f\"\\nScraping tables on page {page_number + 1}...\")\n",
    "            tables = page.find_tables(table_settings={\n",
    "                \"horizontal_strategy\": \"lines\",\n",
    "                \"vertical_strategy\": \"lines\",\n",
    "            })\n",
    "\n",
    "            for table_index, table in enumerate(tables):\n",
    "                tab = table.extract()\n",
    "                if not tab:\n",
    "                    print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\")\n",
    "                    continue  # Skip empty tables\n",
    "\n",
    "                # Get the bounding box of the current table\n",
    "                table_bbox = table.bbox  # (x0, y0, x1, y1)\n",
    "\n",
    "                # Define a bounding box from the top of the page to the top of the table\n",
    "                # Coordinates: (x0, y0, x1, y1)\n",
    "                title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "\n",
    "                # Extract text within the title bounding box\n",
    "                title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "\n",
    "                # Initialize table_title as None\n",
    "                table_title = None\n",
    "\n",
    "                if title_text:\n",
    "                    # Split the title text into lines and reverse to start checking from the closest line to the table\n",
    "                    title_lines = title_text.split('\\n')[::-1]\n",
    "\n",
    "                    for line in title_lines:\n",
    "                        line = line.strip()\n",
    "                        # Check if the line matches the table title pattern (e.g., \"Table 7-1: PTO Upgrade\")\n",
    "                        match = re.match(r\"Table\\s*7[-.]\\d+[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                        if match:\n",
    "                            table_title = match.group(1).strip()\n",
    "                            break  # Stop after finding the closest table title\n",
    "\n",
    "                if table_title:\n",
    "                    # New Table 7 detected\n",
    "                    specific_phrase = extract_specific_phrase(table_title)  # Extracted phrase from the title\n",
    "                    print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\")\n",
    "\n",
    "                    # Extract headers and data rows\n",
    "                    headers = clean_column_headers(tab[0])\n",
    "                    data_rows = tab[1:]\n",
    "\n",
    "                    # Create DataFrame for the new table\n",
    "                    try:\n",
    "                        df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                    except ValueError as ve:\n",
    "                        print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\")\n",
    "                        continue  # Skip this table due to error\n",
    "\n",
    "                    # Special Handling for \"Area Delivery Network Upgrade\" Tables\n",
    "                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                        print(\"Detected 'Area Delivery Network Upgrade' table.\")\n",
    "                        if \"adnu\" in df_new.columns:\n",
    "                            # Rename 'adnu' to 'upgrade'\n",
    "                            if \"upgrade\" in df_new.columns:\n",
    "                                # Avoid duplicate 'upgrade' columns\n",
    "                                df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                print(\"Dropped 'ADNU' column to avoid duplicate 'upgrade' columns.\")\n",
    "                            else:\n",
    "                                df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                print(\"Renamed 'ADNU' to 'upgrade'.\")\n",
    "                        if \"type of upgrade\" not in df_new.columns:\n",
    "                            df_new[\"type of upgrade\"] = specific_phrase\n",
    "                            print(\"Added 'type of upgrade' column with specific phrase.\")\n",
    "                    else:\n",
    "                        # General Handling for other tables\n",
    "                        if \"type of upgrade\" in df_new.columns:\n",
    "                            first_row = df_new.iloc[0]\n",
    "                            if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row on page {page_number + 1}, table {table_index + 1}\")\n",
    "                                df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        else:\n",
    "                            # If \"Type of Upgrade\" column does not exist, add it\n",
    "                            df_new[\"type of upgrade\"] = specific_phrase\n",
    "                            print(f\"'Type of Upgrade' column added with value '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\")\n",
    "\n",
    "                    # Ensure no duplicate columns\n",
    "                    if df_new.columns.duplicated().any():\n",
    "                        print(f\"Duplicate columns detected in new table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\")\n",
    "                        df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                    # Append the new table to the list\n",
    "                    extracted_tables.append(df_new)\n",
    "                else:\n",
    "                    # Continuation Table detected\n",
    "                    if specific_phrase is None:\n",
    "                        print(f\"No previous Table 7 title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\")\n",
    "                        continue  # Skip if no previous title to refer to\n",
    "\n",
    "                    print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\")\n",
    "\n",
    "                    # Treat all rows as data without headers\n",
    "                    data_rows = tab\n",
    "\n",
    "                    # Check if the number of columns matches\n",
    "                    expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                    if expected_columns is None:\n",
    "                        print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\")\n",
    "                        continue  # No table to continue with\n",
    "\n",
    "                    # Define expected columns based on the last extracted table\n",
    "                    expected_headers = extracted_tables[-1].columns.tolist()\n",
    "\n",
    "                    # Define the list of column headers to check in continuation tables\n",
    "                    header_keywords = [\"type of upgrade\", \"adnu\"]\n",
    "\n",
    "                    # Check if the first row contains any header keywords\n",
    "                    first_continuation_row = data_rows[0] if len(data_rows) > 0 else []\n",
    "                    is_header_row = any(\n",
    "                        re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                    )\n",
    "\n",
    "                    if is_header_row:\n",
    "                        print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping the header row.\")\n",
    "                        data_rows = data_rows[1:]  # Drop the header row\n",
    "\n",
    "                    # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                        print(\"Handling continuation for 'Area Delivery Network Upgrade' table.\")\n",
    "                        if \"adnu\" in data_rows[0]:\n",
    "                            # Rename 'ADNU' to 'upgrade' in continuation table.\n",
    "                            print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\")\n",
    "                            data_rows = [ [\"upgrade\"] + row[1:] for row in data_rows ]\n",
    "                        if \"type of upgrade\" not in expected_headers:\n",
    "                            # Insert 'upgrade' column at the beginning\n",
    "                            print(\"Inserting 'type of upgrade' column with specific phrase in continuation table.\")\n",
    "                            data_rows = [ [specific_phrase] + row for row in data_rows ]\n",
    "\n",
    "                    # Handle missing or extra columns\n",
    "                    if len(data_rows[0]) < expected_columns:\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            # For ADNU tables, assume missing \"upgrade\" column\n",
    "                            print(f\"Detected missing 'upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'upgrade' column.\")\n",
    "                            data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                        else:\n",
    "                            # For other tables, assume missing \"Type of Upgrade\" column\n",
    "                            print(f\"Detected missing 'Type of Upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'Type of Upgrade' column.\")\n",
    "                            data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                    elif len(data_rows[0]) > expected_columns:\n",
    "                        # Extra columns detected; adjust accordingly\n",
    "                        print(f\"Detected extra columns in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping extra columns.\")\n",
    "                        data_rows = [row[:expected_columns] for row in data_rows]\n",
    "\n",
    "                    # Create DataFrame for the continuation table\n",
    "                    try:\n",
    "                        df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                    except ValueError as ve:\n",
    "                        print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\")\n",
    "                        continue  # Skip this table due to error\n",
    "\n",
    "                    # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                        if \"type of upgrade\" in df_continuation.columns:\n",
    "                            first_row = df_continuation.iloc[0]\n",
    "                            if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\")\n",
    "                                df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        else:\n",
    "                            # If \"type of upgrade\" column does not exist, add it\n",
    "                            df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                            print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\")\n",
    "                    else:\n",
    "                        # General Handling for other tables\n",
    "                        if \"type of upgrade\" in df_continuation.columns:\n",
    "                            first_row = df_continuation.iloc[0]\n",
    "                            if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\")\n",
    "                                df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        else:\n",
    "                            # If \"Type of Upgrade\" column does not exist, add it\n",
    "                            df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                            print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\")\n",
    "\n",
    "                    # Ensure no duplicate columns\n",
    "                    if df_continuation.columns.duplicated().any():\n",
    "                        print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\")\n",
    "                        df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                    # Merge with the last extracted table\n",
    "                    extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "\n",
    "    # After scraping all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        # To prevent reindexing errors, ensure all DataFrames have unique and consistent columns\n",
    "        # Collect all unique column names\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "        #all_columns = sorted(all_columns)  # Optional: sort columns for consistency\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            # Reindex to have all columns, filling missing ones with NaN\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\")\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\")\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\")\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    pdfs = [    \n",
    "       #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/947/02_phase_1_study/Q947 GV Solar 3_C6Ph I_Appendix A-Individual Study Report_Final.pdf\",\n",
    "      # \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1010/02_phase_1_study/C7PhI Appendix A - Q1010 Dyer Summit Wind Repower.pdf\",\n",
    "      # \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1021/02_phase_1_study/Q1021_StocktonBiomass_PHI_Report-Addendum1.pdf\",\n",
    "       # \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1030/02_phase_1_study/C7PhI - Appendix A - Q1030   South Lake Solar.pdf\",\n",
    "       # \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1035/02_phase_1_study/C7PhI Appendix A - Q1035 Magellan Solar Final.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1098/02_phase_1_study/C8 Ph I Appendix A - Q1098 Birds Landing Wind.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1102/02_phase_1_study/C8 Ph I Appendix A - Q1102 Cornwall Energy Storage.pdf\",\n",
    "       # \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1130/02_phase_1_study/Q1130 Moon Prism 2_Appendix A Report_C8PhI.pdf\",\n",
    "\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1133/02_phase_1_study/Q1133 Oveja Solar Farm_Appendix A Report_C8PhI.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1158/02_phase_1_study/Q1158 Slate_Appendix A Report_C8PhI.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1170/02_phase_1_study/QC8PhI_Q1170_Gateway Energy Storage_Appendix A_1-15-2016.pdf\",\n",
    "        \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1459/02_phase_1_study/Q1459_Mulqueeney_Ranch_Wind_OCR.pdf\"\n",
    "    ]\n",
    "\n",
    "    # Initialize an empty DataFrame to store all results\n",
    "    core = pd.DataFrame()\n",
    "\n",
    "    for pdf_path in pdfs:\n",
    "        print(f\"\\nProcessing PDF: {pdf_path}\")\n",
    "        # Extract base data\n",
    "        project_id = os.path.basename(pdf_path).split('.')[0]  # Modify as appropriate\n",
    "        base_data = extract_base_data(pdf_path, project_id)\n",
    "\n",
    "        # Extract Table 7 data\n",
    "        table7_data = extract_table7(pdf_path)\n",
    "\n",
    "        if table7_data.empty:\n",
    "            print(f\"No Table 7 data found in {pdf_path}. Using base data only.\")\n",
    "            merged_df = base_data\n",
    "        else:\n",
    "            # Merge base data with Table 7 data\n",
    "            overlapping_columns = base_data.columns.intersection(table7_data.columns)\n",
    "            table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "            base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "            try:\n",
    "                merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "                print(f\"Merged base data with Table 7 data for {pdf_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\")\n",
    "                merged_df = base_data  # Fallback to base data only\n",
    "\n",
    "        # Append to core\n",
    "        try:\n",
    "            core = pd.concat([core, merged_df], ignore_index=True, sort=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error appending data from {pdf_path} to core DataFrame: {e}\")\n",
    "\n",
    "    # Clean up the entire DataFrame\n",
    "    core = core.applymap(clean_string_cell)\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    core = reorder_columns(core)\n",
    "    print(\"\\nColumns reordered as per specification.\")\n",
    "\n",
    "    # Display the final combined DataFrame\n",
    "    print(\"\\nFinal combined DataFrame:\")\n",
    "    print(tabulate(core, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "\n",
    "    # Save to CSV if needed\n",
    "    # core.to_csv('final_data.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This PDF is marked as a Tagged PDF. This often indicates that the PDF was generated from an office document and does not need OCR. PDF pages processed by OCRmyPDF may not be tagged correctly.\n",
      "Start processing 8 pages concurrently\n",
      "    1 page already has text! - rasterizing text and running OCR anyway\n",
      "    2 page already has text! - rasterizing text and running OCR anyway\n",
      "    3 page already has text! - rasterizing text and running OCR anyway\n",
      "    4 page already has text! - rasterizing text and running OCR anyway\n",
      "    5 page already has text! - rasterizing text and running OCR anyway\n",
      "    6 page already has text! - rasterizing text and running OCR anyway\n",
      "    7 page already has text! - rasterizing text and running OCR anyway\n",
      "    8 page already has text! - rasterizing text and running OCR anyway\n",
      "    9 page already has text! - rasterizing text and running OCR anyway\n",
      "   10 page already has text! - rasterizing text and running OCR anyway\n",
      "   11 page already has text! - rasterizing text and running OCR anyway\n",
      "   12 page already has text! - rasterizing text and running OCR anyway\n",
      "   13 page already has text! - rasterizing text and running OCR anyway\n",
      "   14 page already has text! - rasterizing text and running OCR anyway\n",
      "   15 page already has text! - rasterizing text and running OCR anyway\n",
      "   16 page already has text! - rasterizing text and running OCR anyway\n",
      "   17 page already has text! - rasterizing text and running OCR anyway\n",
      "   18 page already has text! - rasterizing text and running OCR anyway\n",
      "   19 page already has text! - rasterizing text and running OCR anyway\n",
      "   20 page already has text! - rasterizing text and running OCR anyway\n",
      "   21 page already has text! - rasterizing text and running OCR anyway\n",
      "   22 page already has text! - rasterizing text and running OCR anyway\n",
      "Postprocessing...\n",
      "Image optimization ratio: 1.05 savings: 4.5%\n",
      "Total file size ratio: 0.29 savings: -244.5%\n",
      "The output file size is 3.45\u00d7 larger than the input file.\n",
      "Possible reasons for this include:\n",
      "--force-ocr was issued, causing transcoding.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ocrmypdf', '--force-ocr', '--output-type', 'pdf', '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1656/02_phase_1_study/QC12PhI_Q1656_Gypsy_ESS_Appendix_A_01-15-2020.pdf', '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1656/02_phase_1_study/QC12PhI_Q1656_Gypsy_ESS_Appendix_A_01-15-2020_OCR.pdf'], returncode=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "input_pdf = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1656/02_phase_1_study/QC12PhI_Q1656_Gypsy_ESS_Appendix_A_01-15-2020.pdf\"\n",
    "output_pdf = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1656/02_phase_1_study/QC12PhI_Q1656_Gypsy_ESS_Appendix_A_01-15-2020_OCR.pdf\"\n",
    "\n",
    "# Run ocrmypdf\n",
    "subprocess.run([\"ocrmypdf\", \"--force-ocr\", \"--output-type\", \"pdf\", input_pdf, output_pdf])\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1459,1470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Text:\n",
      "Addendum to Appendix A \u2013\n",
      "Q1470\n",
      "Origis Operating Services, LLC\n",
      "Caballero Storage\n",
      "Addendum #1 To The\n",
      "Cluster 11 Phase II Final Report\n",
      "January 31, 2020\n",
      "Confidential \u2013 Subject to Transmission Planning NDA\n",
      "This study has been completed in coordination with Pacific Gas and Electric per CAISO Tariff\n",
      "Generator Interconnection and Deliverability Allocation Procedures (GIDAP) Appendix DD\n",
      "Page I of 4\n",
      "Page 2 Text:\n",
      "Interconnection Document History\n",
      "Reference\n",
      "Date Document Title Description of Document\n",
      "No.\n",
      "Modifications for Preliminary\n",
      "Addendum #1 to the Cluster Protection Requirements and\n",
      "3 1/31/2020\n",
      "11 Phase II Report Substation and Transmission Line\n",
      "Work Scope\n",
      "Queue Cluster 11 Phase I Final\n",
      "2 11/20/2019 Cluster 11 Phase II Report\n",
      "Report\n",
      "Queue Cluster 11 Phase I Final\n",
      "1 1/15/2019 Cluster 11 Phase I Report\n",
      "Report\n",
      "Page II of 4\n",
      "Page 3 Text:\n",
      "Executive Summary\n",
      "Origis Operating Services, LLC, the Interconnection Customer (IC), received a Cluster 11 Phase II\n",
      "study issued on November 20, 2019 for its Interconnection Request (IR) to the California\n",
      "Independent System Operator Corporation (CAISO) for their proposed Caballero Storage (Project),\n",
      "queue position Q1470.\n",
      "Subsequent to the distribution of the report, CAISO and PG&E have identified necessary\n",
      "modifications to the report.\n",
      "The following changes replace and supersede those in the IC\u2019s Cluster 11 Phase II study report\n",
      "issued on November 20, 2019.\n",
      "Summary of changes:\n",
      "A. Add one IRNU item in Table 8-3\n",
      "Table 8-3: Escalated Cost and Time to Construct for Reliability Network Upgrades\n",
      "Upgrade Estimated\n",
      "Classifica Cost Estimated Escalated Time\n",
      "Type of\n",
      "tion Upgrade Description Allocation Cost Costs (Months) to\n",
      "Upgrade\n",
      "(GRNU, Factor x 1,000 x 1,000 Construct\n",
      "IRNU) (Note 1)\n",
      "IRNU Mesa Substation \u2022 Expand BAAH Bay 1 and install 100.00% $1,450 $1,621 28\n",
      "Reliability (1) 230kV CB & (2) DTT's Rx\n",
      "Network \u2022 Wire Bay 2 & 3 Breaker open\n",
      "Upgrade status and Install Breaker\n",
      "Maintenance Switches\n",
      "IRNU Diablo Canyon \u2022 Install one (1) DTT Transmitter 100.00% $330 $369 13\n",
      "PP\n",
      "IRNU Morro Bay \u2022 Install one (1) DTT Transmitter 100.00% $330 $369 13\n",
      "Substation\n",
      "Total $2,110 $2,359\n",
      "B. Update Table 8-8.\n",
      "Table 8-8: Cost Responsibility Breakdown\n",
      "Caballero Storage Q1470\n",
      "Deliverability Option A\n",
      "A. Phase II ANU Cost Allocation for Current Cost Responsibility (CCR)\n",
      "A.1 GRNU Cost ( x 1,000) 0\n",
      "A.2 LDNU Cost ( x 1,000) 0\n",
      "A.3 IRNU Cost ( x 1,000) 2,359\n",
      "Phase II ANU Cost Allocation for CCR ( x 1,000) (A = A.1 + A.2 + A.3) 2,359\n",
      "B. Phase II ANU Cost Allocation for Maximum Cost Responsibility (MCR)\n",
      "3\n",
      "Page 4 Text:\n",
      "B.1 GRNU Cost ( x 1,000) 0\n",
      "B.2 LDNU Cost ( x 1,000) 0\n",
      "B.3 IRNU Cost ( x 1,000) 2,359\n",
      "Phase II ANU Cost Allocation for MCR ( x 1,000) (B = B.1 + B.2 + B.3) 2,359\n",
      "C. Phase II CANU Cost Allocation\n",
      "C.1 CANU - GRNU ( x 1,000) 0\n",
      "C.2 CANU - LDNU ( x 1,000) 0\n",
      "C.3 CANU - IRNU ( x 1,000) 0\n",
      "Phase II CANU Cost Allocation ( x 1,000) (C = C.1 + C.2 + C.3) 0\n",
      "D. MCR from Phase I\n",
      "D.1 Phase I CCR for ANU ( x 1,000) 15,415\n",
      "D.2 Phase I CANU Cost for Upgrades Becoming ANU in Phase II ( x 1,000) 0\n",
      "Phase I MCR ( x 1,000) (D = D.1 + D.2) 15,415\n",
      "E. Maximum Cost Responsibility ( x 1,000) (E = min{B, D}) 2,359\n",
      "F. Current Cost Responsibility ( x 1,000) (F = min{A, E}) 2,359\n",
      "G. Maximum Cost Exposure ( x 1,000) (G = C + E) 2,359\n",
      "C. Replace Attachment 8 Preliminary Protection Requirements.\n",
      "Modified DTT Requirements with anti-islanding scheme at Mesa Substation.\n",
      "D. Replace Attachment 10 Substation and Transmission Line Work Scope.\n",
      "Added a line item in the network upgrade scope for Mesa Substation:\n",
      "\u2022 Wire Bay 2 and Bay 3 Breaker open status and associated maintenance switches to the\n",
      "SEL 2411 input for the sending DTT to Caballero Storage via the fiber installed for the\n",
      "LCD scheme. Install Breaker Maintenances switches for Mesa Bay 2 and Bay 3 circuit\n",
      "breakers.\n",
      "The remainder of the Cluster 11 Phase I study is unaffected.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r\"\\(cid:\\d+\\)\", \"\", text)\n",
    "\n",
    "\n",
    "with pdfplumber.open(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1459/02_phase_1_study/Q1459 Mulqueeney Ranch Wind 2_Appendix A-C11PhI_Revision1.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text() or \"\"\n",
    "        #text = clean_text(text)\n",
    "        print(f\"Page {page.page_number} Text:\\n{text}\")\n",
    "        if re.search(r\"Table\\s*7\", text, re.IGNORECASE):\n",
    "            print(f\"Found Table 7 on Page {page.page_number}\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for all Cluster 11 projects in this style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1459 has CID charecters even with ocr cannot scrape its revision. 1470 addendum has table 8 thats why it is not scraped.\n",
    "#1499 adnu table requires manual manipulation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: Q1442 Ajo Power Bank_Appendix A-C11PhI.pdf from Project 1442\n",
      "Scraped PDF: Q1443 Angela_Appendix A-C11PhI.pdf from Project 1443\n",
      "Scraped PDF: Q1444 Beauchamp Solar_Appendix A-C11PhI.pdf from Project 1444\n",
      "Scraped PDF: Q1445 Black Diamond Energy Storage 2_Appendix A-C11PhI.pdf from Project 1445\n",
      "Scraped PDF: Q1447 Dernopi Solar_Appendix A-C11PhI.pdf from Project 1447\n",
      "Scraped PDF: Q1449 Falcon Energy Center_Appendix A-C11PhI.pdf from Project 1449\n",
      "Scraped PDF: Q1450 Gaucho Storage_Appendix A-C11PhI.pdf from Project 1450\n",
      "Scraped PDF: Q1451 Holocrystalline Energy Center_Appendix A-C11PhI.pdf from Project 1451\n",
      "Scraped PDF: Q1452 Hood 2_Appendix A-C11PhI.pdf from Project 1452\n",
      "Scraped PDF: Q1453 Hood 3_Appendix A-C11PhI.pdf from Project 1453\n",
      "Scraped PDF: Q1454 Hummingbird Energy Storage_Appendix A-C11PhI.pdf from Project 1454\n",
      "Scraped PDF: Q1455 Janus_Appendix A-C11PhI.pdf from Project 1455\n",
      "Scraped PDF: Q1456_Las_Camas_3_Appendix_A_Addendum2.pdf from Project 1456\n",
      "Scraped PDF: Q1456 Las Camas 3_Appendix A-C11PhI.pdf from Project 1456\n",
      "Scraped PDF: Q1457 Milpa Power Bank_Appendix A-C11PhI.pdf from Project 1457\n",
      "Scraped PDF: Q1458 Montara_Appendix A-C11PhI.pdf from Project 1458\n",
      "Skipped PDF: Q1459 Mulqueeney Ranch Wind 2_Appendix A-C11PhI_Revision1.pdf from Project 1459 (No Table 7)\n",
      "Scraped PDF: Q1459 Mulqueeney Ranch Wind 2_Appendix A-C11PhI.pdf from Project 1459\n",
      "Scraped PDF: Q1460 Pinto Pass_Appendix A-C11PhI.pdf from Project 1460\n",
      "Scraped PDF: Q1461 Reclaimed Wind LLC_Appendix A-C11PhI_Revision 1.pdf from Project 1461\n",
      "Scraped PDF: Q1463 Solano 4 Wind_Appendix A-C11PhI.pdf from Project 1463\n",
      "Scraped PDF: Q1464 Star GL Storage 2_Appendix A-C11PhI.pdf from Project 1464\n",
      "Scraped PDF: Q1465 Stargaze Solar_Appendix A-C11PhI.pdf from Project 1465\n",
      "Scraped PDF: P1RPT-Q1466_Tierra_Buena_Energy_Center_Appendix_AC11PhI_Addm1.pdf from Project 1466\n",
      "Scraped PDF: Q1466 Tierra Buena Energy Center_Appendix A-C11PhI.pdf from Project 1466\n",
      "Skipped PDF: Q1470_Caballero_Storage_Appendix_AC11PhI_Addendum1.pdf from Project 1470 (No Table 7)\n",
      "Scraped PDF: Q1470 Caballero Storage_Appendix A-C11PhI.pdf from Project 1470\n",
      "Scraped PDF: P1RPT-Q1471_Casy_Solar_Appendix_AC11PhI_Revision1.pdf from Project 1471\n",
      "Scraped PDF: Q1472 Dallas Energy Storage_Appendix A-C11PhI.pdf from Project 1472\n",
      "Scraped PDF: Q1473 Del Rancho_Appendix A-C11PhI.pdf from Project 1473\n",
      "Scraped PDF: Q1474_Gemini_Wind_North_Appendix_AC11PhI_Addendum1.pdf from Project 1474\n",
      "Scraped PDF: Q1474 Gemini Wind North_Appendix A-C11PhI.pdf from Project 1474\n",
      "Scraped PDF: Q1475 Gonzaga Ridge Wind Farm 2_Appendix A-C11PhI.pdf from Project 1475\n",
      "Scraped PDF: Q1476 Green Mezzo_Appendix A-C11PhI.pdf from Project 1476\n",
      "Scraped PDF: Q1477 Heartland 11_Appendix A-C11PhI.pdf from Project 1477\n",
      "Scraped PDF: Q1478 Hudson Solar 2_Appendix A-C11PhI.pdf from Project 1478\n",
      "Scraped PDF: Q1479 Key Storage 1_Appendix A-C11PhI.pdf from Project 1479\n",
      "Scraped PDF: Q1481 Kingston Energy Storage_Appendix A-C11PhI.pdf from Project 1481\n",
      "Scraped PDF: Q1482 Mercator_Appendix A-C11PhI.pdf from Project 1482\n",
      "Scraped PDF: Q1483 Paniolo Solar_Appendix A-C11PhI.pdf from Project 1483\n",
      "Scraped PDF: Q1484 Panoche Energy Center_Appendix A-C11PhI_Revision1.pdf from Project 1484\n",
      "Scraped PDF: Q1485 Placid_Appendix A-C11PhI.pdf from Project 1485\n",
      "Scraped PDF: Q1488 Sageberry Solar_Appendix A-C11PhI.pdf from Project 1488\n",
      "Scraped PDF: Q1490 Temettate Advanced CAES_Appendix A-C11PhI.pdf from Project 1490\n",
      "Scraped PDF: Q1491 Tepona Off-Shore Wind_Appendix A-C11PhI.pdf from Project 1491\n",
      "Scraped Addendum PDF: Q1491 Tepona Off-Shore Wind_Appendix A-C11PhI_Addendum1.pdf from Project 1491\n",
      "Scraped PDF: Q1492 Windcharger ESS_Appendix A-C11PhI.pdf from Project 1492\n",
      "Scraped PDF: Q1493 Azalea_Appendix A-C11PhI_Revision1.pdf from Project 1493\n",
      "Scraped PDF: Q1494 Castle Wind Offshore_Appendix A-C11PhI.pdf from Project 1494\n",
      "Scraped PDF: Q1495 Chalan Solar _Appendix A-C11PhI_Revision1.pdf from Project 1495\n",
      "Scraped PDF: Q1496 Descendant Ranch1_Appendix A-C11PhI.pdf from Project 1496\n",
      "Scraped PDF: Q1498 Hood 1_Appendix A-C11PhI.pdf from Project 1498\n",
      "Scraped PDF: Q1499 Jasmine_Appendix A-C11PhI.pdf from Project 1499\n",
      "Scraped PDF: Q1500 Limon Energy Park_Appendix A-C11PhI.pdf from Project 1500\n",
      "Scraped PDF: Q1501 Mammoth Energy Storage_Appendix A-C11PhI.pdf from Project 1501\n",
      "Scraped PDF: Q1502 Mavericks Grid_Appendix A-C11PhI.pdf from Project 1502\n",
      "Scraped PDF: Q1503 Pintail Energy Storage_Appendix A-C11PhI.pdf from Project 1503\n",
      "Scraped PDF: Q1504 Pistachio Energy Park_Appendix A-C11PhI.pdf from Project 1504\n",
      "Scraped PDF: Q1505 Poleline Solar Energy Center_Appendix A-C11PhI.pdf from Project 1505\n",
      "Scraped PDF: Q1506 Poseidon Energy Park_Appendix A-C11PhI.pdf from Project 1506\n",
      "Scraped PDF: Q1507 Prospect Energy Storage_Appendix A-C11PhI.pdf from Project 1507\n",
      "Skipped PDF: Metro-Q1508-TOT877-Cortes-AppendixA-v5-clean.pdf from Project 1508 (No Table 7)\n",
      "Skipped PDF: Attachment 2-Q1508 TOT877 Metro QC11P1.pdf from Project 1508 (No Table 7)\n",
      "Skipped PDF: Metro_Attchment1_TOT877_Q1508_Cortes Grid.pdf from Project 1508 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1509-TOT881-Arrow-Appendix A-Attachment 2.pdf from Project 1509 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1509-TOT881-Arrow-Appendix A-Attachment 1.pdf from Project 1509 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1509-TOT881-Arrow-Appendix A.pdf from Project 1509 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1510-TOT892-Bellefield-Appendix A-Attachment 2.pdf from Project 1510 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1510-TOT892-Bellefield-Appendix A-Attachment 1.pdf from Project 1510 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1510-TOT892-Bellefield-Appendix A.pdf from Project 1510 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1511-TOT889-Cleanpower-Appendix A-Attachment 1.pdf from Project 1511 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1511-TOT889-Cleanpower-Appendix A.pdf from Project 1511 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1511-TOT889-Cleanpower-Appendix A-Attachment 2.pdf from Project 1511 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1512-TOT883-DryCreek-Appendix A.pdf from Project 1512 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1512-TOT883-DryCreek-Appendix A-Attachment 2.pdf from Project 1512 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1512-TOT883-DryCreek-Appendix A-Attachment 1.pdf from Project 1512 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1513-TOT887-Highline-Appendix A-Attachment 1.pdf from Project 1513 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1513-TOT887-Highline-Appendix A-Attachment 2.pdf from Project 1513 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1513-TOT887-Highline-Appendix A.pdf from Project 1513 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1514-TOT871-Longspur-Appendix A-Attachment 2.pdf from Project 1514 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1514-TOT871-Longspur-Appendix A-Attachment 1.pdf from Project 1514 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1514-TOT871-Longspur-Appendix A.pdf from Project 1514 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1515-TOT898-McGrath-Appendix A.pdf from Project 1515 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1515-TOT898-McGrath-Appendix A-Attachment 1.pdf from Project 1515 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1515-TOT898-McGrath-Appendix A-Attachment 2.pdf from Project 1515 (No Table 7)\n",
      "Skipped PDF: C11-Revison2AppendixA-Attachmt1-Q1516-TOT896-Rexford.pdf from Project 1516 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1516-TOT896-Rexford-Appendix A.pdf from Project 1516 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1516-TOT896-Rexford-Appendix A-Attachment 1.pdf from Project 1516 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1516-TOT896-Rexford-Appendix A-Attachment 2.pdf from Project 1516 (No Table 7)\n",
      "Skipped PDF: C11-Revison2AppendixA-Q1516-TOT896-Rexford (clean).pdf from Project 1516 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1517-TOT891-Sagebrush5-Appendix A.pdf from Project 1517 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1517-TOT891-Sagebrush5-Appendix A-Attachment 2.pdf from Project 1517 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1517-TOT891-Sagebrush5-Appendix A-Attachment 1.pdf from Project 1517 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1518-TOT876-Sanborn2-Appendix A.pdf from Project 1518 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1518-TOT876-Sanborn2-Appendix A-Attachment 2.pdf from Project 1518 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Northern-Q1518-TOT876-Sanborn2-Appendix A-Attachment 1.pdf from Project 1518 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-NOL-Q1519-TOT882-BaldyMesa2-AppendixA-Attachment 2.pdf from Project 1519 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-NOL-Q1519-TOT882-BaldyMesa2-AppendixA-Attachment 1.pdf from Project 1519 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-NOL-Q1519-TOT882-BaldyMesa2-AppendixA.pdf from Project 1519 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-NOL-Q1521-TOT890-Moorhern-AppendixA.pdf from Project 1521 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-NOL-Q1521-TOT890-Moorhern-AppendixA-Attachment 1.pdf from Project 1521 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-NOL-Q1521-TOT890-Moorhern-AppendixA-Attachment 2.pdf from Project 1521 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-EOP-Q1522-TOT880-Arida-AppendixA-Atchmnt1.pdf from Project 1522 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-EOP-Q1522-TOT880-Arida-AppendixA.pdf from Project 1522 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-EOP-Q1522-TOT880-Arida-AppendixA-Attachment 2.pdf from Project 1522 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-EOP-Q1523-TOT878-Porthos-AppendixA-Atchmnt1.pdf from Project 1523 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-EOP-Q1523-TOT878-Porthos-AppendixA-Attachment 2.pdf from Project 1523 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-EOP-Q1523-TOT878-Porthos-AppendixA.pdf from Project 1523 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-EOP-Q1524-TOT895-Sunvale-AppendixA-Attachment 2.pdf from Project 1524 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-EOP-Q1524-TOT895-Sunvale-AppendixA-Atchmnt1.pdf from Project 1524 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-EOP-Q1524-TOT895-Sunvale-AppendixA.pdf from Project 1524 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1525-Appendix A.pdf from Project 1525 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1525-Appendix A-Attachment 1.pdf from Project 1525 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1525-Appendix A-Attachment 2.pdf from Project 1525 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1526-Appendix A-Attachment 2.pdf from Project 1526 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1526-Appendix A-Attachment 1.pdf from Project 1526 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1526-Appendix A.pdf from Project 1526 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1527-Appendix A-Attachment 1.pdf from Project 1527 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1527-Appendix A-Attachment 2.pdf from Project 1527 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1527-Appendix A.pdf from Project 1527 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1528-Appendix A-Attachment 2.pdf from Project 1528 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1528-Appendix A-Attachment 1.pdf from Project 1528 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1528-Appendix A.pdf from Project 1528 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1529-Appendix A-Attachment 1.pdf from Project 1529 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1529-Appendix A-Attachment 2.pdf from Project 1529 (No Table 7)\n",
      "Skipped PDF: QC11PI-DCRT-Eastern-Q1529-Appendix A-Attachment 2.pdf from Project 1529 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1529-Appendix A.pdf from Project 1529 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1530-Appendix A-Attachment 2.pdf from Project 1530 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1530-Appendix A-Attachment 1.pdf from Project 1530 (No Table 7)\n",
      "Skipped PDF: QC11PI-DCRT-Eastern-Q1530-Appendix A-Attachment 2.pdf from Project 1530 (No Table 7)\n",
      "Skipped PDF: QC11PI-SDGE-Eastern-Q1530-Appendix A-Attachment 2.pdf from Project 1530 (No Table 7)\n",
      "Skipped PDF: QC11PI-SCE-Eastern-Q1530-Appendix A.pdf from Project 1530 (No Table 7)\n",
      "Scraped PDF: QC11PhI_Q1531_Bateria Del Sur_Appendix A_1-15-2019_final.pdf from Project 1531\n",
      "Scraped PDF: QC11PhI_Q1532_Kettle Solar One_Appendix A_1-15-2019_final.pdf from Project 1532\n",
      "Scraped PDF: QC11PhI_Q1533_San Vicente Energy Storage Facility_Appendix A_1-15-2019_final.pdf from Project 1533\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/rawdata_cluster11_style_G_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/rawdata_cluster11_style_G_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 80\n",
      "Total Projects Scraped: 58\n",
      "Total Projects Skipped: 22\n",
      "Total Projects Missing: 12\n",
      "Total PDFs Accessed: 135\n",
      "Total PDFs Scraped: 62\n",
      "Total PDFs Skipped: 73\n",
      "\n",
      "List of Scraped Projects:\n",
      "[1442, 1443, 1444, 1445, 1447, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1463, 1464, 1465, 1466, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1481, 1482, 1483, 1484, 1485, 1488, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1531, 1532, 1533]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530]\n",
      "\n",
      "List of Missing Projects:\n",
      "[1446, 1448, 1462, 1467, 1468, 1469, 1480, 1486, 1487, 1489, 1497, 1520]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['Q1442 Ajo Power Bank_Appendix A-C11PhI.pdf', 'Q1443 Angela_Appendix A-C11PhI.pdf', 'Q1444 Beauchamp Solar_Appendix A-C11PhI.pdf', 'Q1445 Black Diamond Energy Storage 2_Appendix A-C11PhI.pdf', 'Q1447 Dernopi Solar_Appendix A-C11PhI.pdf', 'Q1449 Falcon Energy Center_Appendix A-C11PhI.pdf', 'Q1450 Gaucho Storage_Appendix A-C11PhI.pdf', 'Q1451 Holocrystalline Energy Center_Appendix A-C11PhI.pdf', 'Q1452 Hood 2_Appendix A-C11PhI.pdf', 'Q1453 Hood 3_Appendix A-C11PhI.pdf', 'Q1454 Hummingbird Energy Storage_Appendix A-C11PhI.pdf', 'Q1455 Janus_Appendix A-C11PhI.pdf', 'Q1456_Las_Camas_3_Appendix_A_Addendum2.pdf', 'Q1456 Las Camas 3_Appendix A-C11PhI.pdf', 'Q1457 Milpa Power Bank_Appendix A-C11PhI.pdf', 'Q1458 Montara_Appendix A-C11PhI.pdf', 'Q1459 Mulqueeney Ranch Wind 2_Appendix A-C11PhI.pdf', 'Q1460 Pinto Pass_Appendix A-C11PhI.pdf', 'Q1461 Reclaimed Wind LLC_Appendix A-C11PhI_Revision 1.pdf', 'Q1463 Solano 4 Wind_Appendix A-C11PhI.pdf', 'Q1464 Star GL Storage 2_Appendix A-C11PhI.pdf', 'Q1465 Stargaze Solar_Appendix A-C11PhI.pdf', 'P1RPT-Q1466_Tierra_Buena_Energy_Center_Appendix_AC11PhI_Addm1.pdf', 'Q1466 Tierra Buena Energy Center_Appendix A-C11PhI.pdf', 'Q1470 Caballero Storage_Appendix A-C11PhI.pdf', 'P1RPT-Q1471_Casy_Solar_Appendix_AC11PhI_Revision1.pdf', 'Q1472 Dallas Energy Storage_Appendix A-C11PhI.pdf', 'Q1473 Del Rancho_Appendix A-C11PhI.pdf', 'Q1474_Gemini_Wind_North_Appendix_AC11PhI_Addendum1.pdf', 'Q1474 Gemini Wind North_Appendix A-C11PhI.pdf', 'Q1475 Gonzaga Ridge Wind Farm 2_Appendix A-C11PhI.pdf', 'Q1476 Green Mezzo_Appendix A-C11PhI.pdf', 'Q1477 Heartland 11_Appendix A-C11PhI.pdf', 'Q1478 Hudson Solar 2_Appendix A-C11PhI.pdf', 'Q1479 Key Storage 1_Appendix A-C11PhI.pdf', 'Q1481 Kingston Energy Storage_Appendix A-C11PhI.pdf', 'Q1482 Mercator_Appendix A-C11PhI.pdf', 'Q1483 Paniolo Solar_Appendix A-C11PhI.pdf', 'Q1484 Panoche Energy Center_Appendix A-C11PhI_Revision1.pdf', 'Q1485 Placid_Appendix A-C11PhI.pdf', 'Q1488 Sageberry Solar_Appendix A-C11PhI.pdf', 'Q1490 Temettate Advanced CAES_Appendix A-C11PhI.pdf', 'Q1491 Tepona Off-Shore Wind_Appendix A-C11PhI.pdf', 'Q1491 Tepona Off-Shore Wind_Appendix A-C11PhI_Addendum1.pdf', 'Q1492 Windcharger ESS_Appendix A-C11PhI.pdf', 'Q1493 Azalea_Appendix A-C11PhI_Revision1.pdf', 'Q1494 Castle Wind Offshore_Appendix A-C11PhI.pdf', 'Q1495 Chalan Solar _Appendix A-C11PhI_Revision1.pdf', 'Q1496 Descendant Ranch1_Appendix A-C11PhI.pdf', 'Q1498 Hood 1_Appendix A-C11PhI.pdf', 'Q1499 Jasmine_Appendix A-C11PhI.pdf', 'Q1500 Limon Energy Park_Appendix A-C11PhI.pdf', 'Q1501 Mammoth Energy Storage_Appendix A-C11PhI.pdf', 'Q1502 Mavericks Grid_Appendix A-C11PhI.pdf', 'Q1503 Pintail Energy Storage_Appendix A-C11PhI.pdf', 'Q1504 Pistachio Energy Park_Appendix A-C11PhI.pdf', 'Q1505 Poleline Solar Energy Center_Appendix A-C11PhI.pdf', 'Q1506 Poseidon Energy Park_Appendix A-C11PhI.pdf', 'Q1507 Prospect Energy Storage_Appendix A-C11PhI.pdf', 'QC11PhI_Q1531_Bateria Del Sur_Appendix A_1-15-2019_final.pdf', 'QC11PhI_Q1532_Kettle Solar One_Appendix A_1-15-2019_final.pdf', 'QC11PhI_Q1533_San Vicente Energy Storage Facility_Appendix A_1-15-2019_final.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['Q1459 Mulqueeney Ranch Wind 2_Appendix A-C11PhI_Revision1.pdf', 'Q1470_Caballero_Storage_Appendix_AC11PhI_Addendum1.pdf', 'Metro-Q1508-TOT877-Cortes-AppendixA-v5-clean.pdf', 'Attachment 2-Q1508 TOT877 Metro QC11P1.pdf', 'Metro_Attchment1_TOT877_Q1508_Cortes Grid.pdf', 'QC11PI-SCE-Northern-Q1509-TOT881-Arrow-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1509-TOT881-Arrow-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1509-TOT881-Arrow-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1510-TOT892-Bellefield-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1510-TOT892-Bellefield-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1510-TOT892-Bellefield-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1511-TOT889-Cleanpower-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1511-TOT889-Cleanpower-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1511-TOT889-Cleanpower-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1512-TOT883-DryCreek-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1512-TOT883-DryCreek-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1512-TOT883-DryCreek-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1513-TOT887-Highline-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1513-TOT887-Highline-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1513-TOT887-Highline-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1514-TOT871-Longspur-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1514-TOT871-Longspur-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1514-TOT871-Longspur-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1515-TOT898-McGrath-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1515-TOT898-McGrath-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1515-TOT898-McGrath-Appendix A-Attachment 2.pdf', 'C11-Revison2AppendixA-Attachmt1-Q1516-TOT896-Rexford.pdf', 'QC11PI-SCE-Northern-Q1516-TOT896-Rexford-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1516-TOT896-Rexford-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1516-TOT896-Rexford-Appendix A-Attachment 2.pdf', 'C11-Revison2AppendixA-Q1516-TOT896-Rexford (clean).pdf', 'QC11PI-SCE-Northern-Q1517-TOT891-Sagebrush5-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1517-TOT891-Sagebrush5-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1517-TOT891-Sagebrush5-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1518-TOT876-Sanborn2-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1518-TOT876-Sanborn2-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1518-TOT876-Sanborn2-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-NOL-Q1519-TOT882-BaldyMesa2-AppendixA-Attachment 2.pdf', 'QC11PI-SCE-NOL-Q1519-TOT882-BaldyMesa2-AppendixA-Attachment 1.pdf', 'QC11PI-SCE-NOL-Q1519-TOT882-BaldyMesa2-AppendixA.pdf', 'QC11PI-SCE-NOL-Q1521-TOT890-Moorhern-AppendixA.pdf', 'QC11PI-SCE-NOL-Q1521-TOT890-Moorhern-AppendixA-Attachment 1.pdf', 'QC11PI-SCE-NOL-Q1521-TOT890-Moorhern-AppendixA-Attachment 2.pdf', 'QC11PI-SCE-EOP-Q1522-TOT880-Arida-AppendixA-Atchmnt1.pdf', 'QC11PI-SCE-EOP-Q1522-TOT880-Arida-AppendixA.pdf', 'QC11PI-SCE-EOP-Q1522-TOT880-Arida-AppendixA-Attachment 2.pdf', 'QC11PI-SCE-EOP-Q1523-TOT878-Porthos-AppendixA-Atchmnt1.pdf', 'QC11PI-SCE-EOP-Q1523-TOT878-Porthos-AppendixA-Attachment 2.pdf', 'QC11PI-SCE-EOP-Q1523-TOT878-Porthos-AppendixA.pdf', 'QC11PI-SCE-EOP-Q1524-TOT895-Sunvale-AppendixA-Attachment 2.pdf', 'QC11PI-SCE-EOP-Q1524-TOT895-Sunvale-AppendixA-Atchmnt1.pdf', 'QC11PI-SCE-EOP-Q1524-TOT895-Sunvale-AppendixA.pdf', 'QC11PI-SCE-Eastern-Q1525-Appendix A.pdf', 'QC11PI-SCE-Eastern-Q1525-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Eastern-Q1525-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1526-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1526-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Eastern-Q1526-Appendix A.pdf', 'QC11PI-SCE-Eastern-Q1527-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Eastern-Q1527-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1527-Appendix A.pdf', 'QC11PI-SCE-Eastern-Q1528-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1528-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Eastern-Q1528-Appendix A.pdf', 'QC11PI-SCE-Eastern-Q1529-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Eastern-Q1529-Appendix A-Attachment 2.pdf', 'QC11PI-DCRT-Eastern-Q1529-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1529-Appendix A.pdf', 'QC11PI-SCE-Eastern-Q1530-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1530-Appendix A-Attachment 1.pdf', 'QC11PI-DCRT-Eastern-Q1530-Appendix A-Attachment 2.pdf', 'QC11PI-SDGE-Eastern-Q1530-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1530-Appendix A.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['Q1456_Las_Camas_3_Appendix_A_Addendum2.pdf', 'P1RPT-Q1466_Tierra_Buena_Energy_Center_Appendix_AC11PhI_Addm1.pdf', 'Q1470_Caballero_Storage_Appendix_AC11PhI_Addendum1.pdf', 'Q1474_Gemini_Wind_North_Appendix_AC11PhI_Addendum1.pdf', 'Q1491 Tepona Off-Shore Wind_Appendix A-C11PhI_Addendum1.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['Q1442 Ajo Power Bank_Appendix A-C11PhI.pdf', 'Q1443 Angela_Appendix A-C11PhI.pdf', 'Q1444 Beauchamp Solar_Appendix A-C11PhI.pdf', 'Q1445 Black Diamond Energy Storage 2_Appendix A-C11PhI.pdf', 'Q1447 Dernopi Solar_Appendix A-C11PhI.pdf', 'Q1449 Falcon Energy Center_Appendix A-C11PhI.pdf', 'Q1450 Gaucho Storage_Appendix A-C11PhI.pdf', 'Q1451 Holocrystalline Energy Center_Appendix A-C11PhI.pdf', 'Q1452 Hood 2_Appendix A-C11PhI.pdf', 'Q1453 Hood 3_Appendix A-C11PhI.pdf', 'Q1454 Hummingbird Energy Storage_Appendix A-C11PhI.pdf', 'Q1455 Janus_Appendix A-C11PhI.pdf', 'Q1456 Las Camas 3_Appendix A-C11PhI.pdf', 'Q1457 Milpa Power Bank_Appendix A-C11PhI.pdf', 'Q1458 Montara_Appendix A-C11PhI.pdf', 'Q1459 Mulqueeney Ranch Wind 2_Appendix A-C11PhI_Revision1.pdf', 'Q1459 Mulqueeney Ranch Wind 2_Appendix A-C11PhI.pdf', 'Q1460 Pinto Pass_Appendix A-C11PhI.pdf', 'Q1461 Reclaimed Wind LLC_Appendix A-C11PhI_Revision 1.pdf', 'Q1463 Solano 4 Wind_Appendix A-C11PhI.pdf', 'Q1464 Star GL Storage 2_Appendix A-C11PhI.pdf', 'Q1465 Stargaze Solar_Appendix A-C11PhI.pdf', 'Q1466 Tierra Buena Energy Center_Appendix A-C11PhI.pdf', 'Q1470 Caballero Storage_Appendix A-C11PhI.pdf', 'P1RPT-Q1471_Casy_Solar_Appendix_AC11PhI_Revision1.pdf', 'Q1472 Dallas Energy Storage_Appendix A-C11PhI.pdf', 'Q1473 Del Rancho_Appendix A-C11PhI.pdf', 'Q1474 Gemini Wind North_Appendix A-C11PhI.pdf', 'Q1475 Gonzaga Ridge Wind Farm 2_Appendix A-C11PhI.pdf', 'Q1476 Green Mezzo_Appendix A-C11PhI.pdf', 'Q1477 Heartland 11_Appendix A-C11PhI.pdf', 'Q1478 Hudson Solar 2_Appendix A-C11PhI.pdf', 'Q1479 Key Storage 1_Appendix A-C11PhI.pdf', 'Q1481 Kingston Energy Storage_Appendix A-C11PhI.pdf', 'Q1482 Mercator_Appendix A-C11PhI.pdf', 'Q1483 Paniolo Solar_Appendix A-C11PhI.pdf', 'Q1484 Panoche Energy Center_Appendix A-C11PhI_Revision1.pdf', 'Q1485 Placid_Appendix A-C11PhI.pdf', 'Q1488 Sageberry Solar_Appendix A-C11PhI.pdf', 'Q1490 Temettate Advanced CAES_Appendix A-C11PhI.pdf', 'Q1491 Tepona Off-Shore Wind_Appendix A-C11PhI.pdf', 'Q1492 Windcharger ESS_Appendix A-C11PhI.pdf', 'Q1493 Azalea_Appendix A-C11PhI_Revision1.pdf', 'Q1494 Castle Wind Offshore_Appendix A-C11PhI.pdf', 'Q1495 Chalan Solar _Appendix A-C11PhI_Revision1.pdf', 'Q1496 Descendant Ranch1_Appendix A-C11PhI.pdf', 'Q1498 Hood 1_Appendix A-C11PhI.pdf', 'Q1499 Jasmine_Appendix A-C11PhI.pdf', 'Q1500 Limon Energy Park_Appendix A-C11PhI.pdf', 'Q1501 Mammoth Energy Storage_Appendix A-C11PhI.pdf', 'Q1502 Mavericks Grid_Appendix A-C11PhI.pdf', 'Q1503 Pintail Energy Storage_Appendix A-C11PhI.pdf', 'Q1504 Pistachio Energy Park_Appendix A-C11PhI.pdf', 'Q1505 Poleline Solar Energy Center_Appendix A-C11PhI.pdf', 'Q1506 Poseidon Energy Park_Appendix A-C11PhI.pdf', 'Q1507 Prospect Energy Storage_Appendix A-C11PhI.pdf', 'Metro-Q1508-TOT877-Cortes-AppendixA-v5-clean.pdf', 'Attachment 2-Q1508 TOT877 Metro QC11P1.pdf', 'Metro_Attchment1_TOT877_Q1508_Cortes Grid.pdf', 'QC11PI-SCE-Northern-Q1509-TOT881-Arrow-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1509-TOT881-Arrow-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1509-TOT881-Arrow-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1510-TOT892-Bellefield-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1510-TOT892-Bellefield-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1510-TOT892-Bellefield-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1511-TOT889-Cleanpower-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1511-TOT889-Cleanpower-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1511-TOT889-Cleanpower-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1512-TOT883-DryCreek-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1512-TOT883-DryCreek-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1512-TOT883-DryCreek-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1513-TOT887-Highline-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1513-TOT887-Highline-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1513-TOT887-Highline-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1514-TOT871-Longspur-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1514-TOT871-Longspur-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1514-TOT871-Longspur-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1515-TOT898-McGrath-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1515-TOT898-McGrath-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1515-TOT898-McGrath-Appendix A-Attachment 2.pdf', 'C11-Revison2AppendixA-Attachmt1-Q1516-TOT896-Rexford.pdf', 'QC11PI-SCE-Northern-Q1516-TOT896-Rexford-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1516-TOT896-Rexford-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1516-TOT896-Rexford-Appendix A-Attachment 2.pdf', 'C11-Revison2AppendixA-Q1516-TOT896-Rexford (clean).pdf', 'QC11PI-SCE-Northern-Q1517-TOT891-Sagebrush5-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1517-TOT891-Sagebrush5-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1517-TOT891-Sagebrush5-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Northern-Q1518-TOT876-Sanborn2-Appendix A.pdf', 'QC11PI-SCE-Northern-Q1518-TOT876-Sanborn2-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Northern-Q1518-TOT876-Sanborn2-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-NOL-Q1519-TOT882-BaldyMesa2-AppendixA-Attachment 2.pdf', 'QC11PI-SCE-NOL-Q1519-TOT882-BaldyMesa2-AppendixA-Attachment 1.pdf', 'QC11PI-SCE-NOL-Q1519-TOT882-BaldyMesa2-AppendixA.pdf', 'QC11PI-SCE-NOL-Q1521-TOT890-Moorhern-AppendixA.pdf', 'QC11PI-SCE-NOL-Q1521-TOT890-Moorhern-AppendixA-Attachment 1.pdf', 'QC11PI-SCE-NOL-Q1521-TOT890-Moorhern-AppendixA-Attachment 2.pdf', 'QC11PI-SCE-EOP-Q1522-TOT880-Arida-AppendixA-Atchmnt1.pdf', 'QC11PI-SCE-EOP-Q1522-TOT880-Arida-AppendixA.pdf', 'QC11PI-SCE-EOP-Q1522-TOT880-Arida-AppendixA-Attachment 2.pdf', 'QC11PI-SCE-EOP-Q1523-TOT878-Porthos-AppendixA-Atchmnt1.pdf', 'QC11PI-SCE-EOP-Q1523-TOT878-Porthos-AppendixA-Attachment 2.pdf', 'QC11PI-SCE-EOP-Q1523-TOT878-Porthos-AppendixA.pdf', 'QC11PI-SCE-EOP-Q1524-TOT895-Sunvale-AppendixA-Attachment 2.pdf', 'QC11PI-SCE-EOP-Q1524-TOT895-Sunvale-AppendixA-Atchmnt1.pdf', 'QC11PI-SCE-EOP-Q1524-TOT895-Sunvale-AppendixA.pdf', 'QC11PI-SCE-Eastern-Q1525-Appendix A.pdf', 'QC11PI-SCE-Eastern-Q1525-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Eastern-Q1525-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1526-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1526-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Eastern-Q1526-Appendix A.pdf', 'QC11PI-SCE-Eastern-Q1527-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Eastern-Q1527-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1527-Appendix A.pdf', 'QC11PI-SCE-Eastern-Q1528-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1528-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Eastern-Q1528-Appendix A.pdf', 'QC11PI-SCE-Eastern-Q1529-Appendix A-Attachment 1.pdf', 'QC11PI-SCE-Eastern-Q1529-Appendix A-Attachment 2.pdf', 'QC11PI-DCRT-Eastern-Q1529-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1529-Appendix A.pdf', 'QC11PI-SCE-Eastern-Q1530-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1530-Appendix A-Attachment 1.pdf', 'QC11PI-DCRT-Eastern-Q1530-Appendix A-Attachment 2.pdf', 'QC11PI-SDGE-Eastern-Q1530-Appendix A-Attachment 2.pdf', 'QC11PI-SCE-Eastern-Q1530-Appendix A.pdf', 'QC11PhI_Q1531_Bateria Del Sur_Appendix A_1-15-2019_final.pdf', 'QC11PhI_Q1532_Kettle Solar One_Appendix A_1-15-2019_final.pdf', 'QC11PhI_Q1533_San Vicente Energy Storage Facility_Appendix A_1-15-2019_final.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 58\n",
      "Number of Addendum PDFs Scraped: 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/rawdata_cluster11_style_G_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/rawdata_cluster11_style_G_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/scraping_cluster11_style_G_log.txt\"\n",
    "PROJECT_RANGE = range(1442, 1534)  # Example range for q_ids in Clusters  11\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 7\"\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*7?\\d*\", text, re.IGNORECASE):\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 7 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2\n",
    "\n",
    "            print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end}\", file=log_file)\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(2).strip()\n",
    "                                break\n",
    "\n",
    "                    if table_title:\n",
    "                        # New Table 7 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        # Handle new ADNU tables (grouping logic)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    # Group all adnu rows into one 'upgrade' row\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    # If 'type of upgrade' exists, just rename adnu if needed\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row none, original logic replaced only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' first row for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            # Non-ADNU new tables\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If exist and none in first row original logic replaced only first row\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for first row in new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(\"Duplicate columns detected in new table. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous Table 7 title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "\n",
    "                        # Check if the number of columns matches\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\")\n",
    "                            continue  # No table to continue with\n",
    "\n",
    "                        # Define expected columns based on the last extracted table\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "\n",
    "                        # Detect header row in continuation\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\", \"MW at POI\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            actual_header_row = data_rows[0]\n",
    "                            actual_headers = clean_column_headers(actual_header_row)\n",
    "                             \n",
    "\n",
    "                            # Go back to previous page bounding box to update table title if needed\n",
    "                            if page_number > 0:\n",
    "                                previous_page = pdf.pages[page_number - 1]\n",
    "                                bbox_lower_region = (0, table_bbox[1], previous_page.width, previous_page.height)\n",
    "                                title_text_previous = previous_page.within_bbox(bbox_lower_region).extract_text() or \"\"\n",
    "                                new_table_title = None\n",
    "                                if title_text_previous:\n",
    "                                    title_lines_prev = title_text_previous.split('\\n')[::-1]\n",
    "                                    for line in title_lines_prev:\n",
    "                                        line = line.strip()\n",
    "                                        match_prev = re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                                        if match_prev:\n",
    "                                            new_table_title = match_prev.group(2).strip()\n",
    "                                            break\n",
    "                                if new_table_title:\n",
    "                                    specific_phrase = extract_specific_phrase(new_table_title)\n",
    "                                    print(f\"Updated table title from previous page: '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                else:\n",
    "                                    print(\"No table title found in previous page region. Using existing specific_phrase.\", file=log_file)\n",
    "                            else:\n",
    "                                print(\"No previous page available for title extraction for continuation table. Using existing specific_phrase.\", file=log_file)\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                            # Handle continuation ADNU or non-ADNU\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                                    \n",
    "                                print(\"Continuation ADNU table detected. No grouping, just rename and type of upgrade handling.\", file=log_file)\n",
    "                                  \n",
    "                                    \n",
    "                                #if \"adnu\" in data_rows[0]:\n",
    "                                    #\n",
    "                                 #   print(\"Handling continuation for 'Area Delivery Netowrk Upgrade' table\")\n",
    "                                 #   print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\")\n",
    "                                 #   \n",
    "                                 #   data_rows = [ [\"upgrade\"] + row[1:] for row in data_rows ]\n",
    "                                    \n",
    "\n",
    "                                if \"type of upgrade\" not in data_rows[0]:\n",
    "                                    # Insert 'type of upgrade' column at the beginning\n",
    "                                    print(\"Inserting 'type of upgrade' column with specific phrase in continuation table.\",file=log_file)\n",
    "                                    data_rows = [ [specific_phrase] + row for row in data_rows ]\n",
    "\n",
    "\n",
    "                                    if \"ADNU\" in data_rows[0]:\n",
    "                                        print(\"Handling continuation for 'Area Delivery Network Upgrade' table\",file=log_file)\n",
    "                                        print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\",file=log_file)\n",
    "                                        # Find the index where \"ADNU\" occurs in the first row\n",
    "                                        adnu_idx = data_rows[0].index(\"ADNU\")\n",
    "                                        # Replace \"ADNU\" with \"upgrade\" in that column for every row\n",
    "                                        #for r in range(len(data_rows)):\n",
    "                                        data_rows[0][adnu_idx] = \"upgrade\"\n",
    "\n",
    "\n",
    "\n",
    "                                    \n",
    "                                \n",
    "                        # Handle missing or extra columns\n",
    "                        if len(data_rows[0]) < expected_columns:\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                    # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                print(f\"Detected missing 'upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'upgrade' column.\",file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                            else:\n",
    "                                    # For other tables, assume missing \"Type of Upgrade\" column\n",
    "                                print(f\"Detected missing 'Type of Upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'Type of Upgrade' column.\",file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                        elif len(data_rows[0]) > expected_columns:\n",
    "                            # Extra columns detected; adjust accordingly\n",
    "                            print(f\"Detected extra columns in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping extra columns.\",file=log_file)\n",
    "                            data_rows = [row[:expected_columns] for row in data_rows]\n",
    "                            \n",
    "                        \n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "                        # Create DataFrame for the continuation table\n",
    "\n",
    "                        if is_header_row:    \n",
    "                            data_rows = data_rows[1:]\n",
    "                            print(f\"Dropped header row from data_rows after modifications for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "    \n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\")\n",
    "                            continue  # Skip this table due to error\n",
    "\n",
    "                        \n",
    "\n",
    "                            # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"type of upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                        else:\n",
    "                            # General Handling for other tables\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\",file=log_file)\n",
    "                            df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)            \n",
    "\n",
    "\n",
    "\n",
    "                                         \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 7 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After processing all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 7 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 7 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 7 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 7.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path, log_file):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            print(f\"Extracted Text: {text}\", file= log_file)  # Debugging step\n",
    "            # Case-insensitive check for 'Addendum'\n",
    "            return \"addendum\" in text.lower()\n",
    "            #return \"Addendum\" in text\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                    # Determine output DataFrame and CSV path based on addendum status\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "\n",
    "                    try:\n",
    "                        has_table7 = check_has_table7(pdf_path)\n",
    "                        if not has_table7:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                            # Print to ipynb output\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            # Extract base data from original PDF\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                        if is_add and base_data_extracted:\n",
    "                            # For addendums, use the extracted base data\n",
    "                            table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table7_data.empty:\n",
    "                                # Merge base data with Table 7 data\n",
    "                                merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                # Optionally, print to ipynb\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            # For originals, extract Table 7 data\n",
    "                            df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                # Optionally, print to ipynb\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        # Optionally, print to ipynb\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.map(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individually doing 1495, 1506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data rows: [None, 'QC11P1RAS-12 Midway\\n500/230 kV transformers\\noverload RAS', '\u2022 QC11P1RAS-12 Modify existing\\nMidway 500/230 kV\\nTransformer SPS to add\\ntransformer outage detection in\\naddition to overload detection\\nbefore tripping generation.', '11.13%', '$223', '$255', '36']\n",
      "Data rows: ['Local\\nDelivery\\nNetwork\\nUpgrade', 'None', '\u2022', '', '', '', '']\n",
      "Scraped PDF: Q1506 Poseidon Energy Park_Appendix A-C11PhI.pdf from Project 1506\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/03_raw/rawdata_cluster11_style_G_1506.csv\n",
      "No data to save for addendums.\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 1\n",
      "Total Projects Scraped: 1\n",
      "Total Projects Skipped: 0\n",
      "Total Projects Missing: 0\n",
      "Total PDFs Accessed: 1\n",
      "Total PDFs Scraped: 1\n",
      "Total PDFs Skipped: 0\n",
      "\n",
      "List of Scraped Projects:\n",
      "[1506]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[]\n",
      "\n",
      "List of Missing Projects:\n",
      "[]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['Q1506 Poseidon Energy Park_Appendix A-C11PhI.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "[]\n",
      "\n",
      "List of Addendum PDFs:\n",
      "[]\n",
      "\n",
      "List of Original PDFs:\n",
      "['Q1506 Poseidon Energy Park_Appendix A-C11PhI.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 1\n",
      "Number of Addendum PDFs Scraped: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/rawdata_cluster11_style_G_1506.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/rawdata_cluster11_style_G_1506_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/scraping_cluster11_style_G_1506_log.txt\"\n",
    "PROJECT_RANGE = range(1506, 1507)  # Example range for q_ids in Clusters  11\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"Other Potential Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrades\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "def adjust_rows_length(data_rows, headers):\n",
    "    \"\"\"Ensure each row in data_rows matches the length of headers by truncating or padding.\"\"\"\n",
    "    col_count = len(headers)\n",
    "    for i in range(len(data_rows)):\n",
    "        row = data_rows[i]\n",
    "        if len(row) > col_count:\n",
    "            data_rows[i] = row[:col_count]\n",
    "        elif len(row) < col_count:\n",
    "            data_rows[i].extend([\"\"]*(col_count - len(row)))\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify pages that contain \"Table 7\"\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*7?\\d*\", text, re.IGNORECASE):\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 7 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 2\n",
    "\n",
    "            print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end}\", file=log_file)\n",
    "\n",
    "            for page_number in range(scrape_start, min(scrape_end, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue\n",
    "\n",
    "                    table_bbox = table.bbox\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                table_title = match.group(2).strip()\n",
    "                                break\n",
    "\n",
    "                    if table_title:\n",
    "                        # New Table 7 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)\n",
    "                        print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        if df_new.empty:\n",
    "                            print(f\"The extracted DataFrame for table {table_index + 1} is empty. added in a place holder row to avoid empty dataframes.\", file=log_file)\n",
    "                            #added in a place holder row to avoid empty dataframes\n",
    "                            df_new.loc[0] = [None] * len(df_new.columns)\n",
    "\n",
    "                        # Handle new ADNU tables (grouping logic)\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table (new).\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                if \"type of upgrade\" not in df_new.columns:\n",
    "                                    # Group all adnu rows into one 'upgrade' row\n",
    "                                    adnu_values = df_new[\"adnu\"].dropna().astype(str).tolist()\n",
    "                                    grouped_adnu = \" \".join(adnu_values)\n",
    "                                    other_columns = df_new.drop(columns=[\"adnu\"]).iloc[0].to_dict()\n",
    "\n",
    "                                    df_grouped = pd.DataFrame({\n",
    "                                        \"upgrade\": [grouped_adnu],\n",
    "                                        \"type of upgrade\": [specific_phrase]\n",
    "                                    })\n",
    "\n",
    "                                    for col, value in other_columns.items():\n",
    "                                        df_grouped[col] = value\n",
    "\n",
    "                                    print(\"Grouped all 'adnu' rows into a single 'upgrade' row for new ADNU table.\", file=log_file)\n",
    "                                    df_new = df_grouped\n",
    "                                else:\n",
    "                                    # If 'type of upgrade' exists, just rename adnu if needed\n",
    "                                    if \"upgrade\" in df_new.columns:\n",
    "                                        df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                        print(\"Dropped 'adnu' column to avoid duplicate 'upgrade'.\", file=log_file)\n",
    "                                    else:\n",
    "                                        df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                        print(\"Renamed 'adnu' to 'upgrade' in new ADNU table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If 'type of upgrade' exists and first row none, original logic replaced only first row if needed\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                                    print(\"Replaced None in 'type of upgrade' first row for new ADNU table.\", file=log_file)\n",
    "                        else:\n",
    "                            # Non-ADNU new tables\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' to all rows in new non-ADNU table.\", file=log_file)\n",
    "                            else:\n",
    "                                # If exist and none in first row original logic replaced only first row\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(\"Replacing None in 'type of upgrade' for first row in new non-ADNU table.\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(\"Duplicate columns detected in new table. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous Table 7 title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        data_rows = tab\n",
    "                        print(f\"Data rows: {data_rows[0]}\")\n",
    "\n",
    "                              \n",
    "\n",
    "                    \n",
    "\n",
    "                        # Check if the number of columns matches\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        print(f\"Expected columns: {extracted_tables[-1].columns.tolist()}\" if extracted_tables else \"Expected columns: None\", file=log_file)\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\")\n",
    "                            continue  # No table to continue with\n",
    "\n",
    "\n",
    "                        # Debug the extracted tables and their columns\n",
    " \n",
    "\n",
    "                        \n",
    "\n",
    "                        # Define expected columns based on the last extracted table\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "\n",
    "                        # Detect header row in continuation\n",
    "                        header_keywords = [\"type of upgrade\", \"adnu\", \"MW at POI\"]\n",
    "                        first_continuation_row = data_rows[0] if data_rows else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "                            actual_header_row = data_rows[0]\n",
    "                            actual_headers = clean_column_headers(actual_header_row)\n",
    "                             \n",
    "\n",
    "                            # Go back to previous page bounding box to update table title if needed\n",
    "                            if page_number > 0:\n",
    "                                previous_page = pdf.pages[page_number - 1]\n",
    "                                bbox_lower_region = (0, table_bbox[1], previous_page.width, previous_page.height)\n",
    "                                title_text_previous = previous_page.within_bbox(bbox_lower_region).extract_text() or \"\"\n",
    "                                new_table_title = None\n",
    "                                if title_text_previous:\n",
    "                                    title_lines_prev = title_text_previous.split('\\n')[::-1]\n",
    "                                    for line in title_lines_prev:\n",
    "                                        line = line.strip()\n",
    "                                        match_prev = re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                                        if match_prev:\n",
    "                                            new_table_title = match_prev.group(2).strip()\n",
    "                                            break\n",
    "                                if new_table_title:\n",
    "                                    specific_phrase = extract_specific_phrase(new_table_title)\n",
    "                                    print(f\"Updated table title from previous page: '{specific_phrase}' for continuation table on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                else:\n",
    "                                    print(\"No table title found in previous page region. Using existing specific_phrase.\", file=log_file)\n",
    "                            else:\n",
    "                                print(\"No previous page available for title extraction for continuation table. Using existing specific_phrase.\", file=log_file)\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                            # Handle continuation ADNU or non-ADNU\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                                    \n",
    "                                print(\"Continuation ADNU table detected. No grouping, just rename and type of upgrade handling.\", file=log_file)\n",
    "                                  \n",
    "                                    \n",
    "                                #if \"adnu\" in data_rows[0]:\n",
    "                                    #\n",
    "                                 #   print(\"Handling continuation for 'Area Delivery Netowrk Upgrade' table\")\n",
    "                                 #   print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\")\n",
    "                                 #   \n",
    "                                 #   data_rows = [ [\"upgrade\"] + row[1:] for row in data_rows ]\n",
    "                                    \n",
    "\n",
    "                                if \"type of upgrade\" not in data_rows[0]:\n",
    "                                    # Insert 'type of upgrade' column at the beginning\n",
    "                                    print(\"Inserting 'type of upgrade' column with specific phrase in continuation table.\",file=log_file)\n",
    "                                    data_rows = [ [specific_phrase] + row for row in data_rows ]\n",
    "\n",
    "\n",
    "                                    if \"ADNU\" in data_rows[0]:\n",
    "                                        print(\"Handling continuation for 'Area Delivery Network Upgrade' table\",file=log_file)\n",
    "                                        print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\",file=log_file)\n",
    "                                        # Find the index where \"ADNU\" occurs in the first row\n",
    "                                        adnu_idx = data_rows[0].index(\"ADNU\")\n",
    "                                        # Replace \"ADNU\" with \"upgrade\" in that column for every row\n",
    "                                        #for r in range(len(data_rows)):\n",
    "                                        data_rows[0][adnu_idx] = \"upgrade\"\n",
    "\n",
    "\n",
    "\n",
    "                                    \n",
    "                                \n",
    "                        # Handle missing or extra columns\n",
    "                        if len(data_rows[0]) < expected_columns and len(data_rows[0]) > 0:\n",
    "                            if re.search(r\"Other\\s*Potentail\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                print(f\"Detected 'Other Potential Network Upgrade' table on page {page_number + 1}, table {table_index + 1}. No changes will be made even if column counts differ.\", file=log_file)\n",
    " \n",
    "                            elif re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                    # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                print(f\"Detected missing 'upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'upgrade' column.\",file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                           \n",
    "\n",
    "                            else:\n",
    "                                    # For other tables, assume missing \"Type of Upgrade\" column\n",
    "                                print(f\"Detected missing 'Type of Upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'Type of Upgrade' column.\",file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                                #row[:7] + [specific_phrase] + row[7:] for row in data_rows\n",
    "                        elif len(data_rows[0]) > expected_columns:\n",
    "                            # Extra columns detected; adjust accordingly\n",
    "                            print(f\"Detected extra columns in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping extra columns.\",file=log_file)\n",
    "                            data_rows = [row[:expected_columns] for row in data_rows]\n",
    "                            \n",
    "                        \n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "                        # Create DataFrame for the continuation table\n",
    "\n",
    "                        if is_header_row:    \n",
    "                            data_rows = data_rows[1:]\n",
    "                            print(f\"Dropped header row from data_rows after modifications for continuation table on page {page_number + 1}, table {table_index + 1}.\", file=log_file)\n",
    "    \n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\")\n",
    "                            continue  # Skip this table due to error\n",
    "\n",
    "                        if df_continuation.empty:\n",
    "                            print(f\"The continuation DataFrame on page {page_number + 1}, table {table_index + 1} is empty. Skipping this table.\", file=log_file)\n",
    "                            continue\n",
    "\n",
    "                        \n",
    "\n",
    "                            # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrades\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'type of upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"type of upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'type of upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                        else:\n",
    "                            # General Handling for other tables\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\",file=log_file)\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\",file=log_file)\n",
    "                            df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)            \n",
    "\n",
    "\n",
    "\n",
    "                                         \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 7 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After processing all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 7 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 7 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 7 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 7.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path, log_file):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            print(f\"Extracted Text: {text}\", file= log_file)  # Debugging step\n",
    "            # Case-insensitive check for 'Addendum'\n",
    "            return \"addendum\" in text.lower()\n",
    "            #return \"Addendum\" in text\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path, log_file)\n",
    "\n",
    "                    # Determine output DataFrame and CSV path based on addendum status\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "\n",
    "                    try:\n",
    "                        has_table7 = check_has_table7(pdf_path)\n",
    "                        if not has_table7:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                            # Print to ipynb output\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            # Extract base data from original PDF\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                        if is_add and base_data_extracted:\n",
    "                            # For addendums, use the extracted base data\n",
    "                            table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table7_data.empty:\n",
    "                                # Merge base data with Table 7 data\n",
    "                                merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                # Optionally, print to ipynb\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            # For originals, extract Table 7 data\n",
    "                            df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                # Optionally, print to ipynb\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        # Optionally, print to ipynb\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.map(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have Individually edit 1459, for some reason the scraper missed the revision, and it refuses to recognize the table 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the individually scraped ones with the original raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sorted dataset successfully created at: /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/03_raw/rawdata_cluster11_style_G_originals.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the datasets\n",
    "existing_dataset_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/rawdata_cluster11_style_G_originals.csv\"  # Update with your existing dataset path\n",
    "new_pdf_data_path_1 = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/rawdata_cluster11_style_G_1495.csv\"      # Update with your first new PDF data path\n",
    "new_pdf_data_path_2 = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/rawdata_cluster11_style_G_1506.csv\"      # Update with your second new PDF data path\n",
    "output_dataset_path = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/rawdata_cluster11_style_G_originals.csv\"      # Update with your desired output dataset path\n",
    "\n",
    "# Load the datasets\n",
    "original_df = pd.read_csv(existing_dataset_path)\n",
    "new_pdf_df_1 = pd.read_csv(new_pdf_data_path_1)\n",
    "new_pdf_df_2 = pd.read_csv(new_pdf_data_path_2)\n",
    "\n",
    "# Combine the two new PDF datasets\n",
    "new_pdf_df = pd.concat([new_pdf_df_1, new_pdf_df_2], ignore_index=True)\n",
    "\n",
    "# Combine the two new PDF datasets\n",
    "new_pdf_df = pd.concat([new_pdf_df_1, new_pdf_df_2], ignore_index=True)\n",
    "\n",
    "# Concatenate all datasets\n",
    "merged_df = pd.concat([original_df, new_pdf_df], ignore_index=True)\n",
    "\n",
    "merged_df = merged_df.sort_values(by=\"q_id\", kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "# Save the concatenated dataset to a new file\n",
    "merged_df.to_csv(output_dataset_path, index=False)\n",
    "\n",
    "print(f\"Final sorted dataset successfully created at: {output_dataset_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to created itemized and totals dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/969974649.py:65: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_1_cluster_11_style_G_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_1_cluster_11_style_G_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU' 'ADNU' 'OPNU']\n",
      "[1442 1443 1444 1445 1447 1449 1450 1451 1452 1453 1454 1455 1456 1457\n",
      " 1458 1459 1460 1461 1463 1464 1465 1466 1470 1471 1472 1473 1474 1475\n",
      " 1476 1477 1478 1479 1481 1482 1483 1484 1485 1488 1490 1491 1492 1493\n",
      " 1494 1495 1496 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1531\n",
      " 1532 1533]\n",
      "[11]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/rawdata_cluster11_style_G_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\"\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"estimated cost x 1000\",\n",
    "            \"allocated_cost\",\n",
    "            \"sum of allocated constant cost\"\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "df.drop('incremental deliverability', axis=1, inplace=True)\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "required_columns = ['type_of_upgrade', 'cost_allocation_factor']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    " \n",
    "\n",
    "# Step 1: Rename 'Grand Total' to 'Total' in total_estimated_cost_x_1000\n",
    "if 'total_estimated_cost_x_1000' in df.columns:\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Grand Total', 'Total')\n",
    "\n",
    "# Step 2: Move 'Total' from total_estimated_cost_x_1000 to cost_allocation_factor\n",
    "if 'total_estimated_cost_x_1000' in df.columns and 'cost_allocation_factor' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: 'Total' if str(row['total_estimated_cost_x_1000']) == 'Total' else row['cost_allocation_factor'],\n",
    "        axis=1\n",
    "    )\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Total', None)\n",
    "\n",
    "\n",
    "'''\n",
    "def clean_currency(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace('$', '').replace(',', '').strip()\n",
    "    return pd.to_numeric(value, errors='coerce')\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade) or 'Total' in upgrade:\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "        if len(rows) == 1:\n",
    "        # Create a total row\n",
    "        total_row = rows.iloc[0].copy()\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "        \n",
    "        # Sum the specified numeric columns if there are multiple rows\n",
    "        if len(rows) > 1:\n",
    "            for col in columns_to_sum:\n",
    "                total_row[col] = rows[col].sum()\n",
    "        \n",
    "        new_rows.append(total_row.to_dict())    \n",
    "'''\n",
    "df['type_of_upgrade'] = (\n",
    "    df['type_of_upgrade']\n",
    "    .fillna('')  # Temporarily replace NaN with an empty string\n",
    "    .str.replace(r'\\(Note \\d+\\)', '', regex=True)  # Remove (Note digit)\n",
    "    .str.strip()  # Strip leading/trailing whitespace\n",
    "    .str.title()  # Capitalize the first letter of each word\n",
    "    .str.replace(r'Upgrades$', 'Upgrade', regex=True)  # Fix plural endings\n",
    "    .replace('', pd.NA)  # Convert empty strings back to NaN\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'Reliability Network Upgrade': 'RNU',\n",
    "    'Local Delivery Network Upgrade': 'LDNU',\n",
    "    'Local Delivery Network': 'LDNU',\n",
    "    'Potential Local Delivery Network Upgrade': 'LDNU',\n",
    "    \"Ptos Interconnection Facilities\": 'PTO_IF',\n",
    "    'Area Delivery Network Upgrade': 'ADNU',\n",
    "    'Reliability Network Upgrade To Physically Interconnect': 'RNU',\n",
    "    \"Reliability Network upgrade To Physically Interconnect\": \"RNU\",\n",
    "     'Pto': 'PTO_IF',\n",
    "    \"Other Potential Network Upgrade\": \"OPNU\",\n",
    "    'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " \n",
    "  \n",
    " 'Total ADNU': 'ADNU',\n",
    "  \n",
    "\n",
    " \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()   \n",
    "\n",
    "    \n",
    "# Remove $ signs and convert to numeric\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000', 'adnu_cost_rate_x_1000_escalated']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000', 'adnu_cost_rate_x_1000_escalated']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade) or 'Total' in upgrade:\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "        \n",
    "        # Check if a Total row already exists\n",
    "        if not (\n",
    "            group['type_of_upgrade'].str.contains(f\"Total \", na=False).any() or \n",
    "            group['cost_allocation_factor'].str.contains(\"Total\", na=False).any()\n",
    "        ):\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "                total_row = rows.iloc[0].copy()\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "                new_rows.append(total_row.to_dict())\n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = rows.iloc[0].copy()\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "                for col in columns_to_sum:\n",
    "                    total_row[col] = rows[col].sum()\n",
    "                new_rows.append(total_row.to_dict())\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "'''        \n",
    "\n",
    "# Step 4: For each q_id and type_of_upgrade, if only one row and no total present, create new Total row\n",
    "new_rows = []\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade) or 'Total' in upgrade:\n",
    "            continue\n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "        if len(rows) == 1:\n",
    "            original_row = rows.iloc[0].copy()\n",
    "            total_row = original_row.copy()\n",
    "            total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "            total_row['item'] = 'no'\n",
    "            new_rows.append(total_row.to_dict())\n",
    "\n",
    "'''            \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 2: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "# Step 3: Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()\n",
    "\n",
    "if 'upgrade' in df.columns and 'type_of_upgrade' in df.columns and 'q_id' in df.columns:\n",
    "    df['upgrade'] = df.groupby(['q_id', 'type_of_upgrade'])['upgrade'].ffill()\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    "# Save itemized and totals separately\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_itemized.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_1_cluster_11_style_G_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_1_cluster_11_style_G_total.csv'.\")\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_1_cluster_10_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_1_cluster_10_total.csv'.\n",
      "['PTO_IF' 'RNU']\n",
      "[1466 1474 1491]\n",
      "[11]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/03_raw/rawdata_cluster11_style_G_addendums.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated without itcca\",\n",
    "            \"escalated cost x 1000\",\n",
    "            \"sum of allocated escalated cost\"\n",
    "        ],\n",
    "        \"estimated_cost_x_1000\": [\n",
    "            \"estimated cost x 1000\",\n",
    "            \"allocated_cost\",\n",
    "            \"sum of allocated constant cost\"\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000\": [\n",
    "            \"total nu cost\",\n",
    "            \"total cost constant\"\n",
    "        ],\n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\",\n",
    "            \"cost rate\",\n",
    "            \"cost rate constant\"\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\",\n",
    "            \"project mw\",\n",
    "            \"mw at poi\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\",\n",
    "            \"cost allocati on factor\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = merge_columns(df)\n",
    "#df.drop('incremental deliverability', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "df = df.map(clean_string_cell)\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "required_columns = ['type_of_upgrade', 'cost_allocation_factor']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    " \n",
    "\n",
    "# Step 1: Rename 'Grand Total' to 'Total' in total_estimated_cost_x_1000\n",
    "if 'total_estimated_cost_x_1000' in df.columns:\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Grand Total', 'Total')\n",
    "\n",
    "# Step 2: Move 'Total' from total_estimated_cost_x_1000 to cost_allocation_factor\n",
    "if 'total_estimated_cost_x_1000' in df.columns and 'cost_allocation_factor' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: 'Total' if str(row['total_estimated_cost_x_1000']) == 'Total' else row['cost_allocation_factor'],\n",
    "        axis=1\n",
    "    )\n",
    "    df['total_estimated_cost_x_1000'] = df['total_estimated_cost_x_1000'].replace('Total', None)\n",
    "\n",
    "\n",
    "'''\n",
    "def clean_currency(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace('$', '').replace(',', '').strip()\n",
    "    return pd.to_numeric(value, errors='coerce')\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade) or 'Total' in upgrade:\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "        if len(rows) == 1:\n",
    "        # Create a total row\n",
    "        total_row = rows.iloc[0].copy()\n",
    "        total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "        total_row['item'] = 'no'\n",
    "        \n",
    "        # Sum the specified numeric columns if there are multiple rows\n",
    "        if len(rows) > 1:\n",
    "            for col in columns_to_sum:\n",
    "                total_row[col] = rows[col].sum()\n",
    "        \n",
    "        new_rows.append(total_row.to_dict())    \n",
    "'''\n",
    "df['type_of_upgrade'] = (\n",
    "    df['type_of_upgrade']\n",
    "    .fillna('')  # Temporarily replace NaN with an empty string\n",
    "    .str.replace(r'\\(Note \\d+\\)', '', regex=True)  # Remove (Note digit)\n",
    "    .str.strip()  # Strip leading/trailing whitespace\n",
    "    .str.title()  # Capitalize the first letter of each word\n",
    "    .str.replace(r'Upgrades$', 'Upgrade', regex=True)  # Fix plural endings\n",
    "    .replace('', pd.NA)  # Convert empty strings back to NaN\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'Reliability Network Upgrade': 'RNU',\n",
    "    'Local Delivery Network Upgrade': 'LDNU',\n",
    "    'Local Delivery Network': 'LDNU',\n",
    "    'Potential Local Delivery Network Upgrade': 'LDNU',\n",
    "    \"Ptos Interconnection Facilities\": 'PTO_IF',\n",
    "    \"Ptos Interconnec- Tion Facilities\": 'PTO_IF', \n",
    "    'Area Delivery Network Upgrade': 'ADNU',\n",
    "    'Reliability Network Upgrade To Physically Interconnect': 'RNU',\n",
    "    \"Reliability Network upgrade To Physically Interconnect\": \"RNU\",\n",
    "     'Pto': 'PTO_IF',\n",
    "    \"Other Potential Network Upgrade\": \"OPNU\",\n",
    "    'Total PTO_IF': 'PTO_IF',\n",
    " 'Total RNU': 'RNU',\n",
    " 'Total LDNU': 'LDNU',\n",
    " 'Total OPNU' : 'OPNU',\n",
    " \n",
    "  \n",
    " 'Total ADNU': 'ADNU',\n",
    "  \n",
    "\n",
    " \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()   \n",
    "\n",
    "    \n",
    "# Remove $ signs and convert to numeric\n",
    "import re\n",
    "\n",
    "def clean_currency(value):\n",
    "    \"\"\"\n",
    "    Cleans a string by explicitly removing $, *, (Note 2), and similar patterns,\n",
    "    then converts it to a numeric value.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Explicitly remove $, *, and any \"(Note ...)\"\n",
    "        value = value.replace('$', '').replace('*', '')\n",
    "        value = re.sub(r'\\(Note \\d+\\)', '', value)  # Remove patterns like \"(Note 2)\"\n",
    "        value = value.replace(',', '').strip()  # Remove commas and extra spaces\n",
    "    try:\n",
    "        return pd.to_numeric(value)\n",
    "    except ValueError:\n",
    "        return pd.NA  # Return NaN for invalid entries\n",
    "\n",
    "\n",
    "# Clean the specific columns\n",
    "for col in ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000', 'adnu_cost_rate_x_1000_escalated']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_currency)\n",
    "\n",
    "# Create Total rows\n",
    "new_rows = []\n",
    "columns_to_sum = ['estimated_cost_x_1000', 'escalated_cost_x_1000', 'total_estimated_cost_x_1000_escalated', 'total_estimated_cost_x_1000', 'adnu_cost_rate_x_1000', 'adnu_cost_rate_x_1000_escalated']\n",
    "\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade) or 'Total' in upgrade:\n",
    "            continue\n",
    "        \n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "        \n",
    "        # Check if a Total row already exists\n",
    "        if not (\n",
    "            group['type_of_upgrade'].str.contains(f\"Total \", na=False).any() or \n",
    "            group['cost_allocation_factor'].str.contains(\"Total\", na=False).any()\n",
    "        ):\n",
    "            # If only one row exists, duplicate it as the total row\n",
    "            if len(rows) == 1:\n",
    "                total_row = rows.iloc[0].copy()\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "                new_rows.append(total_row.to_dict())\n",
    "            \n",
    "            # If multiple rows exist, sum numeric columns and create a total row\n",
    "            elif len(rows) > 1:\n",
    "                total_row = rows.iloc[0].copy()\n",
    "                total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "                total_row['item'] = 'no'\n",
    "                for col in columns_to_sum:\n",
    "                    total_row[col] = rows[col].sum()\n",
    "                new_rows.append(total_row.to_dict())\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "'''        \n",
    "\n",
    "# Step 4: For each q_id and type_of_upgrade, if only one row and no total present, create new Total row\n",
    "new_rows = []\n",
    "for q_id, group in df.groupby('q_id', as_index=False):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "    for upgrade in unique_upgrades:\n",
    "        if pd.isna(upgrade) or 'Total' in upgrade:\n",
    "            continue\n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "        if len(rows) == 1:\n",
    "            original_row = rows.iloc[0].copy()\n",
    "            total_row = original_row.copy()\n",
    "            total_row['type_of_upgrade'] = f\"Total {upgrade}\"\n",
    "            total_row['item'] = 'no'\n",
    "            new_rows.append(total_row.to_dict())\n",
    "\n",
    "'''            \n",
    "\n",
    "if new_rows:\n",
    "    total_rows_df = pd.DataFrame(new_rows)\n",
    "    for col in df.columns:\n",
    "        if col not in total_rows_df.columns:\n",
    "            total_rows_df[col] = None\n",
    "    total_rows_df = total_rows_df[df.columns]\n",
    "    df = pd.concat([df, total_rows_df], ignore_index=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update 'item' column based on Total in type_of_upgrade or cost_allocation_factor\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        'Total' in str(row.get('type_of_upgrade', '')) or \n",
    "        'Total' in str(row.get('cost_allocation_factor', ''))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Step 2: Move 'item' column next to 'type_of_upgrade'\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "# Step 3: Remove \"Total\" values from cost_allocation_factor if they appear in type_of_upgrade\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()\n",
    "\n",
    "if 'upgrade' in df.columns and 'type_of_upgrade' in df.columns and 'q_id' in df.columns:\n",
    "    df['upgrade'] = df.groupby(['q_id', 'type_of_upgrade'])['upgrade'].ffill()\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type_of_upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost_allocation_factor\",\n",
    "        \"estimated_cost_x_1000\",\n",
    "        \"escalated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000\",\n",
    "        \"total_estimated_cost_x_1000_escalated\",\n",
    "        \"estimated_time_to_construct\",\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df        \n",
    "\n",
    "\n",
    "df= reorder_columns(df)\n",
    "\n",
    "\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "     \n",
    "     \n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_itemized_addendums.csv', index=False)\n",
    "\n",
    "    totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "    existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    \n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_total_addendums.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_1_cluster_10_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_1_cluster_10_total.csv'.\")\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the addendum and original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing itemized: q_id=1466, type_of_upgrade=PTO_IF\n",
      "Length of addendum_rows: 4\n",
      "Length of original_rows: 3\n",
      "addendum_rows:\n",
      "   q_id  cluster req_deliverability latitude longitude capacity  \\\n",
      "0  1466       11                                                  \n",
      "1  1466       11                                                  \n",
      "2  1466       11                                                  \n",
      "3  1466       11                                                  \n",
      "\n",
      "  point_of_interconnection type_of_upgrade            upgrade  \\\n",
      "0                                   PTO_IF   Pease Substation   \n",
      "1                                   PTO_IF   Pease Substation   \n",
      "2                                   PTO_IF    Generation Site   \n",
      "3                                   PTO_IF  Transmission Line   \n",
      "\n",
      "                                         description cost_allocation_factor  \\\n",
      "0  Install double-bus/single-breaker line bay, CB...                100.00%   \n",
      "1  Install fiber terminations near the fence for ...                100.00%   \n",
      "2  PG&E Pre-Parallel Inspection, Engineering Revi...                100.00%   \n",
      "3  Install 2 spans of conductors, 1 TSP single ci...                100.00%   \n",
      "\n",
      "   estimated_cost_x_1000  escalated_cost_x_1000 estimated_time_to_construct  \\\n",
      "0                 1360.0                 1521.0                        36.0   \n",
      "1                  170.0                  190.0                        36.0   \n",
      "2                  425.0                  475.0                        36.0   \n",
      "3                  430.0                  481.0                        36.0   \n",
      "\n",
      "  item upgrade_classification estimated caiso_queue project_type  \\\n",
      "0  yes                                                             \n",
      "1  yes                                                             \n",
      "2  yes                                                             \n",
      "3  yes                                                             \n",
      "\n",
      "  dependent_system_upgrade  \n",
      "0                           \n",
      "1                           \n",
      "2                           \n",
      "3                           \n",
      "original_rows:\n",
      "     q_id  cluster req_deliverability   latitude longitude capacity  \\\n",
      "270  1466       11               Full  39.171111    121.66            \n",
      "271  1466       11               Full  39.171111    121.66            \n",
      "272  1466       11               Full  39.171111    121.66            \n",
      "\n",
      "    point_of_interconnection type_of_upgrade            upgrade  \\\n",
      "270   Pease Substation 60 kV          PTO_IF   Pease Substation   \n",
      "271   Pease Substation 60 kV          PTO_IF    Generation Site   \n",
      "272   Pease Substation 60 kV          PTO_IF  Transmission Line   \n",
      "\n",
      "                                           description  ...  \\\n",
      "270  Install double-bus/single- breaker line bay, C...  ...   \n",
      "271  PG&E Pre-Parallel Inspection, Engineering Revi...  ...   \n",
      "272  Install 2 spans of conductors, 1 TSP single ci...  ...   \n",
      "\n",
      "    escalated_cost_x_1000  total_estimated_cost_x_1000  \\\n",
      "270                1521.0                                \n",
      "271                 475.0                                \n",
      "272                 481.0                                \n",
      "\n",
      "     total_estimated_cost_x_1000_escalated estimated_time_to_construct  item  \\\n",
      "270                                    0.0                        36.0   yes   \n",
      "271                                    0.0                        36.0   yes   \n",
      "272                                    0.0                        36.0   yes   \n",
      "\n",
      "    estimated_cost_x_1000_escalated_with_itcca  \\\n",
      "270                                              \n",
      "271                                              \n",
      "272                                              \n",
      "\n",
      "    adnu_cost_rate_x_1000_escalated adnu_cost_rate_x_1000  original  row_order  \n",
      "270                             0.0                   0.0       yes        270  \n",
      "271                             0.0                   0.0       yes        271  \n",
      "272                             0.0                   0.0       yes        272  \n",
      "\n",
      "[3 rows x 22 columns]\n",
      "Before replacement: req_deliverability\n",
      "Addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: req_deliverability, dtype: object\n",
      "Original values:\n",
      "270    Full\n",
      "271    Full\n",
      "272    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "After replacement: req_deliverability\n",
      "Updated addendum values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3        \n",
      "Name: req_deliverability, dtype: object\n",
      "Before replacement: latitude\n",
      "Addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: latitude, dtype: object\n",
      "Original values:\n",
      "270    39.171111\n",
      "271    39.171111\n",
      "272    39.171111\n",
      "Name: latitude, dtype: object\n",
      "After replacement: latitude\n",
      "Updated addendum values:\n",
      "0    39.171111\n",
      "1    39.171111\n",
      "2    39.171111\n",
      "3             \n",
      "Name: latitude, dtype: object\n",
      "Before replacement: longitude\n",
      "Addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: longitude, dtype: object\n",
      "Original values:\n",
      "270    121.66\n",
      "271    121.66\n",
      "272    121.66\n",
      "Name: longitude, dtype: object\n",
      "After replacement: longitude\n",
      "Updated addendum values:\n",
      "0    121.66\n",
      "1    121.66\n",
      "2    121.66\n",
      "3          \n",
      "Name: longitude, dtype: object\n",
      "Before replacement: capacity\n",
      "Addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: capacity, dtype: object\n",
      "Original values:\n",
      "270    \n",
      "271    \n",
      "272    \n",
      "Name: capacity, dtype: object\n",
      "After replacement: capacity\n",
      "Updated addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: capacity, dtype: object\n",
      "Before replacement: point_of_interconnection\n",
      "Addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: point_of_interconnection, dtype: object\n",
      "Original values:\n",
      "270    Pease Substation 60 kV\n",
      "271    Pease Substation 60 kV\n",
      "272    Pease Substation 60 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "After replacement: point_of_interconnection\n",
      "Updated addendum values:\n",
      "0    Pease Substation 60 kV\n",
      "1    Pease Substation 60 kV\n",
      "2    Pease Substation 60 kV\n",
      "3                          \n",
      "Name: point_of_interconnection, dtype: object\n",
      "Processing itemized: q_id=1474, type_of_upgrade=PTO_IF\n",
      "Length of addendum_rows: 6\n",
      "Length of original_rows: 6\n",
      "addendum_rows:\n",
      "   q_id  cluster req_deliverability latitude longitude capacity  \\\n",
      "4  1474       11                                                  \n",
      "5  1474       11                                                  \n",
      "6  1474       11                                                  \n",
      "7  1474       11                                                  \n",
      "8  1474       11                                                  \n",
      "9  1474       11                                                  \n",
      "\n",
      "  point_of_interconnection type_of_upgrade                       upgrade  \\\n",
      "4                                   PTO_IF  Diablo Canyon PP Terminal #1   \n",
      "5                                   PTO_IF  Diablo Canyon PP Terminal #2   \n",
      "6                                   PTO_IF              Diablo Canyon PP   \n",
      "7                                   PTO_IF             Transmission Line   \n",
      "8                                   PTO_IF             Transmission Line   \n",
      "9                                   PTO_IF               Generation Site   \n",
      "\n",
      "                                         description cost_allocation_factor  \\\n",
      "4  Install New Current Differential line relay pa...                100.00%   \n",
      "5  Install New Current Differential line relay pa...                100.00%   \n",
      "6  IT fiber termination and cable from substation...                100.00%   \n",
      "7  Install (3) 500 kV spans of transmission line ...                100.00%   \n",
      "8  Transmission Line Terminal #2: Install (3) 500...                100.00%   \n",
      "9  Engineering Reviews, Metering, Pre-parallel In...                100.00%   \n",
      "\n",
      "   estimated_cost_x_1000  escalated_cost_x_1000 estimated_time_to_construct  \\\n",
      "4                 2400.0                 2810.0                        40.0   \n",
      "5                 2400.0                 2810.0                        40.0   \n",
      "6                  340.0                  398.0                        40.0   \n",
      "7                  870.0                 1019.0                        40.0   \n",
      "8                  870.0                 1019.0                        40.0   \n",
      "9                  370.0                  433.0                        40.0   \n",
      "\n",
      "  item upgrade_classification estimated caiso_queue project_type  \\\n",
      "4  yes                                                             \n",
      "5  yes                                                             \n",
      "6  yes                                                             \n",
      "7  yes                                                             \n",
      "8  yes                                                             \n",
      "9  yes                                                             \n",
      "\n",
      "  dependent_system_upgrade  \n",
      "4                           \n",
      "5                           \n",
      "6                           \n",
      "7                           \n",
      "8                           \n",
      "9                           \n",
      "original_rows:\n",
      "     q_id  cluster req_deliverability   latitude    longitude capacity  \\\n",
      "351  1474       11               Full  35.208779  -120.854101            \n",
      "352  1474       11               Full  35.208779  -120.854101            \n",
      "353  1474       11               Full  35.208779  -120.854101            \n",
      "354  1474       11               Full  35.208779  -120.854101            \n",
      "355  1474       11               Full  35.208779  -120.854101            \n",
      "356  1474       11               Full  35.208779  -120.854101            \n",
      "\n",
      "                              point_of_interconnection type_of_upgrade  \\\n",
      "351  Diablo Canyon Power Plant Switching Station 50...          PTO_IF   \n",
      "352  Diablo Canyon Power Plant Switching Station 50...          PTO_IF   \n",
      "353  Diablo Canyon Power Plant Switching Station 50...          PTO_IF   \n",
      "354  Diablo Canyon Power Plant Switching Station 50...          PTO_IF   \n",
      "355  Diablo Canyon Power Plant Switching Station 50...          PTO_IF   \n",
      "356  Diablo Canyon Power Plant Switching Station 50...          PTO_IF   \n",
      "\n",
      "                          upgrade  \\\n",
      "351  Diablo Canyon PP Terminal #1   \n",
      "352  Diablo Canyon PP Terminal #2   \n",
      "353              Diablo Canyon PP   \n",
      "354    Tierra Buena Energy Center   \n",
      "355             Transmission Line   \n",
      "356               Generation Site   \n",
      "\n",
      "                                           description  ...  \\\n",
      "351  Install New Current Differential line relay pa...  ...   \n",
      "352  Install New Current Differential line relay pa...  ...   \n",
      "353  IT fiber termination and cable from substation...  ...   \n",
      "354  Install (3) 500 kV spans of transmission line ...  ...   \n",
      "355  Transmission Line Terminal #2: Install (3) 500...  ...   \n",
      "356  Engineering Reviews, Metering, Pre-parallel In...  ...   \n",
      "\n",
      "    escalated_cost_x_1000  total_estimated_cost_x_1000  \\\n",
      "351                2810.0                                \n",
      "352                2810.0                                \n",
      "353                 398.0                                \n",
      "354                1019.0                                \n",
      "355                1019.0                                \n",
      "356                 433.0                                \n",
      "\n",
      "     total_estimated_cost_x_1000_escalated estimated_time_to_construct  item  \\\n",
      "351                                    0.0                        40.0   yes   \n",
      "352                                    0.0                        40.0   yes   \n",
      "353                                    0.0                        40.0   yes   \n",
      "354                                    0.0                        40.0   yes   \n",
      "355                                    0.0                        40.0   yes   \n",
      "356                                    0.0                        40.0   yes   \n",
      "\n",
      "    estimated_cost_x_1000_escalated_with_itcca  \\\n",
      "351                                              \n",
      "352                                              \n",
      "353                                              \n",
      "354                                              \n",
      "355                                              \n",
      "356                                              \n",
      "\n",
      "    adnu_cost_rate_x_1000_escalated adnu_cost_rate_x_1000  original  row_order  \n",
      "351                             0.0                   0.0       yes        351  \n",
      "352                             0.0                   0.0       yes        352  \n",
      "353                             0.0                   0.0       yes        353  \n",
      "354                             0.0                   0.0       yes        354  \n",
      "355                             0.0                   0.0       yes        355  \n",
      "356                             0.0                   0.0       yes        356  \n",
      "\n",
      "[6 rows x 22 columns]\n",
      "Before replacement: req_deliverability\n",
      "Addendum values:\n",
      "4    \n",
      "5    \n",
      "6    \n",
      "7    \n",
      "8    \n",
      "9    \n",
      "Name: req_deliverability, dtype: object\n",
      "Original values:\n",
      "351    Full\n",
      "352    Full\n",
      "353    Full\n",
      "354    Full\n",
      "355    Full\n",
      "356    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "After replacement: req_deliverability\n",
      "Updated addendum values:\n",
      "4    Full\n",
      "5    Full\n",
      "6        \n",
      "7        \n",
      "8        \n",
      "9        \n",
      "Name: req_deliverability, dtype: object\n",
      "Before replacement: latitude\n",
      "Addendum values:\n",
      "4    \n",
      "5    \n",
      "6    \n",
      "7    \n",
      "8    \n",
      "9    \n",
      "Name: latitude, dtype: object\n",
      "Original values:\n",
      "351    35.208779\n",
      "352    35.208779\n",
      "353    35.208779\n",
      "354    35.208779\n",
      "355    35.208779\n",
      "356    35.208779\n",
      "Name: latitude, dtype: object\n",
      "After replacement: latitude\n",
      "Updated addendum values:\n",
      "4    35.208779\n",
      "5    35.208779\n",
      "6             \n",
      "7             \n",
      "8             \n",
      "9             \n",
      "Name: latitude, dtype: object\n",
      "Before replacement: longitude\n",
      "Addendum values:\n",
      "4    \n",
      "5    \n",
      "6    \n",
      "7    \n",
      "8    \n",
      "9    \n",
      "Name: longitude, dtype: object\n",
      "Original values:\n",
      "351    -120.854101\n",
      "352    -120.854101\n",
      "353    -120.854101\n",
      "354    -120.854101\n",
      "355    -120.854101\n",
      "356    -120.854101\n",
      "Name: longitude, dtype: object\n",
      "After replacement: longitude\n",
      "Updated addendum values:\n",
      "4    -120.854101\n",
      "5    -120.854101\n",
      "6               \n",
      "7               \n",
      "8               \n",
      "9               \n",
      "Name: longitude, dtype: object\n",
      "Before replacement: capacity\n",
      "Addendum values:\n",
      "4    \n",
      "5    \n",
      "6    \n",
      "7    \n",
      "8    \n",
      "9    \n",
      "Name: capacity, dtype: object\n",
      "Original values:\n",
      "351    \n",
      "352    \n",
      "353    \n",
      "354    \n",
      "355    \n",
      "356    \n",
      "Name: capacity, dtype: object\n",
      "After replacement: capacity\n",
      "Updated addendum values:\n",
      "4    \n",
      "5    \n",
      "6    \n",
      "7    \n",
      "8    \n",
      "9    \n",
      "Name: capacity, dtype: object\n",
      "Before replacement: point_of_interconnection\n",
      "Addendum values:\n",
      "4    \n",
      "5    \n",
      "6    \n",
      "7    \n",
      "8    \n",
      "9    \n",
      "Name: point_of_interconnection, dtype: object\n",
      "Original values:\n",
      "351    Diablo Canyon Power Plant Switching Station 50...\n",
      "352    Diablo Canyon Power Plant Switching Station 50...\n",
      "353    Diablo Canyon Power Plant Switching Station 50...\n",
      "354    Diablo Canyon Power Plant Switching Station 50...\n",
      "355    Diablo Canyon Power Plant Switching Station 50...\n",
      "356    Diablo Canyon Power Plant Switching Station 50...\n",
      "Name: point_of_interconnection, dtype: object\n",
      "After replacement: point_of_interconnection\n",
      "Updated addendum values:\n",
      "4    Diablo Canyon Power Plant Switching Station 50...\n",
      "5    Diablo Canyon Power Plant Switching Station 50...\n",
      "6                                                     \n",
      "7                                                     \n",
      "8                                                     \n",
      "9                                                     \n",
      "Name: point_of_interconnection, dtype: object\n",
      "Processing itemized: q_id=1474, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 3\n",
      "Length of original_rows: 3\n",
      "addendum_rows:\n",
      "    q_id  cluster req_deliverability latitude longitude capacity  \\\n",
      "10  1474       11                                                  \n",
      "11  1474       11                                                  \n",
      "12  1474       11                                                  \n",
      "\n",
      "   point_of_interconnection type_of_upgrade  \\\n",
      "10                                      RNU   \n",
      "11                                      RNU   \n",
      "12                                      RNU   \n",
      "\n",
      "                                              upgrade  \\\n",
      "10                       Diablo Canyon PP Terminal #1   \n",
      "11                       Diablo Canyon PP Terminal #2   \n",
      "12  QC11P1RAS-14 Diablo Canyon 500 kV line outage RAS   \n",
      "\n",
      "                                          description cost_allocation_factor  \\\n",
      "10                               Install (1) 500kV CB                100.00%   \n",
      "11  Install new BAAH bay w/ (2) 500kV CB, grading,...                100.00%   \n",
      "12  QC11P1RAS-14 to trip ~800 MW of Q1474 for loss...                100.00%   \n",
      "\n",
      "    estimated_cost_x_1000  escalated_cost_x_1000 estimated_time_to_construct  \\\n",
      "10                 2800.0                 3279.0                        40.0   \n",
      "11                18000.0                21077.0                        40.0   \n",
      "12                 4500.0                 5032.0                        36.0   \n",
      "\n",
      "   item upgrade_classification estimated caiso_queue project_type  \\\n",
      "10  yes                                                             \n",
      "11  yes                                                             \n",
      "12  yes                                                             \n",
      "\n",
      "   dependent_system_upgrade  \n",
      "10                           \n",
      "11                           \n",
      "12                           \n",
      "original_rows:\n",
      "     q_id  cluster req_deliverability   latitude    longitude capacity  \\\n",
      "357  1474       11               Full  35.208779  -120.854101            \n",
      "358  1474       11               Full  35.208779  -120.854101            \n",
      "359  1474       11               Full  35.208779  -120.854101            \n",
      "\n",
      "                              point_of_interconnection type_of_upgrade  \\\n",
      "357  Diablo Canyon Power Plant Switching Station 50...             RNU   \n",
      "358  Diablo Canyon Power Plant Switching Station 50...             RNU   \n",
      "359  Diablo Canyon Power Plant Switching Station 50...             RNU   \n",
      "\n",
      "                                               upgrade  \\\n",
      "357                       Diablo Canyon PP Terminal #1   \n",
      "358                       Diablo Canyon PP Terminal #2   \n",
      "359  QC11P1RAS-14 Diablo Canyon 500 kV double line ...   \n",
      "\n",
      "                                           description  ...  \\\n",
      "357                               Install (1) 500kV CB  ...   \n",
      "358  Install new BAAH bay w/  (2) 500kV CB, grading...  ...   \n",
      "359  QC11P1RAS-14 to trip up to 800 MW of Q1474 for...  ...   \n",
      "\n",
      "    escalated_cost_x_1000  total_estimated_cost_x_1000  \\\n",
      "357                3279.0                                \n",
      "358               21077.0                                \n",
      "359                5032.0                                \n",
      "\n",
      "     total_estimated_cost_x_1000_escalated estimated_time_to_construct  item  \\\n",
      "357                                    0.0                        40.0   yes   \n",
      "358                                    0.0                        40.0   yes   \n",
      "359                                    0.0                        36.0   yes   \n",
      "\n",
      "    estimated_cost_x_1000_escalated_with_itcca  \\\n",
      "357                                              \n",
      "358                                              \n",
      "359                                              \n",
      "\n",
      "    adnu_cost_rate_x_1000_escalated adnu_cost_rate_x_1000  original  row_order  \n",
      "357                             0.0                   0.0       yes        357  \n",
      "358                             0.0                   0.0       yes        358  \n",
      "359                             0.0                   0.0       yes        359  \n",
      "\n",
      "[3 rows x 22 columns]\n",
      "Before replacement: req_deliverability\n",
      "Addendum values:\n",
      "10    \n",
      "11    \n",
      "12    \n",
      "Name: req_deliverability, dtype: object\n",
      "Original values:\n",
      "357    Full\n",
      "358    Full\n",
      "359    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "After replacement: req_deliverability\n",
      "Updated addendum values:\n",
      "10    \n",
      "11    \n",
      "12    \n",
      "Name: req_deliverability, dtype: object\n",
      "Before replacement: latitude\n",
      "Addendum values:\n",
      "10    \n",
      "11    \n",
      "12    \n",
      "Name: latitude, dtype: object\n",
      "Original values:\n",
      "357    35.208779\n",
      "358    35.208779\n",
      "359    35.208779\n",
      "Name: latitude, dtype: object\n",
      "After replacement: latitude\n",
      "Updated addendum values:\n",
      "10    \n",
      "11    \n",
      "12    \n",
      "Name: latitude, dtype: object\n",
      "Before replacement: longitude\n",
      "Addendum values:\n",
      "10    \n",
      "11    \n",
      "12    \n",
      "Name: longitude, dtype: object\n",
      "Original values:\n",
      "357    -120.854101\n",
      "358    -120.854101\n",
      "359    -120.854101\n",
      "Name: longitude, dtype: object\n",
      "After replacement: longitude\n",
      "Updated addendum values:\n",
      "10    \n",
      "11    \n",
      "12    \n",
      "Name: longitude, dtype: object\n",
      "Before replacement: capacity\n",
      "Addendum values:\n",
      "10    \n",
      "11    \n",
      "12    \n",
      "Name: capacity, dtype: object\n",
      "Original values:\n",
      "357    \n",
      "358    \n",
      "359    \n",
      "Name: capacity, dtype: object\n",
      "After replacement: capacity\n",
      "Updated addendum values:\n",
      "10    \n",
      "11    \n",
      "12    \n",
      "Name: capacity, dtype: object\n",
      "Before replacement: point_of_interconnection\n",
      "Addendum values:\n",
      "10    \n",
      "11    \n",
      "12    \n",
      "Name: point_of_interconnection, dtype: object\n",
      "Original values:\n",
      "357    Diablo Canyon Power Plant Switching Station 50...\n",
      "358    Diablo Canyon Power Plant Switching Station 50...\n",
      "359    Diablo Canyon Power Plant Switching Station 50...\n",
      "Name: point_of_interconnection, dtype: object\n",
      "After replacement: point_of_interconnection\n",
      "Updated addendum values:\n",
      "10    \n",
      "11    \n",
      "12    \n",
      "Name: point_of_interconnection, dtype: object\n",
      "Processing itemized: q_id=1491, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 4\n",
      "Length of original_rows: 3\n",
      "addendum_rows:\n",
      "    q_id  cluster req_deliverability   latitude    longitude capacity  \\\n",
      "13  1491       11               Full  40.740869  -124.210164            \n",
      "14  1491       11               Full  40.740869  -124.210164            \n",
      "15  1491       11               Full  40.740869  -124.210164            \n",
      "16  1491       11               Full  40.740869  -124.210164            \n",
      "\n",
      "      point_of_interconnection type_of_upgrade                 upgrade  \\\n",
      "13  Humboldt Substation 115 kV             RNU     Humboldt Substation   \n",
      "14  Humboldt Substation 115 kV             RNU      Trinity Substation   \n",
      "15  Humboldt Substation 115 kV             RNU   Cottonwood Substation   \n",
      "16  Humboldt Substation 115 kV             RNU  Bridgeville Substation   \n",
      "\n",
      "                                      description cost_allocation_factor  \\\n",
      "13  Expansion of 115kV Buses & Bus Selector Sw's.                100.00%   \n",
      "14                Install one (1) DTT Transmitter                100.00%   \n",
      "15                Install one (1) DTT Transmitter                100.00%   \n",
      "16                Install one (1) DTT Transmitter                100.00%   \n",
      "\n",
      "    estimated_cost_x_1000  escalated_cost_x_1000 estimated_time_to_construct  \\\n",
      "13                  900.0                 1030.0                        38.0   \n",
      "14                  390.0                  446.0                        38.0   \n",
      "15                  390.0                  446.0                        38.0   \n",
      "16                  390.0                  446.0                        38.0   \n",
      "\n",
      "   item upgrade_classification estimated caiso_queue project_type  \\\n",
      "13  yes                                                             \n",
      "14  yes                                                             \n",
      "15  yes                                                             \n",
      "16  yes                                                             \n",
      "\n",
      "   dependent_system_upgrade  \n",
      "13                           \n",
      "14                           \n",
      "15                           \n",
      "16                           \n",
      "original_rows:\n",
      "     q_id  cluster req_deliverability   latitude    longitude capacity  \\\n",
      "647  1491       11               Full  40.740869  -124.210164            \n",
      "648  1491       11               Full  40.740869  -124.210164            \n",
      "649  1491       11               Full  40.740869  -124.210164            \n",
      "\n",
      "       point_of_interconnection type_of_upgrade                 upgrade  \\\n",
      "647  Humboldt Substation 115 kV             RNU     Humboldt Substation   \n",
      "648  Humboldt Substation 115 kV             RNU      Trinity Substation   \n",
      "649  Humboldt Substation 115 kV             RNU  Bridgeville Substation   \n",
      "\n",
      "                                       description  ... escalated_cost_x_1000  \\\n",
      "647  Expansion of 115kV Buses & Bus Selector Sw's.  ...                1030.0   \n",
      "648                Install one (1) DTT Transmitter  ...                 446.0   \n",
      "649                Install one (1) DTT Transmitter  ...                 446.0   \n",
      "\n",
      "     total_estimated_cost_x_1000  total_estimated_cost_x_1000_escalated  \\\n",
      "647                                                                 0.0   \n",
      "648                                                                 0.0   \n",
      "649                                                                 0.0   \n",
      "\n",
      "    estimated_time_to_construct  item  \\\n",
      "647                        38.0   yes   \n",
      "648                        38.0   yes   \n",
      "649                        38.0   yes   \n",
      "\n",
      "    estimated_cost_x_1000_escalated_with_itcca  \\\n",
      "647                                              \n",
      "648                                              \n",
      "649                                              \n",
      "\n",
      "    adnu_cost_rate_x_1000_escalated adnu_cost_rate_x_1000  original  row_order  \n",
      "647                             0.0                   0.0       yes        647  \n",
      "648                             0.0                   0.0       yes        648  \n",
      "649                             0.0                   0.0       yes        649  \n",
      "\n",
      "[3 rows x 22 columns]\n",
      "Before replacement: req_deliverability\n",
      "Addendum values:\n",
      "13    Full\n",
      "14    Full\n",
      "15    Full\n",
      "16    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "Original values:\n",
      "647    Full\n",
      "648    Full\n",
      "649    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "After replacement: req_deliverability\n",
      "Updated addendum values:\n",
      "13    Full\n",
      "14    Full\n",
      "15    Full\n",
      "16    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "Before replacement: latitude\n",
      "Addendum values:\n",
      "13    40.740869\n",
      "14    40.740869\n",
      "15    40.740869\n",
      "16    40.740869\n",
      "Name: latitude, dtype: object\n",
      "Original values:\n",
      "647    40.740869\n",
      "648    40.740869\n",
      "649    40.740869\n",
      "Name: latitude, dtype: object\n",
      "After replacement: latitude\n",
      "Updated addendum values:\n",
      "13    40.740869\n",
      "14    40.740869\n",
      "15    40.740869\n",
      "16    40.740869\n",
      "Name: latitude, dtype: object\n",
      "Before replacement: longitude\n",
      "Addendum values:\n",
      "13    -124.210164\n",
      "14    -124.210164\n",
      "15    -124.210164\n",
      "16    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "Original values:\n",
      "647    -124.210164\n",
      "648    -124.210164\n",
      "649    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "After replacement: longitude\n",
      "Updated addendum values:\n",
      "13    -124.210164\n",
      "14    -124.210164\n",
      "15    -124.210164\n",
      "16    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "Before replacement: capacity\n",
      "Addendum values:\n",
      "13    \n",
      "14    \n",
      "15    \n",
      "16    \n",
      "Name: capacity, dtype: object\n",
      "Original values:\n",
      "647    \n",
      "648    \n",
      "649    \n",
      "Name: capacity, dtype: object\n",
      "After replacement: capacity\n",
      "Updated addendum values:\n",
      "13    \n",
      "14    \n",
      "15    \n",
      "16    \n",
      "Name: capacity, dtype: object\n",
      "Before replacement: point_of_interconnection\n",
      "Addendum values:\n",
      "13    Humboldt Substation 115 kV\n",
      "14    Humboldt Substation 115 kV\n",
      "15    Humboldt Substation 115 kV\n",
      "16    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "Original values:\n",
      "647    Humboldt Substation 115 kV\n",
      "648    Humboldt Substation 115 kV\n",
      "649    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "After replacement: point_of_interconnection\n",
      "Updated addendum values:\n",
      "13    Humboldt Substation 115 kV\n",
      "14    Humboldt Substation 115 kV\n",
      "15    Humboldt Substation 115 kV\n",
      "16    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "Processing itemized: q_id=1466, type_of_upgrade=PTO_IF\n",
      "Length of addendum_rows: 4\n",
      "Length of original_rows: 4\n",
      "addendum_rows:\n",
      "   q_id  cluster req_deliverability   latitude    longitude capacity  \\\n",
      "0  1491       11               Full  40.740869  -124.210164            \n",
      "1  1491       11               Full  40.740869  -124.210164            \n",
      "2  1491       11               Full  40.740869  -124.210164            \n",
      "3  1491       11               Full  40.740869  -124.210164            \n",
      "\n",
      "     point_of_interconnection type_of_upgrade                 upgrade  \\\n",
      "0  Humboldt Substation 115 kV             RNU     Humboldt Substation   \n",
      "1  Humboldt Substation 115 kV             RNU      Trinity Substation   \n",
      "2  Humboldt Substation 115 kV             RNU   Cottonwood Substation   \n",
      "3  Humboldt Substation 115 kV             RNU  Bridgeville Substation   \n",
      "\n",
      "                                     description  ... estimated  caiso_queue  \\\n",
      "0  Expansion of 115kV Buses & Bus Selector Sw's.  ...                          \n",
      "1                Install one (1) DTT Transmitter  ...                          \n",
      "2                Install one (1) DTT Transmitter  ...                          \n",
      "3                Install one (1) DTT Transmitter  ...                          \n",
      "\n",
      "   project_type dependent_system_upgrade  \\\n",
      "0                                          \n",
      "1                                          \n",
      "2                                          \n",
      "3                                          \n",
      "\n",
      "  estimated_cost_x_1000_escalated_with_itcca adnu_cost_rate_x_1000  \\\n",
      "0                                          0                     0   \n",
      "1                                          0                     0   \n",
      "2                                          0                     0   \n",
      "3                                          0                     0   \n",
      "\n",
      "  adnu_cost_rate_x_1000_escalated total_estimated_cost_x_1000  \\\n",
      "0                               0                           0   \n",
      "1                               0                           0   \n",
      "2                               0                           0   \n",
      "3                               0                           0   \n",
      "\n",
      "  total_estimated_cost_x_1000_escalated row_order  \n",
      "0                                     0         0  \n",
      "1                                     0         0  \n",
      "2                                     0         0  \n",
      "3                                     0         0  \n",
      "\n",
      "[4 rows x 26 columns]\n",
      "original_rows:\n",
      "   q_id cluster req_deliverability   latitude    longitude capacity  \\\n",
      "0  1491      11               Full  40.740869  -124.210164            \n",
      "1  1491      11               Full  40.740869  -124.210164            \n",
      "2  1491      11               Full  40.740869  -124.210164            \n",
      "3  <NA>    <NA>                NaN        NaN          NaN      NaN   \n",
      "\n",
      "     point_of_interconnection type_of_upgrade                 upgrade  \\\n",
      "0  Humboldt Substation 115 kV             RNU     Humboldt Substation   \n",
      "1  Humboldt Substation 115 kV             RNU      Trinity Substation   \n",
      "2  Humboldt Substation 115 kV             RNU  Bridgeville Substation   \n",
      "3                         NaN             NaN                     NaN   \n",
      "\n",
      "                                     description  ... escalated_cost_x_1000  \\\n",
      "0  Expansion of 115kV Buses & Bus Selector Sw's.  ...                1030.0   \n",
      "1                Install one (1) DTT Transmitter  ...                 446.0   \n",
      "2                Install one (1) DTT Transmitter  ...                 446.0   \n",
      "3                                            NaN  ...                   NaN   \n",
      "\n",
      "   total_estimated_cost_x_1000  total_estimated_cost_x_1000_escalated  \\\n",
      "0                                                                 0.0   \n",
      "1                                                                 0.0   \n",
      "2                                                                 0.0   \n",
      "3                          NaN                                    NaN   \n",
      "\n",
      "  estimated_time_to_construct  item  \\\n",
      "0                        38.0   yes   \n",
      "1                        38.0   yes   \n",
      "2                        38.0   yes   \n",
      "3                         NaN   NaN   \n",
      "\n",
      "  estimated_cost_x_1000_escalated_with_itcca adnu_cost_rate_x_1000_escalated  \\\n",
      "0                                                                        0.0   \n",
      "1                                                                        0.0   \n",
      "2                                                                        0.0   \n",
      "3                                        NaN                             NaN   \n",
      "\n",
      "  adnu_cost_rate_x_1000  original  row_order  \n",
      "0                   0.0       yes        647  \n",
      "1                   0.0       yes        648  \n",
      "2                   0.0       yes        649  \n",
      "3                   NaN       NaN       <NA>  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "Before replacement: req_deliverability\n",
      "Addendum values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "Original values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3     NaN\n",
      "Name: req_deliverability, dtype: object\n",
      "After replacement: req_deliverability\n",
      "Updated addendum values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "Before replacement: latitude\n",
      "Addendum values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3    40.740869\n",
      "Name: latitude, dtype: object\n",
      "Original values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3          NaN\n",
      "Name: latitude, dtype: object\n",
      "After replacement: latitude\n",
      "Updated addendum values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3    40.740869\n",
      "Name: latitude, dtype: object\n",
      "Before replacement: longitude\n",
      "Addendum values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "Original values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3            NaN\n",
      "Name: longitude, dtype: object\n",
      "After replacement: longitude\n",
      "Updated addendum values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "Before replacement: capacity\n",
      "Addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: capacity, dtype: object\n",
      "Original values:\n",
      "0       \n",
      "1       \n",
      "2       \n",
      "3    NaN\n",
      "Name: capacity, dtype: object\n",
      "After replacement: capacity\n",
      "Updated addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: capacity, dtype: object\n",
      "Before replacement: point_of_interconnection\n",
      "Addendum values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "Original values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3                           NaN\n",
      "Name: point_of_interconnection, dtype: object\n",
      "After replacement: point_of_interconnection\n",
      "Updated addendum values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "Processing itemized: q_id=1474, type_of_upgrade=PTO_IF\n",
      "Length of addendum_rows: 4\n",
      "Length of original_rows: 4\n",
      "addendum_rows:\n",
      "   q_id  cluster req_deliverability   latitude    longitude capacity  \\\n",
      "0  1491       11               Full  40.740869  -124.210164            \n",
      "1  1491       11               Full  40.740869  -124.210164            \n",
      "2  1491       11               Full  40.740869  -124.210164            \n",
      "3  1491       11               Full  40.740869  -124.210164            \n",
      "\n",
      "     point_of_interconnection type_of_upgrade                 upgrade  \\\n",
      "0  Humboldt Substation 115 kV             RNU     Humboldt Substation   \n",
      "1  Humboldt Substation 115 kV             RNU      Trinity Substation   \n",
      "2  Humboldt Substation 115 kV             RNU   Cottonwood Substation   \n",
      "3  Humboldt Substation 115 kV             RNU  Bridgeville Substation   \n",
      "\n",
      "                                     description  ... estimated  caiso_queue  \\\n",
      "0  Expansion of 115kV Buses & Bus Selector Sw's.  ...                          \n",
      "1                Install one (1) DTT Transmitter  ...                          \n",
      "2                Install one (1) DTT Transmitter  ...                          \n",
      "3                Install one (1) DTT Transmitter  ...                          \n",
      "\n",
      "   project_type dependent_system_upgrade  \\\n",
      "0                                          \n",
      "1                                          \n",
      "2                                          \n",
      "3                                          \n",
      "\n",
      "  estimated_cost_x_1000_escalated_with_itcca adnu_cost_rate_x_1000  \\\n",
      "0                                          0                     0   \n",
      "1                                          0                     0   \n",
      "2                                          0                     0   \n",
      "3                                          0                     0   \n",
      "\n",
      "  adnu_cost_rate_x_1000_escalated total_estimated_cost_x_1000  \\\n",
      "0                               0                           0   \n",
      "1                               0                           0   \n",
      "2                               0                           0   \n",
      "3                               0                           0   \n",
      "\n",
      "  total_estimated_cost_x_1000_escalated row_order  \n",
      "0                                     0         0  \n",
      "1                                     0         0  \n",
      "2                                     0         0  \n",
      "3                                     0         0  \n",
      "\n",
      "[4 rows x 26 columns]\n",
      "original_rows:\n",
      "   q_id cluster req_deliverability   latitude    longitude capacity  \\\n",
      "0  1491      11               Full  40.740869  -124.210164            \n",
      "1  1491      11               Full  40.740869  -124.210164            \n",
      "2  1491      11               Full  40.740869  -124.210164            \n",
      "3  <NA>    <NA>                NaN        NaN          NaN      NaN   \n",
      "\n",
      "     point_of_interconnection type_of_upgrade                 upgrade  \\\n",
      "0  Humboldt Substation 115 kV             RNU     Humboldt Substation   \n",
      "1  Humboldt Substation 115 kV             RNU      Trinity Substation   \n",
      "2  Humboldt Substation 115 kV             RNU  Bridgeville Substation   \n",
      "3                         NaN             NaN                     NaN   \n",
      "\n",
      "                                     description  ... escalated_cost_x_1000  \\\n",
      "0  Expansion of 115kV Buses & Bus Selector Sw's.  ...                1030.0   \n",
      "1                Install one (1) DTT Transmitter  ...                 446.0   \n",
      "2                Install one (1) DTT Transmitter  ...                 446.0   \n",
      "3                                            NaN  ...                   NaN   \n",
      "\n",
      "   total_estimated_cost_x_1000  total_estimated_cost_x_1000_escalated  \\\n",
      "0                                                                 0.0   \n",
      "1                                                                 0.0   \n",
      "2                                                                 0.0   \n",
      "3                          NaN                                    NaN   \n",
      "\n",
      "  estimated_time_to_construct  item  \\\n",
      "0                        38.0   yes   \n",
      "1                        38.0   yes   \n",
      "2                        38.0   yes   \n",
      "3                         NaN   NaN   \n",
      "\n",
      "  estimated_cost_x_1000_escalated_with_itcca adnu_cost_rate_x_1000_escalated  \\\n",
      "0                                                                        0.0   \n",
      "1                                                                        0.0   \n",
      "2                                                                        0.0   \n",
      "3                                        NaN                             NaN   \n",
      "\n",
      "  adnu_cost_rate_x_1000  original  row_order  \n",
      "0                   0.0       yes        647  \n",
      "1                   0.0       yes        648  \n",
      "2                   0.0       yes        649  \n",
      "3                   NaN       NaN       <NA>  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "Before replacement: req_deliverability\n",
      "Addendum values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "Original values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3     NaN\n",
      "Name: req_deliverability, dtype: object\n",
      "After replacement: req_deliverability\n",
      "Updated addendum values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "Before replacement: latitude\n",
      "Addendum values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3    40.740869\n",
      "Name: latitude, dtype: object\n",
      "Original values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3          NaN\n",
      "Name: latitude, dtype: object\n",
      "After replacement: latitude\n",
      "Updated addendum values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3    40.740869\n",
      "Name: latitude, dtype: object\n",
      "Before replacement: longitude\n",
      "Addendum values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "Original values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3            NaN\n",
      "Name: longitude, dtype: object\n",
      "After replacement: longitude\n",
      "Updated addendum values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "Before replacement: capacity\n",
      "Addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: capacity, dtype: object\n",
      "Original values:\n",
      "0       \n",
      "1       \n",
      "2       \n",
      "3    NaN\n",
      "Name: capacity, dtype: object\n",
      "After replacement: capacity\n",
      "Updated addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: capacity, dtype: object\n",
      "Before replacement: point_of_interconnection\n",
      "Addendum values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "Original values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3                           NaN\n",
      "Name: point_of_interconnection, dtype: object\n",
      "After replacement: point_of_interconnection\n",
      "Updated addendum values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "Processing itemized: q_id=1474, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 4\n",
      "Length of original_rows: 4\n",
      "addendum_rows:\n",
      "   q_id  cluster req_deliverability   latitude    longitude capacity  \\\n",
      "0  1491       11               Full  40.740869  -124.210164            \n",
      "1  1491       11               Full  40.740869  -124.210164            \n",
      "2  1491       11               Full  40.740869  -124.210164            \n",
      "3  1491       11               Full  40.740869  -124.210164            \n",
      "\n",
      "     point_of_interconnection type_of_upgrade                 upgrade  \\\n",
      "0  Humboldt Substation 115 kV             RNU     Humboldt Substation   \n",
      "1  Humboldt Substation 115 kV             RNU      Trinity Substation   \n",
      "2  Humboldt Substation 115 kV             RNU   Cottonwood Substation   \n",
      "3  Humboldt Substation 115 kV             RNU  Bridgeville Substation   \n",
      "\n",
      "                                     description  ... estimated  caiso_queue  \\\n",
      "0  Expansion of 115kV Buses & Bus Selector Sw's.  ...                          \n",
      "1                Install one (1) DTT Transmitter  ...                          \n",
      "2                Install one (1) DTT Transmitter  ...                          \n",
      "3                Install one (1) DTT Transmitter  ...                          \n",
      "\n",
      "   project_type dependent_system_upgrade  \\\n",
      "0                                          \n",
      "1                                          \n",
      "2                                          \n",
      "3                                          \n",
      "\n",
      "  estimated_cost_x_1000_escalated_with_itcca adnu_cost_rate_x_1000  \\\n",
      "0                                          0                     0   \n",
      "1                                          0                     0   \n",
      "2                                          0                     0   \n",
      "3                                          0                     0   \n",
      "\n",
      "  adnu_cost_rate_x_1000_escalated total_estimated_cost_x_1000  \\\n",
      "0                               0                           0   \n",
      "1                               0                           0   \n",
      "2                               0                           0   \n",
      "3                               0                           0   \n",
      "\n",
      "  total_estimated_cost_x_1000_escalated row_order  \n",
      "0                                     0         0  \n",
      "1                                     0         0  \n",
      "2                                     0         0  \n",
      "3                                     0         0  \n",
      "\n",
      "[4 rows x 26 columns]\n",
      "original_rows:\n",
      "   q_id cluster req_deliverability   latitude    longitude capacity  \\\n",
      "0  1491      11               Full  40.740869  -124.210164            \n",
      "1  1491      11               Full  40.740869  -124.210164            \n",
      "2  1491      11               Full  40.740869  -124.210164            \n",
      "3  <NA>    <NA>                NaN        NaN          NaN      NaN   \n",
      "\n",
      "     point_of_interconnection type_of_upgrade                 upgrade  \\\n",
      "0  Humboldt Substation 115 kV             RNU     Humboldt Substation   \n",
      "1  Humboldt Substation 115 kV             RNU      Trinity Substation   \n",
      "2  Humboldt Substation 115 kV             RNU  Bridgeville Substation   \n",
      "3                         NaN             NaN                     NaN   \n",
      "\n",
      "                                     description  ... escalated_cost_x_1000  \\\n",
      "0  Expansion of 115kV Buses & Bus Selector Sw's.  ...                1030.0   \n",
      "1                Install one (1) DTT Transmitter  ...                 446.0   \n",
      "2                Install one (1) DTT Transmitter  ...                 446.0   \n",
      "3                                            NaN  ...                   NaN   \n",
      "\n",
      "   total_estimated_cost_x_1000  total_estimated_cost_x_1000_escalated  \\\n",
      "0                                                                 0.0   \n",
      "1                                                                 0.0   \n",
      "2                                                                 0.0   \n",
      "3                          NaN                                    NaN   \n",
      "\n",
      "  estimated_time_to_construct  item  \\\n",
      "0                        38.0   yes   \n",
      "1                        38.0   yes   \n",
      "2                        38.0   yes   \n",
      "3                         NaN   NaN   \n",
      "\n",
      "  estimated_cost_x_1000_escalated_with_itcca adnu_cost_rate_x_1000_escalated  \\\n",
      "0                                                                        0.0   \n",
      "1                                                                        0.0   \n",
      "2                                                                        0.0   \n",
      "3                                        NaN                             NaN   \n",
      "\n",
      "  adnu_cost_rate_x_1000  original  row_order  \n",
      "0                   0.0       yes        647  \n",
      "1                   0.0       yes        648  \n",
      "2                   0.0       yes        649  \n",
      "3                   NaN       NaN       <NA>  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "Before replacement: req_deliverability\n",
      "Addendum values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "Original values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3     NaN\n",
      "Name: req_deliverability, dtype: object\n",
      "After replacement: req_deliverability\n",
      "Updated addendum values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "Before replacement: latitude\n",
      "Addendum values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3    40.740869\n",
      "Name: latitude, dtype: object\n",
      "Original values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3          NaN\n",
      "Name: latitude, dtype: object\n",
      "After replacement: latitude\n",
      "Updated addendum values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3    40.740869\n",
      "Name: latitude, dtype: object\n",
      "Before replacement: longitude\n",
      "Addendum values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "Original values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3            NaN\n",
      "Name: longitude, dtype: object\n",
      "After replacement: longitude\n",
      "Updated addendum values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "Before replacement: capacity\n",
      "Addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: capacity, dtype: object\n",
      "Original values:\n",
      "0       \n",
      "1       \n",
      "2       \n",
      "3    NaN\n",
      "Name: capacity, dtype: object\n",
      "After replacement: capacity\n",
      "Updated addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: capacity, dtype: object\n",
      "Before replacement: point_of_interconnection\n",
      "Addendum values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "Original values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3                           NaN\n",
      "Name: point_of_interconnection, dtype: object\n",
      "After replacement: point_of_interconnection\n",
      "Updated addendum values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "Processing itemized: q_id=1491, type_of_upgrade=RNU\n",
      "Length of addendum_rows: 4\n",
      "Length of original_rows: 4\n",
      "addendum_rows:\n",
      "   q_id  cluster req_deliverability   latitude    longitude capacity  \\\n",
      "0  1491       11               Full  40.740869  -124.210164            \n",
      "1  1491       11               Full  40.740869  -124.210164            \n",
      "2  1491       11               Full  40.740869  -124.210164            \n",
      "3  1491       11               Full  40.740869  -124.210164            \n",
      "\n",
      "     point_of_interconnection type_of_upgrade                 upgrade  \\\n",
      "0  Humboldt Substation 115 kV             RNU     Humboldt Substation   \n",
      "1  Humboldt Substation 115 kV             RNU      Trinity Substation   \n",
      "2  Humboldt Substation 115 kV             RNU   Cottonwood Substation   \n",
      "3  Humboldt Substation 115 kV             RNU  Bridgeville Substation   \n",
      "\n",
      "                                     description  ... estimated  caiso_queue  \\\n",
      "0  Expansion of 115kV Buses & Bus Selector Sw's.  ...                          \n",
      "1                Install one (1) DTT Transmitter  ...                          \n",
      "2                Install one (1) DTT Transmitter  ...                          \n",
      "3                Install one (1) DTT Transmitter  ...                          \n",
      "\n",
      "   project_type dependent_system_upgrade  \\\n",
      "0                                          \n",
      "1                                          \n",
      "2                                          \n",
      "3                                          \n",
      "\n",
      "  estimated_cost_x_1000_escalated_with_itcca adnu_cost_rate_x_1000  \\\n",
      "0                                          0                     0   \n",
      "1                                          0                     0   \n",
      "2                                          0                     0   \n",
      "3                                          0                     0   \n",
      "\n",
      "  adnu_cost_rate_x_1000_escalated total_estimated_cost_x_1000  \\\n",
      "0                               0                           0   \n",
      "1                               0                           0   \n",
      "2                               0                           0   \n",
      "3                               0                           0   \n",
      "\n",
      "  total_estimated_cost_x_1000_escalated row_order  \n",
      "0                                     0         0  \n",
      "1                                     0         0  \n",
      "2                                     0         0  \n",
      "3                                     0         0  \n",
      "\n",
      "[4 rows x 26 columns]\n",
      "original_rows:\n",
      "   q_id cluster req_deliverability   latitude    longitude capacity  \\\n",
      "0  1491      11               Full  40.740869  -124.210164            \n",
      "1  1491      11               Full  40.740869  -124.210164            \n",
      "2  1491      11               Full  40.740869  -124.210164            \n",
      "3  <NA>    <NA>                NaN        NaN          NaN      NaN   \n",
      "\n",
      "     point_of_interconnection type_of_upgrade                 upgrade  \\\n",
      "0  Humboldt Substation 115 kV             RNU     Humboldt Substation   \n",
      "1  Humboldt Substation 115 kV             RNU      Trinity Substation   \n",
      "2  Humboldt Substation 115 kV             RNU  Bridgeville Substation   \n",
      "3                         NaN             NaN                     NaN   \n",
      "\n",
      "                                     description  ... escalated_cost_x_1000  \\\n",
      "0  Expansion of 115kV Buses & Bus Selector Sw's.  ...                1030.0   \n",
      "1                Install one (1) DTT Transmitter  ...                 446.0   \n",
      "2                Install one (1) DTT Transmitter  ...                 446.0   \n",
      "3                                            NaN  ...                   NaN   \n",
      "\n",
      "   total_estimated_cost_x_1000  total_estimated_cost_x_1000_escalated  \\\n",
      "0                                                                 0.0   \n",
      "1                                                                 0.0   \n",
      "2                                                                 0.0   \n",
      "3                          NaN                                    NaN   \n",
      "\n",
      "  estimated_time_to_construct  item  \\\n",
      "0                        38.0   yes   \n",
      "1                        38.0   yes   \n",
      "2                        38.0   yes   \n",
      "3                         NaN   NaN   \n",
      "\n",
      "  estimated_cost_x_1000_escalated_with_itcca adnu_cost_rate_x_1000_escalated  \\\n",
      "0                                                                        0.0   \n",
      "1                                                                        0.0   \n",
      "2                                                                        0.0   \n",
      "3                                        NaN                             NaN   \n",
      "\n",
      "  adnu_cost_rate_x_1000  original  row_order  \n",
      "0                   0.0       yes        647  \n",
      "1                   0.0       yes        648  \n",
      "2                   0.0       yes        649  \n",
      "3                   NaN       NaN       <NA>  \n",
      "\n",
      "[4 rows x 22 columns]\n",
      "Before replacement: req_deliverability\n",
      "Addendum values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "Original values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3     NaN\n",
      "Name: req_deliverability, dtype: object\n",
      "After replacement: req_deliverability\n",
      "Updated addendum values:\n",
      "0    Full\n",
      "1    Full\n",
      "2    Full\n",
      "3    Full\n",
      "Name: req_deliverability, dtype: object\n",
      "Before replacement: latitude\n",
      "Addendum values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3    40.740869\n",
      "Name: latitude, dtype: object\n",
      "Original values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3          NaN\n",
      "Name: latitude, dtype: object\n",
      "After replacement: latitude\n",
      "Updated addendum values:\n",
      "0    40.740869\n",
      "1    40.740869\n",
      "2    40.740869\n",
      "3    40.740869\n",
      "Name: latitude, dtype: object\n",
      "Before replacement: longitude\n",
      "Addendum values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "Original values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3            NaN\n",
      "Name: longitude, dtype: object\n",
      "After replacement: longitude\n",
      "Updated addendum values:\n",
      "0    -124.210164\n",
      "1    -124.210164\n",
      "2    -124.210164\n",
      "3    -124.210164\n",
      "Name: longitude, dtype: object\n",
      "Before replacement: capacity\n",
      "Addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: capacity, dtype: object\n",
      "Original values:\n",
      "0       \n",
      "1       \n",
      "2       \n",
      "3    NaN\n",
      "Name: capacity, dtype: object\n",
      "After replacement: capacity\n",
      "Updated addendum values:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "Name: capacity, dtype: object\n",
      "Before replacement: point_of_interconnection\n",
      "Addendum values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n",
      "Original values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3                           NaN\n",
      "Name: point_of_interconnection, dtype: object\n",
      "After replacement: point_of_interconnection\n",
      "Updated addendum values:\n",
      "0    Humboldt Substation 115 kV\n",
      "1    Humboldt Substation 115 kV\n",
      "2    Humboldt Substation 115 kV\n",
      "3    Humboldt Substation 115 kV\n",
      "Name: point_of_interconnection, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:119: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  original_rows = pd.concat([original_rows, extra_rows], ignore_index=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:119: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  original_rows = pd.concat([original_rows, extra_rows], ignore_index=True)\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_row[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:283: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:288: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:283: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:288: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:283: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:288: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:283: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_51906/84979295.py:288: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .apply(lambda group: group.ffill().bfill())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Load a CSV file and ensure specific columns are treated as character, others as numeric.\n",
    "    \"\"\"\n",
    " # Get columns available in the dataset\n",
    "    available_columns = pd.read_csv(file_path, nrows=0).columns\n",
    "    \n",
    "    # Restrict to char_columns that are present in the dataset\n",
    "    char_columns_in_dataset = [col for col in char_columns if col in available_columns]\n",
    "    \n",
    "    # Load the dataset, treating char_columns_in_dataset as strings\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype={col: str for col in char_columns_in_dataset},\n",
    "        na_values=[],  # Disable automatic NaN interpretation\n",
    "        keep_default_na=False  # Prevent treating \"None\" as NaN\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert all other columns to numeric\n",
    "    #for col in df.columns:\n",
    "    #    if col not in char_columns:\n",
    "    #        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_data(df, file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Save a dataframe to a CSV file, ensuring specific columns are treated as character.\n",
    "    \"\"\"\n",
    "    for col in char_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def merge_with_addendums(itemized, itemized_addendums, total, total_addendums):\n",
    "    # Add an 'original' column to the datasets\n",
    "    itemized['original'] = \"yes\"\n",
    "    total['original'] = \"yes\"\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Preserve the original row order\n",
    "    itemized['row_order'] = pd.to_numeric(itemized.index, errors=\"coerce\")\n",
    "    total['row_order'] = pd.to_numeric(total.index, errors=\"coerce\")\n",
    "    \n",
    "    # Ensure q_id is numeric for comparison\n",
    "    itemized['q_id'] = pd.to_numeric(itemized['q_id'], errors=\"coerce\")\n",
    "    itemized_addendums['q_id'] = pd.to_numeric(itemized_addendums['q_id'], errors=\"coerce\")\n",
    "    total['q_id'] = pd.to_numeric(total['q_id'], errors=\"coerce\")\n",
    "    total_addendums['q_id'] = pd.to_numeric(total_addendums['q_id'], errors=\"coerce\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Columns for conditional replacement\n",
    "    conditional_columns = [\"req_deliverability\", \"latitude\", \"longitude\", \"capacity\", \"point_of_interconnection\"]\n",
    "    \n",
    "    # Prepare a list to collect the updated itemized rows\n",
    "    updated_itemized_rows = []\n",
    "    \n",
    "    # Merge itemized and itemized_addendums\n",
    "    for q_id in itemized_addendums['q_id'].unique():\n",
    "        for upgrade_type in itemized_addendums['type_of_upgrade'].unique():\n",
    "            addendum_rows = itemized_addendums[\n",
    "                (itemized_addendums['q_id'] == q_id) &\n",
    "                (itemized_addendums['type_of_upgrade'] == upgrade_type)\n",
    "            ]\n",
    "            if not addendum_rows.empty:\n",
    "                mask = (itemized['q_id'] == q_id) & (itemized['type_of_upgrade'] == upgrade_type)\n",
    "                original_rows = itemized[mask]\n",
    "                print(f\"Processing itemized: q_id={q_id}, type_of_upgrade={upgrade_type}\")\n",
    "                print(f\"Length of addendum_rows: {len(addendum_rows)}\")\n",
    "                print(f\"Length of original_rows: {len(original_rows)}\")\n",
    "                print(f\"addendum_rows:\\n{addendum_rows}\")\n",
    "                print(f\"original_rows:\\n{original_rows}\")\n",
    "\n",
    "                # For specified columns, replace only if addendum values are non-empty\n",
    "                for col in conditional_columns:\n",
    "                    if col in addendum_rows.columns and col in original_rows.columns:\n",
    "                        # Debugging outputs\n",
    "                        print(f\"Before replacement: {col}\")\n",
    "                        print(f\"Addendum values:\\n{addendum_rows[col]}\")\n",
    "                        print(f\"Original values:\\n{original_rows[col]}\")\n",
    "\n",
    "                        # Replace blanks with NaN in addendum\n",
    "                        addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
    "\n",
    "                        # Combine values: prefer addendum if not NaN, else take original\n",
    "                        addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
    "\n",
    "                        # Replace NaN back with blank string for consistency\n",
    "                        addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
    "\n",
    "                        print(f\"After replacement: {col}\")\n",
    "                        print(f\"Updated addendum values:\\n{addendum_rows[col]}\")\n",
    "\n",
    "                # Match row_order with addendum_rows length\n",
    "                # Adjust lengths by truncating the longer one\n",
    "                '''\n",
    "                if len(addendum_rows) > len(original_rows):\n",
    "                    addendum_rows = addendum_rows.iloc[:len(original_rows)]\n",
    "                elif len(addendum_rows) < len(original_rows):\n",
    "                    original_rows = original_rows.iloc[:len(addendum_rows)]\n",
    "                '''\n",
    "\n",
    "                # Align the length of original_rows to addendum_rows\n",
    "                original_rows = original_rows.reset_index(drop=True)\n",
    "                addendum_rows = addendum_rows.reset_index(drop=True)\n",
    "                \n",
    "                if len(addendum_rows) > len(original_rows):\n",
    "                    # Pad original_rows\n",
    "                    extra_rows = pd.DataFrame({col: pd.NA for col in original_rows.columns}, index=range(len(addendum_rows) - len(original_rows)))\n",
    "                    original_rows = pd.concat([original_rows, extra_rows], ignore_index=True)\n",
    "                elif len(addendum_rows) < len(original_rows):\n",
    "                    # Truncate original_rows\n",
    "                    original_rows = original_rows.iloc[:len(addendum_rows)].reset_index(drop=True)\n",
    "                \n",
    "\n",
    "                     \n",
    "                \n",
    "                # Find non-character columns missing in the addendum dataset\n",
    "                missing_cols = set(itemized.columns) - set(addendum_rows.columns) - set(char_columns)\n",
    "                for col in missing_cols:\n",
    "                    addendum_rows[col] = 0 \n",
    "                itemized.loc[mask, 'original'] = \"no\"\n",
    "                updated_itemized_rows.append(addendum_rows.assign(original=\"no\", row_order=original_rows['row_order'].values[:len(addendum_rows)]))\n",
    "                itemized = itemized[~mask]\n",
    "    \n",
    "    if updated_itemized_rows:\n",
    "        updated_itemized = pd.concat([itemized] + updated_itemized_rows, ignore_index=True)\n",
    "    else:\n",
    "        updated_itemized = itemized.copy()\n",
    "    \n",
    "\n",
    "    updated_itemized[\"row_order\"] = pd.to_numeric(updated_itemized[\"row_order\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "    updated_itemized = updated_itemized.sort_values(by=\"row_order\").drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    \n",
    "    updated_total_rows = []\n",
    "    \n",
    "    for q_id in total_addendums['q_id'].unique():\n",
    "        for upgrade_type in total_addendums['type_of_upgrade'].unique():\n",
    "            addendum_row = total_addendums[\n",
    "                (total_addendums['q_id'] == q_id) &\n",
    "                (total_addendums['type_of_upgrade'] == upgrade_type)\n",
    "            ]\n",
    "            if not addendum_row.empty:\n",
    "                mask = (total['q_id'] == q_id) & (total['type_of_upgrade'] == upgrade_type)\n",
    "                original_row = total[mask]\n",
    "                print(f\"Processing itemized: q_id={q_id}, type_of_upgrade={upgrade_type}\")\n",
    "                print(f\"Length of addendum_rows: {len(addendum_rows)}\")\n",
    "                print(f\"Length of original_rows: {len(original_rows)}\")\n",
    "                print(f\"addendum_rows:\\n{addendum_rows}\")\n",
    "                print(f\"original_rows:\\n{original_rows}\")\n",
    "\n",
    "                # For specified columns, replace only if addendum values are non-empty\n",
    "                for col in conditional_columns:\n",
    "                    if col in addendum_rows.columns and col in original_rows.columns:\n",
    "                        # Debugging outputs\n",
    "                        print(f\"Before replacement: {col}\")\n",
    "                        print(f\"Addendum values:\\n{addendum_rows[col]}\")\n",
    "                        print(f\"Original values:\\n{original_rows[col]}\")\n",
    "\n",
    "                        # Replace blanks with NaN in addendum\n",
    "                        addendum_rows[col] = addendum_rows[col].replace(\"\", pd.NA)\n",
    "\n",
    "                        # Combine values: prefer addendum if not NaN, else take original\n",
    "                        addendum_rows[col] = addendum_rows[col].combine_first(original_rows[col].reset_index(drop=True))\n",
    "\n",
    "                        # Replace NaN back with blank string for consistency\n",
    "                        addendum_rows[col] = addendum_rows[col].fillna(\"\")\n",
    "\n",
    "                        print(f\"After replacement: {col}\")\n",
    "                        print(f\"Updated addendum values:\\n{addendum_rows[col]}\")\n",
    "\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        \n",
    "                \n",
    "                # Find non-character columns missing in the addendum dataset\n",
    "                missing_cols = set(total.columns) - set(addendum_row.columns) - set(char_columns)\n",
    "                for col in missing_cols:\n",
    "                    addendum_row[col] = 0 \n",
    "                total.loc[mask, 'original'] = \"no\"\n",
    "                updated_total_rows.append(addendum_row.assign(original=\"no\", row_order=original_row['row_order'].values[:len(addendum_row)]))\n",
    "                total = total[~mask]\n",
    "    \n",
    "    if updated_total_rows:\n",
    "        updated_total = pd.concat([total] + updated_total_rows, ignore_index=True)\n",
    "    else:\n",
    "        updated_total = total.copy()\n",
    "    \n",
    "     \n",
    "    \n",
    "\n",
    "    updated_total[\"row_order\"] = pd.to_numeric(updated_total[\"row_order\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "    updated_total = updated_total.sort_values(by=\"row_order\").drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    # Fill missing columns with zeros in the updated datasets\n",
    "    for col in set(itemized.columns) - set(updated_itemized.columns):\n",
    "        updated_itemized[col] = 0\n",
    "    \n",
    "    for col in set(total.columns) - set(updated_total.columns):\n",
    "        updated_total[col] = 0\n",
    "\n",
    "    # Move the 'original' column to the last position\n",
    "    updated_itemized = updated_itemized[[col for col in updated_itemized.columns if col != 'original'] + ['original']]\n",
    "    updated_total = updated_total[[col for col in updated_total.columns if col != 'original'] + ['original']]\n",
    "\n",
    "     \n",
    "\n",
    "    \n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "        # Drop the row_order column directly\n",
    "    if \"row_order\" in updated_itemized.columns:\n",
    "        updated_itemized = updated_itemized.drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "\n",
    "    if \"row_order\" in updated_total.columns:\n",
    "        updated_total = updated_total.drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "  \n",
    " \n",
    "    \n",
    "    \n",
    "    return updated_itemized, updated_total\n",
    "\n",
    "# Define the character columns\n",
    "char_columns = [\n",
    "    \"req_deliverability\", \"point_of_interconnection\", \"type_of_upgrade\",\n",
    "    \"upgrade\", \"description\", \"estimated_time_to_construct\", \"original\", \"item\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "itemized = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_itemized.csv\", char_columns)\n",
    "itemized_addendums = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_itemized_addendums.csv\", char_columns)\n",
    "total = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_total.csv\", char_columns)\n",
    "total_addendums = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_total_addendums.csv\", char_columns)\n",
    "\n",
    "\n",
    "updated_itemized, updated_total = merge_with_addendums(itemized, itemized_addendums, total, total_addendums)\n",
    "\n",
    "# Drop the specified columns from the updated datasets\n",
    "columns_to_drop = [ \"upgrade_classification\",\"estimated\", \"caiso_queue\", \"project_type\", \"dependent_system_upgrade\"]\n",
    "\n",
    "# For the itemized dataset\n",
    "updated_itemized = updated_itemized.drop(columns=[col for col in columns_to_drop if col in updated_itemized.columns], errors='ignore')\n",
    "\n",
    "# For the total dataset\n",
    "updated_total = updated_total.drop(columns=[col for col in columns_to_drop if col in updated_total.columns], errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "# List of columns to process with ffill and bfill\n",
    "columns_to_fill = [\"point_of_interconnection\", \"latitude\", \"longitude\", \"req_deliverability\", \"capacity\"]\n",
    "\n",
    "# Replace empty strings with NaN for the specified columns\n",
    "for col in columns_to_fill:\n",
    "    updated_itemized[col] = updated_itemized[col].replace('', np.nan)\n",
    "    updated_total[col] = updated_total[col].replace('', np.nan)\n",
    "\n",
    "# Sort by q_id while maintaining other column order (stable sorting)\n",
    "updated_itemized = updated_itemized.sort_values(by=[\"q_id\"], kind=\"stable\").reset_index(drop=True)\n",
    "updated_total = updated_total.sort_values(by=[\"q_id\"], kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "# Apply forward-fill and backward-fill for the specified columns within each q_id group\n",
    "for col in columns_to_fill:\n",
    "    updated_itemized[col] = (\n",
    "        updated_itemized.groupby(\"q_id\")[col]\n",
    "        .apply(lambda group: group.ffill().bfill())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    updated_total[col] = (\n",
    "        updated_total.groupby(\"q_id\")[col]\n",
    "        .apply(lambda group: group.ffill().bfill())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# Replace NaN back with empty strings for consistency\n",
    "for col in columns_to_fill:\n",
    "    updated_itemized[col] = updated_itemized[col].replace(np.nan, '')\n",
    "    updated_total[col] = updated_total[col].replace(np.nan, '')\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the updated datasets\n",
    "save_data(updated_itemized, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/01_clean/costs_phase_1_cluster_11_style_G_itemized_updated.csv\", char_columns)\n",
    "save_data(updated_total, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/01_clean/costs_phase_1_cluster_11_style_G_total_updated.csv\", char_columns)\n",
    "\n",
    "\n",
    "\n",
    "# Save the results\n",
    "save_data(updated_itemized, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/01_clean/costs_phase_1_cluster_11_style_G_itemized_updated.csv\", char_columns)\n",
    "save_data(updated_total, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/01_clean/costs_phase_1_cluster_11_style_G_total_updated.csv\", char_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded itemized data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_itemized.csv\n",
      "Loaded totals data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_itemized.csv\n",
      "\n",
      "Q_ids with missing upgrades:\n",
      "  Q_id 1531 is missing upgrades: OPNU\n",
      "  Q_id 1532 is missing upgrades: OPNU\n",
      "  Q_id 1533 is missing upgrades: OPNU\n",
      "\n",
      "Mismatches saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/mismatches.csv'.\n",
      "\n",
      "Direct Matches (Exact Point of Interconnection Names):\n",
      "                 point_of_interconnection          q_id\n",
      "37                 MESA SUBSTATION 230 KV  [1470, 1490]\n",
      "50         Q1244 SWITCHING STATION 230 KV  [1452, 1453]\n",
      "53  TRANQUILLITY SWITCHING STATION 230 KV  [1471, 1477]\n",
      "\n",
      "Fuzzy Matches (>=80% Similarity in Point of Interconnection):\n",
      "    point_of_interconnection_1 q_ids_1  \\\n",
      "0    ALPAUGH SUBSTATION 115 KV  [1443]   \n",
      "1    ALPAUGH SUBSTATION 115 KV  [1443]   \n",
      "2    ALPAUGH SUBSTATION 115 KV  [1443]   \n",
      "3    ALPAUGH SUBSTATION 115 KV  [1443]   \n",
      "4    ALPAUGH SUBSTATION 115 KV  [1443]   \n",
      "..                         ...     ...   \n",
      "139   MIDWAY SUBSTATION 230 KV  [1500]   \n",
      "140   MIDWAY SUBSTATION 230 KV  [1500]   \n",
      "141   MIDWAY SUBSTATION 230 KV  [1500]   \n",
      "142   NEWARK SUBSTATION 230 KV  [1502]   \n",
      "143   NEWARK SUBSTATION 230 KV  [1502]   \n",
      "\n",
      "                   point_of_interconnection_2 q_ids_2  similarity_score  \n",
      "0                   PANOCHE SUBSTATION 115 KV  [1478]                84  \n",
      "1                    MIDWAY SUBSTATION 115 KV  [1501]                83  \n",
      "2                   CORTINA SUBSTATION 115 KV  [1444]                81  \n",
      "3                 SCHINDLER SUBSTATION 115 KV  [1447]                81  \n",
      "4                  CABRILLO SUBSTATION 115 KV  [1450]                81  \n",
      "..                                        ...     ...               ...  \n",
      "139                  NEWARK SUBSTATION 230 KV  [1502]                83  \n",
      "140               LAKEVILLE SUBSTATION 230 KV  [1503]                83  \n",
      "141  230 KV BUS AT IMPERIAL VALLEY SUBSTATION  [1531]                83  \n",
      "142               LAKEVILLE SUBSTATION 230 KV  [1503]                83  \n",
      "143  230 KV BUS AT IMPERIAL VALLEY SUBSTATION  [1531]                83  \n",
      "\n",
      "[144 rows x 5 columns]\n",
      "Matched Q_ids saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/matched_qids.csv'.\n",
      "\n",
      "Total checks performed: 287\n",
      "Total mismatches found: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "# Paths to the CSV files\n",
    "ITEMIZED_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_itemized.csv'\n",
    "TOTALS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/02_intermediate/costs_phase_1_cluster_11_style_G_itemized.csv'\n",
    "\n",
    "# Columns in totals_df that hold the reported total costs\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "# Upgrade types to check\n",
    "REQUIRED_UPGRADES = ['PTO_IF', 'RNU', 'LDNU', 'ADNU', 'OPNU']\n",
    "\n",
    "# Output paths\n",
    "MISMATCHES_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/mismatches.csv'\n",
    "MATCHED_QIDS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/matched_qids.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "def load_csv(path, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {dataset_name} from {path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Load datasets\n",
    "itemized_df = load_csv(ITEMIZED_CSV_PATH, \"itemized data\")\n",
    "totals_df = load_csv(TOTALS_CSV_PATH, \"totals data\")\n",
    "\n",
    "# ---------------------- Data Cleaning ---------------------- #\n",
    "\n",
    "def clean_text(df, column):\n",
    "    \"\"\"\n",
    "    Cleans text data by stripping leading/trailing spaces and converting to uppercase.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        df[column] = df[column].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        print(f\"Warning: '{column}' column is missing in the dataset. Filling with 'UNKNOWN'.\")\n",
    "        df[column] = 'UNKNOWN'\n",
    "    return df\n",
    "\n",
    "# Clean 'type_of_upgrade' and 'point_of_interconnection' in both datasets\n",
    "itemized_df = clean_text(itemized_df, 'type_of_upgrade')\n",
    "itemized_df = clean_text(itemized_df, 'point_of_interconnection')\n",
    "\n",
    "totals_df = clean_text(totals_df, 'type_of_upgrade')\n",
    "totals_df = clean_text(totals_df, 'point_of_interconnection')\n",
    "\n",
    "# ---------------------- Data Preparation ---------------------- #\n",
    "\n",
    "# Ensure necessary columns exist in itemized_df\n",
    "required_itemized_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', 'estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in required_itemized_columns:\n",
    "    if col not in itemized_df.columns:\n",
    "        print(f\"Warning: '{col}' column is missing in the itemized dataset.\")\n",
    "        if col in ['q_id', 'type_of_upgrade', 'point_of_interconnection']:\n",
    "            itemized_df[col] = 'UNKNOWN'\n",
    "        else:\n",
    "            itemized_df[col] = 0\n",
    "\n",
    "# Ensure necessary columns exist in totals_df\n",
    "required_totals_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in required_totals_columns:\n",
    "    if col not in totals_df.columns:\n",
    "        print(f\"Error: '{col}' column is missing in the totals dataset. Cannot proceed.\")\n",
    "        exit(1)\n",
    "\n",
    "# Convert cost columns to numeric, coercing errors to NaN and filling with 0\n",
    "cost_columns_itemized = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in cost_columns_itemized:\n",
    "    itemized_df[col] = pd.to_numeric(itemized_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "cost_columns_totals = [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in cost_columns_totals:\n",
    "    totals_df[col] = pd.to_numeric(totals_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# ---------------------- Calculate Manual Totals ---------------------- #\n",
    "\n",
    "# Group itemized data by q_id and type_of_upgrade and calculate sums\n",
    "itemized_grouped = itemized_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    'estimated_cost_x_1000': 'sum',\n",
    "    'escalated_cost_x_1000': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "itemized_grouped['manual_total'] = itemized_grouped.apply(\n",
    "    lambda row: row['estimated_cost_x_1000'] if row['estimated_cost_x_1000'] > 0 else row['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Prepare Totals Data ---------------------- #\n",
    "\n",
    "# Group totals data by q_id and type_of_upgrade and calculate sums\n",
    "totals_grouped = totals_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "    TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "totals_grouped['reported_total'] = totals_grouped.apply(\n",
    "    lambda row: row[TOTALS_ESTIMATED_COLUMN] if row[TOTALS_ESTIMATED_COLUMN] > 0 else row[TOTALS_ESCALATED_COLUMN],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Merge Data ---------------------- #\n",
    "\n",
    "# Merge the itemized and totals data on q_id and type_of_upgrade\n",
    "comparison_df = pd.merge(\n",
    "    itemized_grouped,\n",
    "    totals_grouped[['q_id', 'type_of_upgrade', 'reported_total']],\n",
    "    on=['q_id', 'type_of_upgrade'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---------------------- Check for Missing Upgrades ---------------------- #\n",
    "\n",
    "# Identify q_ids that are missing any of the required upgrades\n",
    "missing_upgrades_report = []\n",
    "for q_id in comparison_df['q_id'].unique():\n",
    "    upgrades_present = comparison_df[comparison_df['q_id'] == q_id]['type_of_upgrade'].unique()\n",
    "    missing_upgrades = [upgrade for upgrade in REQUIRED_UPGRADES if upgrade not in upgrades_present]\n",
    "    if missing_upgrades:\n",
    "        missing_upgrades_report.append((q_id, missing_upgrades))\n",
    "\n",
    "# Report missing upgrades\n",
    "if missing_upgrades_report:\n",
    "    print(\"\\nQ_ids with missing upgrades:\")\n",
    "    for q_id, missing in missing_upgrades_report:\n",
    "        print(f\"  Q_id {q_id} is missing upgrades: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nAll q_ids have all required upgrades.\")\n",
    "\n",
    "# ---------------------- Compare Totals and Identify Mismatches ---------------------- #\n",
    "\n",
    "# Initialize list to store mismatches\n",
    "mismatches = []\n",
    "\n",
    "# Iterate through each row to compare manual_total with reported_total\n",
    "for index, row in comparison_df.iterrows():\n",
    "    q_id = row['q_id']\n",
    "    upgrade = row['type_of_upgrade']\n",
    "    manual_total = row['manual_total']\n",
    "    reported_total = row['reported_total']\n",
    "    \n",
    "    # Determine if both manual_total and reported_total are zero\n",
    "    if manual_total == 0.0 and reported_total == 0.0:\n",
    "        continue  # No mismatch\n",
    "    # Determine if manual_total is zero and reported_total is missing or zero\n",
    "    elif manual_total == 0.0 and (pd.isna(row['reported_total']) or reported_total == 0.0):\n",
    "        continue  # No mismatch\n",
    "    # If reported_total is missing (NaN) and manual_total is not zero\n",
    "    elif pd.isna(row['reported_total']) and manual_total != 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: Missing\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': 'Missing'\n",
    "        })\n",
    "    # If manual_total is not zero and reported_total is zero\n",
    "    elif manual_total != 0.0 and reported_total == 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: 0.0\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # If both totals are non-zero but differ beyond tolerance\n",
    "    elif abs(manual_total - reported_total) > 1e-2:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: {reported_total}\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # Else, totals match; do nothing\n",
    "\n",
    "# Create a DataFrame for mismatches\n",
    "mismatches_df = pd.DataFrame(mismatches, columns=['q_id', 'type_of_upgrade', 'manual_total', 'reported_total'])\n",
    "\n",
    "# Save mismatches to a CSV file\n",
    "try:\n",
    "    mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "    print(f\"\\nMismatches saved to '{MISMATCHES_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving mismatches CSV: {e}\")\n",
    "\n",
    "# ---------------------- Point of Interconnection Matching ---------------------- #\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from itemized dataset\n",
    "itemized_poi = itemized_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from totals dataset\n",
    "totals_poi = totals_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Merge both to have a complete list of q_id and point_of_interconnection\n",
    "all_poi = pd.concat([itemized_poi, totals_poi]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ---------------------- Direct Match Identification ---------------------- #\n",
    "\n",
    "# Group by point_of_interconnection to find q_ids sharing the same point_of_interconnection\n",
    "direct_matches = all_poi.groupby('point_of_interconnection')['q_id'].apply(list).reset_index()\n",
    "\n",
    "# Filter groups with more than one q_id (i.e., shared points_of_interconnection)\n",
    "direct_matches = direct_matches[direct_matches['q_id'].apply(len) > 1]\n",
    "\n",
    "print(\"\\nDirect Matches (Exact Point of Interconnection Names):\")\n",
    "if not direct_matches.empty:\n",
    "    print(direct_matches)\n",
    "else:\n",
    "    print(\"No direct matches found.\")\n",
    "\n",
    "# ---------------------- Fuzzy Match Identification ---------------------- #\n",
    "\n",
    "# Prepare list of points_of_interconnection for fuzzy matching\n",
    "poi_list = all_poi['point_of_interconnection'].unique().tolist()\n",
    "\n",
    "# Initialize list to store fuzzy matches\n",
    "fuzzy_matches = []\n",
    "\n",
    "# Iterate through each point_of_interconnection to find similar ones\n",
    "for i, poi in enumerate(poi_list):\n",
    "    # Compare with the rest of the points to avoid redundant comparisons\n",
    "    similar_pois = process.extract(poi, poi_list[i+1:], scorer=fuzz.token_set_ratio)\n",
    "    \n",
    "    # Filter matches with similarity >= 80%\n",
    "    for match_poi, score in similar_pois:\n",
    "        if score >= 80:\n",
    "            # Retrieve q_ids for both points_of_interconnection\n",
    "            qids_poi1 = all_poi[all_poi['point_of_interconnection'] == poi]['q_id'].tolist()\n",
    "            qids_poi2 = all_poi[all_poi['point_of_interconnection'] == match_poi]['q_id'].tolist()\n",
    "            \n",
    "            # Append the matched pairs with their points_of_interconnection and similarity score\n",
    "            fuzzy_matches.append({\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_ids_1': qids_poi1,\n",
    "                'point_of_interconnection_2': match_poi,\n",
    "                'q_ids_2': qids_poi2,\n",
    "                'similarity_score': score\n",
    "            })\n",
    "\n",
    "# Convert fuzzy matches to DataFrame\n",
    "fuzzy_matches_df = pd.DataFrame(fuzzy_matches)\n",
    "\n",
    "print(\"\\nFuzzy Matches (>=80% Similarity in Point of Interconnection):\")\n",
    "if not fuzzy_matches_df.empty:\n",
    "    print(fuzzy_matches_df)\n",
    "else:\n",
    "    print(\"No fuzzy matches found.\")\n",
    "\n",
    "# ---------------------- Save Matched Q_ids to CSV ---------------------- #\n",
    "\n",
    "# For clarity, create a combined DataFrame for direct and fuzzy matches\n",
    "\n",
    "# Direct matches: list each pair of q_ids sharing the same point_of_interconnection\n",
    "direct_matches_expanded = []\n",
    "for _, row in direct_matches.iterrows():\n",
    "    qids = row['q_id']\n",
    "    poi = row['point_of_interconnection']\n",
    "    # Generate all possible unique pairs\n",
    "    for i in range(len(qids)):\n",
    "        for j in range(i+1, len(qids)):\n",
    "            direct_matches_expanded.append({\n",
    "                'match_type': 'Direct',\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_id_1': qids[i],\n",
    "                'point_of_interconnection_2': poi,\n",
    "                'q_id_2': qids[j],\n",
    "                'similarity_score': 100\n",
    "            })\n",
    "\n",
    "# Fuzzy matches: already have pairs\n",
    "fuzzy_matches_expanded = []\n",
    "for _, row in fuzzy_matches_df.iterrows():\n",
    "    fuzzy_matches_expanded.append({\n",
    "        'match_type': 'Fuzzy',\n",
    "        'point_of_interconnection_1': row['point_of_interconnection_1'],\n",
    "        'q_id_1': row['q_ids_1'],\n",
    "        'point_of_interconnection_2': row['point_of_interconnection_2'],\n",
    "        'q_id_2': row['q_ids_2'],\n",
    "        'similarity_score': row['similarity_score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "matched_qids_df = pd.DataFrame(direct_matches_expanded + fuzzy_matches_expanded)\n",
    "\n",
    "# Save matched q_ids to CSV\n",
    "try:\n",
    "    matched_qids_df.to_csv(MATCHED_QIDS_CSV_PATH, index=False)\n",
    "    print(f\"Matched Q_ids saved to '{MATCHED_QIDS_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving matched Q_ids CSV: {e}\")\n",
    "\n",
    "# ---------------------- Summary ---------------------- #\n",
    "\n",
    "# Print a summary\n",
    "total_checked = len(comparison_df)\n",
    "total_mismatches = len(mismatches_df)\n",
    "print(f\"\\nTotal checks performed: {total_checked}\")\n",
    "print(f\"Total mismatches found: {total_mismatches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking scraped data for final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded itemized data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/01_clean/costs_phase_1_cluster_11_style_G_itemized_updated.csv\n",
      "Loaded totals data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/01_clean/costs_phase_1_cluster_11_style_G_itemized_updated.csv\n",
      "\n",
      "Q_ids with missing upgrades:\n",
      "  Q_id 1531 is missing upgrades: OPNU\n",
      "  Q_id 1532 is missing upgrades: OPNU\n",
      "  Q_id 1533 is missing upgrades: OPNU\n",
      "\n",
      "Mismatches saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/mismatches.csv'.\n",
      "\n",
      "Direct Matches (Exact Point of Interconnection Names):\n",
      "                 point_of_interconnection          q_id\n",
      "37                 MESA SUBSTATION 230 KV  [1470, 1490]\n",
      "50         Q1244 SWITCHING STATION 230 KV  [1452, 1453]\n",
      "53  TRANQUILLITY SWITCHING STATION 230 KV  [1471, 1477]\n",
      "\n",
      "Fuzzy Matches (>=80% Similarity in Point of Interconnection):\n",
      "    point_of_interconnection_1 q_ids_1  \\\n",
      "0    ALPAUGH SUBSTATION 115 KV  [1443]   \n",
      "1    ALPAUGH SUBSTATION 115 KV  [1443]   \n",
      "2    ALPAUGH SUBSTATION 115 KV  [1443]   \n",
      "3    ALPAUGH SUBSTATION 115 KV  [1443]   \n",
      "4    ALPAUGH SUBSTATION 115 KV  [1443]   \n",
      "..                         ...     ...   \n",
      "139   MIDWAY SUBSTATION 230 KV  [1500]   \n",
      "140   MIDWAY SUBSTATION 230 KV  [1500]   \n",
      "141   MIDWAY SUBSTATION 230 KV  [1500]   \n",
      "142   NEWARK SUBSTATION 230 KV  [1502]   \n",
      "143   NEWARK SUBSTATION 230 KV  [1502]   \n",
      "\n",
      "                   point_of_interconnection_2 q_ids_2  similarity_score  \n",
      "0                   PANOCHE SUBSTATION 115 KV  [1478]                84  \n",
      "1                    MIDWAY SUBSTATION 115 KV  [1501]                83  \n",
      "2                   CORTINA SUBSTATION 115 KV  [1444]                81  \n",
      "3                 SCHINDLER SUBSTATION 115 KV  [1447]                81  \n",
      "4                  CABRILLO SUBSTATION 115 KV  [1450]                81  \n",
      "..                                        ...     ...               ...  \n",
      "139                  NEWARK SUBSTATION 230 KV  [1502]                83  \n",
      "140               LAKEVILLE SUBSTATION 230 KV  [1503]                83  \n",
      "141  230 KV BUS AT IMPERIAL VALLEY SUBSTATION  [1531]                83  \n",
      "142               LAKEVILLE SUBSTATION 230 KV  [1503]                83  \n",
      "143  230 KV BUS AT IMPERIAL VALLEY SUBSTATION  [1531]                83  \n",
      "\n",
      "[144 rows x 5 columns]\n",
      "Matched Q_ids saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 11/matched_qids.csv'.\n",
      "\n",
      "Total checks performed: 287\n",
      "Total mismatches found: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "# Paths to the CSV files\n",
    "ITEMIZED_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/01_clean/costs_phase_1_cluster_11_style_G_itemized_updated.csv'\n",
    "TOTALS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/01_clean/costs_phase_1_cluster_11_style_G_itemized_updated.csv'\n",
    "\n",
    "# Columns in totals_df that hold the reported total costs\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "# Upgrade types to check\n",
    "REQUIRED_UPGRADES = ['PTO_IF', 'RNU', 'LDNU', 'ADNU', 'OPNU']\n",
    "\n",
    "# Output paths\n",
    "MISMATCHES_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/mismatches.csv'\n",
    "MATCHED_QIDS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 11/matched_qids.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "def load_csv(path, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {dataset_name} from {path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Load datasets\n",
    "itemized_df = load_csv(ITEMIZED_CSV_PATH, \"itemized data\")\n",
    "totals_df = load_csv(TOTALS_CSV_PATH, \"totals data\")\n",
    "\n",
    "# ---------------------- Data Cleaning ---------------------- #\n",
    "\n",
    "def clean_text(df, column):\n",
    "    \"\"\"\n",
    "    Cleans text data by stripping leading/trailing spaces and converting to uppercase.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        df[column] = df[column].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        print(f\"Warning: '{column}' column is missing in the dataset. Filling with 'UNKNOWN'.\")\n",
    "        df[column] = 'UNKNOWN'\n",
    "    return df\n",
    "\n",
    "# Clean 'type_of_upgrade' and 'point_of_interconnection' in both datasets\n",
    "itemized_df = clean_text(itemized_df, 'type_of_upgrade')\n",
    "itemized_df = clean_text(itemized_df, 'point_of_interconnection')\n",
    "\n",
    "totals_df = clean_text(totals_df, 'type_of_upgrade')\n",
    "totals_df = clean_text(totals_df, 'point_of_interconnection')\n",
    "\n",
    "# ---------------------- Data Preparation ---------------------- #\n",
    "\n",
    "# Ensure necessary columns exist in itemized_df\n",
    "required_itemized_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', 'estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in required_itemized_columns:\n",
    "    if col not in itemized_df.columns:\n",
    "        print(f\"Warning: '{col}' column is missing in the itemized dataset.\")\n",
    "        if col in ['q_id', 'type_of_upgrade', 'point_of_interconnection']:\n",
    "            itemized_df[col] = 'UNKNOWN'\n",
    "        else:\n",
    "            itemized_df[col] = 0\n",
    "\n",
    "# Ensure necessary columns exist in totals_df\n",
    "required_totals_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in required_totals_columns:\n",
    "    if col not in totals_df.columns:\n",
    "        print(f\"Error: '{col}' column is missing in the totals dataset. Cannot proceed.\")\n",
    "        exit(1)\n",
    "\n",
    "# Convert cost columns to numeric, coercing errors to NaN and filling with 0\n",
    "cost_columns_itemized = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in cost_columns_itemized:\n",
    "    itemized_df[col] = pd.to_numeric(itemized_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "cost_columns_totals = [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in cost_columns_totals:\n",
    "    totals_df[col] = pd.to_numeric(totals_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# ---------------------- Calculate Manual Totals ---------------------- #\n",
    "\n",
    "# Group itemized data by q_id and type_of_upgrade and calculate sums\n",
    "itemized_grouped = itemized_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    'estimated_cost_x_1000': 'sum',\n",
    "    'escalated_cost_x_1000': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "itemized_grouped['manual_total'] = itemized_grouped.apply(\n",
    "    lambda row: row['estimated_cost_x_1000'] if row['estimated_cost_x_1000'] > 0 else row['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Prepare Totals Data ---------------------- #\n",
    "\n",
    "# Group totals data by q_id and type_of_upgrade and calculate sums\n",
    "totals_grouped = totals_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "    TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "totals_grouped['reported_total'] = totals_grouped.apply(\n",
    "    lambda row: row[TOTALS_ESTIMATED_COLUMN] if row[TOTALS_ESTIMATED_COLUMN] > 0 else row[TOTALS_ESCALATED_COLUMN],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Merge Data ---------------------- #\n",
    "\n",
    "# Merge the itemized and totals data on q_id and type_of_upgrade\n",
    "comparison_df = pd.merge(\n",
    "    itemized_grouped,\n",
    "    totals_grouped[['q_id', 'type_of_upgrade', 'reported_total']],\n",
    "    on=['q_id', 'type_of_upgrade'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---------------------- Check for Missing Upgrades ---------------------- #\n",
    "\n",
    "# Identify q_ids that are missing any of the required upgrades\n",
    "missing_upgrades_report = []\n",
    "for q_id in comparison_df['q_id'].unique():\n",
    "    upgrades_present = comparison_df[comparison_df['q_id'] == q_id]['type_of_upgrade'].unique()\n",
    "    missing_upgrades = [upgrade for upgrade in REQUIRED_UPGRADES if upgrade not in upgrades_present]\n",
    "    if missing_upgrades:\n",
    "        missing_upgrades_report.append((q_id, missing_upgrades))\n",
    "\n",
    "# Report missing upgrades\n",
    "if missing_upgrades_report:\n",
    "    print(\"\\nQ_ids with missing upgrades:\")\n",
    "    for q_id, missing in missing_upgrades_report:\n",
    "        print(f\"  Q_id {q_id} is missing upgrades: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nAll q_ids have all required upgrades.\")\n",
    "\n",
    "# ---------------------- Compare Totals and Identify Mismatches ---------------------- #\n",
    "\n",
    "# Initialize list to store mismatches\n",
    "mismatches = []\n",
    "\n",
    "# Iterate through each row to compare manual_total with reported_total\n",
    "for index, row in comparison_df.iterrows():\n",
    "    q_id = row['q_id']\n",
    "    upgrade = row['type_of_upgrade']\n",
    "    manual_total = row['manual_total']\n",
    "    reported_total = row['reported_total']\n",
    "    \n",
    "    # Determine if both manual_total and reported_total are zero\n",
    "    if manual_total == 0.0 and reported_total == 0.0:\n",
    "        continue  # No mismatch\n",
    "    # Determine if manual_total is zero and reported_total is missing or zero\n",
    "    elif manual_total == 0.0 and (pd.isna(row['reported_total']) or reported_total == 0.0):\n",
    "        continue  # No mismatch\n",
    "    # If reported_total is missing (NaN) and manual_total is not zero\n",
    "    elif pd.isna(row['reported_total']) and manual_total != 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: Missing\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': 'Missing'\n",
    "        })\n",
    "    # If manual_total is not zero and reported_total is zero\n",
    "    elif manual_total != 0.0 and reported_total == 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: 0.0\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # If both totals are non-zero but differ beyond tolerance\n",
    "    elif abs(manual_total - reported_total) > 1e-2:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: {reported_total}\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # Else, totals match; do nothing\n",
    "\n",
    "# Create a DataFrame for mismatches\n",
    "mismatches_df = pd.DataFrame(mismatches, columns=['q_id', 'type_of_upgrade', 'manual_total', 'reported_total'])\n",
    "\n",
    "# Save mismatches to a CSV file\n",
    "try:\n",
    "    mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "    print(f\"\\nMismatches saved to '{MISMATCHES_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving mismatches CSV: {e}\")\n",
    "\n",
    "# ---------------------- Point of Interconnection Matching ---------------------- #\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from itemized dataset\n",
    "itemized_poi = itemized_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from totals dataset\n",
    "totals_poi = totals_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Merge both to have a complete list of q_id and point_of_interconnection\n",
    "all_poi = pd.concat([itemized_poi, totals_poi]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ---------------------- Direct Match Identification ---------------------- #\n",
    "\n",
    "# Group by point_of_interconnection to find q_ids sharing the same point_of_interconnection\n",
    "direct_matches = all_poi.groupby('point_of_interconnection')['q_id'].apply(list).reset_index()\n",
    "\n",
    "# Filter groups with more than one q_id (i.e., shared points_of_interconnection)\n",
    "direct_matches = direct_matches[direct_matches['q_id'].apply(len) > 1]\n",
    "\n",
    "print(\"\\nDirect Matches (Exact Point of Interconnection Names):\")\n",
    "if not direct_matches.empty:\n",
    "    print(direct_matches)\n",
    "else:\n",
    "    print(\"No direct matches found.\")\n",
    "\n",
    "# ---------------------- Fuzzy Match Identification ---------------------- #\n",
    "\n",
    "# Prepare list of points_of_interconnection for fuzzy matching\n",
    "poi_list = all_poi['point_of_interconnection'].unique().tolist()\n",
    "\n",
    "# Initialize list to store fuzzy matches\n",
    "fuzzy_matches = []\n",
    "\n",
    "# Iterate through each point_of_interconnection to find similar ones\n",
    "for i, poi in enumerate(poi_list):\n",
    "    # Compare with the rest of the points to avoid redundant comparisons\n",
    "    similar_pois = process.extract(poi, poi_list[i+1:], scorer=fuzz.token_set_ratio)\n",
    "    \n",
    "    # Filter matches with similarity >= 80%\n",
    "    for match_poi, score in similar_pois:\n",
    "        if score >= 80:\n",
    "            # Retrieve q_ids for both points_of_interconnection\n",
    "            qids_poi1 = all_poi[all_poi['point_of_interconnection'] == poi]['q_id'].tolist()\n",
    "            qids_poi2 = all_poi[all_poi['point_of_interconnection'] == match_poi]['q_id'].tolist()\n",
    "            \n",
    "            # Append the matched pairs with their points_of_interconnection and similarity score\n",
    "            fuzzy_matches.append({\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_ids_1': qids_poi1,\n",
    "                'point_of_interconnection_2': match_poi,\n",
    "                'q_ids_2': qids_poi2,\n",
    "                'similarity_score': score\n",
    "            })\n",
    "\n",
    "# Convert fuzzy matches to DataFrame\n",
    "fuzzy_matches_df = pd.DataFrame(fuzzy_matches)\n",
    "\n",
    "print(\"\\nFuzzy Matches (>=80% Similarity in Point of Interconnection):\")\n",
    "if not fuzzy_matches_df.empty:\n",
    "    print(fuzzy_matches_df)\n",
    "else:\n",
    "    print(\"No fuzzy matches found.\")\n",
    "\n",
    "# ---------------------- Save Matched Q_ids to CSV ---------------------- #\n",
    "\n",
    "# For clarity, create a combined DataFrame for direct and fuzzy matches\n",
    "\n",
    "# Direct matches: list each pair of q_ids sharing the same point_of_interconnection\n",
    "direct_matches_expanded = []\n",
    "for _, row in direct_matches.iterrows():\n",
    "    qids = row['q_id']\n",
    "    poi = row['point_of_interconnection']\n",
    "    # Generate all possible unique pairs\n",
    "    for i in range(len(qids)):\n",
    "        for j in range(i+1, len(qids)):\n",
    "            direct_matches_expanded.append({\n",
    "                'match_type': 'Direct',\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_id_1': qids[i],\n",
    "                'point_of_interconnection_2': poi,\n",
    "                'q_id_2': qids[j],\n",
    "                'similarity_score': 100\n",
    "            })\n",
    "\n",
    "# Fuzzy matches: already have pairs\n",
    "fuzzy_matches_expanded = []\n",
    "for _, row in fuzzy_matches_df.iterrows():\n",
    "    fuzzy_matches_expanded.append({\n",
    "        'match_type': 'Fuzzy',\n",
    "        'point_of_interconnection_1': row['point_of_interconnection_1'],\n",
    "        'q_id_1': row['q_ids_1'],\n",
    "        'point_of_interconnection_2': row['point_of_interconnection_2'],\n",
    "        'q_id_2': row['q_ids_2'],\n",
    "        'similarity_score': row['similarity_score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "matched_qids_df = pd.DataFrame(direct_matches_expanded + fuzzy_matches_expanded)\n",
    "\n",
    "# Save matched q_ids to CSV\n",
    "try:\n",
    "    matched_qids_df.to_csv(MATCHED_QIDS_CSV_PATH, index=False)\n",
    "    print(f\"Matched Q_ids saved to '{MATCHED_QIDS_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving matched Q_ids CSV: {e}\")\n",
    "\n",
    "# ---------------------- Summary ---------------------- #\n",
    "\n",
    "# Print a summary\n",
    "total_checked = len(comparison_df)\n",
    "total_mismatches = len(mismatches_df)\n",
    "print(f\"\\nTotal checks performed: {total_checked}\")\n",
    "print(f\"Total mismatches found: {total_mismatches}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}