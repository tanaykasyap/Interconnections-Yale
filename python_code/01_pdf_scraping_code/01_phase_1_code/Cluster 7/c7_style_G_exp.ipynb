{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#  These code are for style G from clusters 6-8.\n",
    "\n",
    "#The idea is to write similar code for the other styles in these clusters and then combine them into one single script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code: IGNORE: This is for a subset of projects-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the new code which works for a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing PDF: /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1010/02_phase_1_study/C7PhI Appendix A - Q1010 Dyer Summit Wind Repower.pdf\n",
      "Extracting base data from PDF...\n",
      "Extracted Queue ID: 1010\n",
      "Extracted Cluster Number: 7\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "\n",
      "Processing /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1010/02_phase_1_study/C7PhI Appendix A - Q1010 Dyer Summit Wind Repower.pdf for Table 1 extraction...\n",
      "Table 1 starts on page 4 and ends on page 5\n",
      "\n",
      "Scraping tables on page 4 for Table 1...\n",
      "\n",
      "Attempt 1 with table settings: {'horizontal_strategy': 'text', 'vertical_strategy': 'lines', 'snap_tolerance': 1}\n",
      "Found 1 table(s) on page 4 with current settings.\n",
      "\n",
      "--- Table 1 on Page 4 ---\n",
      "Row 1: ['Project Parameters', 'Project Specific Data']\n",
      "Row 2: ['', '']\n",
      "Row 3: ['', 'Tracy, Alameda County, CA']\n",
      "Row 4: ['', '']\n",
      "Row 5: ['Project Location', '']\n",
      "Row 6: ['', '']\n",
      "Row 7: ['', 'GPS Coordinates: 37.76122\u00b0, -121.690893\u00b0']\n",
      "Row 8: ['', '']\n",
      "Row 9: ['Participating TO\u2019s Planning Area', 'North Area']\n",
      "Row 10: ['', '']\n",
      "Row 11: ['Number and Type of Generators', '22 x 2.1 MW Wind Turbines']\n",
      "Row 12: ['', '']\n",
      "Row 13: ['Interconnection Voltage', '60 kV']\n",
      "Row 14: ['', '']\n",
      "Row 15: ['Maximum Generator Output at', '']\n",
      "Row 16: ['', '']\n",
      "Row 17: ['', '46.2 MW']\n",
      "Row 18: ['', '']\n",
      "Row 19: ['generator Terminal', '']\n",
      "Row 20: ['', '']\n",
      "Row 21: ['Generator Auxiliary Load', '0.1 MW']\n",
      "Row 22: ['', '']\n",
      "Row 23: ['Maximum Net Output', '45 MW']\n",
      "Row 24: ['', '']\n",
      "Row 25: ['Power Factor Range', '0.94 lag to 0.94 lead']\n",
      "Row 26: ['', '']\n",
      "Row 27: ['', '1 x 60/34.5 kV, rated for 50 MVA with Z=8% at 50']\n",
      "Row 28: ['', '']\n",
      "Row 29: ['Step-up Transformer(s)', '']\n",
      "Row 30: ['', '']\n",
      "Row 31: ['', 'MVA']\n",
      "\n",
      "Attempt 2 with table settings: {'horizontal_strategy': 'lines', 'vertical_strategy': 'lines', 'snap_tolerance': 2}\n",
      "Found 1 table(s) on page 4 with current settings.\n",
      "\n",
      "--- Table 1 on Page 4 ---\n",
      "Row 1: ['Project Parameters', 'Project Specific Data']\n",
      "Row 2: ['Project Location', 'Tracy, Alameda County, CA\\nGPS Coordinates: 37.76122\u00b0, -121.690893\u00b0']\n",
      "Row 3: ['Participating TO\u2019s Planning Area', 'North Area']\n",
      "Row 4: ['Number and Type of Generators', '22 x 2.1 MW Wind Turbines']\n",
      "Row 5: ['Interconnection Voltage', '60 kV']\n",
      "Row 6: ['Maximum Generator Output at\\ngenerator Terminal', '46.2 MW']\n",
      "Row 7: ['Generator Auxiliary Load', '0.1 MW']\n",
      "Row 8: ['Maximum Net Output', '45 MW']\n",
      "Row 9: ['Power Factor Range', '0.94 lag to 0.94 lead']\n",
      "Row 10: ['Step-up Transformer(s)', '1 x 60/34.5 kV, rated for 50 MVA with Z=8% at 50\\nMVA']\n",
      "\n",
      "Scraping tables on page 5 for Table 1...\n",
      "\n",
      "Attempt 1 with table settings: {'horizontal_strategy': 'text', 'vertical_strategy': 'lines', 'snap_tolerance': 1}\n",
      "Found 1 table(s) on page 5 with current settings.\n",
      "\n",
      "--- Table 1 on Page 5 ---\n",
      "Row 1: ['', 'US Windpower #1 60 kV Tap Line which connects']\n",
      "Row 2: ['', '']\n",
      "Row 3: ['Point of Interconnection', '']\n",
      "Row 4: ['', '']\n",
      "Row 5: ['', 'to the Vasco-Herdlyn 60 kV Line']\n",
      "Row 6: ['', '']\n",
      "Row 7: ['Interconnection Customer Requested', '']\n",
      "Row 8: ['', '']\n",
      "Row 9: ['', '12/1/2015']\n",
      "Row 10: ['', '']\n",
      "Row 11: ['Commercial Operation Date', '']\n",
      "\n",
      "Point of Interconnection label found but adjacent value is empty (Page 5, Table 1, Row 3).\n",
      "Scanning rows 1 to 5 for POI value parts.\n",
      "Found POI part in row 1: 'US Windpower #1 60 kV Tap Line which connects'\n",
      "Found POI part in row 5: 'to the Vasco-Herdlyn 60 kV Line'\n",
      "\n",
      "Concatenated Point of Interconnection: 'US Windpower #1 60 kV Tap Line which connects to the Vasco-Herdlyn 60 kV Line' (Page 5, Table 1)\n",
      "Searching for GPS coordinates...\n",
      "GPS coordinates not found.\n",
      "Base data extracted: {'q_id': ['1010'], 'cluster': ['7'], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': ['US Windpower #1 60 kV Tap Line which connects to the Vasco-Herdlyn 60 kV Line']}\n",
      "\n",
      "Processing /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1010/02_phase_1_study/C7PhI Appendix A - Q1010 Dyer Summit Wind Repower.pdf for Table 7 extraction...\n",
      "Table 7 starts on page 12 and ends on page 16\n",
      "\n",
      "Scraping tables on page 12...\n",
      "New Table 7 detected: 'Escalated Cost and Time to Construct for Interconnection Facilities - IF' on page 12, table 1\n",
      "\n",
      "Scraping tables on page 13...\n",
      "New Table 7 detected: 'Reliability Network Upgrade' on page 13, table 1\n",
      "\n",
      "Scraping tables on page 14...\n",
      "Continuation Table detected on page 14, table 1\n",
      "Detected header row in continuation table on page 14, table 1. Dropping the header row.\n",
      "\n",
      "Scraping tables on page 15...\n",
      "New Table 7 detected: 'Area Delivery Network Upgrade' on page 15, table 1\n",
      "Detected 'Area Delivery Network Upgrade' table.\n",
      "\n",
      "Scraping tables on page 16...\n",
      "\n",
      "Concatenating all extracted Table 7 data...\n",
      "Merged base data with Table 7 data for /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1010/02_phase_1_study/C7PhI Appendix A - Q1010 Dyer Summit Wind Repower.pdf\n",
      "\n",
      "Processing PDF: /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1021/02_phase_1_study/Q1021_StocktonBiomass_PHI_Report-Addendum1.pdf\n",
      "Extracting base data from PDF...\n",
      "Extracted Queue ID: 1021\n",
      "Extracted Cluster Number: None\n",
      "Extracted Deliverability Status: Full\n",
      "Extracted Capacity: None\n",
      "\n",
      "Processing /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1021/02_phase_1_study/Q1021_StocktonBiomass_PHI_Report-Addendum1.pdf for Table 1 extraction...\n",
      "No Table 1 found in the PDF.\n",
      "Searching for GPS coordinates...\n",
      "GPS coordinates not found.\n",
      "Base data extracted: {'q_id': ['1021'], 'cluster': [None], 'req_deliverability': ['Full'], 'latitude': [None], 'longitude': [None], 'capacity': [None], 'point_of_interconnection': [None]}\n",
      "\n",
      "Processing /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1021/02_phase_1_study/Q1021_StocktonBiomass_PHI_Report-Addendum1.pdf for Table 7 extraction...\n",
      "Table 7 starts on page 4 and ends on page 5\n",
      "\n",
      "Scraping tables on page 4...\n",
      "No previous Table 7 title found for continuation on page 4, table 1. Skipping.\n",
      "No previous Table 7 title found for continuation on page 4, table 2. Skipping.\n",
      "\n",
      "Scraping tables on page 5...\n",
      "No Table 7 data extracted.\n",
      "No Table 7 data found in /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/data/1021/02_phase_1_study/Q1021_StocktonBiomass_PHI_Report-Addendum1.pdf. Using base data only.\n",
      "\n",
      "Columns reordered as per specification.\n",
      "\n",
      "Final combined DataFrame:\n",
      "\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n",
      "\u2502   q_id \u2502   cluster \u2502 req_deliverability   \u2502 latitude   \u2502 longitude   \u2502 capacity   \u2502 point_of_interconnection                                                      \u2502 type of upgrade                           \u2502 upgrade            \u2502 description                                                                                                                                       \u2502 cost allocation factor   \u2502 escalated costs x 1000   \u2502 estimated time  to construct   \u2502 estimated cost x 1000   \u2502\n",
      "\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n",
      "\u2502   1010 \u2502         7 \u2502 Full                 \u2502            \u2502             \u2502            \u2502 US Windpower #1 60 kV Tap Line which connects to the Vasco-Herdlyn 60 kV Line \u2502 PTO\u2019s Interconnection Facilities (Note 2) \u2502 Generation Site    \u2502 \u2022 Engineering Reviews, Metering, Pre-parallel Inspection, and Project Management \u2022 One (1) DTT Receiver, SCADA RTU Installation, and ISTS Testing \u2502 100.00%                  \u2502 $430                     \u2502 12                             \u2502 $404                    \u2502\n",
      "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
      "\u2502   1010 \u2502         7 \u2502 Full                 \u2502            \u2502             \u2502            \u2502 US Windpower #1 60 kV Tap Line which connects to the Vasco-Herdlyn 60 kV Line \u2502                                           \u2502                    \u2502                                                                                                                                                   \u2502 Total                    \u2502 $430                     \u2502                                \u2502 $404                    \u2502\n",
      "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
      "\u2502   1010 \u2502         7 \u2502 Full                 \u2502            \u2502             \u2502            \u2502 US Windpower #1 60 kV Tap Line which connects to the Vasco-Herdlyn 60 kV Line \u2502 Reliability Network Upgrade               \u2502 Herdlyn Substation \u2502 \u2022 One (1) DTT Transmitter (RFL Upgrade Only)                                                                                                      \u2502 100.00%                  \u2502 $58                      \u2502 12                             \u2502 $55                     \u2502\n",
      "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
      "\u2502   1010 \u2502         7 \u2502 Full                 \u2502            \u2502             \u2502            \u2502 US Windpower #1 60 kV Tap Line which connects to the Vasco-Herdlyn 60 kV Line \u2502                                           \u2502                    \u2502                                                                                                                                                   \u2502 Total                    \u2502 $58                      \u2502                                \u2502 $55                     \u2502\n",
      "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
      "\u2502   1010 \u2502         7 \u2502 Full                 \u2502            \u2502             \u2502            \u2502 US Windpower #1 60 kV Tap Line which connects to the Vasco-Herdlyn 60 kV Line \u2502 Local Delivery Network Upgrade            \u2502 None               \u2502                                                                                                                                                   \u2502                          \u2502                          \u2502                                \u2502                         \u2502\n",
      "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
      "\u2502   1010 \u2502         7 \u2502 Full                 \u2502            \u2502             \u2502            \u2502 US Windpower #1 60 kV Tap Line which connects to the Vasco-Herdlyn 60 kV Line \u2502                                           \u2502                    \u2502                                                                                                                                                   \u2502 Total                    \u2502                          \u2502                                \u2502                         \u2502\n",
      "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
      "\u2502   1010 \u2502         7 \u2502 Full                 \u2502            \u2502             \u2502            \u2502 US Windpower #1 60 kV Tap Line which connects to the Vasco-Herdlyn 60 kV Line \u2502 Area Delivery Network Upgrade             \u2502 None               \u2502                                                                                                                                                   \u2502                          \u2502                          \u2502                                \u2502                         \u2502\n",
      "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
      "\u2502   1010 \u2502         7 \u2502 Full                 \u2502            \u2502             \u2502            \u2502 US Windpower #1 60 kV Tap Line which connects to the Vasco-Herdlyn 60 kV Line \u2502                                           \u2502                    \u2502                                                                                                                                                   \u2502 Total                    \u2502                          \u2502                                \u2502                         \u2502\n",
      "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
      "\u2502   1021 \u2502           \u2502 Full                 \u2502            \u2502             \u2502            \u2502                                                                               \u2502 nan                                       \u2502 nan                \u2502 nan                                                                                                                                               \u2502 nan                      \u2502 nan                      \u2502 nan                            \u2502 nan                     \u2502\n",
      "\u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_75037/2247125055.py:648: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  core = core.applymap(clean_string_cell)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "from tabulate import tabulate\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def search_gps_coordinates(text):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    print(\"Searching for GPS coordinates...\")\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\")\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\")\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\")\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\")\n",
    "    return (None, None)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def extract_table1(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\")\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Identify all pages that contain \"Table 1\"\n",
    "        table1_pages = []\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text() or \"\"\n",
    "            if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                table1_pages.append(i)\n",
    "\n",
    "        if not table1_pages:\n",
    "            print(\"No Table 1 found in the PDF.\")\n",
    "            return None  # Return None if no Table 1 found\n",
    "\n",
    "        first_page = table1_pages[0]\n",
    "        last_page = table1_pages[-1]\n",
    "        scrape_start = first_page\n",
    "        scrape_end = last_page + 1  # Plus one to include the next page if needed\n",
    "\n",
    "        print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\")\n",
    "\n",
    "        # Flag to indicate if extraction was successful\n",
    "        extraction_successful = False\n",
    "\n",
    "        # Iterate through the specified page range\n",
    "        for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "            page = pdf.pages[page_number]\n",
    "            print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\")\n",
    "\n",
    "            for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                print(f\"\\nAttempt {attempt} with table settings: {table_settings}\")\n",
    "                tables = page.find_tables(table_settings=table_settings)\n",
    "                print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\")\n",
    "\n",
    "                for table_index, table in enumerate(tables, start=1):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\")\n",
    "                        continue  # Skip empty tables\n",
    "\n",
    "                    print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\")\n",
    "                    for row_num, row in enumerate(tab, start=1):\n",
    "                        print(f\"Row {row_num}: {row}\")\n",
    "\n",
    "                    # Iterate through each row in the table\n",
    "                    for row_index, row in enumerate(tab, start=1):\n",
    "                        # Iterate through each cell in the row\n",
    "                        for cell_index, cell in enumerate(row, start=1):\n",
    "                            if cell and poi_pattern.search(cell):\n",
    "                                # Assuming the next column contains the value\n",
    "                                poi_col_index = cell_index  # 1-based index\n",
    "                                adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                if adjacent_col_index <= len(row):\n",
    "                                    poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                    if poi_value:  # Check if the value is not empty\n",
    "                                        point_of_interconnection = poi_value\n",
    "                                        print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\")\n",
    "                                        extraction_successful = True\n",
    "                                        break  # Exit the cell loop\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\")\n",
    "                                        # Proceed to scan surrounding rows for the value\n",
    "                                        poi_value_parts = []\n",
    "\n",
    "                                        # Define the range to scan: two rows above and two rows below\n",
    "                                        # Convert to 0-based index\n",
    "                                        current_row_idx = row_index - 1\n",
    "                                        start_scan = max(0, current_row_idx - 2)\n",
    "                                        end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                        print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\")\n",
    "\n",
    "                                        for scan_row_index in range(start_scan, end_scan):\n",
    "                                            # Skip the current row where the label was found\n",
    "                                            if scan_row_index == current_row_idx:\n",
    "                                                continue\n",
    "\n",
    "                                            scan_row = tab[scan_row_index]\n",
    "                                            # Ensure the adjacent column exists in the scan row\n",
    "                                            if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                    poi_value_parts.append(scan_cell)\n",
    "                                                    print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\")\n",
    "                                                elif poi_pattern.search(scan_cell):\n",
    "                                                    # If another POI label is found, skip it\n",
    "                                                    print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\")\n",
    "                                                    continue\n",
    "\n",
    "                                        if poi_value_parts:\n",
    "                                            # Concatenate the parts to form the complete POI value\n",
    "                                            point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                            print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index})\")\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\")\n",
    "                                            # Do not return immediately; proceed to retry\n",
    "                                else:\n",
    "                                    print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                          f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\")\n",
    "                                    # Do not return immediately; proceed to retry\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the row loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the table loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the attempt loop\n",
    "            if extraction_successful:\n",
    "                break  # Exit the page loop\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\")\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\")\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_base_data(pdf_path, project_id):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\")\n",
    "    with open(pdf_path, 'rb') as pdf:\n",
    "        reader = PyPDF2.PdfReader(pdf)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text\n",
    "\n",
    "    text = clean_string_cell(text)\n",
    "\n",
    "    queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "    queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "    print(f\"Extracted Queue ID: {queue_id}\")\n",
    "\n",
    "    cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "    cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "    print(f\"Extracted Cluster Number: {cluster_number}\")\n",
    "\n",
    "    deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "    deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "    print(f\"Extracted Deliverability Status: {deliverability_status}\")\n",
    "\n",
    "    # Extract Capacity\n",
    "    capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "    if capacity:\n",
    "        capacity = int(capacity.group(1))\n",
    "    else:\n",
    "        capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "        capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "    print(f\"Extracted Capacity: {capacity}\")\n",
    "\n",
    "    # Removed Point of Interconnection extraction as per instruction\n",
    "    point_of_interconnection = extract_table1(pdf_path)\n",
    "    #point_of_interconnection = None\n",
    "    latitude, longitude = search_gps_coordinates(text)\n",
    "\n",
    "    # Initialize base data dictionary\n",
    "    base_data = {\n",
    "        \"q_id\": [queue_id],\n",
    "        \"cluster\": [cluster_number],\n",
    "        \"req_deliverability\": [deliverability_status],\n",
    "        \"latitude\": [latitude],\n",
    "        \"longitude\": [longitude],\n",
    "        \"capacity\": [capacity],\n",
    "        \"point_of_interconnection\": [point_of_interconnection]\n",
    "    }\n",
    "\n",
    "    print(\"Base data extracted:\", base_data)\n",
    "    return pd.DataFrame(base_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_table7(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\")\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None  # To store the specific phrase from the table title\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Identify all pages that contain \"Table 7\" or \"Table 7-\"\n",
    "        table7_pages = []\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text() or \"\"\n",
    "            if re.search(r\"Table\\s*7\", text, re.IGNORECASE): #[-\\s]\n",
    "                table7_pages.append(i)\n",
    "\n",
    "        if not table7_pages:\n",
    "            print(\"No Table 7 found in the PDF.\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame if no Table 7 found\n",
    "\n",
    "        first_page = table7_pages[0]\n",
    "        last_page = table7_pages[-1]\n",
    "        scrape_start = first_page\n",
    "        scrape_end = last_page + 1  # Plus one to include the next page\n",
    "\n",
    "        print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\")\n",
    "\n",
    "        # Iterate through the specified page range\n",
    "        for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "            page = pdf.pages[page_number]\n",
    "            print(f\"\\nScraping tables on page {page_number + 1}...\")\n",
    "            tables = page.find_tables(table_settings={\n",
    "                \"horizontal_strategy\": \"lines\",\n",
    "                \"vertical_strategy\": \"lines\",\n",
    "            })\n",
    "\n",
    "            for table_index, table in enumerate(tables):\n",
    "                tab = table.extract()\n",
    "                if not tab:\n",
    "                    print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\")\n",
    "                    continue  # Skip empty tables\n",
    "\n",
    "                # Get the bounding box of the current table\n",
    "                table_bbox = table.bbox  # (x0, y0, x1, y1)\n",
    "\n",
    "                # Define a bounding box from the top of the page to the top of the table\n",
    "                # Coordinates: (x0, y0, x1, y1)\n",
    "                title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "\n",
    "                # Extract text within the title bounding box\n",
    "                title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "\n",
    "                # Initialize table_title as None\n",
    "                table_title = None\n",
    "\n",
    "                if title_text:\n",
    "                    # Split the title text into lines and reverse to start checking from the closest line to the table\n",
    "                    title_lines = title_text.split('\\n')[::-1]\n",
    "\n",
    "                    for line in title_lines:\n",
    "                        line = line.strip()\n",
    "                        # Check if the line matches the table title pattern (e.g., \"Table 7-1: PTO Upgrade\")\n",
    "                        match = re.match(r\"Table\\s*7[-.]\\d+[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                        if match:\n",
    "                            table_title = match.group(1).strip()\n",
    "                            break  # Stop after finding the closest table title\n",
    "\n",
    "                if table_title:\n",
    "                    # New Table 7 detected\n",
    "                    specific_phrase = extract_specific_phrase(table_title)  # Extracted phrase from the title\n",
    "                    print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\")\n",
    "\n",
    "                    # Extract headers and data rows\n",
    "                    headers = clean_column_headers(tab[0])\n",
    "                    data_rows = tab[1:]\n",
    "\n",
    "                    # Create DataFrame for the new table\n",
    "                    try:\n",
    "                        df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                    except ValueError as ve:\n",
    "                        print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\")\n",
    "                        continue  # Skip this table due to error\n",
    "\n",
    "                    # Special Handling for \"Area Delivery Network Upgrade\" Tables\n",
    "                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                        print(\"Detected 'Area Delivery Network Upgrade' table.\")\n",
    "                        if \"adnu\" in df_new.columns:\n",
    "                            # Rename 'adnu' to 'upgrade'\n",
    "                            if \"upgrade\" in df_new.columns:\n",
    "                                # Avoid duplicate 'upgrade' columns\n",
    "                                df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                print(\"Dropped 'ADNU' column to avoid duplicate 'upgrade' columns.\")\n",
    "                            else:\n",
    "                                df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                print(\"Renamed 'ADNU' to 'upgrade'.\")\n",
    "                        if \"type of upgrade\" not in df_new.columns:\n",
    "                            df_new[\"type of upgrade\"] = specific_phrase\n",
    "                            print(\"Added 'type of upgrade' column with specific phrase.\")\n",
    "                    else:\n",
    "                        # General Handling for other tables\n",
    "                        if \"type of upgrade\" in df_new.columns:\n",
    "                            first_row = df_new.iloc[0]\n",
    "                            if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row on page {page_number + 1}, table {table_index + 1}\")\n",
    "                                df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        else:\n",
    "                            # If \"Type of Upgrade\" column does not exist, add it\n",
    "                            df_new[\"type of upgrade\"] = specific_phrase\n",
    "                            print(f\"'Type of Upgrade' column added with value '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\")\n",
    "\n",
    "                    # Ensure no duplicate columns\n",
    "                    if df_new.columns.duplicated().any():\n",
    "                        print(f\"Duplicate columns detected in new table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\")\n",
    "                        df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                    # Append the new table to the list\n",
    "                    extracted_tables.append(df_new)\n",
    "                else:\n",
    "                    # Continuation Table detected\n",
    "                    if specific_phrase is None:\n",
    "                        print(f\"No previous Table 7 title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\")\n",
    "                        continue  # Skip if no previous title to refer to\n",
    "\n",
    "                    print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\")\n",
    "\n",
    "                    # Treat all rows as data without headers\n",
    "                    data_rows = tab\n",
    "\n",
    "                    # Check if the number of columns matches\n",
    "                    expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                    if expected_columns is None:\n",
    "                        print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\")\n",
    "                        continue  # No table to continue with\n",
    "\n",
    "                    # Define expected columns based on the last extracted table\n",
    "                    expected_headers = extracted_tables[-1].columns.tolist()\n",
    "\n",
    "                    # Define the list of column headers to check in continuation tables\n",
    "                    header_keywords = [\"type of upgrade\", \"upgrade\", \"adnu\"]\n",
    "\n",
    "                    # Check if the first row contains any header keywords\n",
    "                    first_continuation_row = data_rows[0] if len(data_rows) > 0 else []\n",
    "                    is_header_row = any(\n",
    "                        re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                    )\n",
    "\n",
    "                    if is_header_row:\n",
    "                        print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping the header row.\")\n",
    "                        data_rows = data_rows[1:]  # Drop the header row\n",
    "\n",
    "                    # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                        print(\"Handling continuation for 'Area Delivery Network Upgrade' table.\")\n",
    "                        if \"adnu\" in data_rows[0]:\n",
    "                            # Rename 'ADNU' to 'upgrade' in continuation table.\n",
    "                            print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\")\n",
    "                            data_rows = [ [\"upgrade\"] + row[1:] for row in data_rows ]\n",
    "                        if \"upgrade\" not in expected_headers:\n",
    "                            # Insert 'upgrade' column at the beginning\n",
    "                            print(\"Inserting 'upgrade' column with specific phrase in continuation table.\")\n",
    "                            data_rows = [ [specific_phrase] + row for row in data_rows ]\n",
    "\n",
    "                    # Handle missing or extra columns\n",
    "                    if len(data_rows[0]) < expected_columns:\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            # For ADNU tables, assume missing \"upgrade\" column\n",
    "                            print(f\"Detected missing 'upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'upgrade' column.\")\n",
    "                            data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                        else:\n",
    "                            # For other tables, assume missing \"Type of Upgrade\" column\n",
    "                            print(f\"Detected missing 'Type of Upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'Type of Upgrade' column.\")\n",
    "                            data_rows = [[\"Type of Upgrade\"] + row for row in data_rows]\n",
    "                    elif len(data_rows[0]) > expected_columns:\n",
    "                        # Extra columns detected; adjust accordingly\n",
    "                        print(f\"Detected extra columns in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping extra columns.\")\n",
    "                        data_rows = [row[:expected_columns] for row in data_rows]\n",
    "\n",
    "                    # Create DataFrame for the continuation table\n",
    "                    try:\n",
    "                        df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                    except ValueError as ve:\n",
    "                        print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\")\n",
    "                        continue  # Skip this table due to error\n",
    "\n",
    "                    # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                    if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                        if \"upgrade\" in df_continuation.columns:\n",
    "                            first_row = df_continuation.iloc[0]\n",
    "                            if pd.isna(first_row[\"upgrade\"]) or first_row[\"upgrade\"] == \"\":\n",
    "                                print(f\"Replacing 'None' in 'upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\")\n",
    "                                df_continuation.at[0, \"upgrade\"] = specific_phrase\n",
    "                        else:\n",
    "                            # If \"upgrade\" column does not exist, add it\n",
    "                            df_continuation[\"upgrade\"] = specific_phrase\n",
    "                            print(f\"'upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\")\n",
    "                    else:\n",
    "                        # General Handling for other tables\n",
    "                        if \"type of upgrade\" in df_continuation.columns:\n",
    "                            first_row = df_continuation.iloc[0]\n",
    "                            if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\")\n",
    "                                df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                        else:\n",
    "                            # If \"Type of Upgrade\" column does not exist, add it\n",
    "                            df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                            print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\")\n",
    "\n",
    "                    # Ensure no duplicate columns\n",
    "                    if df_continuation.columns.duplicated().any():\n",
    "                        print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\")\n",
    "                        df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                    # Merge with the last extracted table\n",
    "                    extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "\n",
    "    # After scraping all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        # To prevent reindexing errors, ensure all DataFrames have unique and consistent columns\n",
    "        # Collect all unique column names\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "        #all_columns = sorted(all_columns)  # Optional: sort columns for consistency\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            # Reindex to have all columns, filling missing ones with NaN\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\")\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\")\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\")\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    pdfs = [    \n",
    "       #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/947/02_phase_1_study/Q947 GV Solar 3_C6Ph I_Appendix A-Individual Study Report_Final.pdf\",\n",
    "       \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1010/02_phase_1_study/C7PhI Appendix A - Q1010 Dyer Summit Wind Repower.pdf\",\n",
    "       \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1021/02_phase_1_study/Q1021_StocktonBiomass_PHI_Report-Addendum1.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1030/02_phase_1_study/C7PhI - Appendix A - Q1030   South Lake Solar.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1035/02_phase_1_study/C7PhI Appendix A - Q1035 Magellan Solar Final.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1098/02_phase_1_study/C8 Ph I Appendix A - Q1098 Birds Landing Wind.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1130/02_phase_1_study/Q1130 Moon Prism 2_Appendix A Report_C8PhI.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1133/02_phase_1_study/Q1133 Oveja Solar Farm_Appendix A Report_C8PhI.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1158/02_phase_1_study/Q1158 Slate_Appendix A Report_C8PhI.pdf\",\n",
    "        #\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/1170/02_phase_1_study/QC8PhI_Q1170_Gateway Energy Storage_Appendix A_1-15-2016.pdf\",\n",
    "    ]\n",
    "\n",
    "    # Initialize an empty DataFrame to store all results\n",
    "    core = pd.DataFrame()\n",
    "\n",
    "    for pdf_path in pdfs:\n",
    "        print(f\"\\nProcessing PDF: {pdf_path}\")\n",
    "        # Extract base data\n",
    "        project_id = os.path.basename(pdf_path).split('.')[0]  # Modify as appropriate\n",
    "        base_data = extract_base_data(pdf_path, project_id)\n",
    "\n",
    "        # Extract Table 7 data\n",
    "        table7_data = extract_table7(pdf_path)\n",
    "\n",
    "        if table7_data.empty:\n",
    "            print(f\"No Table 7 data found in {pdf_path}. Using base data only.\")\n",
    "            merged_df = base_data\n",
    "        else:\n",
    "            # Merge base data with Table 7 data\n",
    "            overlapping_columns = base_data.columns.intersection(table7_data.columns)\n",
    "            table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "            base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "            try:\n",
    "                merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "                print(f\"Merged base data with Table 7 data for {pdf_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\")\n",
    "                merged_df = base_data  # Fallback to base data only\n",
    "\n",
    "        # Append to core\n",
    "        try:\n",
    "            core = pd.concat([core, merged_df], ignore_index=True, sort=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error appending data from {pdf_path} to core DataFrame: {e}\")\n",
    "\n",
    "    # Clean up the entire DataFrame\n",
    "    core = core.applymap(clean_string_cell)\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    core = reorder_columns(core)\n",
    "    print(\"\\nColumns reordered as per specification.\")\n",
    "\n",
    "    # Display the final combined DataFrame\n",
    "    print(\"\\nFinal combined DataFrame:\")\n",
    "    print(tabulate(core, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "\n",
    "    # Save to CSV if needed\n",
    "    # core.to_csv('final_data.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Project Parameters Pro', 'ject Specific Data'], ['Alpa\\nProject Location\\nGPS', 'ugh, Tulare County, CA\\nCoordinates: 35\u00b0 49.285 N, 119\u00b0 27.317 W'], ['Participating TO\u2019s Planning Area Kern', ''], ['240\\nNumber and Type of Generators\\nInve', 'x 0.5 MW PV Advanced Energy Solaron\\nrter Units'], ['Interconnection Voltage 115', 'kV'], ['Maximum Generator Output at\\n120\\ngenerator Terminal', 'MW'], ['Generator Auxiliary Load 0.5', 'MW'], ['Maximum Net Output 120', 'MW'], ['Power Factor Range 0.95', 'leading to 0.95 lagging'], ['thre\\nStep-up Transformer(s)\\nwith', 'e-phase 115/34.5, rated for 75/100/125 MVA\\nZ=8.5% at 75 MVA'], ['Collector System Equivalent\\nR1\\nImpedance P', '=0.0083 X1 =0.0067 B1 =0.0240\\nU PU PU'], ['Point of Interconnection Oliv', 'e 115kV Substation'], ['Interconnection Customer Requested\\nDec\\nCommercial Operation Date', 'ember 1, 2017']]\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data/947/02_phase_1_study/Q947 GV Solar 3_C6Ph I_Appendix A-Individual Study Report_Final.pdf\") as pdf:\n",
    "    page = pdf.pages[4]\n",
    "    table = page.extract_table()\n",
    "    print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Projects in the cluster code: this code fixed the issues with the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Codes Please IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note some projects require some manual attention like 1098, 1099, 1102,1133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets skip these projects for now 1133,1225,1227,1229,1234,1236,1237,1249,1252,1256,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Code for Cluster 7 Style G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped PDF: C7PhI Appendix A - Q1007 Coyote Creek Energy Storage.pdf from Project 1007\n",
      "Scraped PDF: C7PhI Appendix A - Q1010 Dyer Summit Wind Repower.pdf from Project 1010\n",
      "Scraped PDF: C7PhI Appendix A - Q1011 GHS Project.pdf from Project 1011\n",
      "Scraped PDF: C7PhI Appendix A - Q1013 Lake Clementine Hydroelectric.pdf from Project 1013\n",
      "Scraped PDF: C7PhI Appendix A - Q1014 Paradise Cut.pdf from Project 1014\n",
      "Scraped PDF: C7PhI Appendix A - Q1015 Pleasant Grove - Grid Stabilization Project.pdf from Project 1015\n",
      "Scraped PDF: C7PhI Appendix A - Q1016 Richmond Steam Turbine.pdf from Project 1016\n",
      "Scraped PDF: C7PhI Appendix A - Q1018 SGS South San Francisco.pdf from Project 1018\n",
      "Scraped PDF: C7PhI Appendix A - Q1019 Shingle Springs - Grid Stabilization Project.pdf from Project 1019\n",
      "Scraped PDF: C7PhI Appendix A - Q1020 Sky Valley Wind.pdf from Project 1020\n",
      "Scraped PDF: C7PhI Appendix A - Q1021 Stockton Biomass.pdf from Project 1021\n",
      "Scraped Addendum PDF: Q1021_StocktonBiomass_PHI_Report-Addendum1.pdf from Project 1021\n",
      "Scraped PDF: C7PhI Appendix A - Q1023 Umbarger Evergreen Energy Storage.pdf from Project 1023\n",
      "Scraped PDF: C7PhI Appendix A - Q1024 Victor Grid Stabilization Project.pdf from Project 1024\n",
      "Scraped PDF: C7PhI Appendix A - Q1026 Weber Grid Stabilization Project.pdf from Project 1026\n",
      "Scraped PDF: C7PhI - Appendix A -  Q1027 Blackbriar.pdf from Project 1027\n",
      "Scraped PDF: 14AS868993-C7PhI_Appendix_A__Q1028_Little_Bear_Solar_1__Revision_1_27Jan2015.pdf from Project 1028\n",
      "Scraped PDF: 14AS869615-C7PhI_Appendix_A__Q1029_Little_Bear_Solar_2__Revision_1_27Jan2015.pdf from Project 1029\n",
      "Scraped PDF: C7PhI - Appendix A - Q1030   South Lake Solar.pdf from Project 1030\n",
      "Scraped PDF: C7PhI - Appendix A - Q1031   Stonecrop.pdf from Project 1031\n",
      "Scraped PDF: C7PhI - Appendix A - Q1032 RE Tranquillity 8.pdf from Project 1032\n",
      "Scraped PDF: C7PhI Appendix A - Q1033 Broken Spoke Final.pdf from Project 1033\n",
      "Scraped PDF: C7PhI Appendix A - Q1035 Magellan Solar Final.pdf from Project 1035\n",
      "Scraped PDF: C7PhI - Appendix A - Q1036 RE Mustang Two.pdf from Project 1036\n",
      "Scraped PDF: C7PhI Appendix A - Q1037 New HECA_Final.pdf from Project 1037\n",
      "Scraped PDF: C7PhI Appendix A - Q1038 Voyager Solar_Final.pdf from Project 1038\n",
      "Scraped PDF: QC7PhI_Q1040_Arlington Valley Solar Energy 2_Appendix A_12-17-2014.pdf from Project 1040\n",
      "Scraped PDF: QC7PhI_Q1043_Canasta_Appendix A_12-17-2014.pdf from Project 1043\n",
      "Scraped PDF: QC7PhI_Q1045_Chula Vista Reliablity Appendix A_12-17-2014.pdf from Project 1045\n",
      "Scraped PDF: QC7PhI_Q1046_ECO Energy Storage Center_Appendix A_12-17-2014.pdf from Project 1046\n",
      "Scraped PDF: QC7PhI_Q1047_El Cajon Reliability_Appendix A_12-17-2014.pdf from Project 1047\n",
      "Scraped PDF: QC7PhI_Q1048_Escondido Reliability_Appendix A_12-17-2014.pdf from Project 1048\n",
      "Scraped PDF: QC7PhI_Q1049_Hawkwind_Appendix A_12-17-2014.pdf from Project 1049\n",
      "Scraped PDF: QC7PhI_Q1050_Honey Badger_Appendix A_12-17-2014.pdf from Project 1050\n",
      "Scraped PDF: QC7PhI_Q1051_Kearny Hybrid Power_Appendix A_12-17-2014.pdf from Project 1051\n",
      "Scraped PDF: QC7PhI_Q1052_Mesa Rojo_Appendix A_12-17-2014.pdf from Project 1052\n",
      "Scraped PDF: Revised QC7PhI_Q1053_Mesquite Solar 3_Appendix A_02-04-2015.pdf from Project 1053\n",
      "Scraped PDF: QC7PhI_Q1055_NCC Hybrid Power_Appendix A_12-17-2014.pdf from Project 1055\n",
      "Scraped PDF: Revised QC7PhI_Q1056_NCI Hybrid Power_Appendix A_02-04-2015.pdf from Project 1056\n",
      "Scraped PDF: QC7PhI_Q1057_Otay Mesa Energy Center AGP_Appendix A_12-17-2014.pdf from Project 1057\n",
      "Scraped PDF: QC7PhI_Q1058_Pio Pico Energy Center 2_Appendix A_12-17-2014.pdf from Project 1058\n",
      "Scraped PDF: QC7PhI_Q1059_Sea Lion Energy Storage_Appendix A_12-17-2014.pdf from Project 1059\n",
      "Scraped PDF: QC7PhI_Q1060_SLW_Appendix A_12-17-2014.pdf from Project 1060\n",
      "Scraped PDF: QC7PhI_Q1061_Vista Energy Storage_Appendix A_12-17-2014.pdf from Project 1061\n",
      "Scraped PDF: QC7PhI_Q1062_White Wing Ranch Solar _Appendix A_12-17-2014.pdf from Project 1062\n",
      "Scraped PDF: QC7PhI_Q1063_Wiley Road_Appendix A_12-17-2014.pdf from Project 1063\n",
      "Skipped PDF: QC7P1-VEA-EOP-Q1064-ARES-Appendix A-Attachment 2.pdf from Project 1064 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1064-ARES-Appendix A-Attachment 2.pdf from Project 1064 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1064-ARES-Appendix A-Attachment 2-SCD.pdf from Project 1064 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1064-ARES-Appendix A-Attachment 1.pdf from Project 1064 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1064-ARES-Appendix A.pdf from Project 1064 (No Table 7)\n",
      "Skipped PDF: QC7P1-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2_Addendum01.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7P1-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2-SCD.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 1.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A_Addendum01.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 1_Addendum01.pdf from Project 1065 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2-SCD.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 1.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7P1-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 2.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A_Addendum01.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 1_Addendum01.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7P1-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 2_Addendum01.pdf from Project 1066 (No Table 7)\n",
      "Skipped PDF: QC7P1-VEA-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 2.pdf from Project 1067 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-VEA-EOP-Q1067-Spring Mountain Solar Project-Appendix A_Addendum01.pdf from Project 1067 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 1.pdf from Project 1067 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 2-SCD.pdf from Project 1067 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1067-Spring Mountain Solar-Appendix A.pdf from Project 1067 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 2.pdf from Project 1067 (No Table 7)\n",
      "Skipped PDF: QC7P1-VEA-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 2.pdf from Project 1068 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-VEA-EOP-Q1068-Sunshine Valley Solar 3 -Appendix A_Addendum01.pdf from Project 1068 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 1.pdf from Project 1068 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 2.pdf from Project 1068 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 2-SCD.pdf from Project 1068 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1068-Sunshine Valley Solar 3-Appendix A.pdf from Project 1068 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2-SCD.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1069-Boulder Solar-Appendix A-Attachment 1.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: QC7P1-VEA-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2_Addendum01.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-VEA-EOP-Q1069-Boulder Solar -Appendix A_Addendum01.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: QC7P1-SCE-VEA-EOP-Q1069-Boulder Solar-Appendix A.pdf from Project 1069 (No Table 7)\n",
      "Skipped PDF: Q1070-Appendix A_Attachment 1-Interconnection Network Facilities.pdf from Project 1070 (No Table 7)\n",
      "Skipped PDF: Q1070-Appendix A_Attachment 2-SCD Cost Allocation.pdf from Project 1070 (No Table 7)\n",
      "Skipped PDF: Q1070-Appendix A-Jupiter Solar.pdf from Project 1070 (No Table 7)\n",
      "Skipped PDF: Q1070-Appendix A_Attachment 2-Escalated Cost.pdf from Project 1070 (No Table 7)\n",
      "Skipped PDF: Q1071-Appendix A_Attachment 1-Interconnection Network Facilities.pdf from Project 1071 (No Table 7)\n",
      "Skipped PDF: Q1071-Appendix A_Attachment 2-SCD Cost Allocation.pdf from Project 1071 (No Table 7)\n",
      "Skipped PDF: Q1071-Appendix A_Attachment 2-Escalated Cost.pdf from Project 1071 (No Table 7)\n",
      "Skipped PDF: Q1071-Appendix A-Quartz 4 Solar.pdf from Project 1071 (No Table 7)\n",
      "Skipped PDF: Q1072-Appendix A-Vermillion.pdf from Project 1072 (No Table 7)\n",
      "Skipped PDF: Q1072-Appendix A_Attachment 1-Interconnection Network Facilities.pdf from Project 1072 (No Table 7)\n",
      "Skipped PDF: Q1072-Appendix A_Attachment 2-Escalated Cost.pdf from Project 1072 (No Table 7)\n",
      "Skipped PDF: Q1072-Appendix A_Attachment 2-SCD Cost Allocation.pdf from Project 1072 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A-Attachment 1.pdf from Project 1073 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A-Attachment 2 SCD.pdf from Project 1073 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A-Attachment 2.pdf from Project 1073 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A.pdf from Project 1073 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A-Attachment 2.pdf from Project 1074 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A-Attachment 1.pdf from Project 1074 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A.pdf from Project 1074 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A Addendum.pdf from Project 1074 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A-Attachment 2 SCD.pdf from Project 1074 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A.pdf from Project 1075 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A-Attachment 2 SCD.pdf from Project 1075 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A-Attachment 1.pdf from Project 1075 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A-Attachment 2.pdf from Project 1075 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A-Attachment 2 SCD.pdf from Project 1076 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A-Attachment 1.pdf from Project 1076 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A-Attachment 2.pdf from Project 1076 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A.pdf from Project 1076 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A-Attachment 2 SCD.pdf from Project 1077 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A-Attachment 2.pdf from Project 1077 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A-Attachment 1.pdf from Project 1077 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A.pdf from Project 1077 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1080-Birch Street Generation Facility-Appendix A-Attachment 2 SCD.pdf from Project 1080 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1080 Birch Stret Generation Facility-Revised Appendix A-Attachment 2.pdf from Project 1080 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1080-Birch Street Generation Facility-Appendix A-Attachment 2.pdf from Project 1080 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1080-Birch Street Generation Facility-Appendix A-Attachment 1.pdf from Project 1080 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1080 Birch Street Generation Facility-Revised Appendix A.pdf from Project 1080 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A-Attachment 2.pdf from Project 1083 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A-Attachment 1.pdf from Project 1083 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A-Attachment 2 SCD.pdf from Project 1083 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A.pdf from Project 1083 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A-Attachment 2 SCD.pdf from Project 1084 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A.pdf from Project 1084 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A-Attachment 1.pdf from Project 1084 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A-Attachment 2.pdf from Project 1084 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A.pdf from Project 1087 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A-Attachment 2 SCD.pdf from Project 1087 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A-Attachment 2.pdf from Project 1087 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A-Attachment 1.pdf from Project 1087 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A-Attachment 1.pdf from Project 1088 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A-Attachment 2.pdf from Project 1088 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A-Attachment 2 SCD.pdf from Project 1088 (No Table 7)\n",
      "Skipped PDF: QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A.pdf from Project 1088 (No Table 7)\n",
      "\n",
      "Columns reordered for originals as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/rawdata_cluster7_style_G_originals.csv\n",
      "\n",
      "Columns reordered for addendums as per specification.\n",
      "\n",
      "Data successfully saved to /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/rawdata_cluster7_style_G_addendums.csv\n",
      "\n",
      "=== Scraping Summary ===\n",
      "Total Projects Processed: 64\n",
      "Total Projects Scraped: 45\n",
      "Total Projects Skipped: 19\n",
      "Total Projects Missing: 18\n",
      "Total PDFs Accessed: 140\n",
      "Total PDFs Scraped: 46\n",
      "Total PDFs Skipped: 94\n",
      "\n",
      "List of Scraped Projects:\n",
      "[1007, 1010, 1011, 1013, 1014, 1015, 1016, 1018, 1019, 1020, 1021, 1023, 1024, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1035, 1036, 1037, 1038, 1040, 1043, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063]\n",
      "\n",
      "List of Skipped Projects:\n",
      "[1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1080, 1083, 1084, 1087, 1088]\n",
      "\n",
      "List of Missing Projects:\n",
      "[1008, 1009, 1012, 1017, 1022, 1025, 1034, 1039, 1041, 1042, 1044, 1054, 1078, 1079, 1081, 1082, 1085, 1086]\n",
      "\n",
      "List of Scraped PDFs:\n",
      "['C7PhI Appendix A - Q1007 Coyote Creek Energy Storage.pdf', 'C7PhI Appendix A - Q1010 Dyer Summit Wind Repower.pdf', 'C7PhI Appendix A - Q1011 GHS Project.pdf', 'C7PhI Appendix A - Q1013 Lake Clementine Hydroelectric.pdf', 'C7PhI Appendix A - Q1014 Paradise Cut.pdf', 'C7PhI Appendix A - Q1015 Pleasant Grove - Grid Stabilization Project.pdf', 'C7PhI Appendix A - Q1016 Richmond Steam Turbine.pdf', 'C7PhI Appendix A - Q1018 SGS South San Francisco.pdf', 'C7PhI Appendix A - Q1019 Shingle Springs - Grid Stabilization Project.pdf', 'C7PhI Appendix A - Q1020 Sky Valley Wind.pdf', 'C7PhI Appendix A - Q1021 Stockton Biomass.pdf', 'Q1021_StocktonBiomass_PHI_Report-Addendum1.pdf', 'C7PhI Appendix A - Q1023 Umbarger Evergreen Energy Storage.pdf', 'C7PhI Appendix A - Q1024 Victor Grid Stabilization Project.pdf', 'C7PhI Appendix A - Q1026 Weber Grid Stabilization Project.pdf', 'C7PhI - Appendix A -  Q1027 Blackbriar.pdf', '14AS868993-C7PhI_Appendix_A__Q1028_Little_Bear_Solar_1__Revision_1_27Jan2015.pdf', '14AS869615-C7PhI_Appendix_A__Q1029_Little_Bear_Solar_2__Revision_1_27Jan2015.pdf', 'C7PhI - Appendix A - Q1030   South Lake Solar.pdf', 'C7PhI - Appendix A - Q1031   Stonecrop.pdf', 'C7PhI - Appendix A - Q1032 RE Tranquillity 8.pdf', 'C7PhI Appendix A - Q1033 Broken Spoke Final.pdf', 'C7PhI Appendix A - Q1035 Magellan Solar Final.pdf', 'C7PhI - Appendix A - Q1036 RE Mustang Two.pdf', 'C7PhI Appendix A - Q1037 New HECA_Final.pdf', 'C7PhI Appendix A - Q1038 Voyager Solar_Final.pdf', 'QC7PhI_Q1040_Arlington Valley Solar Energy 2_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1043_Canasta_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1045_Chula Vista Reliablity Appendix A_12-17-2014.pdf', 'QC7PhI_Q1046_ECO Energy Storage Center_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1047_El Cajon Reliability_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1048_Escondido Reliability_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1049_Hawkwind_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1050_Honey Badger_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1051_Kearny Hybrid Power_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1052_Mesa Rojo_Appendix A_12-17-2014.pdf', 'Revised QC7PhI_Q1053_Mesquite Solar 3_Appendix A_02-04-2015.pdf', 'QC7PhI_Q1055_NCC Hybrid Power_Appendix A_12-17-2014.pdf', 'Revised QC7PhI_Q1056_NCI Hybrid Power_Appendix A_02-04-2015.pdf', 'QC7PhI_Q1057_Otay Mesa Energy Center AGP_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1058_Pio Pico Energy Center 2_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1059_Sea Lion Energy Storage_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1060_SLW_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1061_Vista Energy Storage_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1062_White Wing Ranch Solar _Appendix A_12-17-2014.pdf', 'QC7PhI_Q1063_Wiley Road_Appendix A_12-17-2014.pdf']\n",
      "\n",
      "List of Skipped PDFs:\n",
      "['QC7P1-VEA-EOP-Q1064-ARES-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1064-ARES-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1064-ARES-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-VEA-EOP-Q1064-ARES-Appendix A-Attachment 1.pdf', 'QC7P1-SCE-VEA-EOP-Q1064-ARES-Appendix A.pdf', 'QC7P1-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2_Addendum01.pdf', 'QC7P1-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A.pdf', 'QC7P1-SCE-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A_Addendum01.pdf', 'QC7P1-SCE-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 1_Addendum01.pdf', 'QC7P1-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 1.pdf', 'QC7P1-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A.pdf', 'QC7P1-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A_Addendum01.pdf', 'QC7P1-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 1_Addendum01.pdf', 'QC7P1-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 2_Addendum01.pdf', 'QC7P1-VEA-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-VEA-EOP-Q1067-Spring Mountain Solar Project-Appendix A_Addendum01.pdf', 'QC7P1-SCE-VEA-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 1.pdf', 'QC7P1-SCE-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-VEA-EOP-Q1067-Spring Mountain Solar-Appendix A.pdf', 'QC7P1-SCE-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 2.pdf', 'QC7P1-VEA-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-VEA-EOP-Q1068-Sunshine Valley Solar 3 -Appendix A_Addendum01.pdf', 'QC7P1-SCE-VEA-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 1.pdf', 'QC7P1-SCE-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-VEA-EOP-Q1068-Sunshine Valley Solar 3-Appendix A.pdf', 'QC7P1-SCE-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-VEA-EOP-Q1069-Boulder Solar-Appendix A-Attachment 1.pdf', 'QC7P1-VEA-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2_Addendum01.pdf', 'QC7PI-SCE-VEA-EOP-Q1069-Boulder Solar -Appendix A_Addendum01.pdf', 'QC7P1-SCE-VEA-EOP-Q1069-Boulder Solar-Appendix A.pdf', 'Q1070-Appendix A_Attachment 1-Interconnection Network Facilities.pdf', 'Q1070-Appendix A_Attachment 2-SCD Cost Allocation.pdf', 'Q1070-Appendix A-Jupiter Solar.pdf', 'Q1070-Appendix A_Attachment 2-Escalated Cost.pdf', 'Q1071-Appendix A_Attachment 1-Interconnection Network Facilities.pdf', 'Q1071-Appendix A_Attachment 2-SCD Cost Allocation.pdf', 'Q1071-Appendix A_Attachment 2-Escalated Cost.pdf', 'Q1071-Appendix A-Quartz 4 Solar.pdf', 'Q1072-Appendix A-Vermillion.pdf', 'Q1072-Appendix A_Attachment 1-Interconnection Network Facilities.pdf', 'Q1072-Appendix A_Attachment 2-Escalated Cost.pdf', 'Q1072-Appendix A_Attachment 2-SCD Cost Allocation.pdf', 'QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A.pdf', 'QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A.pdf', 'QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A Addendum.pdf', 'QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A.pdf', 'QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A.pdf', 'QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A.pdf', 'QC7PI-SCE-Metro-Q1080-Birch Street Generation Facility-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1080 Birch Stret Generation Facility-Revised Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1080-Birch Street Generation Facility-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1080-Birch Street Generation Facility-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1080 Birch Street Generation Facility-Revised Appendix A.pdf', 'QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A.pdf', 'QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A.pdf', 'QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A.pdf', 'QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A.pdf']\n",
      "\n",
      "List of Addendum PDFs:\n",
      "['Q1021_StocktonBiomass_PHI_Report-Addendum1.pdf', 'QC7P1-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2_Addendum01.pdf', 'QC7PI-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A_Addendum01.pdf', 'QC7P1-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 1_Addendum01.pdf', 'QC7PI-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A_Addendum01.pdf', 'QC7P1-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 1_Addendum01.pdf', 'QC7P1-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 2_Addendum01.pdf', 'QC7PI-SCE-VEA-EOP-Q1067-Spring Mountain Solar Project-Appendix A_Addendum01.pdf', 'QC7PI-SCE-VEA-EOP-Q1068-Sunshine Valley Solar 3 -Appendix A_Addendum01.pdf', 'QC7PI-SCE-VEA-EOP-Q1069-Boulder Solar -Appendix A_Addendum01.pdf', 'QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A Addendum.pdf']\n",
      "\n",
      "List of Original PDFs:\n",
      "['C7PhI Appendix A - Q1007 Coyote Creek Energy Storage.pdf', 'C7PhI Appendix A - Q1010 Dyer Summit Wind Repower.pdf', 'C7PhI Appendix A - Q1011 GHS Project.pdf', 'C7PhI Appendix A - Q1013 Lake Clementine Hydroelectric.pdf', 'C7PhI Appendix A - Q1014 Paradise Cut.pdf', 'C7PhI Appendix A - Q1015 Pleasant Grove - Grid Stabilization Project.pdf', 'C7PhI Appendix A - Q1016 Richmond Steam Turbine.pdf', 'C7PhI Appendix A - Q1018 SGS South San Francisco.pdf', 'C7PhI Appendix A - Q1019 Shingle Springs - Grid Stabilization Project.pdf', 'C7PhI Appendix A - Q1020 Sky Valley Wind.pdf', 'C7PhI Appendix A - Q1021 Stockton Biomass.pdf', 'C7PhI Appendix A - Q1023 Umbarger Evergreen Energy Storage.pdf', 'C7PhI Appendix A - Q1024 Victor Grid Stabilization Project.pdf', 'C7PhI Appendix A - Q1026 Weber Grid Stabilization Project.pdf', 'C7PhI - Appendix A -  Q1027 Blackbriar.pdf', '14AS868993-C7PhI_Appendix_A__Q1028_Little_Bear_Solar_1__Revision_1_27Jan2015.pdf', '14AS869615-C7PhI_Appendix_A__Q1029_Little_Bear_Solar_2__Revision_1_27Jan2015.pdf', 'C7PhI - Appendix A - Q1030   South Lake Solar.pdf', 'C7PhI - Appendix A - Q1031   Stonecrop.pdf', 'C7PhI - Appendix A - Q1032 RE Tranquillity 8.pdf', 'C7PhI Appendix A - Q1033 Broken Spoke Final.pdf', 'C7PhI Appendix A - Q1035 Magellan Solar Final.pdf', 'C7PhI - Appendix A - Q1036 RE Mustang Two.pdf', 'C7PhI Appendix A - Q1037 New HECA_Final.pdf', 'C7PhI Appendix A - Q1038 Voyager Solar_Final.pdf', 'QC7PhI_Q1040_Arlington Valley Solar Energy 2_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1043_Canasta_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1045_Chula Vista Reliablity Appendix A_12-17-2014.pdf', 'QC7PhI_Q1046_ECO Energy Storage Center_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1047_El Cajon Reliability_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1048_Escondido Reliability_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1049_Hawkwind_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1050_Honey Badger_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1051_Kearny Hybrid Power_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1052_Mesa Rojo_Appendix A_12-17-2014.pdf', 'Revised QC7PhI_Q1053_Mesquite Solar 3_Appendix A_02-04-2015.pdf', 'QC7PhI_Q1055_NCC Hybrid Power_Appendix A_12-17-2014.pdf', 'Revised QC7PhI_Q1056_NCI Hybrid Power_Appendix A_02-04-2015.pdf', 'QC7PhI_Q1057_Otay Mesa Energy Center AGP_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1058_Pio Pico Energy Center 2_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1059_Sea Lion Energy Storage_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1060_SLW_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1061_Vista Energy Storage_Appendix A_12-17-2014.pdf', 'QC7PhI_Q1062_White Wing Ranch Solar _Appendix A_12-17-2014.pdf', 'QC7PhI_Q1063_Wiley Road_Appendix A_12-17-2014.pdf', 'QC7P1-VEA-EOP-Q1064-ARES-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1064-ARES-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1064-ARES-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-VEA-EOP-Q1064-ARES-Appendix A-Attachment 1.pdf', 'QC7P1-SCE-VEA-EOP-Q1064-ARES-Appendix A.pdf', 'QC7P1-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A.pdf', 'QC7P1-SCE-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-VEA-EOP-Q1065-Caballo Loco-Appendix A-Attachment 1.pdf', 'QC7P1-SCE-EOP-Q1065-Caballo Loco-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 1.pdf', 'QC7P1-SCE-VEA-EOP-Q1066-Roadrunner-Appendix A.pdf', 'QC7P1-VEA-EOP-Q1066-Roadrunner-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1066-Roadrunner-Appendix A-Attachment 2.pdf', 'QC7P1-VEA-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-VEA-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 1.pdf', 'QC7P1-SCE-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-VEA-EOP-Q1067-Spring Mountain Solar-Appendix A.pdf', 'QC7P1-SCE-EOP-Q1067-Spring Mountain Solar-Appendix A-Attachment 2.pdf', 'QC7P1-VEA-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-VEA-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 1.pdf', 'QC7P1-SCE-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1068-Sunshine Valley Solar 3-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-VEA-EOP-Q1068-Sunshine Valley Solar 3-Appendix A.pdf', 'QC7P1-SCE-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2-SCD.pdf', 'QC7P1-SCE-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-VEA-EOP-Q1069-Boulder Solar-Appendix A-Attachment 1.pdf', 'QC7P1-VEA-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2.pdf', 'QC7P1-SCE-EOP-Q1069-Boulder Solar-Appendix A-Attachment 2_Addendum01.pdf', 'QC7P1-SCE-VEA-EOP-Q1069-Boulder Solar-Appendix A.pdf', 'Q1070-Appendix A_Attachment 1-Interconnection Network Facilities.pdf', 'Q1070-Appendix A_Attachment 2-SCD Cost Allocation.pdf', 'Q1070-Appendix A-Jupiter Solar.pdf', 'Q1070-Appendix A_Attachment 2-Escalated Cost.pdf', 'Q1071-Appendix A_Attachment 1-Interconnection Network Facilities.pdf', 'Q1071-Appendix A_Attachment 2-SCD Cost Allocation.pdf', 'Q1071-Appendix A_Attachment 2-Escalated Cost.pdf', 'Q1071-Appendix A-Quartz 4 Solar.pdf', 'Q1072-Appendix A-Vermillion.pdf', 'Q1072-Appendix A_Attachment 1-Interconnection Network Facilities.pdf', 'Q1072-Appendix A_Attachment 2-Escalated Cost.pdf', 'Q1072-Appendix A_Attachment 2-SCD Cost Allocation.pdf', 'QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Northern-Q1073 Mission Rock Energy Center-Appendix A.pdf', 'QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A.pdf', 'QC7PI-SCE-Northern-Q1074 RE Garland 2-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A.pdf', 'QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Northern-Q1075 Rose Meadow Wind Bank-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Northern-Q1076 Willow Spring Solar 3-Appendix A.pdf', 'QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1077-Alamitos Energy Storage-Appendix A.pdf', 'QC7PI-SCE-Metro-Q1080-Birch Street Generation Facility-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1080 Birch Stret Generation Facility-Revised Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1080-Birch Street Generation Facility-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1080-Birch Street Generation Facility-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1080 Birch Street Generation Facility-Revised Appendix A.pdf', 'QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1083 Grand Ave Geeneration Facility-Appendix A.pdf', 'QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A.pdf', 'QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1084 Johanna Energy Center-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A.pdf', 'QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1087 Santa Ana Energy Center-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A-Attachment 1.pdf', 'QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A-Attachment 2.pdf', 'QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A-Attachment 2 SCD.pdf', 'QC7PI-SCE-Metro-Q1088 Viejo Hybrid Power-Appendix A.pdf']\n",
      "\n",
      "Number of Original PDFs Scraped: 45\n",
      "Number of Addendum PDFs Scraped: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import PyPDF2\n",
    "import traceback\n",
    "\n",
    "# Define paths and project range\n",
    "BASE_DIRECTORY = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/03_data\"\n",
    "OUTPUT_CSV_PATH_ORIGINAL = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/03_raw/rawdata_cluster7_style_G_originals.csv\"\n",
    "OUTPUT_CSV_PATH_ADDENDUM = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/03_raw/rawdata_cluster7_style_G_addendums.csv\"\n",
    "LOG_FILE_PATH = \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/03_raw/scraping_cluster7_style_G_log.txt\"\n",
    "PROJECT_RANGE = range(1007, 1089)  # Example range for q_ids in Clusters 6, 7, 8\n",
    "\n",
    "# Initialize DataFrames\n",
    "core_originals = pd.DataFrame()\n",
    "core_addendums = pd.DataFrame()\n",
    "\n",
    "# Initialize tracking variables\n",
    "scraped_projects = set()\n",
    "skipped_projects = set()\n",
    "missing_projects = set()\n",
    "scraped_pdfs = []\n",
    "skipped_pdfs = []\n",
    "addendum_pdfs = []\n",
    "original_pdfs = []\n",
    "total_pdfs_accessed = 0\n",
    "total_pdfs_scraped = 0\n",
    "total_pdfs_skipped = 0\n",
    "\n",
    "def clean_column_headers(headers):\n",
    "    \"\"\"Cleans column headers by normalizing and removing unwanted characters.\"\"\"\n",
    "    cleaned_headers = []\n",
    "    for header in headers:\n",
    "        if header is None:\n",
    "            header = \"\"\n",
    "        elif isinstance(header, str):\n",
    "            header = header.lower()\n",
    "            header = re.sub(r'\\s+', ' ', header)\n",
    "            header = re.sub(r'\\(.*?\\)', '', header)\n",
    "            header = re.sub(r'[^a-zA-Z0-9\\s]', '', header)\n",
    "            header = header.strip()\n",
    "        cleaned_headers.append(header)\n",
    "    return cleaned_headers\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    \"\"\"Cleans string cells by removing newlines and trimming spaces.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "def contains_phrase(row, phrase):\n",
    "    \"\"\"Checks if any cell in a row contains a specific phrase.\"\"\"\n",
    "    regex_pattern = re.sub(r\"\\s+\", r\"\\\\s*\", phrase)\n",
    "    pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "    return row.astype(str).apply(lambda cell: bool(pattern.search(cell))).any()\n",
    "\n",
    "def extract_specific_phrase(title):\n",
    "    \"\"\"\n",
    "    Extracts a specific phrase from the table title based on predefined keywords.\n",
    "\n",
    "    Args:\n",
    "        title (str): The table title string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted specific phrase if found, else the original title.\n",
    "    \"\"\"\n",
    "    phrases = [\n",
    "        \"PTO\",\n",
    "        \"Reliability Network Upgrade\",\n",
    "        \"Area Delivery Network Upgrade\",\n",
    "        \"Local Delivery Network\",\n",
    "        \"ADNU\",\n",
    "        \"LDNU\",\n",
    "        \"RNU\"\n",
    "    ]\n",
    "\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{re.escape(phrase)}\\b\", title, re.IGNORECASE):\n",
    "            return phrase\n",
    "    return title  # Fallback to the entire title if no specific phrase is found\n",
    "\n",
    "def reorder_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to reorder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The reordered DataFrame.\n",
    "    \"\"\"\n",
    "    desired_order = [\n",
    "        \"q_id\",\n",
    "        \"cluster\",\n",
    "        \"req_deliverability\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"capacity\",\n",
    "        \"point_of_interconnection\",\n",
    "        \"type of upgrade\",\n",
    "        \"upgrade\",\n",
    "        \"description\",\n",
    "        \"cost allocation factor\"\n",
    "    ]\n",
    "\n",
    "    # Start with desired columns that exist in the DataFrame\n",
    "    existing_desired = [col for col in desired_order if col in df.columns]\n",
    "\n",
    "    # Then add the remaining columns\n",
    "    remaining = [col for col in df.columns if col not in existing_desired]\n",
    "\n",
    "    # Combine the two lists\n",
    "    new_order = existing_desired + remaining\n",
    "\n",
    "    # Reorder the DataFrame\n",
    "    df = df[new_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "def search_gps_coordinates(text, log_file):\n",
    "    \"\"\"Search for GPS coordinates using multiple patterns.\"\"\"\n",
    "    gps_coords = re.search(r\"gps coordinates:\\s*([\\d\\.\\-]+),\\s*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if gps_coords:\n",
    "        print(f\"Found GPS coordinates: {gps_coords.groups()}\", file=log_file)\n",
    "        return gps_coords.groups()\n",
    "\n",
    "    project_coords = re.search(r\"latitude[:\\s]*([\\d\\.\\-]+)[^\\d]+longitude[:\\s]*([\\d\\.\\-]+)\", text, re.IGNORECASE)\n",
    "    if project_coords:\n",
    "        print(f\"Found project coordinates: {project_coords.groups()}\", file=log_file)\n",
    "        return project_coords.groups()\n",
    "\n",
    "    gps_coords_directional = re.search(\n",
    "        r\"gps coordinates:\\s*([\\d\\.\\-]+)\\s*[nNsS],\\s*([\\d\\.\\-]+)\\s*[eEwW]\", text, re.IGNORECASE)\n",
    "    if gps_coords_directional:\n",
    "        lat, lon = gps_coords_directional.groups()\n",
    "        latitude = lat if \"N\" in text.upper() else f\"-{lat}\"  # Adjust latitude sign\n",
    "        longitude = lon if \"E\" in text.upper() else f\"-{lon}\"  # Adjust longitude sign\n",
    "        print(f\"Found directional GPS coordinates: {(latitude, longitude)}\", file=log_file)\n",
    "        return (latitude, longitude)\n",
    "\n",
    "    print(\"GPS coordinates not found.\", file=log_file)\n",
    "    return (None, None)\n",
    "\n",
    "def extract_table1(pdf_path, log_file):\n",
    "    \"\"\"\n",
    "    Extracts the Point of Interconnection from Table 1 in the provided PDF.\n",
    "    Implements a retry mechanism with different table extraction settings if initial attempts fail.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted Point of Interconnection value,\n",
    "             \"Value Missing\" if label found but no value,\n",
    "             or None if not found.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 1 extraction...\", file=log_file)\n",
    "    point_of_interconnection = None\n",
    "\n",
    "    # Define the regex pattern for 'Point of Interconnection' (case-insensitive)\n",
    "    poi_pattern = re.compile(r\"Point\\s+of\\s+Interconnection\", re.IGNORECASE)\n",
    "\n",
    "    # Define different table extraction settings to try\n",
    "    table_settings_list = [\n",
    "        {\n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"horizontal_strategy\": \"lines\",\n",
    "            \"vertical_strategy\": \"lines\",\n",
    "            \"snap_tolerance\": 2,  # Increased tolerance for retry\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 1\"\n",
    "            table1_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"Table\\s*1\\b\", text, re.IGNORECASE):\n",
    "                    table1_pages.append(i)\n",
    "\n",
    "            if not table1_pages:\n",
    "                print(\"No Table 1 found in the PDF.\", file=log_file)\n",
    "                return None  # Return None if no Table 1 found\n",
    "\n",
    "            first_page = table1_pages[0]\n",
    "            last_page = table1_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1  # Plus one to include the next page if needed\n",
    "\n",
    "            print(f\"Table 1 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Flag to indicate if extraction was successful\n",
    "            extraction_successful = False\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1} for Table 1...\", file=log_file)\n",
    "\n",
    "                for attempt, table_settings in enumerate(table_settings_list, start=1):\n",
    "                    print(f\"\\nAttempt {attempt} with table settings: {table_settings}\", file=log_file)\n",
    "                    tables = page.find_tables(table_settings=table_settings)\n",
    "                    print(f\"Found {len(tables)} table(s) on page {page_number + 1} with current settings.\", file=log_file)\n",
    "\n",
    "                    for table_index, table in enumerate(tables, start=1):\n",
    "                        tab = table.extract()\n",
    "                        if not tab:\n",
    "                            print(f\"Table {table_index} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                            continue  # Skip empty tables\n",
    "\n",
    "                        print(f\"\\n--- Table {table_index} on Page {page_number + 1} ---\", file=log_file)\n",
    "                        for row_num, row in enumerate(tab, start=1):\n",
    "                            print(f\"Row {row_num}: {row}\", file=log_file)\n",
    "\n",
    "                        # Iterate through each row in the table\n",
    "                        for row_index, row in enumerate(tab, start=1):\n",
    "                            # Iterate through each cell in the row\n",
    "                            for cell_index, cell in enumerate(row, start=1):\n",
    "                                if cell and poi_pattern.search(cell):\n",
    "                                    # Assuming the next column contains the value\n",
    "                                    poi_col_index = cell_index  # 1-based index\n",
    "                                    adjacent_col_index = poi_col_index + 1  # Next column\n",
    "\n",
    "                                    if adjacent_col_index <= len(row):\n",
    "                                        poi_value = clean_string_cell(row[adjacent_col_index - 1])\n",
    "                                        if poi_value:  # Check if the value is not empty\n",
    "                                            point_of_interconnection = poi_value\n",
    "                                            print(f\"\\nFound Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index})\", file=log_file)\n",
    "                                            extraction_successful = True\n",
    "                                            break  # Exit the cell loop\n",
    "                                        else:\n",
    "                                            print(f\"\\nPoint of Interconnection label found but adjacent value is empty \"\n",
    "                                                  f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                            # Proceed to scan surrounding rows for the value\n",
    "                                            poi_value_parts = []\n",
    "\n",
    "                                            # Define the range to scan: two rows above and two rows below\n",
    "                                            # Convert to 0-based index\n",
    "                                            current_row_idx = row_index - 1\n",
    "                                            start_scan = max(0, current_row_idx - 2)\n",
    "                                            end_scan = min(len(tab), current_row_idx + 3)  # Exclusive\n",
    "\n",
    "                                            print(f\"Scanning rows {start_scan + 1} to {end_scan} for POI value parts.\", file=log_file)\n",
    "\n",
    "                                            for scan_row_index in range(start_scan, end_scan):\n",
    "                                                # Skip the current row where the label was found\n",
    "                                                if scan_row_index == current_row_idx:\n",
    "                                                    continue\n",
    "\n",
    "                                                scan_row = tab[scan_row_index]\n",
    "                                                # Ensure the adjacent column exists in the scan row\n",
    "                                                if adjacent_col_index - 1 < len(scan_row):\n",
    "                                                    scan_cell = clean_string_cell(scan_row[adjacent_col_index - 1])\n",
    "                                                    if scan_cell and not poi_pattern.search(scan_cell):\n",
    "                                                        poi_value_parts.append(scan_cell)\n",
    "                                                        print(f\"Found POI part in row {scan_row_index + 1}: '{scan_cell}'\", file=log_file)\n",
    "                                                    elif poi_pattern.search(scan_cell):\n",
    "                                                        # If another POI label is found, skip it\n",
    "                                                        print(f\"Encountered another POI label in row {scan_row_index + 1}. Skipping this row.\", file=log_file)\n",
    "                                                        continue\n",
    "\n",
    "                                            if poi_value_parts:\n",
    "                                                # Concatenate the parts to form the complete POI value\n",
    "                                                point_of_interconnection = \" \".join(poi_value_parts)\n",
    "                                                print(f\"\\nConcatenated Point of Interconnection: '{point_of_interconnection}' \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index})\", file=log_file)\n",
    "                                                extraction_successful = True\n",
    "                                                break  # Exit the cell loop\n",
    "                                            else:\n",
    "                                                print(f\"\\nNo POI value found in the surrounding rows \"\n",
    "                                                      f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                                # Do not return immediately; proceed to retry\n",
    "                                    else:\n",
    "                                        print(f\"\\nPoint of Interconnection label found but no adjacent column \"\n",
    "                                              f\"(Page {page_number + 1}, Table {table_index}, Row {row_index}).\", file=log_file)\n",
    "                                        # Do not return immediately; proceed to retry\n",
    "                            if extraction_successful:\n",
    "                                break  # Exit the row loop\n",
    "                        if extraction_successful:\n",
    "                            break  # Exit the table loop\n",
    "                    if extraction_successful:\n",
    "                        break  # Exit the attempt loop\n",
    "                if extraction_successful:\n",
    "                    break  # Exit the page loop\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 1 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return None\n",
    "\n",
    "    if not extraction_successful:\n",
    "        # After all attempts, determine the appropriate return value\n",
    "        if point_of_interconnection is not None and point_of_interconnection != \"\":\n",
    "            # Label was found but no value\n",
    "            print(\"Point of Interconnection label found but no adjacent value.\", file=log_file)\n",
    "            return \"Value Missing\"\n",
    "        else:\n",
    "            # Label not found\n",
    "            print(\"Point of Interconnection not found in Table 1.\", file=log_file)\n",
    "            return None\n",
    "\n",
    "    return point_of_interconnection\n",
    "\n",
    "def extract_base_data(pdf_path, project_id, log_file):\n",
    "    \"\"\"Extract base data from the PDF and return as a DataFrame.\"\"\"\n",
    "    print(\"Extracting base data from PDF...\", file=log_file)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        text = clean_string_cell(text)\n",
    "\n",
    "        queue_id = re.search(r\"q[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        queue_id = queue_id.group(1) if queue_id else str(project_id)  # Use project_id if queue_id is not found\n",
    "        print(f\"Extracted Queue ID: {queue_id}\", file=log_file)\n",
    "\n",
    "        cluster_number = re.search(r\"queue[\\s_-]*cluster[\\s_-]*(\\d+)\", text, re.IGNORECASE)\n",
    "        cluster_number = cluster_number.group(1) if cluster_number else None\n",
    "        print(f\"Extracted Cluster Number: {cluster_number}\", file=log_file)\n",
    "\n",
    "        deliverability_status = re.search(r\"(\\w+)\\s*capacity deliverability status\", text, re.IGNORECASE)\n",
    "        deliverability_status = deliverability_status.group(1) if deliverability_status else None\n",
    "        print(f\"Extracted Deliverability Status: {deliverability_status}\", file=log_file)\n",
    "\n",
    "        # Extract Capacity\n",
    "        capacity = re.search(r\"total rated output of (\\d+)\\s*mw\", text, re.IGNORECASE)\n",
    "        if capacity:\n",
    "            capacity = int(capacity.group(1))\n",
    "        else:\n",
    "            capacity2 = re.search(r\"(\\d+)\\s*mw\", text)\n",
    "            capacity = int(capacity2.group(1)) if capacity2 else None\n",
    "        print(f\"Extracted Capacity: {capacity}\", file=log_file)\n",
    "\n",
    "        # Extract Point of Interconnection\n",
    "        point_of_interconnection = extract_table1(pdf_path, log_file)\n",
    "\n",
    "        latitude, longitude = search_gps_coordinates(text, log_file)\n",
    "\n",
    "        # Initialize base data dictionary\n",
    "        base_data = {\n",
    "            \"q_id\": [queue_id],\n",
    "            \"cluster\": [cluster_number],\n",
    "            \"req_deliverability\": [deliverability_status],\n",
    "            \"latitude\": [latitude],\n",
    "            \"longitude\": [longitude],\n",
    "            \"capacity\": [capacity],\n",
    "            \"point_of_interconnection\": [point_of_interconnection]\n",
    "        }\n",
    "\n",
    "        print(\"Base data extracted:\", file=log_file)\n",
    "        print(base_data, file=log_file)\n",
    "        return pd.DataFrame(base_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base data from {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def extract_table7(pdf_path, log_file, is_addendum=False):\n",
    "    \"\"\"\n",
    "    Extracts Table 7 data from the provided PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        log_file (file object): Log file to write print statements.\n",
    "        is_addendum (bool): Whether the PDF is an addendum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted Table 7 data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {pdf_path} for Table 7 extraction...\", file=log_file)\n",
    "    extracted_tables = []\n",
    "    specific_phrase = None  # To store the specific phrase from the table title\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Identify all pages that contain \"Table 7\" or variations like \"Modification of Table 7\"\n",
    "            table7_pages = []\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or \"\"\n",
    "                # Use re.search to find \"Table 7\" anywhere in the text, allowing for prefixes\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*\", text, re.IGNORECASE):\n",
    "                    table7_pages.append(i)\n",
    "\n",
    "            if not table7_pages:\n",
    "                print(\"No Table 7 found in the PDF.\", file=log_file)\n",
    "                return pd.DataFrame()  # Return empty DataFrame if no Table 7 found\n",
    "\n",
    "            first_page = table7_pages[0]\n",
    "            last_page = table7_pages[-1]\n",
    "            scrape_start = first_page\n",
    "            scrape_end = last_page + 1  # Plus one to include the next page\n",
    "\n",
    "            print(f\"Table 7 starts on page {scrape_start + 1} and ends on page {scrape_end + 1}\", file=log_file)\n",
    "\n",
    "            # Iterate through the specified page range\n",
    "            for page_number in range(scrape_start, min(scrape_end + 1, len(pdf.pages))):\n",
    "                page = pdf.pages[page_number]\n",
    "                print(f\"\\nScraping tables on page {page_number + 1}...\", file=log_file)\n",
    "                tables = page.find_tables(table_settings={\n",
    "                    \"horizontal_strategy\": \"lines\",\n",
    "                    \"vertical_strategy\": \"lines\",\n",
    "                })\n",
    "\n",
    "                for table_index, table in enumerate(tables):\n",
    "                    tab = table.extract()\n",
    "                    if not tab:\n",
    "                        print(f\"Table {table_index + 1} on page {page_number + 1} is empty. Skipping.\", file=log_file)\n",
    "                        continue  # Skip empty tables\n",
    "\n",
    "                    # Get the bounding box of the current table\n",
    "                    table_bbox = table.bbox  # (x0, y0, x1, y1)\n",
    "\n",
    "                    # Define a bounding box from the top of the page to the top of the table\n",
    "                    # Coordinates: (x0, y0, x1, y1)\n",
    "                    title_bbox = (0, 0, page.width, table_bbox[1])\n",
    "\n",
    "                    # Extract text within the title bounding box\n",
    "                    title_text = page.within_bbox(title_bbox).extract_text() or \"\"\n",
    "\n",
    "                    # Initialize table_title as None\n",
    "                    table_title = None\n",
    "\n",
    "                    if title_text:\n",
    "                        # Split the title text into lines and reverse to start checking from the closest line to the table\n",
    "                        title_lines = title_text.split('\\n')[::-1]\n",
    "\n",
    "                        for line in title_lines:\n",
    "                            line = line.strip()\n",
    "                            # Use re.search instead of re.match to find \"Table 7\" anywhere in the line\n",
    "                            match = re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*[:\\-\\s]*(.*)\", line, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                # Capture the specific phrase after \"Table 7\" and possible number\n",
    "                                table_title = match.group(2).strip()\n",
    "                                break  # Stop after finding the closest table title\n",
    "\n",
    "                    if table_title:\n",
    "                        # New Table 7 detected\n",
    "                        specific_phrase = extract_specific_phrase(table_title)  # Extracted phrase from the title\n",
    "                        print(f\"New Table 7 detected: '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        # Extract headers and data rows\n",
    "                        headers = clean_column_headers(tab[0])\n",
    "                        data_rows = tab[1:]\n",
    "\n",
    "                        # Create DataFrame for the new table\n",
    "                        try:\n",
    "                            df_new = pd.DataFrame(data_rows, columns=headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for new table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue  # Skip this table due to error\n",
    "\n",
    "                        # Special Handling for \"Area Delivery Network Upgrade\" Tables\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Detected 'Area Delivery Network Upgrade' table.\", file=log_file)\n",
    "                            if \"adnu\" in df_new.columns:\n",
    "                                # Rename 'adnu' to 'upgrade'\n",
    "                                if \"upgrade\" in df_new.columns:\n",
    "                                    # Avoid duplicate 'upgrade' columns\n",
    "                                    df_new.drop(columns=['adnu'], inplace=True)\n",
    "                                    print(\"Dropped 'ADNU' column to avoid duplicate 'upgrade' columns.\", file=log_file)\n",
    "                                else:\n",
    "                                    df_new.rename(columns={'adnu': 'upgrade'}, inplace=True)\n",
    "                                    print(\"Renamed 'ADNU' to 'upgrade'.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in df_new.columns:\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(\"Added 'type of upgrade' column with specific phrase.\", file=log_file)\n",
    "                        else:\n",
    "                            # General Handling for other tables\n",
    "                            if \"type of upgrade\" in df_new.columns:\n",
    "                                first_row = df_new.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_new.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                df_new[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'Type of Upgrade' column added with value '{specific_phrase}' on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_new.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in new table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\", file=log_file)\n",
    "                            df_new = df_new.loc[:, ~df_new.columns.duplicated()]\n",
    "\n",
    "                        # Append the new table to the list\n",
    "                        extracted_tables.append(df_new)\n",
    "                    else:\n",
    "                        # Continuation Table detected\n",
    "                        if specific_phrase is None:\n",
    "                            print(f\"No previous Table 7 title found for continuation on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue  # Skip if no previous title to refer to\n",
    "\n",
    "                        print(f\"Continuation Table detected on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        # Treat all rows as data without headers\n",
    "                        data_rows = tab\n",
    "\n",
    "                        # Check if the number of columns matches\n",
    "                        expected_columns = len(extracted_tables[-1].columns) if extracted_tables else None\n",
    "                        if expected_columns is None:\n",
    "                            print(f\"No existing table to continue with on page {page_number + 1}, table {table_index + 1}. Skipping.\", file=log_file)\n",
    "                            continue  # No table to continue with\n",
    "\n",
    "                        # Define expected columns based on the last extracted table\n",
    "                        expected_headers = extracted_tables[-1].columns.tolist()\n",
    "\n",
    "                        # Define the list of column headers to check in continuation tables\n",
    "                        header_keywords = [\"type of upgrade\", \"upgrade\", \"adnu\"]\n",
    "\n",
    "                        # Check if the first row contains any header keywords\n",
    "                        first_continuation_row = data_rows[0] if len(data_rows) > 0 else []\n",
    "                        is_header_row = any(\n",
    "                            re.search(rf\"\\b{kw}\\b\", str(cell), re.IGNORECASE) for kw in header_keywords for cell in first_continuation_row\n",
    "                        )\n",
    "\n",
    "                        if is_header_row:\n",
    "                            print(f\"Detected header row in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping the header row.\", file=log_file)\n",
    "                            data_rows = data_rows[1:]  # Drop the header row\n",
    "\n",
    "                        # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            print(\"Handling continuation for 'Area Delivery Network Upgrade' table.\", file=log_file)\n",
    "                            if \"adnu\" in data_rows[0]:\n",
    "                                # Rename 'ADNU' to 'upgrade' in continuation table.\n",
    "                                print(\"Renaming 'ADNU' to 'upgrade' in continuation table.\", file=log_file)\n",
    "                                data_rows = [[\"upgrade\"] + row[1:] for row in data_rows]\n",
    "                            if \"upgrade\" not in expected_headers:\n",
    "                                # Insert 'upgrade' column at the beginning\n",
    "                                print(\"Inserting 'upgrade' column with specific phrase in continuation table.\", file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                        else:\n",
    "                            # General Handling for other tables\n",
    "                            if \"type of upgrade\" in expected_headers:\n",
    "                                # Ensure the first data row has the specific phrase\n",
    "                                print(\"Ensuring 'type of upgrade' column has the specific phrase in continuation table.\", file=log_file)\n",
    "                            if \"type of upgrade\" not in expected_headers:\n",
    "                                # Insert 'Type of Upgrade' column with specific phrase\n",
    "                                print(\"Inserting 'Type of Upgrade' column with specific phrase in continuation table.\", file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "\n",
    "                        # Handle missing or extra columns\n",
    "                        if len(data_rows[0]) < expected_columns:\n",
    "                            if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                                # For ADNU tables, assume missing \"upgrade\" column\n",
    "                                print(f\"Detected missing 'upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'upgrade' column.\", file=log_file)\n",
    "                                data_rows = [[specific_phrase] + row for row in data_rows]\n",
    "                            else:\n",
    "                                # For other tables, assume missing \"Type of Upgrade\" column\n",
    "                                print(f\"Detected missing 'Type of Upgrade' column in continuation table on page {page_number + 1}, table {table_index + 1}. Inserting 'Type of Upgrade' column.\", file=log_file)\n",
    "                                data_rows = [[\"Type of Upgrade\"] + row for row in data_rows]\n",
    "                        elif len(data_rows[0]) > expected_columns:\n",
    "                            # Extra columns detected; adjust accordingly\n",
    "                            print(f\"Detected extra columns in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping extra columns.\", file=log_file)\n",
    "                            data_rows = [row[:expected_columns] for row in data_rows]\n",
    "\n",
    "                        # Create DataFrame for the continuation table\n",
    "                        try:\n",
    "                            df_continuation = pd.DataFrame(data_rows, columns=expected_headers)\n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error creating DataFrame for continuation table on page {page_number + 1}, table {table_index + 1}: {ve}\", file=log_file)\n",
    "                            continue  # Skip this table due to error\n",
    "\n",
    "                        # Special Handling for \"Area Delivery Network Upgrade\" Tables in Continuation\n",
    "                        if re.search(r\"Area\\s*Delivery\\s*Network\\s*Upgrade\", specific_phrase, re.IGNORECASE):\n",
    "                            if \"upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"upgrade\"]) or first_row[\"upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"upgrade\"] = specific_phrase\n",
    "                                print(f\"'upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                        else:\n",
    "                            # General Handling for other tables\n",
    "                            if \"type of upgrade\" in df_continuation.columns:\n",
    "                                first_row = df_continuation.iloc[0]\n",
    "                                if pd.isna(first_row[\"type of upgrade\"]) or first_row[\"type of upgrade\"] == \"\":\n",
    "                                    print(f\"Replacing 'None' in 'Type of Upgrade' for the first data row of continuation on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "                                    df_continuation.at[0, \"type of upgrade\"] = specific_phrase\n",
    "                            else:\n",
    "                                # If \"Type of Upgrade\" column does not exist, add it\n",
    "                                df_continuation[\"type of upgrade\"] = specific_phrase\n",
    "                                print(f\"'Type of Upgrade' column added with value '{specific_phrase}' for continuation on page {page_number + 1}, table {table_index + 1}\", file=log_file)\n",
    "\n",
    "                        # Ensure no duplicate columns\n",
    "                        if df_continuation.columns.duplicated().any():\n",
    "                            print(f\"Duplicate columns detected in continuation table on page {page_number + 1}, table {table_index + 1}. Dropping duplicates.\", file=log_file)\n",
    "                            df_continuation = df_continuation.loc[:, ~df_continuation.columns.duplicated()]\n",
    "\n",
    "                        # Merge with the last extracted table\n",
    "                        extracted_tables[-1] = pd.concat([extracted_tables[-1], df_continuation], ignore_index=True, sort=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Table 7 in {pdf_path}: {e}\", file=log_file)\n",
    "        print(traceback.format_exc(), file=log_file)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # After scraping all tables, concatenate them\n",
    "    if extracted_tables:\n",
    "        # To prevent reindexing errors, ensure all DataFrames have unique and consistent columns\n",
    "        # Collect all unique column names\n",
    "        all_columns = set()\n",
    "        for df in extracted_tables:\n",
    "            all_columns.update(df.columns.tolist())\n",
    "        #all_columns = sorted(all_columns)  # Optional: sort columns for consistency\n",
    "\n",
    "        standardized_tables = []\n",
    "        for df in extracted_tables:\n",
    "            # Reindex to have all columns, filling missing ones with NaN\n",
    "            standardized_df = df.reindex(columns=all_columns)\n",
    "            standardized_tables.append(standardized_df)\n",
    "\n",
    "        print(\"\\nConcatenating all extracted Table 7 data...\", file=log_file)\n",
    "        try:\n",
    "            table7_data = pd.concat(standardized_tables, ignore_index=True, sort=False)\n",
    "            print(f\"Successfully concatenated {len(standardized_tables)} tables.\", file=log_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating tables: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            table7_data = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No Table 7 data extracted.\", file=log_file)\n",
    "        table7_data = pd.DataFrame()\n",
    "\n",
    "    return table7_data\n",
    "\n",
    "def extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=False):\n",
    "    \"\"\"Extracts Table 7 data and merges with base data.\"\"\"\n",
    "    base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "    table7_data = extract_table7(pdf_path, log_file, is_addendum)\n",
    "\n",
    "    if table7_data.empty:\n",
    "        return base_data\n",
    "    else:\n",
    "        # Identify overlapping columns excluding 'point_of_interconnection'\n",
    "        overlapping_columns = base_data.columns.intersection(table7_data.columns).difference(['point_of_interconnection'])\n",
    "        table7_data = table7_data.drop(columns=overlapping_columns, errors='ignore')\n",
    "        \n",
    "        # Repeat base data for each row in table7_data\n",
    "        base_data_repeated = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Concatenate base data with Table 7 data along columns\n",
    "            merged_df = pd.concat([base_data_repeated, table7_data], axis=1, sort=False)\n",
    "            \n",
    "            # Ensure 'point_of_interconnection' is present and correctly populated\n",
    "            if 'point_of_interconnection' not in merged_df.columns:\n",
    "                merged_df['point_of_interconnection'] = base_data['point_of_interconnection'].iloc[0]\n",
    "                print(f\"Added 'point_of_interconnection' to merged data for {pdf_path}.\", file=log_file)\n",
    "            \n",
    "            print(f\"Merged base data with Table 7 data for {pdf_path}.\", file=log_file)\n",
    "            return merged_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging base data with Table 7 data for {pdf_path}: {e}\", file=log_file)\n",
    "            print(traceback.format_exc(), file=log_file)\n",
    "            return base_data  # Fallback to base data only\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def check_has_table7(pdf_path):\n",
    "    \"\"\"Checks if the PDF contains Table 7.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                if re.search(r\"(Modification\\s+of\\s+)?Table\\s*7[-.]?\\d*\", text, re.IGNORECASE):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_addendum(pdf_path):\n",
    "    \"\"\"Checks if the PDF is an addendum by searching 'Addendum' on the first page.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            if len(pdf.pages) == 0:\n",
    "                return False\n",
    "            first_page = pdf.pages[0]\n",
    "            text = first_page.extract_text() or \"\"\n",
    "            return \"Addendum\" in text\n",
    "    except Exception as e:\n",
    "        # Handle potential errors when opening PDF\n",
    "        return False\n",
    "\n",
    "def process_pdfs_in_folder():\n",
    "    \"\"\"Processes all PDFs in the specified project range and directory.\"\"\"\n",
    "    global core_originals, core_addendums, total_pdfs_accessed, total_pdfs_scraped, total_pdfs_skipped\n",
    "\n",
    "    # Ensure the log file directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
    "\n",
    "    with open(LOG_FILE_PATH, 'w') as log_file:\n",
    "        for project_id in PROJECT_RANGE:\n",
    "            project_path = os.path.join(BASE_DIRECTORY, str(project_id), \"02_phase_1_study\")\n",
    "            if not os.path.exists(project_path):\n",
    "                missing_projects.add(project_id)\n",
    "                print(f\"Project path does not exist: {project_path}\", file=log_file)\n",
    "                continue\n",
    "\n",
    "            project_scraped = False  # Flag to track if any PDF in the project was scraped\n",
    "            base_data_extracted = False\n",
    "            base_data = pd.DataFrame()\n",
    "\n",
    "            for pdf_name in os.listdir(project_path):\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(project_path, pdf_name)\n",
    "                    total_pdfs_accessed += 1\n",
    "                    is_add = is_addendum(pdf_path)\n",
    "\n",
    "                    # Determine output DataFrame and CSV path based on addendum status\n",
    "                    if is_add:\n",
    "                        addendum_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Addendum PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "                    else:\n",
    "                        original_pdfs.append(pdf_name)\n",
    "                        print(f\"Accessing Original PDF: {pdf_name} from Project {project_id}\", file=log_file)\n",
    "\n",
    "                    try:\n",
    "                        has_table7 = check_has_table7(pdf_path)\n",
    "                        if not has_table7:\n",
    "                            skipped_pdfs.append(pdf_name)\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\", file=log_file)\n",
    "                            # Print to ipynb output\n",
    "                            print(f\"Skipped PDF: {pdf_name} from Project {project_id} (No Table 7)\")\n",
    "                            total_pdfs_skipped += 1\n",
    "                            continue\n",
    "\n",
    "                        if not is_add and not base_data_extracted:\n",
    "                            # Extract base data from original PDF\n",
    "                            base_data = extract_base_data(pdf_path, project_id, log_file)\n",
    "                            base_data_extracted = True\n",
    "                            print(f\"Extracted base data from original PDF: {pdf_name}\", file=log_file)\n",
    "\n",
    "                        if is_add and base_data_extracted:\n",
    "                            # For addendums, use the extracted base data\n",
    "                            table7_data = extract_table7(pdf_path, log_file, is_addendum=is_add)\n",
    "                            if not table7_data.empty:\n",
    "                                # Merge base data with Table 7 data\n",
    "                                merged_df = pd.concat([base_data] * len(table7_data), ignore_index=True)\n",
    "                                merged_df = pd.concat([merged_df, table7_data], axis=1, sort=False)\n",
    "                                core_addendums = pd.concat([core_addendums, merged_df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped Addendum PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                # Optionally, print to ipynb\n",
    "                                print(f\"Skipped Addendum PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "                        else:\n",
    "                            # For originals, extract Table 7 data\n",
    "                            df = extract_table7_and_replace_none(pdf_path, project_id, log_file, is_addendum=is_add)\n",
    "                            if not df.empty:\n",
    "                                if is_add:\n",
    "                                    core_addendums = pd.concat([core_addendums, df], ignore_index=True)\n",
    "                                else:\n",
    "                                    core_originals = pd.concat([core_originals, df], ignore_index=True)\n",
    "                                scraped_pdfs.append(pdf_name)\n",
    "                                scraped_projects.add(project_id)\n",
    "                                project_scraped = True\n",
    "                                total_pdfs_scraped += 1\n",
    "                                print(f\"Scraped PDF: {pdf_name} from Project {project_id}\")\n",
    "                            else:\n",
    "                                skipped_pdfs.append(pdf_name)\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\", file=log_file)\n",
    "                                # Optionally, print to ipynb\n",
    "                                print(f\"Skipped PDF: {pdf_name} from Project {project_id} (Empty Data)\")\n",
    "                                total_pdfs_skipped += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        skipped_pdfs.append(pdf_name)\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\", file=log_file)\n",
    "                        print(traceback.format_exc(), file=log_file)\n",
    "                        # Optionally, print to ipynb\n",
    "                        print(f\"Skipped PDF: {pdf_name} from Project {project_id} due to an error: {e}\")\n",
    "                        total_pdfs_skipped += 1\n",
    "\n",
    "            # After processing all PDFs for this project, check if any PDF was scraped\n",
    "            if not project_scraped and os.path.exists(project_path):\n",
    "                skipped_projects.add(project_id)\n",
    "\n",
    "\n",
    "    # After processing all PDFs, save to CSV\n",
    "    save_to_csv(core_originals, OUTPUT_CSV_PATH_ORIGINAL, \"originals\")\n",
    "    save_to_csv(core_addendums, OUTPUT_CSV_PATH_ADDENDUM, \"addendums\")\n",
    "\n",
    "    # Calculate total projects processed\n",
    "    total_projects_processed = len(scraped_projects) + len(skipped_projects)\n",
    "\n",
    "    # Print summary to ipynb\n",
    "    print(\"\\n=== Scraping Summary ===\")\n",
    "    print(f\"Total Projects Processed: {total_projects_processed}\")\n",
    "    print(f\"Total Projects Scraped: {len(scraped_projects)}\")\n",
    "    print(f\"Total Projects Skipped: {len(skipped_projects)}\")\n",
    "    print(f\"Total Projects Missing: {len(missing_projects)}\")\n",
    "    print(f\"Total PDFs Accessed: {total_pdfs_accessed}\")\n",
    "    print(f\"Total PDFs Scraped: {total_pdfs_scraped}\")\n",
    "    print(f\"Total PDFs Skipped: {total_pdfs_skipped}\")\n",
    "\n",
    "    print(\"\\nList of Scraped Projects:\")\n",
    "    print(sorted(scraped_projects))\n",
    "\n",
    "    print(\"\\nList of Skipped Projects:\")\n",
    "    print(sorted(skipped_projects))\n",
    "\n",
    "    print(\"\\nList of Missing Projects:\")\n",
    "    print(sorted(missing_projects))\n",
    "\n",
    "    print(\"\\nList of Scraped PDFs:\")\n",
    "    print(scraped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Skipped PDFs:\")\n",
    "    print(skipped_pdfs)\n",
    "\n",
    "    print(\"\\nList of Addendum PDFs:\")\n",
    "    print(addendum_pdfs)\n",
    "\n",
    "    print(\"\\nList of Original PDFs:\")\n",
    "    print(original_pdfs)\n",
    "\n",
    "    print(\"\\nNumber of Original PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in original_pdfs]))\n",
    "    print(\"Number of Addendum PDFs Scraped:\", len([pdf for pdf in scraped_pdfs if pdf in addendum_pdfs]))\n",
    "\n",
    "def save_to_csv(df, output_csv_path, data_type):\n",
    "    \"\"\"Cleans the DataFrame and saves it to a CSV file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"No data to save for {data_type}.\")\n",
    "        return\n",
    "\n",
    "    # Clean up the entire DataFrame by cleaning string cells\n",
    "    df = df.map(clean_string_cell)\n",
    "\n",
    "    # Drop rows that contain specific phrases (e.g., \"Type of Upgrade\")\n",
    "    df = df[~df.apply(lambda row: contains_phrase(row, \"Type of Upgrade\"), axis=1)]\n",
    "\n",
    "    # Reorder columns as specified\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\nColumns reordered for {data_type} as per specification.\")\n",
    "\n",
    "    # Ensure q_id is numeric for sorting, replace missing values with None\n",
    "    if 'q_id' in df.columns:\n",
    "        df['q_id'] = pd.to_numeric(df['q_id'], errors='coerce')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData successfully saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {data_type} data to CSV: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the PDF scraping process.\"\"\"\n",
    "    process_pdfs_in_folder()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Original and Addendums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_24577/2448823523.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load original and addendum data\n",
    "df_original = pd.read_csv(\n",
    "    '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/03_raw/rawdata_cluster7_style_G_originals.csv',\n",
    "    dtype={'estimated_time_to_construct': str}\n",
    ")\n",
    "df_addendum = pd.read_csv(\n",
    "    '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/03_raw/rawdata_cluster7_style_G_addendums.csv',\n",
    "    dtype={'estimated_time_to_construct': str}\n",
    ")\n",
    "\n",
    "# Step 1: Align column names and merge similar columns\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated with itcca\"\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\"\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "\n",
    "    if unnamed_columns:\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "            df.drop(columns=[col for col in existing_cols if col != new_col], inplace=True)\n",
    "    return df\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "df_original = merge_columns(df_original)\n",
    "df_addendum = merge_columns(df_addendum)\n",
    "\n",
    "# Convert column names to snake_case\n",
    "df_original.columns = [convert_to_snake_case(col) for col in df_original.columns]\n",
    "df_addendum.columns = [convert_to_snake_case(col) for col in df_addendum.columns]\n",
    "\n",
    "# Step 2: Replace and merge data using group-based approach\n",
    "\n",
    "# Identify the key columns\n",
    "key_columns = ['q_id', 'type_of_upgrade', 'upgrade']\n",
    "\n",
    "# Ensure key columns exist in both dataframes\n",
    "for col in key_columns:\n",
    "    if col not in df_original.columns or col not in df_addendum.columns:\n",
    "        raise ValueError(f\"Key column '{col}' is missing from one of the dataframes.\")\n",
    "\n",
    "# Identify (q_id, type_of_upgrade) pairs present in addendums\n",
    "addendum_groups = df_addendum[['q_id', 'type_of_upgrade']].drop_duplicates()\n",
    "\n",
    "# Filter out rows from the original that have (q_id, type_of_upgrade) present in addendums\n",
    "original_filtered = df_original.merge(\n",
    "    addendum_groups,\n",
    "    on=['q_id', 'type_of_upgrade'],\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "original_filtered = original_filtered[original_filtered['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# Concatenate the filtered original with addendums\n",
    "df_updated = pd.concat([original_filtered, df_addendum], ignore_index=True)\n",
    "\n",
    "# Optional: Sort the updated dataframe based on keys for better readability\n",
    "df_updated.sort_values(by=key_columns, inplace=True)\n",
    "df_updated.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 3: Save the updated file\n",
    "updated_file = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/03_raw/rawdata_cluster7_style_G_updated.csv'\n",
    "df_updated.to_csv(updated_file, index=False)\n",
    "print(\"Updated file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Code to create itemized and total dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_1_cluster_7_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_1_cluster_7_total.csv'.\n",
      "['PTO_IF' 'RNU' 'LDNU' 'ADNU']\n",
      "[1007 1010 1011 1013 1014 1015 1016 1018 1019 1020 1021 1023 1024 1026\n",
      " 1027 1028 1029 1030 1031 1032 1033 1035 1036 1037 1038 1040 1043 1045\n",
      " 1046 1047 1048 1049 1050 1051 1052 1053 1055 1056 1057 1058 1059 1060\n",
      " 1061 1062 1063]\n",
      "[7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_75037/408863846.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "# df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6-8/C_6_7_8_data.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/03_raw/rawdata_cluster7_style_G_originals.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated with itcca\"\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\"\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns (columns with no name or those starting with \"Unnamed\")\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "\n",
    "    if unnamed_columns:\n",
    "        # Merge unnamed columns into 'description' if they exist\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    # Iterate over the dictionary to merge columns\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        # Get the list of columns that exist in the DataFrame\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "\n",
    "        if existing_cols:\n",
    "            # Create a new column by backfilling data from the existing old columns\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "            # Drop the old columns, except for the new one we just created\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the merge_columns function to the DataFrame\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        # Normalize Unicode to NFKD and remove non-ASCII characters\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        # Remove newlines and extra spaces\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "# Apply the clean_string_cell function to all cells using applymap\n",
    "df = df.map(clean_string_cell)\n",
    "\n",
    "\n",
    "# Apply the function to all columns to convert column names to snake_case\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "# Ensure that the necessary columns exist before proceeding\n",
    "required_columns = ['type_of_upgrade', 'cost_allocation_factor']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None  # or any default value you prefer\n",
    "\n",
    "\n",
    "# Step 1: Create the 'item' column based on whether the 'type_of_upgrade' row or 'cost_allocation_factor' contains 'Total'\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 2: Move 'item' column next to 'type_of_upgrade' column if both exist\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "# Step 3: Remove the 'Total' values from the 'cost_allocation_factor' column if they are already in the 'type_of_upgrade' column\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Step 4: For each `q_id` and `type_of_upgrade`, if it has only one row and no 'Total' is present in 'cost_allocation_factor', create a new `Total` row\n",
    "new_rows = []\n",
    "for q_id, group in df.groupby('q_id'):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "\n",
    "    # Check if there's already a \"Total\" in 'cost_allocation_factor' for this q_id\n",
    "    if any('Total' in str(x) for x in group.get('cost_allocation_factor', [])):\n",
    "        continue  # If Total exists under 'cost_allocation_factor', skip creating new total row for this q_id\n",
    "\n",
    "    for upgrade in unique_upgrades:\n",
    "        # Skip if \"Total\" is already present for this upgrade or if the upgrade is NaN\n",
    "        if pd.isna(upgrade) or 'Total' in str(upgrade):\n",
    "            continue\n",
    "\n",
    "        # Get rows corresponding to this specific upgrade\n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        if len(rows) == 1:\n",
    "            # Duplicate the row and create a new row with \"Total\" under 'type_of_upgrade'\n",
    "            original_row = rows.iloc[0].copy()\n",
    "            total_row = original_row.copy()\n",
    "\n",
    "            # Modify the total_row to reflect \"Total\"\n",
    "            total_row['type_of_upgrade'] = 'Total'\n",
    "            total_row['item'] = 'no'\n",
    "\n",
    "            # Append the Total row immediately after the original row\n",
    "            original_index = df[(df['q_id'] == q_id) & (df['type_of_upgrade'] == upgrade)].index[0]\n",
    "            new_rows.append((original_index + 1, total_row))\n",
    "\n",
    "# Step 5: Insert the new rows into the DataFrame in the correct order\n",
    "for idx, row in sorted(new_rows, reverse=True):\n",
    "    if idx < 0 or idx > len(df):\n",
    "        continue  # Skip invalid indices\n",
    "    df = pd.concat([df.iloc[:idx], pd.DataFrame([row]), df.iloc[idx:]]).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Move \"Total\" from 'cost_allocation_factor' column to 'type_of_upgrade' column\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "# Step 7: After moving \"Total\" to 'type_of_upgrade', clear it from 'cost_allocation_factor'\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "'''\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'\\s*months?\\s*', '', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "'''\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove any word after the number, preserving hyphens\n",
    "        # This will strip words like 'months', 'years', etc., but keep hyphens\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'Reliability Network Upgrade': 'RNU',\n",
    "    'Reliability Network upgrade': 'RNU',\n",
    "    'Local Delivery Network Upgrade': 'LDNU',\n",
    "    'Local Delivery Network': 'LDNU',\n",
    "    \"PTOs Interconnection Facilities\": 'PTO_IF',\n",
    "    'Area Delivery Network Upgrade': 'ADNU',\n",
    "    'Reliability Network upgrade to Physically Interconnect': 'RNU',\n",
    "    'PTO': 'PTO_IF',\n",
    "}\n",
    "\n",
    "# Apply transformations using a lambda function\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    # Step 8: Forward fill the non-empty values in 'type_of_upgrade' to replace empty cells with the previous value\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()\n",
    "\n",
    "\n",
    "if 'upgrade' in df.columns and 'type_of_upgrade' in df.columns and 'q_id' in df.columns:\n",
    "    df['upgrade'] = df.groupby(['q_id', 'type_of_upgrade'])['upgrade'].ffill()\n",
    "\n",
    "\n",
    "# Step 9: Ensure 'Total' is correctly replaced in 'type_of_upgrade' when present\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "# Step 10: Fill NaN in descriptive columns with 'None' and NaN in numeric columns with 0\n",
    "# List of numeric columns and non-numeric columns based on dataset structure\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "# Replace '-' with NaN in numeric columns (so they can be converted to 0)\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "\n",
    "# Replace NaN in numeric columns with 0\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "# Step 11: Sort the DataFrame by q_id and original order if 'original_order' exists\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    # Optional: Drop the original_order column if not needed anymore\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "# Step 12: Create the itemized dataset (rows where item == 'yes') and save to CSV\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_itemized.csv', index=False)\n",
    "\n",
    "# Step 13: Create the totals dataset (rows where item == 'no') and drop unwanted columns\n",
    "totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "if 'item' in df.columns:\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_total.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_1_cluster_7_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_1_cluster_7_total.csv'.\")\n",
    "\n",
    "# Print unique values if columns exist\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemized rows saved to 'costs_phase_1_cluster_7_itemized.csv'.\n",
      "Filtered Total rows saved to 'costs_phase_1_cluster_7_total.csv'.\n",
      "['PTO_IF' 'RNU']\n",
      "[1021]\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Load the CSV file\n",
    "# df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 6-8/C_6_7_8_data.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df = pd.read_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/03_raw/rawdata_cluster7_style_G_addendums.csv', dtype={'estimated_time_to_construct': str})\n",
    "\n",
    "df['q_id'] = df['q_id'].astype('Int64')\n",
    "df['cluster'] = df['cluster'].astype('Int64')\n",
    "\n",
    "\n",
    "def merge_columns(df):\n",
    "    merge_columns_dict = {\n",
    "        \"escalated_cost_x_1000\": [\n",
    "            \"escalated costs x 1000\",\n",
    "            \"estimated cost x 1000 escalated\",\n",
    "            \"estimated cost x 1000 escalated with itcca\"\n",
    "        ],\n",
    "        \"estimated_time_to_construct\": [\n",
    "            \"estimated time to construct\",\n",
    "            \"estimated time  to construct\",\n",
    "            \"estimated time\"\n",
    "        ],\n",
    "        \"total_estimated_cost_x_1000_escalated\": [\n",
    "            \"total estimated cost x 1000 escalalted\",\n",
    "            \"total estimated cost x 1000 escalated\"\n",
    "        ],\n",
    "        \"adnu_cost_rate_x_1000\": [\n",
    "            \"adnu cost rate x 1000\",\n",
    "            \"cost rate x 1000\"\n",
    "        ],\n",
    "        \"description\": [\"description\"],\n",
    "        \"capacity\": [\n",
    "            \"capacity\",\n",
    "            \"project size\"\n",
    "        ],\n",
    "        \"cost_allocation_factor\": [\n",
    "            \"cost allocation factor\",\n",
    "            \"cost allocatio n factor\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Identify unnamed columns (columns with no name or those starting with \"Unnamed\")\n",
    "    unnamed_columns = [col for col in df.columns if pd.isna(col) or col.strip() == \"\" or col.startswith(\"Unnamed\")]\n",
    "\n",
    "    if unnamed_columns:\n",
    "        # Merge unnamed columns into 'description' if they exist\n",
    "        merge_columns_dict[\"description\"].extend(unnamed_columns)\n",
    "\n",
    "    # Iterate over the dictionary to merge columns\n",
    "    for new_col, old_cols in merge_columns_dict.items():\n",
    "        # Get the list of columns that exist in the DataFrame\n",
    "        existing_cols = [col for col in old_cols if col in df.columns]\n",
    "\n",
    "        if existing_cols:\n",
    "            # Create a new column by backfilling data from the existing old columns\n",
    "            df[new_col] = df[existing_cols].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "            # Drop the old columns, except for the new one we just created\n",
    "            cols_to_drop = [col for col in existing_cols if col != new_col]\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the merge_columns function to the DataFrame\n",
    "df = merge_columns(df)\n",
    "\n",
    "\n",
    "def convert_to_snake_case(column_name):\n",
    "    column_name = column_name.strip().lower()\n",
    "    column_name = re.sub(r'[\\s\\-]+', '_', column_name)\n",
    "    column_name = re.sub(r'[^\\w]', '', column_name)\n",
    "    return column_name\n",
    "\n",
    "\n",
    "def clean_string_cell(value):\n",
    "    if isinstance(value, str):\n",
    "        # Normalize Unicode to NFKD and remove non-ASCII characters\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "        # Remove newlines and extra spaces\n",
    "        value = value.replace('\\n', ' ').strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "# Apply the clean_string_cell function to all cells using applymap\n",
    "df = df.map(clean_string_cell)\n",
    "\n",
    "\n",
    "# Apply the function to all columns to convert column names to snake_case\n",
    "df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "\n",
    "# Ensure that the necessary columns exist before proceeding\n",
    "required_columns = ['type_of_upgrade', 'cost_allocation_factor']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None  # or any default value you prefer\n",
    "\n",
    "\n",
    "# Step 1: Create the 'item' column based on whether the 'type_of_upgrade' row or 'cost_allocation_factor' contains 'Total'\n",
    "df['item'] = df.apply(\n",
    "    lambda row: 'no' if (\n",
    "        (pd.notna(row.get('type_of_upgrade')) and 'Total' in str(row['type_of_upgrade'])) or\n",
    "        (pd.notna(row.get('cost_allocation_factor')) and 'Total' in str(row['cost_allocation_factor']))\n",
    "    ) else 'yes',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 2: Move 'item' column next to 'type_of_upgrade' column if both exist\n",
    "if 'item' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    cols = df.columns.tolist()\n",
    "    item_index = cols.index('item')\n",
    "    type_index = cols.index('type_of_upgrade')\n",
    "    if item_index < type_index:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    else:\n",
    "        cols.insert(type_index + 1, cols.pop(item_index))\n",
    "    df = df[cols]\n",
    "\n",
    "# Step 3: Remove the 'Total' values from the 'cost_allocation_factor' column if they are already in the 'type_of_upgrade' column\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if (\n",
    "            pd.notna(row['type_of_upgrade']) and 'Total' in str(row['type_of_upgrade'])\n",
    "        ) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Step 4: For each `q_id` and `type_of_upgrade`, if it has only one row and no 'Total' is present in 'cost_allocation_factor', create a new `Total` row\n",
    "new_rows = []\n",
    "for q_id, group in df.groupby('q_id'):\n",
    "    unique_upgrades = group['type_of_upgrade'].dropna().unique()\n",
    "\n",
    "    # Check if there's already a \"Total\" in 'cost_allocation_factor' for this q_id\n",
    "    if any('Total' in str(x) for x in group.get('cost_allocation_factor', [])):\n",
    "        continue  # If Total exists under 'cost_allocation_factor', skip creating new total row for this q_id\n",
    "\n",
    "    for upgrade in unique_upgrades:\n",
    "        # Skip if \"Total\" is already present for this upgrade or if the upgrade is NaN\n",
    "        if pd.isna(upgrade) or 'Total' in str(upgrade):\n",
    "            continue\n",
    "\n",
    "        # Get rows corresponding to this specific upgrade\n",
    "        rows = group[group['type_of_upgrade'] == upgrade]\n",
    "\n",
    "        if len(rows) == 1:\n",
    "            # Duplicate the row and create a new row with \"Total\" under 'type_of_upgrade'\n",
    "            original_row = rows.iloc[0].copy()\n",
    "            total_row = original_row.copy()\n",
    "\n",
    "            # Modify the total_row to reflect \"Total\"\n",
    "            total_row['type_of_upgrade'] = 'Total'\n",
    "            total_row['item'] = 'no'\n",
    "\n",
    "            # Append the Total row immediately after the original row\n",
    "            original_index = df[(df['q_id'] == q_id) & (df['type_of_upgrade'] == upgrade)].index[0]\n",
    "            new_rows.append((original_index + 1, total_row))\n",
    "\n",
    "# Step 5: Insert the new rows into the DataFrame in the correct order\n",
    "for idx, row in sorted(new_rows, reverse=True):\n",
    "    if idx < 0 or idx > len(df):\n",
    "        continue  # Skip invalid indices\n",
    "    df = pd.concat([df.iloc[:idx], pd.DataFrame([row]), df.iloc[idx:]]).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Move \"Total\" from 'cost_allocation_factor' column to 'type_of_upgrade' column\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "# Step 7: After moving \"Total\" to 'type_of_upgrade', clear it from 'cost_allocation_factor'\n",
    "if 'cost_allocation_factor' in df.columns and 'type_of_upgrade' in df.columns:\n",
    "    df['cost_allocation_factor'] = df.apply(\n",
    "        lambda row: None if 'Total' in str(row.get('cost_allocation_factor', '')) else row.get('cost_allocation_factor'),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "'''\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'\\s*months?\\s*', '', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "'''\n",
    "\n",
    "def clean_estimated_time(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove any word after the number, preserving hyphens\n",
    "        # This will strip words like 'months', 'years', etc., but keep hyphens\n",
    "        value = re.sub(r'(\\d+(?:-\\w+)*)\\s+\\w+.*$', r'\\1', value, flags=re.IGNORECASE).strip()\n",
    "    return value\n",
    "\n",
    "\n",
    "if 'estimated_time_to_construct' in df.columns:\n",
    "    df['estimated_time_to_construct'] = df['estimated_time_to_construct'].apply(clean_estimated_time)\n",
    "\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\s*\\(note \\d+\\)', '', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'Reliability Network Upgrade': 'RNU',\n",
    "    'Reliability Network upgrade': 'RNU',\n",
    "    'Local Delivery Network Upgrade': 'LDNU',\n",
    "    'Local Delivery Network': 'LDNU',\n",
    "    \"PTOs Interconnection Facilities\": 'PTO_IF',\n",
    "    'Area Delivery Network Upgrade': 'ADNU',\n",
    "    'Reliability Network upgrade to Physically Interconnect': 'RNU',\n",
    "    'PTO': 'PTO_IF',\n",
    "}\n",
    "\n",
    "# Apply transformations using a lambda function\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: re.sub(r'\\bupgrades\\b', 'upgrade', x, flags=re.IGNORECASE).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].apply(\n",
    "        lambda x: mappings.get(x, x) if isinstance(x, str) else x\n",
    "    )\n",
    "    # Step 8: Forward fill the non-empty values in 'type_of_upgrade' to replace empty cells with the previous value\n",
    "    df['type_of_upgrade'] = df['type_of_upgrade'].ffill()\n",
    "\n",
    "\n",
    "if 'upgrade' in df.columns and 'type_of_upgrade' in df.columns and 'q_id' in df.columns:\n",
    "    df['upgrade'] = df.groupby(['q_id', 'type_of_upgrade'])['upgrade'].ffill()\n",
    "\n",
    "\n",
    "# Step 9: Ensure 'Total' is correctly replaced in 'type_of_upgrade' when present\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    previous_type_of_upgrade = None\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i, 'type_of_upgrade'] == 'Total':\n",
    "            if previous_type_of_upgrade is not None:\n",
    "                df.at[i, 'type_of_upgrade'] = previous_type_of_upgrade\n",
    "        else:\n",
    "            previous_type_of_upgrade = df.at[i, 'type_of_upgrade']\n",
    "\n",
    "# Step 10: Fill NaN in descriptive columns with 'None' and NaN in numeric columns with 0\n",
    "# List of numeric columns and non-numeric columns based on dataset structure\n",
    "numeric_columns = [\n",
    "    'cost_allocation_factor',\n",
    "    'estimated_cost_x_1000',\n",
    "    'estimated_time_to_construct',\n",
    "    'total_estimated_cost_x_1000_escalated',\n",
    "    'adnu_cost_rate_x_1000',\n",
    "    'escalated_cost_x_1000',\n",
    "    'estimated_cost_x_1000_escalated_without_itcca',\n",
    "    'adnu_cost_rate_x_1000_escalated'\n",
    "]\n",
    "non_numeric_columns = ['type_of_upgrade', 'upgrade', 'description']\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "# Replace '-' with NaN in numeric columns (so they can be converted to 0)\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('-', pd.NA)\n",
    "\n",
    "# Replace NaN in numeric columns with 0\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "# Step 11: Sort the DataFrame by q_id and original order if 'original_order' exists\n",
    "if 'original_order' in df.columns and 'q_id' in df.columns:\n",
    "    df['original_order'] = df.index\n",
    "    df = df.sort_values(by=['q_id', 'original_order'], ascending=[True, True])\n",
    "    # Optional: Drop the original_order column if not needed anymore\n",
    "    df = df.drop(columns=['original_order'])\n",
    "\n",
    "# Step 12: Create the itemized dataset (rows where item == 'yes') and save to CSV\n",
    "if 'item' in df.columns:\n",
    "    itemized_df = df[df['item'] == 'yes']\n",
    "    itemized_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_itemized_addendums.csv', index=False)\n",
    "\n",
    "# Step 13: Create the totals dataset (rows where item == 'no') and drop unwanted columns\n",
    "totals_columns = ['upgrade', 'description', 'cost_allocation_factor', 'estimated_time_to_construct']\n",
    "existing_totals_columns = [col for col in totals_columns if col in df.columns]\n",
    "if 'item' in df.columns:\n",
    "    totals_df = df[df['item'] == 'no'].drop(columns=existing_totals_columns, errors='ignore')\n",
    "    totals_df.to_csv('/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_total_addendums.csv', index=False)\n",
    "\n",
    "print(f\"Itemized rows saved to 'costs_phase_1_cluster_7_itemized.csv'.\")\n",
    "print(f\"Filtered Total rows saved to 'costs_phase_1_cluster_7_total.csv'.\")\n",
    "\n",
    "# Print unique values if columns exist\n",
    "if 'type_of_upgrade' in df.columns:\n",
    "    print(df['type_of_upgrade'].unique())\n",
    "\n",
    "if 'q_id' in df.columns:\n",
    "    print(df['q_id'].unique())\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    print(df['cluster'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to merge the itemized and total datasets for original and addendum files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_24577/2689210007.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_24577/2689210007.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_24577/2689210007.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_24577/2689210007.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_24577/2689210007.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = 0\n",
      "/var/folders/l5/3g5pj1nj2j108yb2yjngfmcmcmdwpt/T/ipykernel_24577/2689210007.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  addendum_rows[col] = 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Load a CSV file and ensure specific columns are treated as character, others as numeric.\n",
    "    \"\"\"\n",
    " # Get columns available in the dataset\n",
    "    available_columns = pd.read_csv(file_path, nrows=0).columns\n",
    "    \n",
    "    # Restrict to char_columns that are present in the dataset\n",
    "    char_columns_in_dataset = [col for col in char_columns if col in available_columns]\n",
    "    \n",
    "    # Load the dataset, treating char_columns_in_dataset as strings\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        dtype={col: str for col in char_columns_in_dataset},\n",
    "        na_values=[],  # Disable automatic NaN interpretation\n",
    "        keep_default_na=False  # Prevent treating \"None\" as NaN\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert all other columns to numeric\n",
    "    #for col in df.columns:\n",
    "    #    if col not in char_columns:\n",
    "    #        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_data(df, file_path, char_columns):\n",
    "    \"\"\"\n",
    "    Save a dataframe to a CSV file, ensuring specific columns are treated as character.\n",
    "    \"\"\"\n",
    "    for col in char_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def merge_with_addendums(itemized, itemized_addendums, total, total_addendums):\n",
    "    # Add an 'original' column to the datasets\n",
    "    itemized['original'] = \"yes\"\n",
    "    total['original'] = \"yes\"\n",
    "    \n",
    "    # Preserve the original row order\n",
    "    itemized['row_order'] = itemized.index\n",
    "    total['row_order'] = total.index\n",
    "    \n",
    "    # Prepare a list to collect the updated itemized rows\n",
    "    updated_itemized_rows = []\n",
    "    \n",
    "    # Merge itemized and itemized_addendums\n",
    "    for q_id in itemized_addendums['q_id'].unique():\n",
    "        for upgrade_type in itemized_addendums['type_of_upgrade'].unique():\n",
    "            addendum_rows = itemized_addendums[\n",
    "                (itemized_addendums['q_id'] == q_id) &\n",
    "                (itemized_addendums['type_of_upgrade'] == upgrade_type)\n",
    "            ]\n",
    "            if not addendum_rows.empty:\n",
    "                mask = (itemized['q_id'] == q_id) & (itemized['type_of_upgrade'] == upgrade_type)\n",
    "                original_rows = itemized[mask]\n",
    "\n",
    "                # Find non-character columns missing in the addendum dataset\n",
    "                missing_cols = set(itemized.columns) - set(addendum_rows.columns) - set(char_columns)\n",
    "                for col in missing_cols:\n",
    "                    addendum_rows[col] = 0 \n",
    "                itemized.loc[mask, 'original'] = \"no\"\n",
    "                updated_itemized_rows.append(addendum_rows.assign(original=\"no\", row_order=original_rows['row_order'].values[:len(addendum_rows)]))\n",
    "                itemized = itemized[~mask]\n",
    "\n",
    "            \n",
    "    \n",
    "    if updated_itemized_rows:\n",
    "        updated_itemized = pd.concat([itemized] + updated_itemized_rows, ignore_index=True)\n",
    "    else:\n",
    "        updated_itemized = itemized.copy()\n",
    "    \n",
    "    updated_itemized = updated_itemized.sort_values(by=\"row_order\").drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    \n",
    "    updated_total_rows = []\n",
    "    \n",
    "    for q_id in total_addendums['q_id'].unique():\n",
    "        for upgrade_type in total_addendums['type_of_upgrade'].unique():\n",
    "            addendum_row = total_addendums[\n",
    "                (total_addendums['q_id'] == q_id) &\n",
    "                (total_addendums['type_of_upgrade'] == upgrade_type)\n",
    "            ]\n",
    "            if not addendum_row.empty:\n",
    "                mask = (total['q_id'] == q_id) & (total['type_of_upgrade'] == upgrade_type)\n",
    "                original_row = total[mask]\n",
    "\n",
    "                # Find non-character columns missing in the addendum dataset\n",
    "                missing_cols = set(itemized.columns) - set(addendum_rows.columns) - set(char_columns)\n",
    "                for col in missing_cols:\n",
    "                    addendum_rows[col] = 0 \n",
    "                total.loc[mask, 'original'] = \"no\"\n",
    "                updated_total_rows.append(addendum_row.assign(original=\"no\", row_order=original_row['row_order'].values[:len(addendum_row)]))\n",
    "                total = total[~mask]\n",
    "    \n",
    "    if updated_total_rows:\n",
    "        updated_total = pd.concat([total] + updated_total_rows, ignore_index=True)\n",
    "    else:\n",
    "        updated_total = total.copy()\n",
    "    \n",
    "    updated_total = updated_total.sort_values(by=\"row_order\").drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Fill missing columns with zeros in the updated datasets\n",
    "    for col in set(itemized.columns) - set(updated_itemized.columns):\n",
    "        updated_itemized[col] = 0\n",
    "    \n",
    "    for col in set(total.columns) - set(updated_total.columns):\n",
    "        updated_total[col] = 0\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Move the 'original' column to the last position\n",
    "    updated_itemized = updated_itemized[[col for col in updated_itemized.columns if col != 'original'] + ['original']]\n",
    "    updated_total = updated_total[[col for col in updated_total.columns if col != 'original'] + ['original']]\n",
    "\n",
    "        # Drop the row_order column directly\n",
    "    if \"row_order\" in updated_itemized.columns:\n",
    "        updated_itemized = updated_itemized.drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "\n",
    "    if \"row_order\" in updated_total.columns:\n",
    "        updated_total = updated_total.drop(columns=[\"row_order\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "  \n",
    " \n",
    "    \n",
    "    \n",
    "    return updated_itemized, updated_total\n",
    "\n",
    "# Define the character columns\n",
    "char_columns = [\n",
    "    \"req_deliverability\", \"point_of_interconnection\", \"type_of_upgrade\",\n",
    "    \"upgrade\", \"description\", \"estimated_time_to_construct\", \"original\", \"item\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "itemized = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_itemized.csv\", char_columns)\n",
    "itemized_addendums = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_itemized_addendums.csv\", char_columns)\n",
    "total = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_total.csv\", char_columns)\n",
    "total_addendums = load_data(\"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_total_addendums.csv\", char_columns)\n",
    "\n",
    "\n",
    "updated_itemized, updated_total = merge_with_addendums(itemized, itemized_addendums, total, total_addendums)\n",
    "\n",
    "\n",
    "# Save the results\n",
    "save_data(updated_itemized, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/01_clean/costs_phase_1_cluster_7_style_G_itemized_updated.csv\", char_columns)\n",
    "save_data(updated_total, \"/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/01_clean/costs_phase_1_cluster_7_style_G_total_updated.csv\", char_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded itemized data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 7/costs_phase_1_cluster_7_style_G_itemized.csv\n",
      "Loaded totals data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 7/costs_phase_1_cluster_7_style_G_total.csv\n",
      "\n",
      "All q_ids have all required upgrades.\n",
      "Mismatch: Q_id 1014, Upgrade 'PTO_IF' - Manual Total: 616.0, Reported Total: 0.0\n",
      "Mismatch: Q_id 1014, Upgrade 'RNU' - Manual Total: 2035.9, Reported Total: 0.0\n",
      "Mismatch: Q_id 1027, Upgrade 'RNU' - Manual Total: 1058.1, Reported Total: 0.0\n",
      "\n",
      "Mismatches saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 7/mismatches.csv'.\n",
      "\n",
      "Direct Matches (Exact Point of Interconnection Names):\n",
      "       point_of_interconnection          q_id\n",
      "3   230 KV OTAY MESA SWITCHYARD  [1057, 1058]\n",
      "19         ARCO SUBSTATION 70KV  [1035, 1038]\n",
      "21       GATES SUBSTATION 230KV  [1027, 1031]\n",
      "27    MENDOTA SUBSTATION 115 KV  [1028, 1029]\n",
      "\n",
      "Fuzzy Matches (>=80% Similarity in Point of Interconnection):\n",
      "                           point_of_interconnection_1 q_ids_1  \\\n",
      "0                       LOS ESTEROS SUBSTATION 115 KV  [1007]   \n",
      "1                              TESLA SUBSTATION 115KV  [1011]   \n",
      "2                              TESLA SUBSTATION 115KV  [1011]   \n",
      "3                             HALSEY SUBSTATION 60 KV  [1013]   \n",
      "4                             HALSEY SUBSTATION 60 KV  [1013]   \n",
      "5                             HALSEY SUBSTATION 60 KV  [1013]   \n",
      "6                    PLEASANT GROVE SUBSTATION 115 KV  [1015]   \n",
      "7                           EVERGREEN SUBSTATION 60KV  [1023]   \n",
      "8                             VICTOR SUBSTATION 60 KV  [1024]   \n",
      "9   PANOCHE-HELM AND PANOCHE-KEARNEY 230 KV LINES ...  [1032]   \n",
      "10                              ARCO SUBSTATION 70 KV  [1033]   \n",
      "11                              ARCO SUBSTATION 70 KV  [1033]   \n",
      "12                              ARCO SUBSTATION 70 KV  [1033]   \n",
      "13          500KV COMMON BUS AT HASSAYAMPA SWITCHYARD  [1040]   \n",
      "14                          69 KV CREELMAN SUBSTATION  [1043]   \n",
      "15                          69 KV CREELMAN SUBSTATION  [1043]   \n",
      "16                          69 KV CREELMAN SUBSTATION  [1043]   \n",
      "17                          69 KV CREELMAN SUBSTATION  [1043]   \n",
      "18                          69 KV CREELMAN SUBSTATION  [1043]   \n",
      "19                              69 KV OTAY SUBSTATION  [1045]   \n",
      "20                              69 KV OTAY SUBSTATION  [1045]   \n",
      "21                              69 KV OTAY SUBSTATION  [1045]   \n",
      "22                              69 KV OTAY SUBSTATION  [1045]   \n",
      "23                              69 KV OTAY SUBSTATION  [1045]   \n",
      "24                      138 KV EAST COUNTY SUBSTATION  [1046]   \n",
      "25                      138 KV EAST COUNTY SUBSTATION  [1046]   \n",
      "26  69 KV EL CAJON SUBSTATION ON EXISTING GEN TIE ...  [1047]   \n",
      "27  69 KV EL CAJON SUBSTATION ON EXISTING GEN TIE ...  [1047]   \n",
      "28  69 KV EL CAJON SUBSTATION ON EXISTING GEN TIE ...  [1047]   \n",
      "29  69 KV ESCONDIDO SUBSTATION, VIA TL6933 (GEN TI...  [1048]   \n",
      "30  69 KV ESCONDIDO SUBSTATION, VIA TL6933 (GEN TI...  [1048]   \n",
      "31  69 KV ESCONDIDO SUBSTATION, VIA TL6933 (GEN TI...  [1048]   \n",
      "32                              138 KV ECO SUBSTATION  [1049]   \n",
      "33                           69 KV CAMERON SUBSTATION  [1050]   \n",
      "34                           69 KV CAMERON SUBSTATION  [1050]   \n",
      "35                           69 KV CAMERON SUBSTATION  [1050]   \n",
      "36                           69 KV CAMERON SUBSTATION  [1050]   \n",
      "37                           69 KV CAMERON SUBSTATION  [1050]   \n",
      "38                            69 KV KEARNY SUBSTATION  [1051]   \n",
      "39                            69 KV KEARNY SUBSTATION  [1051]   \n",
      "40                            69 KV KEARNY SUBSTATION  [1051]   \n",
      "41                            69 KV KEARNY SUBSTATION  [1051]   \n",
      "42                      69 KV SANTA YSABEL SUBSTATION  [1052]   \n",
      "43                     230 KV SAN LUIS REY SUBSTATION  [1055]   \n",
      "44                      69 KV SAN LUIS REY SUBSTATION  [1059]   \n",
      "45                           69 KV MELROSE SUBSTATION  [1061]   \n",
      "\n",
      "                           point_of_interconnection_2       q_ids_2  \\\n",
      "0                           MENDOTA SUBSTATION 115 KV  [1028, 1029]   \n",
      "1                    SHINGLE SPRINGS SUBSTATION 115KV        [1019]   \n",
      "2                           MENDOTA SUBSTATION 115 KV  [1028, 1029]   \n",
      "3                             VICTOR SUBSTATION 60 KV        [1024]   \n",
      "4                               69 KV OTAY SUBSTATION        [1045]   \n",
      "5                            69 KV MELROSE SUBSTATION        [1061]   \n",
      "6                           MENDOTA SUBSTATION 115 KV  [1028, 1029]   \n",
      "7                               WEBER SUBSTATION 60KV        [1026]   \n",
      "8                               ARCO SUBSTATION 70 KV        [1033]   \n",
      "9   GATES-GREGG AND GATES-MCCALL 230 KV LINES VIA ...        [1036]   \n",
      "10                               ARCO SUBSTATION 70KV  [1035, 1038]   \n",
      "11                              138 KV ECO SUBSTATION        [1049]   \n",
      "12                           69 KV CAMERON SUBSTATION        [1050]   \n",
      "13         500 KV COMMON BUS AT HASSAYAMPA SWITCHYARD        [1053]   \n",
      "14                              69 KV OTAY SUBSTATION        [1045]   \n",
      "15                        69 KV SALT CREEK SUBSTATION        [1063]   \n",
      "16                            69 KV KEARNY SUBSTATION        [1051]   \n",
      "17                           69 KV CAMERON SUBSTATION        [1050]   \n",
      "18                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "19  69 KV EL CAJON SUBSTATION ON EXISTING GEN TIE ...        [1047]   \n",
      "20  69 KV ESCONDIDO SUBSTATION, VIA TL6933 (GEN TI...        [1048]   \n",
      "21                           69 KV CAMERON SUBSTATION        [1050]   \n",
      "22                            69 KV KEARNY SUBSTATION        [1051]   \n",
      "23                      69 KV SANTA YSABEL SUBSTATION        [1052]   \n",
      "24                              138 KV ECO SUBSTATION        [1049]   \n",
      "25                   138 KV BOULEVARD EAST SUBSTATION        [1060]   \n",
      "26                            69 KV KEARNY SUBSTATION        [1051]   \n",
      "27                           69 KV CAMERON SUBSTATION        [1050]   \n",
      "28                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "29                            69 KV KEARNY SUBSTATION        [1051]   \n",
      "30                           69 KV CAMERON SUBSTATION        [1050]   \n",
      "31                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "32                   138 KV BOULEVARD EAST SUBSTATION        [1060]   \n",
      "33                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "34                            69 KV KEARNY SUBSTATION        [1051]   \n",
      "35                      69 KV SANTA YSABEL SUBSTATION        [1052]   \n",
      "36                      69 KV SAN LUIS REY SUBSTATION        [1059]   \n",
      "37                        69 KV SALT CREEK SUBSTATION        [1063]   \n",
      "38                      69 KV SANTA YSABEL SUBSTATION        [1052]   \n",
      "39                      69 KV SAN LUIS REY SUBSTATION        [1059]   \n",
      "40                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "41                        69 KV SALT CREEK SUBSTATION        [1063]   \n",
      "42                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "43                      69 KV SAN LUIS REY SUBSTATION        [1059]   \n",
      "44                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "45                        69 KV SALT CREEK SUBSTATION        [1063]   \n",
      "\n",
      "    similarity_score  \n",
      "0                 81  \n",
      "1                 84  \n",
      "2                 81  \n",
      "3                 82  \n",
      "4                 82  \n",
      "5                 81  \n",
      "6                 81  \n",
      "7                 83  \n",
      "8                 82  \n",
      "9                 83  \n",
      "10                98  \n",
      "11                81  \n",
      "12                80  \n",
      "13                99  \n",
      "14                86  \n",
      "15                85  \n",
      "16                83  \n",
      "17                82  \n",
      "18                80  \n",
      "19                86  \n",
      "20                86  \n",
      "21                86  \n",
      "22                86  \n",
      "23                86  \n",
      "24                89  \n",
      "25                86  \n",
      "26                82  \n",
      "27                80  \n",
      "28                80  \n",
      "29                82  \n",
      "30                80  \n",
      "31                80  \n",
      "32                89  \n",
      "33                88  \n",
      "34                85  \n",
      "35                80  \n",
      "36                80  \n",
      "37                80  \n",
      "38                82  \n",
      "39                82  \n",
      "40                82  \n",
      "41                82  \n",
      "42                80  \n",
      "43                95  \n",
      "44                80  \n",
      "45                80  \n",
      "Matched Q_ids saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 7/matched_qids.csv'.\n",
      "\n",
      "Total checks performed: 180\n",
      "Total mismatches found: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "# Paths to the CSV files\n",
    "ITEMIZED_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_itemized.csv'\n",
    "TOTALS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/02_intermediate/costs_phase_1_cluster_7_style_G_total.csv'\n",
    "\n",
    "# Columns in totals_df that hold the reported total costs\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "# Upgrade types to check\n",
    "REQUIRED_UPGRADES = ['PTO_IF', 'RNU', 'LDNU', 'ADNU']\n",
    "\n",
    "# Output paths\n",
    "MISMATCHES_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/mismatches.csv'\n",
    "MATCHED_QIDS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/matched_qids.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "def load_csv(path, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {dataset_name} from {path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Load datasets\n",
    "itemized_df = load_csv(ITEMIZED_CSV_PATH, \"itemized data\")\n",
    "totals_df = load_csv(TOTALS_CSV_PATH, \"totals data\")\n",
    "\n",
    "# ---------------------- Data Cleaning ---------------------- #\n",
    "\n",
    "def clean_text(df, column):\n",
    "    \"\"\"\n",
    "    Cleans text data by stripping leading/trailing spaces and converting to uppercase.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        df[column] = df[column].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        print(f\"Warning: '{column}' column is missing in the dataset. Filling with 'UNKNOWN'.\")\n",
    "        df[column] = 'UNKNOWN'\n",
    "    return df\n",
    "\n",
    "# Clean 'type_of_upgrade' and 'point_of_interconnection' in both datasets\n",
    "itemized_df = clean_text(itemized_df, 'type_of_upgrade')\n",
    "itemized_df = clean_text(itemized_df, 'point_of_interconnection')\n",
    "\n",
    "totals_df = clean_text(totals_df, 'type_of_upgrade')\n",
    "totals_df = clean_text(totals_df, 'point_of_interconnection')\n",
    "\n",
    "# ---------------------- Data Preparation ---------------------- #\n",
    "\n",
    "# Ensure necessary columns exist in itemized_df\n",
    "required_itemized_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', 'estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in required_itemized_columns:\n",
    "    if col not in itemized_df.columns:\n",
    "        print(f\"Warning: '{col}' column is missing in the itemized dataset.\")\n",
    "        if col in ['q_id', 'type_of_upgrade', 'point_of_interconnection']:\n",
    "            itemized_df[col] = 'UNKNOWN'\n",
    "        else:\n",
    "            itemized_df[col] = 0\n",
    "\n",
    "# Ensure necessary columns exist in totals_df\n",
    "required_totals_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in required_totals_columns:\n",
    "    if col not in totals_df.columns:\n",
    "        print(f\"Error: '{col}' column is missing in the totals dataset. Cannot proceed.\")\n",
    "        exit(1)\n",
    "\n",
    "# Convert cost columns to numeric, coercing errors to NaN and filling with 0\n",
    "cost_columns_itemized = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in cost_columns_itemized:\n",
    "    itemized_df[col] = pd.to_numeric(itemized_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "cost_columns_totals = [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in cost_columns_totals:\n",
    "    totals_df[col] = pd.to_numeric(totals_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# ---------------------- Calculate Manual Totals ---------------------- #\n",
    "\n",
    "# Group itemized data by q_id and type_of_upgrade and calculate sums\n",
    "itemized_grouped = itemized_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    'estimated_cost_x_1000': 'sum',\n",
    "    'escalated_cost_x_1000': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "itemized_grouped['manual_total'] = itemized_grouped.apply(\n",
    "    lambda row: row['estimated_cost_x_1000'] if row['estimated_cost_x_1000'] > 0 else row['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Prepare Totals Data ---------------------- #\n",
    "\n",
    "# Group totals data by q_id and type_of_upgrade and calculate sums\n",
    "totals_grouped = totals_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "    TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "totals_grouped['reported_total'] = totals_grouped.apply(\n",
    "    lambda row: row[TOTALS_ESTIMATED_COLUMN] if row[TOTALS_ESTIMATED_COLUMN] > 0 else row[TOTALS_ESCALATED_COLUMN],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Merge Data ---------------------- #\n",
    "\n",
    "# Merge the itemized and totals data on q_id and type_of_upgrade\n",
    "comparison_df = pd.merge(\n",
    "    itemized_grouped,\n",
    "    totals_grouped[['q_id', 'type_of_upgrade', 'reported_total']],\n",
    "    on=['q_id', 'type_of_upgrade'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---------------------- Check for Missing Upgrades ---------------------- #\n",
    "\n",
    "# Identify q_ids that are missing any of the required upgrades\n",
    "missing_upgrades_report = []\n",
    "for q_id in comparison_df['q_id'].unique():\n",
    "    upgrades_present = comparison_df[comparison_df['q_id'] == q_id]['type_of_upgrade'].unique()\n",
    "    missing_upgrades = [upgrade for upgrade in REQUIRED_UPGRADES if upgrade not in upgrades_present]\n",
    "    if missing_upgrades:\n",
    "        missing_upgrades_report.append((q_id, missing_upgrades))\n",
    "\n",
    "# Report missing upgrades\n",
    "if missing_upgrades_report:\n",
    "    print(\"\\nQ_ids with missing upgrades:\")\n",
    "    for q_id, missing in missing_upgrades_report:\n",
    "        print(f\"  Q_id {q_id} is missing upgrades: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nAll q_ids have all required upgrades.\")\n",
    "\n",
    "# ---------------------- Compare Totals and Identify Mismatches ---------------------- #\n",
    "\n",
    "# Initialize list to store mismatches\n",
    "mismatches = []\n",
    "\n",
    "# Iterate through each row to compare manual_total with reported_total\n",
    "for index, row in comparison_df.iterrows():\n",
    "    q_id = row['q_id']\n",
    "    upgrade = row['type_of_upgrade']\n",
    "    manual_total = row['manual_total']\n",
    "    reported_total = row['reported_total']\n",
    "    \n",
    "    # Determine if both manual_total and reported_total are zero\n",
    "    if manual_total == 0.0 and reported_total == 0.0:\n",
    "        continue  # No mismatch\n",
    "    # Determine if manual_total is zero and reported_total is missing or zero\n",
    "    elif manual_total == 0.0 and (pd.isna(row['reported_total']) or reported_total == 0.0):\n",
    "        continue  # No mismatch\n",
    "    # If reported_total is missing (NaN) and manual_total is not zero\n",
    "    elif pd.isna(row['reported_total']) and manual_total != 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: Missing\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': 'Missing'\n",
    "        })\n",
    "    # If manual_total is not zero and reported_total is zero\n",
    "    elif manual_total != 0.0 and reported_total == 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: 0.0\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # If both totals are non-zero but differ beyond tolerance\n",
    "    elif abs(manual_total - reported_total) > 1e-2:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: {reported_total}\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # Else, totals match; do nothing\n",
    "\n",
    "# Create a DataFrame for mismatches\n",
    "mismatches_df = pd.DataFrame(mismatches, columns=['q_id', 'type_of_upgrade', 'manual_total', 'reported_total'])\n",
    "\n",
    "# Save mismatches to a CSV file\n",
    "try:\n",
    "    mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "    print(f\"\\nMismatches saved to '{MISMATCHES_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving mismatches CSV: {e}\")\n",
    "\n",
    "# ---------------------- Point of Interconnection Matching ---------------------- #\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from itemized dataset\n",
    "itemized_poi = itemized_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from totals dataset\n",
    "totals_poi = totals_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Merge both to have a complete list of q_id and point_of_interconnection\n",
    "all_poi = pd.concat([itemized_poi, totals_poi]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ---------------------- Direct Match Identification ---------------------- #\n",
    "\n",
    "# Group by point_of_interconnection to find q_ids sharing the same point_of_interconnection\n",
    "direct_matches = all_poi.groupby('point_of_interconnection')['q_id'].apply(list).reset_index()\n",
    "\n",
    "# Filter groups with more than one q_id (i.e., shared points_of_interconnection)\n",
    "direct_matches = direct_matches[direct_matches['q_id'].apply(len) > 1]\n",
    "\n",
    "print(\"\\nDirect Matches (Exact Point of Interconnection Names):\")\n",
    "if not direct_matches.empty:\n",
    "    print(direct_matches)\n",
    "else:\n",
    "    print(\"No direct matches found.\")\n",
    "\n",
    "# ---------------------- Fuzzy Match Identification ---------------------- #\n",
    "\n",
    "# Prepare list of points_of_interconnection for fuzzy matching\n",
    "poi_list = all_poi['point_of_interconnection'].unique().tolist()\n",
    "\n",
    "# Initialize list to store fuzzy matches\n",
    "fuzzy_matches = []\n",
    "\n",
    "# Iterate through each point_of_interconnection to find similar ones\n",
    "for i, poi in enumerate(poi_list):\n",
    "    # Compare with the rest of the points to avoid redundant comparisons\n",
    "    similar_pois = process.extract(poi, poi_list[i+1:], scorer=fuzz.token_set_ratio)\n",
    "    \n",
    "    # Filter matches with similarity >= 80%\n",
    "    for match_poi, score in similar_pois:\n",
    "        if score >= 80:\n",
    "            # Retrieve q_ids for both points_of_interconnection\n",
    "            qids_poi1 = all_poi[all_poi['point_of_interconnection'] == poi]['q_id'].tolist()\n",
    "            qids_poi2 = all_poi[all_poi['point_of_interconnection'] == match_poi]['q_id'].tolist()\n",
    "            \n",
    "            # Append the matched pairs with their points_of_interconnection and similarity score\n",
    "            fuzzy_matches.append({\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_ids_1': qids_poi1,\n",
    "                'point_of_interconnection_2': match_poi,\n",
    "                'q_ids_2': qids_poi2,\n",
    "                'similarity_score': score\n",
    "            })\n",
    "\n",
    "# Convert fuzzy matches to DataFrame\n",
    "fuzzy_matches_df = pd.DataFrame(fuzzy_matches)\n",
    "\n",
    "print(\"\\nFuzzy Matches (>=80% Similarity in Point of Interconnection):\")\n",
    "if not fuzzy_matches_df.empty:\n",
    "    print(fuzzy_matches_df)\n",
    "else:\n",
    "    print(\"No fuzzy matches found.\")\n",
    "\n",
    "# ---------------------- Save Matched Q_ids to CSV ---------------------- #\n",
    "\n",
    "# For clarity, create a combined DataFrame for direct and fuzzy matches\n",
    "\n",
    "# Direct matches: list each pair of q_ids sharing the same point_of_interconnection\n",
    "direct_matches_expanded = []\n",
    "for _, row in direct_matches.iterrows():\n",
    "    qids = row['q_id']\n",
    "    poi = row['point_of_interconnection']\n",
    "    # Generate all possible unique pairs\n",
    "    for i in range(len(qids)):\n",
    "        for j in range(i+1, len(qids)):\n",
    "            direct_matches_expanded.append({\n",
    "                'match_type': 'Direct',\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_id_1': qids[i],\n",
    "                'point_of_interconnection_2': poi,\n",
    "                'q_id_2': qids[j],\n",
    "                'similarity_score': 100\n",
    "            })\n",
    "\n",
    "# Fuzzy matches: already have pairs\n",
    "fuzzy_matches_expanded = []\n",
    "for _, row in fuzzy_matches_df.iterrows():\n",
    "    fuzzy_matches_expanded.append({\n",
    "        'match_type': 'Fuzzy',\n",
    "        'point_of_interconnection_1': row['point_of_interconnection_1'],\n",
    "        'q_id_1': row['q_ids_1'],\n",
    "        'point_of_interconnection_2': row['point_of_interconnection_2'],\n",
    "        'q_id_2': row['q_ids_2'],\n",
    "        'similarity_score': row['similarity_score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "matched_qids_df = pd.DataFrame(direct_matches_expanded + fuzzy_matches_expanded)\n",
    "\n",
    "# Save matched q_ids to CSV\n",
    "try:\n",
    "    matched_qids_df.to_csv(MATCHED_QIDS_CSV_PATH, index=False)\n",
    "    print(f\"Matched Q_ids saved to '{MATCHED_QIDS_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving matched Q_ids CSV: {e}\")\n",
    "\n",
    "# ---------------------- Summary ---------------------- #\n",
    "\n",
    "# Print a summary\n",
    "total_checked = len(comparison_df)\n",
    "total_mismatches = len(mismatches_df)\n",
    "print(f\"\\nTotal checks performed: {total_checked}\")\n",
    "print(f\"Total mismatches found: {total_mismatches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded itemized data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 7/01_clean/costs_phase_1_cluster_7_style_G_itemized_updated.csv\n",
      "Loaded totals data from /Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 7/01_clean/costs_phase_1_cluster_7_style_G_total_updated.csv\n",
      "\n",
      "All q_ids have all required upgrades.\n",
      "Mismatch: Q_id 1014, Upgrade 'PTO_IF' - Manual Total: 616.0, Reported Total: 0.0\n",
      "Mismatch: Q_id 1014, Upgrade 'RNU' - Manual Total: 2035.9, Reported Total: 0.0\n",
      "Mismatch: Q_id 1027, Upgrade 'RNU' - Manual Total: 1058.1, Reported Total: 0.0\n",
      "\n",
      "Mismatches saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 7/mismatches.csv'.\n",
      "\n",
      "Direct Matches (Exact Point of Interconnection Names):\n",
      "       point_of_interconnection          q_id\n",
      "3   230 KV OTAY MESA SWITCHYARD  [1057, 1058]\n",
      "19         ARCO SUBSTATION 70KV  [1035, 1038]\n",
      "21       GATES SUBSTATION 230KV  [1027, 1031]\n",
      "27    MENDOTA SUBSTATION 115 KV  [1028, 1029]\n",
      "\n",
      "Fuzzy Matches (>=80% Similarity in Point of Interconnection):\n",
      "                           point_of_interconnection_1 q_ids_1  \\\n",
      "0                       LOS ESTEROS SUBSTATION 115 KV  [1007]   \n",
      "1                              TESLA SUBSTATION 115KV  [1011]   \n",
      "2                              TESLA SUBSTATION 115KV  [1011]   \n",
      "3                             HALSEY SUBSTATION 60 KV  [1013]   \n",
      "4                             HALSEY SUBSTATION 60 KV  [1013]   \n",
      "5                             HALSEY SUBSTATION 60 KV  [1013]   \n",
      "6                    PLEASANT GROVE SUBSTATION 115 KV  [1015]   \n",
      "7                           EVERGREEN SUBSTATION 60KV  [1023]   \n",
      "8                             VICTOR SUBSTATION 60 KV  [1024]   \n",
      "9   PANOCHE-HELM AND PANOCHE-KEARNEY 230 KV LINES ...  [1032]   \n",
      "10                              ARCO SUBSTATION 70 KV  [1033]   \n",
      "11                              ARCO SUBSTATION 70 KV  [1033]   \n",
      "12                              ARCO SUBSTATION 70 KV  [1033]   \n",
      "13          500KV COMMON BUS AT HASSAYAMPA SWITCHYARD  [1040]   \n",
      "14                          69 KV CREELMAN SUBSTATION  [1043]   \n",
      "15                          69 KV CREELMAN SUBSTATION  [1043]   \n",
      "16                          69 KV CREELMAN SUBSTATION  [1043]   \n",
      "17                          69 KV CREELMAN SUBSTATION  [1043]   \n",
      "18                          69 KV CREELMAN SUBSTATION  [1043]   \n",
      "19                              69 KV OTAY SUBSTATION  [1045]   \n",
      "20                              69 KV OTAY SUBSTATION  [1045]   \n",
      "21                              69 KV OTAY SUBSTATION  [1045]   \n",
      "22                              69 KV OTAY SUBSTATION  [1045]   \n",
      "23                              69 KV OTAY SUBSTATION  [1045]   \n",
      "24                      138 KV EAST COUNTY SUBSTATION  [1046]   \n",
      "25                      138 KV EAST COUNTY SUBSTATION  [1046]   \n",
      "26  69 KV EL CAJON SUBSTATION ON EXISTING GEN TIE ...  [1047]   \n",
      "27  69 KV EL CAJON SUBSTATION ON EXISTING GEN TIE ...  [1047]   \n",
      "28  69 KV EL CAJON SUBSTATION ON EXISTING GEN TIE ...  [1047]   \n",
      "29  69 KV ESCONDIDO SUBSTATION, VIA TL6933 (GEN TI...  [1048]   \n",
      "30  69 KV ESCONDIDO SUBSTATION, VIA TL6933 (GEN TI...  [1048]   \n",
      "31  69 KV ESCONDIDO SUBSTATION, VIA TL6933 (GEN TI...  [1048]   \n",
      "32                              138 KV ECO SUBSTATION  [1049]   \n",
      "33                           69 KV CAMERON SUBSTATION  [1050]   \n",
      "34                           69 KV CAMERON SUBSTATION  [1050]   \n",
      "35                           69 KV CAMERON SUBSTATION  [1050]   \n",
      "36                           69 KV CAMERON SUBSTATION  [1050]   \n",
      "37                           69 KV CAMERON SUBSTATION  [1050]   \n",
      "38                            69 KV KEARNY SUBSTATION  [1051]   \n",
      "39                            69 KV KEARNY SUBSTATION  [1051]   \n",
      "40                            69 KV KEARNY SUBSTATION  [1051]   \n",
      "41                            69 KV KEARNY SUBSTATION  [1051]   \n",
      "42                      69 KV SANTA YSABEL SUBSTATION  [1052]   \n",
      "43                     230 KV SAN LUIS REY SUBSTATION  [1055]   \n",
      "44                      69 KV SAN LUIS REY SUBSTATION  [1059]   \n",
      "45                           69 KV MELROSE SUBSTATION  [1061]   \n",
      "\n",
      "                           point_of_interconnection_2       q_ids_2  \\\n",
      "0                           MENDOTA SUBSTATION 115 KV  [1028, 1029]   \n",
      "1                    SHINGLE SPRINGS SUBSTATION 115KV        [1019]   \n",
      "2                           MENDOTA SUBSTATION 115 KV  [1028, 1029]   \n",
      "3                             VICTOR SUBSTATION 60 KV        [1024]   \n",
      "4                               69 KV OTAY SUBSTATION        [1045]   \n",
      "5                            69 KV MELROSE SUBSTATION        [1061]   \n",
      "6                           MENDOTA SUBSTATION 115 KV  [1028, 1029]   \n",
      "7                               WEBER SUBSTATION 60KV        [1026]   \n",
      "8                               ARCO SUBSTATION 70 KV        [1033]   \n",
      "9   GATES-GREGG AND GATES-MCCALL 230 KV LINES VIA ...        [1036]   \n",
      "10                               ARCO SUBSTATION 70KV  [1035, 1038]   \n",
      "11                              138 KV ECO SUBSTATION        [1049]   \n",
      "12                           69 KV CAMERON SUBSTATION        [1050]   \n",
      "13         500 KV COMMON BUS AT HASSAYAMPA SWITCHYARD        [1053]   \n",
      "14                              69 KV OTAY SUBSTATION        [1045]   \n",
      "15                        69 KV SALT CREEK SUBSTATION        [1063]   \n",
      "16                            69 KV KEARNY SUBSTATION        [1051]   \n",
      "17                           69 KV CAMERON SUBSTATION        [1050]   \n",
      "18                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "19  69 KV EL CAJON SUBSTATION ON EXISTING GEN TIE ...        [1047]   \n",
      "20  69 KV ESCONDIDO SUBSTATION, VIA TL6933 (GEN TI...        [1048]   \n",
      "21                           69 KV CAMERON SUBSTATION        [1050]   \n",
      "22                            69 KV KEARNY SUBSTATION        [1051]   \n",
      "23                      69 KV SANTA YSABEL SUBSTATION        [1052]   \n",
      "24                              138 KV ECO SUBSTATION        [1049]   \n",
      "25                   138 KV BOULEVARD EAST SUBSTATION        [1060]   \n",
      "26                            69 KV KEARNY SUBSTATION        [1051]   \n",
      "27                           69 KV CAMERON SUBSTATION        [1050]   \n",
      "28                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "29                            69 KV KEARNY SUBSTATION        [1051]   \n",
      "30                           69 KV CAMERON SUBSTATION        [1050]   \n",
      "31                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "32                   138 KV BOULEVARD EAST SUBSTATION        [1060]   \n",
      "33                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "34                            69 KV KEARNY SUBSTATION        [1051]   \n",
      "35                      69 KV SANTA YSABEL SUBSTATION        [1052]   \n",
      "36                      69 KV SAN LUIS REY SUBSTATION        [1059]   \n",
      "37                        69 KV SALT CREEK SUBSTATION        [1063]   \n",
      "38                      69 KV SANTA YSABEL SUBSTATION        [1052]   \n",
      "39                      69 KV SAN LUIS REY SUBSTATION        [1059]   \n",
      "40                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "41                        69 KV SALT CREEK SUBSTATION        [1063]   \n",
      "42                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "43                      69 KV SAN LUIS REY SUBSTATION        [1059]   \n",
      "44                           69 KV MELROSE SUBSTATION        [1061]   \n",
      "45                        69 KV SALT CREEK SUBSTATION        [1063]   \n",
      "\n",
      "    similarity_score  \n",
      "0                 81  \n",
      "1                 84  \n",
      "2                 81  \n",
      "3                 82  \n",
      "4                 82  \n",
      "5                 81  \n",
      "6                 81  \n",
      "7                 83  \n",
      "8                 82  \n",
      "9                 83  \n",
      "10                98  \n",
      "11                81  \n",
      "12                80  \n",
      "13                99  \n",
      "14                86  \n",
      "15                85  \n",
      "16                83  \n",
      "17                82  \n",
      "18                80  \n",
      "19                86  \n",
      "20                86  \n",
      "21                86  \n",
      "22                86  \n",
      "23                86  \n",
      "24                89  \n",
      "25                86  \n",
      "26                82  \n",
      "27                80  \n",
      "28                80  \n",
      "29                82  \n",
      "30                80  \n",
      "31                80  \n",
      "32                89  \n",
      "33                88  \n",
      "34                85  \n",
      "35                80  \n",
      "36                80  \n",
      "37                80  \n",
      "38                82  \n",
      "39                82  \n",
      "40                82  \n",
      "41                82  \n",
      "42                80  \n",
      "43                95  \n",
      "44                80  \n",
      "45                80  \n",
      "Matched Q_ids saved to '/Users/vk365/Dropbox/Interconnections_data/data/pdf_scraper/output/Cluster 7/matched_qids.csv'.\n",
      "\n",
      "Total checks performed: 180\n",
      "Total mismatches found: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# ---------------------- Configuration ---------------------- #\n",
    "\n",
    "# Paths to the CSV files\n",
    "ITEMIZED_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/01_clean/costs_phase_1_cluster_7_style_G_itemized_updated.csv'\n",
    "TOTALS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/01_clean/costs_phase_1_cluster_7_style_G_total_updated.csv'\n",
    "\n",
    "# Columns in totals_df that hold the reported total costs\n",
    "TOTALS_ESTIMATED_COLUMN = 'estimated_cost_x_1000'\n",
    "TOTALS_ESCALATED_COLUMN = 'escalated_cost_x_1000'\n",
    "\n",
    "# Upgrade types to check\n",
    "REQUIRED_UPGRADES = ['PTO_IF', 'RNU', 'LDNU', 'ADNU']\n",
    "\n",
    "# Output paths\n",
    "MISMATCHES_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/mismatches.csv'\n",
    "MATCHED_QIDS_CSV_PATH = '/Users/vk365/Dropbox/Interconnections_data/data/ic_studies/raw/04_intermediate_scraped_data/phase_1_cost_data/Cluster 7/matched_qids.csv'\n",
    "\n",
    "# ---------------------- Load Data ---------------------- #\n",
    "\n",
    "def load_csv(path, dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {dataset_name} from {path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Load datasets\n",
    "itemized_df = load_csv(ITEMIZED_CSV_PATH, \"itemized data\")\n",
    "totals_df = load_csv(TOTALS_CSV_PATH, \"totals data\")\n",
    "\n",
    "# ---------------------- Data Cleaning ---------------------- #\n",
    "\n",
    "def clean_text(df, column):\n",
    "    \"\"\"\n",
    "    Cleans text data by stripping leading/trailing spaces and converting to uppercase.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        df[column] = df[column].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        print(f\"Warning: '{column}' column is missing in the dataset. Filling with 'UNKNOWN'.\")\n",
    "        df[column] = 'UNKNOWN'\n",
    "    return df\n",
    "\n",
    "# Clean 'type_of_upgrade' and 'point_of_interconnection' in both datasets\n",
    "itemized_df = clean_text(itemized_df, 'type_of_upgrade')\n",
    "itemized_df = clean_text(itemized_df, 'point_of_interconnection')\n",
    "\n",
    "totals_df = clean_text(totals_df, 'type_of_upgrade')\n",
    "totals_df = clean_text(totals_df, 'point_of_interconnection')\n",
    "\n",
    "# ---------------------- Data Preparation ---------------------- #\n",
    "\n",
    "# Ensure necessary columns exist in itemized_df\n",
    "required_itemized_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', 'estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in required_itemized_columns:\n",
    "    if col not in itemized_df.columns:\n",
    "        print(f\"Warning: '{col}' column is missing in the itemized dataset.\")\n",
    "        if col in ['q_id', 'type_of_upgrade', 'point_of_interconnection']:\n",
    "            itemized_df[col] = 'UNKNOWN'\n",
    "        else:\n",
    "            itemized_df[col] = 0\n",
    "\n",
    "# Ensure necessary columns exist in totals_df\n",
    "required_totals_columns = ['q_id', 'type_of_upgrade', 'point_of_interconnection', TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in required_totals_columns:\n",
    "    if col not in totals_df.columns:\n",
    "        print(f\"Error: '{col}' column is missing in the totals dataset. Cannot proceed.\")\n",
    "        exit(1)\n",
    "\n",
    "# Convert cost columns to numeric, coercing errors to NaN and filling with 0\n",
    "cost_columns_itemized = ['estimated_cost_x_1000', 'escalated_cost_x_1000']\n",
    "for col in cost_columns_itemized:\n",
    "    itemized_df[col] = pd.to_numeric(itemized_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "cost_columns_totals = [TOTALS_ESTIMATED_COLUMN, TOTALS_ESCALATED_COLUMN]\n",
    "for col in cost_columns_totals:\n",
    "    totals_df[col] = pd.to_numeric(totals_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# ---------------------- Calculate Manual Totals ---------------------- #\n",
    "\n",
    "# Group itemized data by q_id and type_of_upgrade and calculate sums\n",
    "itemized_grouped = itemized_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    'estimated_cost_x_1000': 'sum',\n",
    "    'escalated_cost_x_1000': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "itemized_grouped['manual_total'] = itemized_grouped.apply(\n",
    "    lambda row: row['estimated_cost_x_1000'] if row['estimated_cost_x_1000'] > 0 else row['escalated_cost_x_1000'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Prepare Totals Data ---------------------- #\n",
    "\n",
    "# Group totals data by q_id and type_of_upgrade and calculate sums\n",
    "totals_grouped = totals_df.groupby(['q_id', 'type_of_upgrade']).agg({\n",
    "    TOTALS_ESTIMATED_COLUMN: 'sum',\n",
    "    TOTALS_ESCALATED_COLUMN: 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Apply preference: Use estimated_cost_x_1000 if sum > 0, else use escalated_cost_x_1000\n",
    "totals_grouped['reported_total'] = totals_grouped.apply(\n",
    "    lambda row: row[TOTALS_ESTIMATED_COLUMN] if row[TOTALS_ESTIMATED_COLUMN] > 0 else row[TOTALS_ESCALATED_COLUMN],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------- Merge Data ---------------------- #\n",
    "\n",
    "# Merge the itemized and totals data on q_id and type_of_upgrade\n",
    "comparison_df = pd.merge(\n",
    "    itemized_grouped,\n",
    "    totals_grouped[['q_id', 'type_of_upgrade', 'reported_total']],\n",
    "    on=['q_id', 'type_of_upgrade'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ---------------------- Check for Missing Upgrades ---------------------- #\n",
    "\n",
    "# Identify q_ids that are missing any of the required upgrades\n",
    "missing_upgrades_report = []\n",
    "for q_id in comparison_df['q_id'].unique():\n",
    "    upgrades_present = comparison_df[comparison_df['q_id'] == q_id]['type_of_upgrade'].unique()\n",
    "    missing_upgrades = [upgrade for upgrade in REQUIRED_UPGRADES if upgrade not in upgrades_present]\n",
    "    if missing_upgrades:\n",
    "        missing_upgrades_report.append((q_id, missing_upgrades))\n",
    "\n",
    "# Report missing upgrades\n",
    "if missing_upgrades_report:\n",
    "    print(\"\\nQ_ids with missing upgrades:\")\n",
    "    for q_id, missing in missing_upgrades_report:\n",
    "        print(f\"  Q_id {q_id} is missing upgrades: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nAll q_ids have all required upgrades.\")\n",
    "\n",
    "# ---------------------- Compare Totals and Identify Mismatches ---------------------- #\n",
    "\n",
    "# Initialize list to store mismatches\n",
    "mismatches = []\n",
    "\n",
    "# Iterate through each row to compare manual_total with reported_total\n",
    "for index, row in comparison_df.iterrows():\n",
    "    q_id = row['q_id']\n",
    "    upgrade = row['type_of_upgrade']\n",
    "    manual_total = row['manual_total']\n",
    "    reported_total = row['reported_total']\n",
    "    \n",
    "    # Determine if both manual_total and reported_total are zero\n",
    "    if manual_total == 0.0 and reported_total == 0.0:\n",
    "        continue  # No mismatch\n",
    "    # Determine if manual_total is zero and reported_total is missing or zero\n",
    "    elif manual_total == 0.0 and (pd.isna(row['reported_total']) or reported_total == 0.0):\n",
    "        continue  # No mismatch\n",
    "    # If reported_total is missing (NaN) and manual_total is not zero\n",
    "    elif pd.isna(row['reported_total']) and manual_total != 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: Missing\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': 'Missing'\n",
    "        })\n",
    "    # If manual_total is not zero and reported_total is zero\n",
    "    elif manual_total != 0.0 and reported_total == 0.0:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: 0.0\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # If both totals are non-zero but differ beyond tolerance\n",
    "    elif abs(manual_total - reported_total) > 1e-2:\n",
    "        print(f\"Mismatch: Q_id {q_id}, Upgrade '{upgrade}' - Manual Total: {manual_total}, Reported Total: {reported_total}\")\n",
    "        mismatches.append({\n",
    "            'q_id': q_id,\n",
    "            'type_of_upgrade': upgrade,\n",
    "            'manual_total': manual_total,\n",
    "            'reported_total': reported_total\n",
    "        })\n",
    "    # Else, totals match; do nothing\n",
    "\n",
    "# Create a DataFrame for mismatches\n",
    "mismatches_df = pd.DataFrame(mismatches, columns=['q_id', 'type_of_upgrade', 'manual_total', 'reported_total'])\n",
    "\n",
    "# Save mismatches to a CSV file\n",
    "try:\n",
    "    mismatches_df.to_csv(MISMATCHES_CSV_PATH, index=False)\n",
    "    print(f\"\\nMismatches saved to '{MISMATCHES_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving mismatches CSV: {e}\")\n",
    "\n",
    "# ---------------------- Point of Interconnection Matching ---------------------- #\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from itemized dataset\n",
    "itemized_poi = itemized_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Extract unique q_id and point_of_interconnection from totals dataset\n",
    "totals_poi = totals_df[['q_id', 'point_of_interconnection']].drop_duplicates()\n",
    "\n",
    "# Merge both to have a complete list of q_id and point_of_interconnection\n",
    "all_poi = pd.concat([itemized_poi, totals_poi]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# ---------------------- Direct Match Identification ---------------------- #\n",
    "\n",
    "# Group by point_of_interconnection to find q_ids sharing the same point_of_interconnection\n",
    "direct_matches = all_poi.groupby('point_of_interconnection')['q_id'].apply(list).reset_index()\n",
    "\n",
    "# Filter groups with more than one q_id (i.e., shared points_of_interconnection)\n",
    "direct_matches = direct_matches[direct_matches['q_id'].apply(len) > 1]\n",
    "\n",
    "print(\"\\nDirect Matches (Exact Point of Interconnection Names):\")\n",
    "if not direct_matches.empty:\n",
    "    print(direct_matches)\n",
    "else:\n",
    "    print(\"No direct matches found.\")\n",
    "\n",
    "# ---------------------- Fuzzy Match Identification ---------------------- #\n",
    "\n",
    "# Prepare list of points_of_interconnection for fuzzy matching\n",
    "poi_list = all_poi['point_of_interconnection'].unique().tolist()\n",
    "\n",
    "# Initialize list to store fuzzy matches\n",
    "fuzzy_matches = []\n",
    "\n",
    "# Iterate through each point_of_interconnection to find similar ones\n",
    "for i, poi in enumerate(poi_list):\n",
    "    # Compare with the rest of the points to avoid redundant comparisons\n",
    "    similar_pois = process.extract(poi, poi_list[i+1:], scorer=fuzz.token_set_ratio)\n",
    "    \n",
    "    # Filter matches with similarity >= 80%\n",
    "    for match_poi, score in similar_pois:\n",
    "        if score >= 80:\n",
    "            # Retrieve q_ids for both points_of_interconnection\n",
    "            qids_poi1 = all_poi[all_poi['point_of_interconnection'] == poi]['q_id'].tolist()\n",
    "            qids_poi2 = all_poi[all_poi['point_of_interconnection'] == match_poi]['q_id'].tolist()\n",
    "            \n",
    "            # Append the matched pairs with their points_of_interconnection and similarity score\n",
    "            fuzzy_matches.append({\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_ids_1': qids_poi1,\n",
    "                'point_of_interconnection_2': match_poi,\n",
    "                'q_ids_2': qids_poi2,\n",
    "                'similarity_score': score\n",
    "            })\n",
    "\n",
    "# Convert fuzzy matches to DataFrame\n",
    "fuzzy_matches_df = pd.DataFrame(fuzzy_matches)\n",
    "\n",
    "print(\"\\nFuzzy Matches (>=80% Similarity in Point of Interconnection):\")\n",
    "if not fuzzy_matches_df.empty:\n",
    "    print(fuzzy_matches_df)\n",
    "else:\n",
    "    print(\"No fuzzy matches found.\")\n",
    "\n",
    "# ---------------------- Save Matched Q_ids to CSV ---------------------- #\n",
    "\n",
    "# For clarity, create a combined DataFrame for direct and fuzzy matches\n",
    "\n",
    "# Direct matches: list each pair of q_ids sharing the same point_of_interconnection\n",
    "direct_matches_expanded = []\n",
    "for _, row in direct_matches.iterrows():\n",
    "    qids = row['q_id']\n",
    "    poi = row['point_of_interconnection']\n",
    "    # Generate all possible unique pairs\n",
    "    for i in range(len(qids)):\n",
    "        for j in range(i+1, len(qids)):\n",
    "            direct_matches_expanded.append({\n",
    "                'match_type': 'Direct',\n",
    "                'point_of_interconnection_1': poi,\n",
    "                'q_id_1': qids[i],\n",
    "                'point_of_interconnection_2': poi,\n",
    "                'q_id_2': qids[j],\n",
    "                'similarity_score': 100\n",
    "            })\n",
    "\n",
    "# Fuzzy matches: already have pairs\n",
    "fuzzy_matches_expanded = []\n",
    "for _, row in fuzzy_matches_df.iterrows():\n",
    "    fuzzy_matches_expanded.append({\n",
    "        'match_type': 'Fuzzy',\n",
    "        'point_of_interconnection_1': row['point_of_interconnection_1'],\n",
    "        'q_id_1': row['q_ids_1'],\n",
    "        'point_of_interconnection_2': row['point_of_interconnection_2'],\n",
    "        'q_id_2': row['q_ids_2'],\n",
    "        'similarity_score': row['similarity_score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "matched_qids_df = pd.DataFrame(direct_matches_expanded + fuzzy_matches_expanded)\n",
    "\n",
    "# Save matched q_ids to CSV\n",
    "try:\n",
    "    matched_qids_df.to_csv(MATCHED_QIDS_CSV_PATH, index=False)\n",
    "    print(f\"Matched Q_ids saved to '{MATCHED_QIDS_CSV_PATH}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving matched Q_ids CSV: {e}\")\n",
    "\n",
    "# ---------------------- Summary ---------------------- #\n",
    "\n",
    "# Print a summary\n",
    "total_checked = len(comparison_df)\n",
    "total_mismatches = len(mismatches_df)\n",
    "print(f\"\\nTotal checks performed: {total_checked}\")\n",
    "print(f\"Total mismatches found: {total_mismatches}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}